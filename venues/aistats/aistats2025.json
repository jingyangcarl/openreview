[
    {
        "id": "0FvarE8SMs",
        "title": "Integer Programming Based Methods and Heuristics for Causal Graph Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Acyclic directed mixed graphs (ADMG) \u2013\ngraphs that contain both directed and bidi-\nrected edges but no directed cycles \u2013 are used\nto model causal and conditional independence\nrelationships between a set of random vari-\nables in the presence of latent or unmeasured\nvariables. Bow-free ADMGs, Arid ADMGs,\nand Ancestral ADMGs (AADMG) are three\nwidely studied classes of ADMGs where each\nclass is contained in the previously mentioned\nclass. There are a number of published meth-\nods \u2013 primarily heuristic ones \u2013 to find score-\nmaximizing AADMGs from data. Bow-free\nand Arid ADMGs can model certain equal-\nity restrictions \u2013 such as Verma constraints\n\u2013 between observed variables that maximal\nAADMGs cannot. In this work, we develop\nthe first exact methods \u2013 based on integer\nprogramming \u2013 to find score-maximizing Bow-\nfree and Arid ADMGs. Our methods work for\ndata that follows a continuous Gaussian distri-\nbution and for scores that linearly decompose\ninto the sum of scores of c-components of\nan ADMG. To improve scaling, we develop\nan effective linear-programming based heuris-\ntic that yields solutions with high parent set\nsizes and/or large districts. We show that\nour proposed algorithms obtain better scores\nthan other state-of-the-art methods and re-\nturn graphs that have excellent fits to data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sanjeeb Dash;Joao Goncalves;Tian Gao",
        "authorids": "~Sanjeeb_Dash1;~Joao_Goncalves1;~Tian_Gao1",
        "gender": ";;",
        "homepage": "https://researcher.watson.ibm.com/researcher/view.php?person=us-sanjeebd;https://ibm.com;https://sites.google.com/view/tiangao/home",
        "dblp": "09/294;;",
        "google_scholar": "NJV8UUoAAAAJ;;5rweipAAAAAJ",
        "orcid": ";;0000-0002-0337-6682",
        "linkedin": ";;",
        "or_profile": "~Sanjeeb_Dash1;~Joao_Goncalves1;~Tian_Gao1",
        "aff": ";International Business Machines;International Business Machines+Rensselaer Polytechnic Institute",
        "aff_domain": ";ibm.com;ibm.com+rpi.edu",
        "position": ";Engineer;Reseach Staff Member+PhD student",
        "bibtex": "@inproceedings{\ndash2025integer,\ntitle={Integer Programming Based Methods and Heuristics for Causal Graph Learning},\nauthor={Sanjeeb Dash and Joao Goncalves and Tian Gao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0FvarE8SMs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0FvarE8SMs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0Pl4libZsC",
        "title": "Stochastic Compositional Minimax Optimization with Provable Convergence Guarantees",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Stochastic compositional minimax problems are prevalent in machine learning, yet there exist only limited established findings on the convergence of this class of problems. In this paper, we propose a formal definition of the stochastic compositional minimax problem, which involves optimizing a minimax loss with a compositional structure either in primal, dual, or both primal and dual variables. We introduce a simple yet effective algorithm, stochastically Corrected stOchastic gradient Descent Ascent (CODA), which is a primal-dual type algorithm with compositional correction steps, and establish its convergence rate in the aforementioned three settings. We also propose a variance reduced variant, CODA+, which achieves the best-known rate on nonconvex-strongly-concave and nonconvex-concave compositional minimax problems. This work initiates the theoretical study of the stochastic compositional minimax problem in various settings and may inform modern machine learning scenarios such as domain adaptation or robust model-agnostic meta-learning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuyang Deng;Fuli Qiao;Mehrdad Mahdavi",
        "authorids": "~Yuyang_Deng3;~Fuli_Qiao1;~Mehrdad_Mahdavi2",
        "gender": "M;F;M",
        "homepage": "https://sites.psu.edu/yuyangdeng/;;http://www.cse.psu.edu/~mzm616/",
        "dblp": "261/9253;;88/4321",
        "google_scholar": "bfV3XWUAAAAJ;https://scholar.google.com/citations?view_op=list_works;HzxnwocAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yuyang_Deng3;~Fuli_Qiao1;~Mehrdad_Mahdavi2",
        "aff": "Columbia University;Pennsylvania State University;Pennsylvania State University+Toyota Technological Institute at Chicago",
        "aff_domain": "columbia.edu;psu.edu;psu.edu+ttic.edu",
        "position": "Postdoc;PhD student;Associate Professor+Researcher",
        "bibtex": "@inproceedings{\ndeng2025stochastic,\ntitle={Stochastic Compositional Minimax Optimization with Provable Convergence Guarantees},\nauthor={Yuyang Deng and Fuli Qiao and Mehrdad Mahdavi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0Pl4libZsC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0Pl4libZsC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0ebspjSSXk",
        "title": "Some Targets Are Harder to Identify than Others: Quantifying the Target-dependent Membership Leakage",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "In a Membership Inference (MI) game, an attacker tries to infer whether a target point was included or not in the input of an algorithm. Existing works show that some target points are easier to identify, while others are harder. This paper explains the target-dependent hardness of membership attacks by studying the powers of the optimal attacks in a *fixed-target* MI game. \nWe characterise the optimal advantage and trade-off functions of attacks against the empirical mean in terms of the Mahalanobis distance between the target point and the data-generating distribution. We further derive the impacts of two privacy defences, i.e. adding Gaussian noise and sub-sampling, and that of target misspecification on optimal attacks. As by-products of our novel analysis of the Likelihood Ratio (LR) test, we provide a new covariance attack which generalises and improves the scalar product attack. Also, we propose a new optimal canary-choosing strategy for auditing privacy in the white-box federated learning setting. Our experiments validate that the Mahalanobis score explains the hardness of *fixed-target* MI games.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Achraf Azize;Debabrota Basu",
        "authorids": "~Achraf_Azize1;~Debabrota_Basu1",
        "gender": "M;",
        "homepage": "https://achraf-azize.github.io/;https://debabrota-basu.github.io/",
        "dblp": "287/4270;126/2209",
        "google_scholar": "9RKFStAAAAAJ;https://scholar.google.co.in/citations?user=e26Maa4AAAAJ",
        "orcid": ";",
        "linkedin": "achraf-azize/;",
        "or_profile": "~Achraf_Azize1;~Debabrota_Basu1",
        "aff": "Ecole Nationale de la Statistique et de l'Administration Economique;INRIA",
        "aff_domain": "ensae.fr;inria.fr",
        "position": "Postdoc;Faculty",
        "bibtex": "@inproceedings{\nazize2025some,\ntitle={Some Targets Are Harder to Identify than Others: Quantifying the Target-dependent Membership Leakage},\nauthor={Achraf Azize and Debabrota Basu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0ebspjSSXk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0ebspjSSXk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0gEjlLdjK9",
        "title": "SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Causal discovery can be computationally demanding for large numbers of variables. \nIf we only wish to estimate the causal effects on a small subset of target variables, we might not need to learn the causal graph for all variables, but only a small subgraph that includes the targets and their adjustment sets. \nIn this paper, we focus on identifying causal effects between target variables in a computationally and statistically efficient way.\nThis task combines causal discovery and effect estimation, aligning the discovery objective with the effects to be estimated.\nWe show that definite non-ancestors of the targets are unnecessary to learn causal relations between the targets and to identify efficient adjustments sets.\nWe sequentially identify and prune these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP) framework, which can be used either as a preprocessing step to standard causal discovery methods, or as a standalone sound and complete causal discovery algorithm.\nOur results on synthetic and real data show that both approaches substantially reduce the number of independence tests and the computation time without compromising the quality of causal effect estimations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "M\u00e1ty\u00e1s Schubert;Tom Claassen;Sara Magliacane",
        "authorids": "~M\u00e1ty\u00e1s_Schubert1;~Tom_Claassen1;~Sara_Magliacane1",
        "gender": "M;M;F",
        "homepage": ";http://www.cs.ru.nl/~tomc/;http://saramagliacane.github.io",
        "dblp": ";00/10452;120/5256",
        "google_scholar": ";WpOKcPMAAAAJ;https://scholar.google.nl/citations?user=H3j_zQ4AAAAJ",
        "orcid": ";0000-0003-2046-8447;",
        "linkedin": "matyas-schubert/;;magliacane/",
        "or_profile": "~M\u00e1ty\u00e1s_Schubert1;~Tom_Claassen1;~Sara_Magliacane1",
        "aff": "University of Amsterdam;Radboud University Nijmegen;University of Amsterdam",
        "aff_domain": "ivi.uva.nl;ru.nl;uva.nl",
        "position": "PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nschubert2025snap,\ntitle={{SNAP}: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph},\nauthor={M{\\'a}ty{\\'a}s Schubert and Tom Claassen and Sara Magliacane},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0gEjlLdjK9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0gEjlLdjK9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0mHke8bhEg",
        "title": "Offline Multi-task Transfer RL with Representational Penalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of representational transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.\n\nWe propose an algorithm to compute pointwise uncertainty measures for the learnt representation in low-rank MDPs, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meaningful transfer by existing offline algorithms. We complement our theoretical results with empirical evaluation on a rich-observation MDP which requires many samples for complete coverage. Our findings illustrate the benefits of penalizing and quantifying the uncertainty in the learnt representation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Avinandan Bose;Simon Shaolei Du;Maryam Fazel",
        "authorids": "~Avinandan_Bose1;~Simon_Shaolei_Du1;~Maryam_Fazel1",
        "gender": "M;M;F",
        "homepage": "https://avinandan22.github.io/;http://simonshaoleidu.com;",
        "dblp": "305/7490;176/5602;10/2309",
        "google_scholar": "https://scholar.google.com/citations?pli=1;OttawxUAAAAJ;vlN_kRoAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Avinandan_Bose1;~Simon_Shaolei_Du1;~Maryam_Fazel1",
        "aff": "Department of Computer Science;University of Washington;University of Washington, Seattle",
        "aff_domain": "cs.washington.edu;washington.edu;uw.edu",
        "position": "PhD student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nbose2025offline,\ntitle={Offline Multi-task Transfer {RL} with Representational Penalization},\nauthor={Avinandan Bose and Simon Shaolei Du and Maryam Fazel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0mHke8bhEg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0mHke8bhEg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0tOl3nmVGz",
        "title": "To Give or Not to Give? The Impacts of Strategically Withheld Recourse",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Individuals often aim to reverse undesired outcomes in interactions with automated systems, like loan denials, by either implementing system-recommended actions (recourse), or manipulating their features. While providing recourse benefits users and enhances system utility, it also provides information about the decision process that can be used for more effective strategic manipulation, especially when the individuals collectively share such information with each other. We show that this tension leads rational utility-maximizing systems to frequently withhold recourse, resulting in decreased population utility, particularly impacting sensitive groups. To mitigate these effects, we explore the role of recourse subsidies, finding them effective in increasing the provision of recourse actions by rational systems, as well as lowering the potential social cost and mitigating unfairness caused by recourse withholding.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yatong Chen;Andrew Estornell;Yevgeniy Vorobeychik;Yang Liu",
        "authorids": "~Yatong_Chen1;~Andrew_Estornell1;~Yevgeniy_Vorobeychik1;~Yang_Liu3",
        "gender": "F;;M;M",
        "homepage": "https://yatongchen.github.io/;;http://vorobeychik.com;http://www.yliuu.com",
        "dblp": "202/8466;;70/2217;51/3710-18",
        "google_scholar": "yoExm_UAAAAJ;;https://scholar.google.com.tw/citations?user=ptI-HHkAAAAJ;jKrIVCIAAAAJ",
        "orcid": ";;;0000-0001-8420-6011",
        "linkedin": ";;;",
        "or_profile": "~Yatong_Chen1;~Andrew_Estornell1;~Yevgeniy_Vorobeychik1;~Yang_Liu3",
        "aff": "Max Planck Institute for Intelligent Systems, Max-Planck Institute;;Washington University, St. Louis;University of California, Santa Cruz",
        "aff_domain": "tuebingen.mpg.de;;wustl.edu;ucsc.edu",
        "position": "Postdoc;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nchen2025to,\ntitle={To Give or Not to Give? The Impacts of Strategically Withheld Recourse},\nauthor={Yatong Chen and Andrew Estornell and Yevgeniy Vorobeychik and Yang Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0tOl3nmVGz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0tOl3nmVGz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0ynSy2dwNi",
        "title": "Sample Compression Unleashed: New Generalization Bounds for Real Valued Losses",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The sample compression theory provides generalization guarantees for predictors that can be fully defined using a subset of the training dataset and a (short) message string, generally defined as a binary sequence. Previous works provided generalization bounds for the zero-one loss, which is restrictive notably when applied to deep learning approaches. In this paper, we present a general framework for deriving new sample compression bounds that hold for real-valued unbounded losses. Using the Pick-To-Learn (P2L) meta-algorithm, which transforms the training method of any machine-learning predictor to yield sample-compressed predictors, we empirically demonstrate the tightness of the bounds and their versatility by evaluating them on random forests and multiple types of neural networks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mathieu Bazinet;Valentina Zantedeschi;Pascal Germain",
        "authorids": "~Mathieu_Bazinet1;~Valentina_Zantedeschi2;~Pascal_Germain1",
        "gender": "M;F;M",
        "homepage": "https://mathieubazinet.github.io/eng/;http://vzantedeschi.com/;http://www.pascalgermain.info/",
        "dblp": ";179/2187;31/6421",
        "google_scholar": "https://scholar.google.ca/citations?user=26mmxRkAAAAJ;tdUUrS8AAAAJ;mgOIj_4AAAAJ",
        "orcid": "0009-0008-4105-5733;;0000-0003-3998-9533",
        "linkedin": "mathieu-bazinet-196523a6;valentina-zantedeschi-36a65a83/;germainml/",
        "or_profile": "~Mathieu_Bazinet1;~Valentina_Zantedeschi2;~Pascal_Germain1",
        "aff": "Universit\u00e9 Laval;ServiceNow Inc;Universit\u00e9 Laval",
        "aff_domain": "ulaval.ca;servicenow.com;ift.ulaval.ca",
        "position": "PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nbazinet2025sample,\ntitle={Sample compression unleashed: New generalization bounds for real valued losses},\nauthor={Mathieu Bazinet and Valentina Zantedeschi and Pascal Germain},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=0ynSy2dwNi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0ynSy2dwNi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "14C3JhW2jk",
        "title": "Generalized Criterion for Identifiability of Additive Noise Models Using Majorization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The discovery of causal relationships from observational data is very challenging. Many recent approaches rely on complexity or uncertainty concepts to impose constraints on probability distributions, aiming to identify specific classes of directed acyclic graph (DAG) models. In this paper, we introduce a novel identifiability criterion for DAGs that places constraints on the conditional variances of additive noise models. We demonstrate that this criterion extends and generalizes existing identifiability criteria in the literature that employ (conditional) variances as measures of uncertainty in (conditional) distributions. For linear structural equation models, we present a new algorithm that leverages the concept of weak majorization applied to the diagonal elements of the Cholesky factor of the covariance matrix to learn a topological ordering of variables. Through extensive simulations and the analysis of bank connectivity data, we provide evidence of the effectiveness of our approach in successfully recovering DAGs. The code for reproducing the results in this paper is available in Supplementary Materials.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aramayis Dallakyan;Yang Ni",
        "authorids": "~Aramayis_Dallakyan1;~Yang_Ni2",
        "gender": "M;M",
        "homepage": "https://adallak.github.io/;https://www.stat.tamu.edu/~yni/",
        "dblp": "271/0237;https://dblp.org/rec/conf/nips/ChoiCN20",
        "google_scholar": ";",
        "orcid": "0000-0002-3897-8261;0000-0003-0636-2363",
        "linkedin": ";",
        "or_profile": "~Aramayis_Dallakyan1;~Yang_Ni2",
        "aff": "StataCorp;Texas A&M University Main Campus+Texas A&M",
        "aff_domain": "stata.com;+tamu.edu",
        "position": "Researcher;+Assistant Professor",
        "bibtex": "@inproceedings{\ndallakyan2025generalized,\ntitle={Generalized Criterion for Identifiability of Additive Noise Models Using Majorization},\nauthor={Aramayis Dallakyan and Yang Ni},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=14C3JhW2jk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=14C3JhW2jk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1DH5pxs3xg",
        "title": "Estimation of Large Zipfian Distributions with Sort and Snap",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the estimation of Zipfian distributions under $L_1$ loss, and provide near minimax optimal bounds in several regimes.  Specifically, we assume observations arrive from a known alphabet, and with a known decay rate parametrizing the Zipfian, but we do not know a priori which alphabet elements have larger probability than others.  We present a novel Sort and Snap estimator, which uses the empirical proportions to sort the alphabet, and then snaps them to the associated term from the Zipfian distribution.  For arbitrary decay rates and smaller alphabet sizes, as well as for large decay rates and large alphabet sizes, we show an exact or minor variant of this estimator is near minimax optimal and has exponential improvement over the standard empirical proportion estimator. However, for small decay rates and larger alphabet sizes a simulation study indicates the standard empirical proportion estimator is competitive with Sort and Snap procedures. In addition to providing nearly tight bounds for important high-dimensional estimation problems, we believe the Sort and Snap estimator, and its analysis, will have independent interest.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peter Matthew Jacobs;Anirban Bhattacharya;Debdeep Pati;Lekha Patel;Jeff M. Phillips",
        "authorids": "~Peter_Matthew_Jacobs1;~Anirban_Bhattacharya1;~Debdeep_Pati2;~Lekha_Patel1;~Jeff_M._Phillips1",
        "gender": ";M;M;F;",
        "homepage": ";https://sites.google.com/view/anirban-bhattacharya-tamu/;https://pages.stat.wisc.edu/~dpati2/;https://lekhapatel.com;",
        "dblp": ";13/10009;128/4106;236/3960;",
        "google_scholar": "UyiiLiUAAAAJ;okljcvgAAAAJ;Qpi_-cwAAAAJ;ZKS9dtkAAAAJ;",
        "orcid": ";0000-0001-6197-2055;;0000-0003-3508-0672;",
        "linkedin": ";;;;",
        "or_profile": "~Peter_Matthew_Jacobs1;~Anirban_Bhattacharya1;~Debdeep_Pati2;~Lekha_Patel1;~Jeff_M._Phillips1",
        "aff": "University of Utah;Texas A&M University - College Station;University of Wisconsin - Madison;Sandia National Laboratories;",
        "aff_domain": "utah.edu;tamu.edu;wisc.edu;sandia.gov;",
        "position": "PhD student;Full Professor;Full Professor;Researcher;",
        "bibtex": "@inproceedings{\njacobs2025estimation,\ntitle={Estimation of Large Zipfian Distributions with Sort and Snap},\nauthor={Peter Matthew Jacobs and Jeff M. Phillips and Anirban Bhattacharya and Debdeep Pati and Lekha Patel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1DH5pxs3xg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1DH5pxs3xg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1MnUyJoVNC",
        "title": "Nonparametric estimation of Hawkes processes with RKHSs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper addresses nonparametric estimation of nonlinear multivariate Hawkes processes, where the interaction functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). Motivated by applications in neuroscience, the model allows complex interaction functions, in order to express exciting and inhibiting effects, but also a combination of both (which is particularly interesting to model the refractory period of neurons), and considers in return that conditional intensities are rectified by the ReLU function. The latter feature incurs several methodological challenges, for which workarounds are proposed in this paper. In particular, it is shown that a representer theorem can be obtained for approximated versions of the log-likelihood and the least-squares criteria. Based on it, we propose an estimation method, that relies on two common approximations (of the ReLU function and of the integral operator). We provide a bound that controls the impact of these approximations. Numerical results on synthetic data confirm this fact as well as the good asymptotic behavior of the proposed estimator. It also shows that our method achieves a better performance compared to related nonparametric estimation techniques and suits neuronal applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anna Bonnet;Maxime Sangnier",
        "authorids": "anna.bonnet@sorbonne-universite.fr;~Maxime_Sangnier1",
        "gender": ";",
        "homepage": ";",
        "dblp": ";136/5087.html",
        "google_scholar": ";https://scholar.google.fr/citations?user=ABKTexcAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "anna.bonnet@sorbonne-universite.fr;~Maxime_Sangnier1",
        "aff": ";Sorbonne Universit\u00e9, LPSM, UMR 8001",
        "aff_domain": ";sorbonne-universite.fr",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nbonnet2025nonparametric,\ntitle={Nonparametric estimation of Hawkes processes with {RKHS}s},\nauthor={Anna Bonnet and Maxime Sangnier},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1MnUyJoVNC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1MnUyJoVNC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1QovyEffl5",
        "title": "Federated Causal Inference: Multi-Study ATE Estimation beyond Meta-Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study Federated Causal Inference, an approach to estimate treatment effects from decentralized data across centers. We compare three classes of Average Treatment Effect (ATE) estimators derived from the Plug-in G-Formula, ranging from simple meta-analysis to one-shot and multi-shot federated learning, the latter leveraging the full data to learn the outcome model (albeit requiring more communication). Focusing on Randomized Controlled Trials (RCTs), we derive the asymptotic variance of these estimators for linear models. Our results provide practical guidance on selecting the appropriate estimator for various scenarios, including heterogeneity in sample sizes, covariate distributions, treatment assignment schemes, and center effects. We validate these findings with a simulation study.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "R\u00e9mi Khellaf;Aur\u00e9lien Bellet;Julie Josse",
        "authorids": "~R\u00e9mi_Khellaf1;~Aur\u00e9lien_Bellet1;~Julie_Josse1",
        "gender": "M;;F",
        "homepage": "https://remikhellaf.github.io/;http://researchers.lille.inria.fr/abellet/;http://juliejosse.com/",
        "dblp": ";61/8017;22/6376",
        "google_scholar": ";https://scholar.google.fr/citations?user=j8svx3IAAAAJ;AlIkUSAAAAAJ",
        "orcid": ";0000-0003-3440-1251;",
        "linkedin": "remi-khellaf/;;",
        "or_profile": "~R\u00e9mi_Khellaf1;~Aur\u00e9lien_Bellet1;~Julie_Josse1",
        "aff": "INRIA;INRIA;INRIA",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "position": "PhD student;Tenured researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nkhellaf2025federated,\ntitle={Federated Causal Inference: Multi-Centric {ATE} Estimation beyond Meta-Analysis},\nauthor={R{\\'e}mi Khellaf and Aur{\\'e}lien Bellet and Julie Josse},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1QovyEffl5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1QovyEffl5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1XTwh6x8ob",
        "title": "Pick-to-Learn and Self-Certified Gaussian Process Approximations",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Generalisation bounds are crucial for providing data-driven models with performance and safety guarantees. In this respect, bounds that do not require a held-out test set are particularly valuable as they allow the use of all data for training. While many such bounds do not improve upon the train-test approach, which remains the gold standard, the P2L algorithm (Paccagnan et al., 2023) has shown great potential. However, P2L comes with limitations, including computational overhead, reliance on consistent data, and restriction to non-Bayesian settings. In this work, we overcome these challenges in general settings and employ the corresponding results to show that classical Gaussian process (GP) training procedures can be interpreted as instantiations of P2L, thus inheriting tight, self-certified bounds. Three contributions underpin these conclusions. First, we introduce early stopping in P2L, equipping it with a tight generalisation bound to reduce training costs and address the non-consistent case. Second, we adapt P2L to the Bayesian setting and demonstrate its equivalence to posterior updating in a hierarchical model. Third, we show that greedy subset-of-data GPs are special P2L instantiations. Numerical evidence shows that the resulting P2L bounds we obtain compare favourably with the train-test and PAC-Bayes approaches on various real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Marks;Dario Paccagnan",
        "authorids": "~Daniel_Marks1;~Dario_Paccagnan1",
        "gender": "M;Not Specified",
        "homepage": ";https://www.doc.ic.ac.uk/~dp414/",
        "dblp": ";175/9343",
        "google_scholar": ";https://scholar.google.ch/citations?user=Y7wBGG8AAAAJ",
        "orcid": ";",
        "linkedin": "marksdan/;",
        "or_profile": "~Daniel_Marks1;~Dario_Paccagnan1",
        "aff": ";Imperial College London",
        "aff_domain": ";imperial.ac.uk",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nmarks2025picktolearn,\ntitle={Pick-to-Learn and Self-Certified Gaussian Process Approximations},\nauthor={Daniel Marks and Dario Paccagnan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1XTwh6x8ob}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1XTwh6x8ob",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1YNxc4j8sd",
        "title": "Causal discovery in mixed additive noise models",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Uncovering causal relationships in datasets that include both categorical and continuous variables is a challenging problem. The overwhelming majority of existing methods restrict their application to dealing with a single type of variable. Our contribution is a structural causal model designed to handle mixed-type data through a general function class. We present a theoretical foundation that specifies the conditions under which the directed acyclic graph underlying the causal model can be identified from observed data. In addition, we propose Mixed-type data Extension for Regression and Independence Testing (MERIT), enabling the discovery of causal connections in real-world classification settings. Our empirical studies demonstrate that MERIT outperforms its state-of-the-art competitor in causal discovery on relatively low-dimensional data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruicong Yao;Tim Verdonck;Jakob Raymaekers",
        "authorids": "~Ruicong_Yao1;~Tim_Verdonck1;~Jakob_Raymaekers1",
        "gender": "M;M;",
        "homepage": ";https://www.uantwerpen.be/en/staff/tim-verdonck/;",
        "dblp": "339/8800;95/1823;201/6075",
        "google_scholar": "https://scholar.google.com/citations?hl=en;hJ4MY3gAAAAJ;AG-W7eUAAAAJ",
        "orcid": "0000-0001-7581-1797;0000-0003-1105-2028;",
        "linkedin": ";tim-verdonck-205a8a1/;",
        "or_profile": "~Ruicong_Yao1;~Tim_Verdonck1;~Jakob_Raymaekers1",
        "aff": "KU Leuven;Universiteit Antwerpen;Universiteit Antwerpen",
        "aff_domain": "kuleuven.be;uantwerpen.be;uantwerpen.be",
        "position": "PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyao2025causal,\ntitle={Causal discovery in mixed additive noise models},\nauthor={Ruicong Yao and Jakob Raymaekers and Tim Verdonck},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1YNxc4j8sd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1YNxc4j8sd",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1YicSQLwjR",
        "title": "Max-Rank: Efficient Multiple Testing for Conformal Prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multiple hypothesis testing (MHT) frequently arises in scientific inquiries, and concurrent testing of multiple hypotheses inflates the risk of Type-I errors or false positives, rendering MHT corrections essential. This paper addresses MHT in the context of conformal prediction, a flexible framework for predictive uncertainty quantification. Some conformal applications give rise to simultaneous testing, and positive dependencies among tests typically exist. We introduce max-rank, a novel correction that exploits these dependencies whilst efficiently controlling the family-wise error rate. Inspired by existing permutation-based corrections, max-rank leverages rank order information to improve performance and integrates readily with any conformal procedure. We establish its theoretical and empirical advantages over the common Bonferroni correction and its compatibility with conformal prediction, highlighting the potential to strengthen predictive uncertainty estimates.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexander Timans;Christoph-Nikolas Straehle;Kaspar Sakmann;Christian A. Naesseth;Eric Nalisnick",
        "authorids": "~Alexander_Timans1;~Christoph-Nikolas_Straehle1;~Kaspar_Sakmann1;~Christian_A._Naesseth1;~Eric_Nalisnick1",
        "gender": "M;;;;M",
        "homepage": "https://alextimans.github.io/;;https://ksakmann.github.io/;;https://enalisnick.github.io",
        "dblp": "354/8299;;177/0713.html;;136/4057",
        "google_scholar": "tgiKFH4AAAAJ;;cmIHK9UAAAAJ;;cb1ZN7AAAAAJ",
        "orcid": "0009-0006-9395-5560;;0000-0002-5342-5921;;",
        "linkedin": "alexander-timans/;;kasparsakmann/;;",
        "or_profile": "~Alexander_Timans1;~Christoph-Nikolas_Straehle1;~Kaspar_Sakmann1;~Christian_A._Naesseth1;~Eric_Nalisnick1",
        "aff": "University of Amsterdam;;Robert Bosch GmbH, Bosch;;Johns Hopkins University",
        "aff_domain": "uva.nl;;de.bosch.com;;jhu.edu",
        "position": "PhD student;;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\ntimans2025maxrank,\ntitle={Max-Rank: Efficient Multiple Testing for Conformal Prediction},\nauthor={Alexander Timans and Christoph-Nikolas Straehle and Kaspar Sakmann and Christian A. Naesseth and Eric Nalisnick},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1YicSQLwjR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1YicSQLwjR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1av51ZlsuL",
        "title": "The Local Learning Coefficient: A Singularity-Aware Complexity Measure",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The Local Learning Coefficient (LLC) is introduced as a novel complexity measure for deep neural networks (DNNs). Recognizing the limitations of traditional complexity measures, the LLC leverages Singular Learning Theory (SLT), which has long recognized the significance of singularities in the loss landscape geometry. This paper provides an extensive exploration of the LLC's theoretical underpinnings, offering both a clear definition and intuitive insights into its application. Moreover, we propose a new scalable estimator for the LLC, which is then effectively applied across diverse architectures including deep linear networks up to 100M parameters, ResNet image models, and transformer language models. Empirical evidence suggests that the LLC provides valuable insights into how training heuristics might influence the effective complexity of DNNs. Ultimately, the LLC emerges as a crucial tool for reconciling the apparent contradiction between deep learning's complexity and the principle of parsimony.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Edmund Lau;Zach Furman;George Wang;Daniel Murfet;Susan Wei",
        "authorids": "~Edmund_Lau1;~Zach_Furman1;~George_Wang1;~Daniel_Murfet1;~Susan_Wei1",
        "gender": "M;;M;M;F",
        "homepage": "https://edmundlth.github.io/;;https://www.georgeyw.com;http://therisingsea.org;https://www.suswei.com/",
        "dblp": ";342/7253;;;203/8878",
        "google_scholar": ";;;;Udv9jsIAAAAJ",
        "orcid": ";;;;0000-0002-6842-2352",
        "linkedin": ";zach-furman-4936a0a5/;;;",
        "or_profile": "~Edmund_Lau1;~Zach_Furman1;~George_Wang1;~Daniel_Murfet1;~Susan_Wei1",
        "aff": "University of Melbourne;Boston University;Timaeus;The University of Melbourne;Monash University",
        "aff_domain": "unimelb.edu;bu.edu;timaeus.co;unimelb.edu.au;monash.edu",
        "position": "PhD student;Undergrad student;Researcher;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nlau2025the,\ntitle={The Local Learning Coefficient: A Singularity-Aware Complexity Measure},\nauthor={Edmund Lau and Zach Furman and George Wang and Daniel Murfet and Susan Wei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1av51ZlsuL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1av51ZlsuL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1y9UG4YOnm",
        "title": "Near-Optimal Algorithm for Non-Stationary Kernelized Bandits",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "This paper studies a non-stationary kernelized bandit (KB) problem, also called time-varying Bayesian optimization, where one seeks to minimize the regret under an unknown reward function that varies over time. In particular, we focus on a near-optimal algorithm whose regret upper bound matches the regret lower bound. For this goal, we show the first algorithm-independent regret lower bound for non-stationary KB with squared exponential and Mat\\'ern kernels, which reveals that an existing optimization-based KB algorithm with slight modification is near-optimal. However, this existing algorithm suffers from feasibility issues due to its huge computational cost.\nTherefore, we propose a novel near-optimal algorithm called restarting phased elimination with random permutation (R-PERP), which bypasses the huge computational cost. A technical key point is the simple permutation procedures of query candidates, which enable us to derive a novel tighter confidence bound tailored to the non-stationary problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shogo Iwazaki;Shion Takeno",
        "authorids": "~Shogo_Iwazaki1;~Shion_Takeno1",
        "gender": "M;M",
        "homepage": ";https://takeno1995.github.io/myhomepage/",
        "dblp": "251/9091;234/8990",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=oGaC1SgAAAAJ",
        "orcid": ";0009-0000-3638-8658",
        "linkedin": "shogo-iwazaki-0692a1185/;",
        "or_profile": "~Shogo_Iwazaki1;~Shion_Takeno1",
        "aff": "MI-6 Ltd.+LY Corporation;Nagoya University+RIKEN",
        "aff_domain": "mi-6.co.jp+lycorp.co.jp;nagoya-u.ac.jp+riken.jp",
        "position": "Researcher+Researcher;Assistant Professor+Visiting Reseacher",
        "bibtex": "@inproceedings{\niwazaki2025nearoptimal,\ntitle={Near-Optimal Algorithm for Non-Stationary Kernelized Bandits},\nauthor={Shogo Iwazaki and Shion Takeno},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=1y9UG4YOnm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1y9UG4YOnm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "23OrwVz5d5",
        "title": "Continuous Structure Constraint Integration for Robust Causal Discovery",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Causal discovery aims to infer a Directed Acyclic Graph (DAG) from observational data to represent causal relationships among variables. Traditional combinatorial methods search DAG spaces to identify optimal structures, while recent advances in continuous optimization improve this search process. However, integrating structural constraints informed by prior knowledge into these methods remains a substantial challenge. Existing methods typically integrate prior knowledge in a hard way, demanding precise information about causal relationships and struggling with erroneous priors. Such rigidity can lead to significant inaccuracies, especially when the priors are flawed. \nIn response to these challenges, this work introduces the Edge Constraint Adaptive (ECA) method, a novel approach that softly represents the presence of edges, allowing for a differentiable representation of prior constraint loss. This soft integration can more flexibly adjust to both accurate and erroneous priors, enhancing both robustness and adaptability. Empirical evaluations demonstrate that our approach effectively leverages prior to improve causal structure accuracy while maintaining resilience against prior errors, thus offering significant advancements in the field of causal discovery.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lyuzhou Chen;Taiyu Ban;Derui Lyu;Yijia Sun;Kangtao Hu;Xiangyu Wang;Huanhuan Chen",
        "authorids": "~Lyuzhou_Chen1;~Taiyu_Ban1;~Derui_Lyu1;~Yijia_Sun2;~Kangtao_Hu1;~Xiangyu_Wang7;~Huanhuan_Chen1",
        "gender": "M;M;M;M;M;M;",
        "homepage": "https://scholar.google.com/citations?hl=zh-CN&user=K0i72_4AAAAJ;;;;;;",
        "dblp": "336/0856;;340/0074;;;02/6128-16.html?q=Xiangyu%20Wang%200016;",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;;8y_rd1IAAAAJ;https://scholar.google.com.hk/citations?hl=zh-CN;https://scholar.google.com.hk/citations?view_op=list_works;https://scholar.google.com/citations?view_op=list_works;",
        "orcid": ";0000-0002-1379-7528;0000-0002-6416-6013;;;0000-0001-9843-5982;",
        "linkedin": ";;;;;;",
        "or_profile": "~Lyuzhou_Chen1;~Taiyu_Ban1;~Derui_Lyu1;~Yijia_Sun2;~Kangtao_Hu1;~Xiangyu_Wang7;~Huanhuan_Chen1",
        "aff": "University of Science and Technology of China;;University of Science and Technology of China;University of Science and Technology of China;;University of Science and Technology of China;",
        "aff_domain": "ustc.edu.cn;;ustc.edu.cn;mail.ustc.edu.cn;;ustc.edu.cn;",
        "position": "PhD student;;PhD student;MS student;;Assistant Professor;",
        "bibtex": "@inproceedings{\nchen2025continuous,\ntitle={Continuous Structrue Constraint Integration for Robust Causal Discovery},\nauthor={Lyuzhou Chen and Taiyu Ban and Derui Lyu and Yijia Sun and Kangtao Hu and Xiangyu Wang and Huanhuan Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=23OrwVz5d5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=23OrwVz5d5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "23yqeRmadg",
        "title": "Entropic Matching for Expectation Propagation of Markov Jump Processes",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We propose a novel, tractable latent state inference scheme for Markov jump processes, for which exact inference is often intractable. Our approach is based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm.\nWe demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology.\nMoreover, we derive closed-form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. \nWe evaluate our method across various chemical reaction networks and compare it to multiple baseline approaches, demonstrating superior performance in approximating the mean of the posterior process. Finally, we discuss the limitations of our method and potential avenues for future improvement, highlighting its promising direction for addressing complex continuous-time Bayesian inference problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yannick Eich;Bastian Alt;Heinz Koeppl",
        "authorids": "~Yannick_Eich1;~Bastian_Alt1;~Heinz_Koeppl1",
        "gender": ";M;M",
        "homepage": "https://www.bcs.tu-darmstadt.de/team_sos/eichyannick.en.jsp;;",
        "dblp": "329/5274;207/9742;41/6084",
        "google_scholar": "LU68-WsAAAAJ;https://scholar.google.de/citations?user=ZEj8FekAAAAJ;https://scholar.google.de/citations?user=WaPW80kAAAAJ",
        "orcid": ";0000-0002-1522-5400;",
        "linkedin": ";bastian-alt/;",
        "or_profile": "~Yannick_Eich1;~Bastian_Alt1;~Heinz_Koeppl1",
        "aff": "Technische Universit\u00e4t Darmstadt;DB Systel GmbH;TU Darmstadt",
        "aff_domain": "tu-darmstadt.de;deutschebahn.de;tu-darmstadt.de",
        "position": "PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\neich2025entropic,\ntitle={Entropic Matching for Expectation Propagation of Markov Jump Processes},\nauthor={Yannick Eich and Bastian Alt and Heinz Koeppl},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=23yqeRmadg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=23yqeRmadg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "25oDoZG4a5",
        "title": "Bridging Domains with Approximately Shared Features",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Machine learning models can suffer from performance degradation when applied to new tasks due to distribution shifts. Feature representation learning offers a robust solution to this issue. However, a fundamental challenge remains in devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. For better understanding, we propose a statistical framework that evaluates the utilities of the features (i.e., how differently the features are used in each source task) based on the variance of their correlation to $y$ across different domains. Under our framework, we design and analyze a learning procedure consisting of learning\n    content features (comprising both invariant and approximately shared features) from source tasks and fine-tuning them on the target task. Our theoretical analysis highlights the significance of learning approximately shared features\u2014beyond strictly invariant ones\u2014when distribution shifts occur. Our analysis also yields an improved population risk on target tasks compared to previous results. Inspired by our theory, we introduce ProjectionNet, a practical method to distinguish content features from environmental features via \\textit{explicit feature space control}, further consolidating our theoretical findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziliang Samuel Zhong;Xiang Pan;Qi Lei",
        "authorids": "~Ziliang_Samuel_Zhong1;~Xiang_Pan3;~Qi_Lei1",
        "gender": ";M;F",
        "homepage": "https://samzhong0702.github.io/;https://xiangpan.info;https://cecilialeiqi.github.io/",
        "dblp": ";59/749-1.html;",
        "google_scholar": "S-rSjJAAAAAJ;SxU03foAAAAJ;kGOgaowAAAAJ",
        "orcid": ";0000-0002-9828-5416;",
        "linkedin": ";;",
        "or_profile": "~Ziliang_Samuel_Zhong1;~Xiang_Pan3;~Qi_Lei1",
        "aff": "New York University;New York University;New York University",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nzhong2025bridging,\ntitle={Bridging Domains with Approximately Shared Features},\nauthor={Ziliang Samuel Zhong and Xiang Pan and Qi Lei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=25oDoZG4a5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=25oDoZG4a5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2831ZzaTL2",
        "title": "Is Merging Worth It? Securely Evaluating the Information Gain for Causal Dataset Acquisition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Merging datasets across institutions is a lengthy and costly procedure, especially when it involves private information. \nData hosts may therefore want to prospectively gauge which datasets are most beneficial to merge with, without revealing sensitive information. For causal estimation this is particularly challenging as the value of a merge depends not only on reduction in epistemic uncertainty but also on improvement in overlap. To address this challenge, we introduce the first \\emph{cryptographically secure} information-theoretic approach for quantifying the value of a merge in the context of heterogeneous treatment effect estimation. We do this by evaluating the \\emph{Expected Information Gain} (EIG) using multi-party computation to ensure that no raw data is revealed. We further demonstrate that our approach can be combined with differential privacy (DP) to meet arbitrary privacy requirements whilst preserving more accurate computation compared to DP alone. To the best of our knowledge, this work presents the first privacy-preserving method for dataset acquisition tailored to causal estimation.Code is publicly available: \\url{https://github.com/LucileTerminassian/causal_prospective_merge}.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jake Fawkes;Lucile Ter-Minassian;Desi R. Ivanova;Uri Shalit;Christopher C. Holmes",
        "authorids": "~Jake_Fawkes1;~Lucile_Ter-Minassian1;~Desi_R._Ivanova1;~Uri_Shalit1;~Christopher_C._Holmes1",
        "gender": "M;F;F;M;M",
        "homepage": "http://csml.stats.ox.ac.uk/people/;;https://desirivanova.com;;",
        "dblp": ";;286/8335;87/7049;08/6129",
        "google_scholar": ";PXyI6qkAAAAJ;AmX6sMIAAAAJ;https://scholar.google.co.il/citations?user=aeGDj-IAAAAJ;",
        "orcid": ";;;0000-0002-4026-2692;",
        "linkedin": ";lucile-ter-minassian-94428a12b/;dr-ivanova/;;",
        "or_profile": "~Jake_Fawkes1;~Lucile_Ter-Minassian1;~Desi_R._Ivanova1;~Uri_Shalit1;~Christopher_C._Holmes1",
        "aff": "University of Oxford;;Oxofrd, University of Oxford;Tel Aviv University+Google;University of Oxford",
        "aff_domain": "oxford.ac.uk;;stats.ox.ac.uk;tau.ac.il+google.com;ox.ac.uk",
        "position": "PhD student;;Assistant Professor;Associate Professor+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nfawkes2025is,\ntitle={Is merging worth it? Securely evaluating the information gain for causal dataset acquisition},\nauthor={Jake Fawkes and Lucile Ter-Minassian and Desi R. Ivanova and Uri Shalit and Christopher C. Holmes},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2831ZzaTL2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2831ZzaTL2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "287x97mSrq",
        "title": "Gaussian Mean Testing under Truncation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the task of Gaussian mean testing, that is, of testing whether a high-dimensional vector perturbed by white noise has large magnitude, or is the zero vector. This question, originating from the signal processing community, has recently seen a surge of interest from the machine learning and theoretical computer science community, and is by now fairly well understood. What is much less understood, and the focus of our work, is how to perform this task under *truncation*: that is, when the observations (i.i.d. samples from the underlying high-dimensional Gaussian) are only observed when they fall in an given subset of the domain $\\mathbb{R}^d$. This truncation model, previously studied in the context of *learning* (instead of *testing*) the mean vector, has a range of applications, in particular in Economics and Social Sciences. As our work shows, sample truncations affect the complexity of the testing task in a rather subtle and surprising way.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Clement Louis Canonne;Themis Gouleakis;Yuhao Wang;Qiping Yang",
        "authorids": "~Clement_Louis_Canonne1;~Themis_Gouleakis2;~Yuhao_Wang2;~Qiping_Yang1",
        "gender": "M;;F;M",
        "homepage": "https://ccanonne.github.io/;;https://www.yohannawang.com/;https://twitter.com/nerd_qp",
        "dblp": "28/9840L;;54/518.html;298/4926",
        "google_scholar": "u_OXsBIAAAAJ;;LHHyKJAAAAAJ;",
        "orcid": "0000-0001-7153-5211;;;0009-0009-6841-9370",
        "linkedin": ";;;",
        "or_profile": "~Clement_Louis_Canonne1;~Themis_Gouleakis2;~Yuhao_Wang2;~Qiping_Yang1",
        "aff": "University of Sydney;;National University of Singapore;University of Sydney",
        "aff_domain": "sydney.edu.au;;nus.edu.sg;sydney.edu.au",
        "position": "Lecturer;;PhD student;PhD student",
        "bibtex": "@inproceedings{\ncanonne2025gaussian,\ntitle={Gaussian Mean Testing under Truncation},\nauthor={Clement Louis Canonne and Themis Gouleakis and Yuhao Wang and Qiping Yang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=287x97mSrq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=287x97mSrq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "29cqyUl2bI",
        "title": "Variational Combinatorial Sequential Monte Carlo for Bayesian Phylogenetics in Hyperbolic Space",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Hyperbolic space naturally encodes hierarchical structures such as phylogenies (binary trees), where inward-bending geodesics reflect paths through least common ancestors, and the exponential growth of neighborhoods mirrors the super-exponential scaling of topologies. This scaling challenge limits the efficiency of Euclidean-based approximate Bayesian inference methods. Motivated by the geometric connections between trees and hyperbolic space, we develop novel hyperbolic extensions of two sequential search algorithms: Combinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc} and \\textsc{Ncsmc}). Our approach introduces consistent and unbiased estimators, along with variational inference methods (\\textsc{H-Vcsmc} and \\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical results demonstrate improved speed, scalability and performance in high-dimensional Bayesian phylogenetic inference tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alex Chen;Philippe Chlenski;Kenneth Munyuza;Antonio Khalil Moretti;Christian A. Naesseth;Itsik Pe'er",
        "authorids": "~Alex_Chen2;~Philippe_Chlenski1;~Kenneth_Munyuza1;~Antonio_Khalil_Moretti1;~Christian_A._Naesseth1;~Itsik_Pe'er1",
        "gender": "M;M;M;M;;M",
        "homepage": ";http://www.chlenski.com;https://kennethmunyuza.netlify.app/;;;http://www.cs.columbia.edu/~itsik/",
        "dblp": ";;;;;43/6414.html",
        "google_scholar": ";_8s9f44AAAAJ;;zOHnboAAAAAJ;;k7gYh7gAAAAJ",
        "orcid": ";0000-0002-2951-4385;;;;0000-0002-6128-7231",
        "linkedin": "axchen7/;;;;;itsik-pe-er-99b26a1/",
        "or_profile": "~Alex_Chen2;~Philippe_Chlenski1;~Kenneth_Munyuza1;~Antonio_Khalil_Moretti1;~Christian_A._Naesseth1;~Itsik_Pe'er1",
        "aff": "Columbia University;Apple+Columbia University;Columbia University;Spelman College;;Columbia University",
        "aff_domain": "columbia.edu;apple.com+columbia.edu;columbia.edu;spelman.edu;;columbia.edu",
        "position": "Undergrad student;Intern+PhD student;MS student;Assistant Professor;;Full Professor",
        "bibtex": "@inproceedings{\nchen2025variational,\ntitle={Variational Combinatorial Sequential Monte Carlo for Bayesian Phylogenetics in Hyperbolic Space},\nauthor={Alex Chen and Philippe Chlenski and Kenneth Munyuza and Antonio Khalil Moretti and Christian A. Naesseth and Itsik Pe'er},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=29cqyUl2bI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=29cqyUl2bI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2A4X4AxLXu",
        "title": "Reliable and Scalable Variable Importance Estimation via Warm-start and Early Stopping",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As opaque black-box predictive models such as neural networks become more prevalent, the need to develop interpretations for these models is of great interest. The concept of $\\textit{variable importance}$ is an interpretability measure that applies to any predictive model and assesses how much a variable or set of variables improves prediction performance. When the number of variables is large, estimating variable importance presents a significant challenge because re-training neural networks or other black-box algorithms requires significant additional computation. In this paper, we address this challenge for algorithms using gradient descent and gradient boosting (e.g. neural networks, gradient-boosted decision trees). By using the ideas of early stopping of gradient-based methods in combination with warm-start using the $\\textit{dropout}$ method, we develop a scalable method to estimate variable importance for any algorithm that can be expressed as an $\\textit{iterative kernel update equation}$.  Importantly, we provide theoretical guarantees by using the  theory for early stopping of kernel-based methods for  neural networks with sufficient large width and gradient-boosting decision trees that use symmetric tree as a weaker learner. We also demonstrate the efficacy of our methods through simulations and a real data example which illustrates the computational benefit of early stopping rather than fully re-training the model as well as the increased accuracy of taking initial steps from the dropout solution.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zexuan Sun;Garvesh Raskutti",
        "authorids": "~Zexuan_Sun1;~Garvesh_Raskutti3",
        "gender": ";M",
        "homepage": ";https://pages.cs.wisc.edu/~raskutti/",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": "zexuansun/;",
        "or_profile": "~Zexuan_Sun1;~Garvesh_Raskutti3",
        "aff": "University of Wisconsin - Madison;",
        "aff_domain": "wisc.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nsun2025reliable,\ntitle={Reliable and scalable variable importance estimation via warm-start and early stopping},\nauthor={Zexuan Sun and Garvesh Raskutti},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2A4X4AxLXu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2A4X4AxLXu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2EhGTwqwX2",
        "title": "DeCaf: A Causal Decoupling Framework for OOD Generalization on Node Classification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graph Neural Networks (GNNs) are susceptible to distribution shifts, creating vulnerability and security issues in critical domains. There is a pressing need to enhance the generalizability of GNNs on out-of-distribution (OOD) test data. Existing methods that target learning an invariant (feature, structure)-label mapping often depend on oversimplified assumptions about the data generation process, which do not adequately reflect the actual dynamics of distribution shifts in graphs. In this paper, we introduce a more realistic graph data generation model using Structural Causal Models (SCMs), allowing us to redefine distribution shifts by pinpointing their origins within the generation process. Building on this, we propose a casual decoupling framework, DeCaf, that independently learns unbiased feature-label and structure-label mappings. We provide a detailed theoretical framework that shows how our approach can effectively mitigate the impact of various distribution shifts. We evaluate DeCaf across both real-world and synthetic datasets that demonstrate different patterns of shifts, confirming its efficacy in enhancing the generalizability of GNNs. Our code is available at: https://github.com/hanxiaoxue114/DeCaf-GraphOOD.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaoxue Han;Huzefa Rangwala;Yue Ning",
        "authorids": "~Xiaoxue_Han1;~Huzefa_Rangwala2;~Yue_Ning1",
        "gender": "F;M;F",
        "homepage": "https://hanxiaoxue114.github.io/;http://www.cs.gmu.edu/~rangwala;https://yue-ning.github.io/",
        "dblp": "219/1935;30/444;74/9990-1.html",
        "google_scholar": ";yWJ9BqEAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Xiaoxue_Han1;~Huzefa_Rangwala2;~Yue_Ning1",
        "aff": "Stevens Institute of Technology;Amazon;Stevens Institute of Technology",
        "aff_domain": "stevens.edu;amazon.com;stevens.edu",
        "position": "PhD student;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nhan2025decaf,\ntitle={DeCaf: A Causal Decoupling Framework for {OOD} Generalization on Node Classification},\nauthor={Xiaoxue Han and Huzefa Rangwala and Yue Ning},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2EhGTwqwX2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2EhGTwqwX2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2YDdWw3qWu",
        "title": "Graph-based Complexity for Causal Effect by Empirical Plug-in",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper focuses on the computational complexity of computing empirical plug-in estimates for causal effect queries.\nGiven a causal graph and observational data, any identifiable causal query can be estimated from an expression over the observed variables, called the estimand. The estimand can then be evaluated by plugging in probabilities computed empirically from data. \nIn contrast to conventional wisdom which assumes that high dimensional probabilistic functions will lead to exponential evaluation time, we show that estimand evaluation can be done efficiently, potentially in time linear in the data size, depending on the estimand's hypergraph.\n\nIn particular, we show that both the $\\it{treewidth}$ and $\\it{hypertree width}$ of the estimand's structure bound the evaluation complexity, analogous to their role in bounding the complexity of inference in probabilistic graphical models. In settings with high dimensional functions, the hypertree width often provides a more effective bound, since the empirical distributions are sparse.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rina Dechter;Anna K Raichev;Jin Tian;Alexander Ihler",
        "authorids": "~Rina_Dechter1;~Anna_K_Raichev1;~Jin_Tian1;~Alexander_Ihler1",
        "gender": "female;F;M;M",
        "homepage": ";;https://mbzuai.ac.ae/study/faculty/jin-tian/;http://www.ics.uci.edu/~ihler/",
        "dblp": "d/RDechter;;04/4658-1;39/1313",
        "google_scholar": ";;T0crkfoAAAAJ;https://scholar.google.com.tw/citations?user=ffdt7gMAAAAJ",
        "orcid": ";;0000-0001-5313-1600;0000-0002-4331-1015",
        "linkedin": ";annie-raichev-378376128/;;ihler/",
        "or_profile": "~Rina_Dechter1;~Anna_K_Raichev1;~Jin_Tian1;~Alexander_Ihler1",
        "aff": ";;Mohamed bin Zayed University of Artificial Intelligence;University of California, Irvine",
        "aff_domain": "uci;;mbzuai.ac.ae;uci.edu",
        "position": "Professor;;Full Professor;Professor",
        "bibtex": "@inproceedings{\ndechter2025graphbased,\ntitle={Graph-based Complexity for Causal Effect Evaluation by Plug-in},\nauthor={Rina Dechter and Anna K Raichev and Alexander Ihler and Jin Tian},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2YDdWw3qWu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2YDdWw3qWu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2cZ1xs5UUL",
        "title": "Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "By leveraging the representation power of deep neural networks, neural upper confidence bound (UCB) algorithms have shown success in contextual bandits. To further balance the exploration and exploitation, we propose Neural-$\\sigma^2$-LinearUCB, a variance-aware algorithm that utilizes $\\sigma^2_t$, i.e., an upper bound of the reward noise variance at round $t$, to enhance the uncertainty quantification quality of the UCB, resulting in a regret performance improvement. We provide an oracle version for our algorithm characterized by an oracle variance upper bound $\\sigma^2_t$ and a practical version with a novel estimation for this variance bound. Theoretically, we provide rigorous regret analysis for both versions and prove that our oracle algorithm achieves a better regret guarantee than other neural-UCB algorithms in the neural contextual bandits setting. Empirically, our practical method enjoys a similar computational efficiency, while outperforming state-of-the-art techniques by having a better calibration and lower regret across multiple standard settings, including on the synthetic, UCI, MNIST, and CIFAR-10 datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ha Manh Bui;Enrique Mallada;Anqi Liu",
        "authorids": "~Ha_Manh_Bui1;~Enrique_Mallada1;~Anqi_Liu2",
        "gender": ";M;F",
        "homepage": ";http://mallada.ece.jhu.edu;https://anqiliu-ai.github.io/",
        "dblp": ";;",
        "google_scholar": ";ZvRFA04AAAAJ;Q8yp6zQAAAAJ",
        "orcid": ";0000-0003-1568-1833;0000-0002-0468-5698",
        "linkedin": ";emallada/;",
        "or_profile": "~Ha_Manh_Bui1;~Enrique_Mallada1;~Anqi_Liu2",
        "aff": ";Johns Hopkins University;Johns Hopkins University+California Institute of Technology+University of Illinois, Chicago",
        "aff_domain": ";jhu.edu;jhu.edu+caltech.edu+uic.edu",
        "position": ";Associate Professor;Assistant Professor+Postdoc+PhD student",
        "bibtex": "@inproceedings{\nbui2025varianceaware,\ntitle={Variance-Aware Linear {UCB} with Deep Representation for Neural Contextual Bandits},\nauthor={Ha Manh Bui and Enrique Mallada and Anqi Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2cZ1xs5UUL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2cZ1xs5UUL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2dQL1TBu5q",
        "title": "Locally Optimal Descent for Dynamic Stepsize Scheduling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce a novel dynamic learning-rate scheduling scheme grounded in theory with the goal of simplifying the manual and time-consuming tuning of schedules in practice.  Our approach is based on estimating the locally-optimal stepsize, guaranteeing maximal descent in the direction of the stochastic gradient of the current step.  We first establish theoretical convergence bounds for our method within the context of smooth non-convex stochastic optimization. We then present a practical implementation of our algorithm and conduct systematic experiments across diverse datasets and optimization algorithms, comparing our scheme with existing state-of-the-art learning-rate schedulers. Our findings indicate that our method needs minimal tuning when compared to existing approaches. Thus, removing the need for auxiliary manual schedules and warm-up phases and achieving comparable performance with drastically reduced parameter tuning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gilad Yehudai;Alon Cohen;Amit Daniely;Yoel Drori;Tomer Koren;Mariano Schain",
        "authorids": "~Gilad_Yehudai2;~Alon_Cohen1;~Amit_Daniely1;~Yoel_Drori1;~Tomer_Koren1;~Mariano_Schain1",
        "gender": "M;M;;;M;",
        "homepage": ";https://sites.google.com/site/aloncohentechnion/;;;https://tomerkoren.github.io;",
        "dblp": "239/4344;133/2021;;115/7818;12/10044;",
        "google_scholar": "opVT1qkAAAAJ;shoYR_AAAAAJ;;7pRQY3MAAAAJ;wGG1voYAAAAJ;",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Gilad_Yehudai2;~Alon_Cohen1;~Amit_Daniely1;~Yoel_Drori1;~Tomer_Koren1;~Mariano_Schain1",
        "aff": "New York University;Tel Aviv University+Google;Hebrew U and Google;Google;Tel Aviv University;",
        "aff_domain": "nyu.edu;tauex.tau.ac.il+google.com;ac.il;google.com;tau.ac.il;",
        "position": "Postdoc;Assistant Professor+Researcher;Assistant Professor;Researcher;Associate Professor;",
        "bibtex": "@inproceedings{\nyehudai2025locally,\ntitle={Locally Optimal Descent for Dynamic Stepsize Scheduling},\nauthor={Gilad Yehudai and Alon Cohen and Amit Daniely and Yoel Drori and Tomer Koren and Mariano Schain},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2dQL1TBu5q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2dQL1TBu5q",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2deM2CJQz5",
        "title": "Tensor Network-Constrained Kernel Machines as Gaussian Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper we establish a new connection between Tensor Network (TN)-constrained kernel machines and Gaussian Processes (GPs). We prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor\nTrain (TT)-constrained kernel machines converge in the limit of large ranks to the same GP which we fully characterize, when specifying appropriate i.i.d. priors across their components. We show that TT-constrained models achieve faster convergence to the GP compared to their CPD counterparts for the\nsame number of model parameters. The convergence to the GP occurs as the ranks tend to\ninfinity, as opposed to the standard approach\nwhich introduces TNs as an additional constraint on the posterior. This implies that the\nnewly established priors allow the models to\nlearn features more freely as they necessitate\ninfinitely more parameters to converge to a\nGP, which is characterized by a fixed learning\nrepresentation and thus no feature learning.\nAs a consequence, the newly derived priors yield more flexible models which can better fit the data, albeit at increased risk of overfitting. We demonstrate these considerations by means of two numerical experiments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Frederiek Wesel;kim batselier",
        "authorids": "~Frederiek_Wesel1;~kim_batselier1",
        "gender": "Not Specified;",
        "homepage": ";",
        "dblp": ";128/5511",
        "google_scholar": ";",
        "orcid": ";0000-0001-7381-2630",
        "linkedin": "https://linkedin.com/in/frederiek-wesel;",
        "or_profile": "~Frederiek_Wesel1;~kim_batselier1",
        "aff": "Delft University of Technology;Delft University of Technology",
        "aff_domain": "tudelft.nl;tudelft.nl",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nwesel2025tensor,\ntitle={Tensor Network-Constrained Kernel Machines as Gaussian Processes},\nauthor={Frederiek Wesel and kim batselier},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2deM2CJQz5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2deM2CJQz5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2kIYWdFU0x",
        "title": "Multi-Agent Credit Assignment with Pretrained Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the \"over-representation\" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel \"disentangled\" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thought that can suggest potential goals, provide suitable goal decomposition and subgoal allocation as well as self-reflection-based replanning. Additionally, SAMA incorporates language-grounded MARL to train each agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages in sample efficiency compared to state-of-the-art ASG methods, as evidenced by its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS. The code is available at https://anonymous.4open.science/r/SAMA/.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenhao Li;Dan Qiao;Baoxiang Wang;Xiangfeng Wang;Wei;Hao Shen;Bo Jin;Hongyuan Zha",
        "authorids": "~Wenhao_Li2;~Dan_Qiao3;~Baoxiang_Wang1;~Xiangfeng_Wang1;~Wei3;~Hao_Shen1;~Bo_Jin1;~Hongyuan_Zha1",
        "gender": "M;;;M;M;M;;",
        "homepage": "https://tomaxent.com;https://qiaodan-cuhk.github.io;;https://xfwang87.github.io/;https://www.helloyinwei.com/;;;",
        "dblp": "11/444-1;152/4915-3.html;;;;26/2210-2;;z/HongyuanZha",
        "google_scholar": "HAtzuaYAAAAJ;;;YpGMkgsAAAAJ;;Kce9W-8AAAAJ;;n1DQMIsAAAAJ",
        "orcid": "0000-0003-2985-1098;0000-0002-2364-6897;;0000-0003-1802-4425;;;;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Wenhao_Li2;~Dan_Qiao3;~Baoxiang_Wang1;~Xiangfeng_Wang1;~Wei3;~Hao_Shen1;~Bo_Jin1;~Hongyuan_Zha1",
        "aff": "Tongji University;Chinese University of Hong Kong, Shen Zhen;;East China Normal University;Bank of Communications;Fortiss GmbH;;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "tongji.edu.cn;link.cuhk.edu.cn;;ecnu.edu.cn;bankcomm.com;fortiss.org;;cuhk.edu.cn",
        "position": "Assistant Professor;PhD student;;Full Professor;Researcher;Principal Researcher;;Full Professor",
        "bibtex": "@inproceedings{\nli2025multiagent,\ntitle={Multi-Agent Credit Assignment with Pretrained Language Models},\nauthor={Wenhao Li and Dan Qiao and Baoxiang Wang and Xiangfeng Wang and Wei and Hao Shen and Bo Jin and Hongyuan Zha},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2kIYWdFU0x}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2kIYWdFU0x",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2soEM2biI3",
        "title": "Memory-Efficient Optimization with Factorized Hamiltonian Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern deep learning heavily depends on adaptive optimizers such as Adam and its variants, which are renowned for their capacity to handle model scaling and streamline hyperparameter tuning. However, these algorithms typically experience high memory overhead caused by the accumulation of optimization states, leading to a critical challenge in training large-scale network models. In this study, we introduce a novel adaptive optimizer, H-Fac, which incorporates a memory-efficient factorization approach to address this challenge. By employing a rank-1 parameterization for both momentum and scaling parameter estimators, H-Fac reduces memory costs to a sublinear level while maintaining competitive performance across a wide range of architectures. We develop our algorithms based on principles derived from Hamiltonian dynamics, providing robust theoretical underpinnings in optimization dynamics and convergence guarantees. These optimization algorithms are designed to be both straightforward and adaptable, facilitating easy implementation in diverse settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Son Nguyen;Lizhang Chen;Bo Liu;qiang liu",
        "authorids": "~Son_Nguyen2;~Lizhang_Chen1;~Bo_Liu13;~qiang_liu4",
        "gender": "M;M;M;",
        "homepage": "https://sonpeter.github.io/;https://l-z-chen.github.io/;https://cranial-xix.github.io/;",
        "dblp": ";225/1559;;",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Son_Nguyen2;~Lizhang_Chen1;~Bo_Liu13;~qiang_liu4",
        "aff": "University of Texas at Austin;University of Texas at Austin;University of Texas, Austin;",
        "aff_domain": "utexas.edu;utexas.edu;cs.utexas.edu;",
        "position": "PhD student;PhD student;PhD student;",
        "bibtex": "@inproceedings{\nnguyen2025memoryefficient,\ntitle={Memory-Efficient Optimization with Factorized Hamiltonian Descent},\nauthor={Son Nguyen and Lizhang Chen and Bo Liu and qiang liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=2soEM2biI3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2soEM2biI3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "38atnV3R2h",
        "title": "Zero-Shot Action Generalization with Limited Observations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement Learning (RL) has demonstrated remarkable success in solving sequential decision-making problems. However, in real-world scenarios, RL agents often struggle to generalize when faced with unseen actions that were not encountered during training. Some previous works on zero-shot action generalization rely on large datasets of action observations to capture the behaviors of new actions, making them impractical for real-world applications. In this paper, we introduce a novel zero-shot framework, Action Generalization from Limited Observations (AGLO). Our framework has two main components: an action representation learning module and a policy learning module. The action representation learning module extracts discriminative embeddings of actions from limited observations, while the policy learning module leverages the learned action representations, along with augmented synthetic action representations, to learn a policy capable of handling tasks with unseen actions. The experimental results demonstrate that our framework significantly outperforms state-of-the-art methods for zero-shot action generalization across multiple benchmark tasks, showcasing its effectiveness in generalizing to new actions with minimal action observations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abdullah Alchihabi;Hanping Zhang;Yuhong Guo",
        "authorids": "~Abdullah_Alchihabi1;~Hanping_Zhang1;~Yuhong_Guo1",
        "gender": ";M;",
        "homepage": ";https://jajajag.github.io/;",
        "dblp": ";230/3460;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Abdullah_Alchihabi1;~Hanping_Zhang1;~Yuhong_Guo1",
        "aff": ";Carleton University;",
        "aff_domain": ";carleton.ca;",
        "position": ";PhD student;",
        "bibtex": "@inproceedings{\nalchihabi2025zeroshot,\ntitle={Zero-Shot Action Generalization with Limited Observations},\nauthor={Abdullah Alchihabi and Hanping Zhang and Yuhong Guo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=38atnV3R2h}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=38atnV3R2h",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3E9HooMHRV",
        "title": "Global Ground Metric Learning with Applications to scRNA data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Optimal transport (OT) provides a robust framework for comparing probability distributions. Its effectiveness is significantly influenced by the choice of the underlying ground metric. Traditionally, the ground metric has either been (i) predefined, e.g., as a Euclidean metric, or (ii) learned in a supervised way, by utilizing labeled data to learn a suitable ground metric for enhanced task-specific performance. Yet, predefined metrics typically cannot account for the inherent structure and varying significance of different features in the data, and existing supervised ground metric learning methods often fail to generalize across multiple classes or are limited to distributions with shared supports.   To address this issue, this paper introduces a novel approach for learning metrics for arbitrary distributions over a shared metric space. Our method provides a distance between individual points (samples) like a global metric, but requires only class labels on a distribution-level for training. The resulting learned global ground metric enables more accurate OT distances, which can significantly improve clustering and classification tasks. Further, we can create task-specific shared embeddings for elements (samples) from different distributions, including unseen data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Damin K\u00fchn;Michael T Schaub",
        "authorids": "~Damin_K\u00fchn1;~Michael_T_Schaub1",
        "gender": "M;",
        "homepage": "https://daminkuehn.de;https://michaelschaub.github.io/",
        "dblp": ";72/10263",
        "google_scholar": "KudwlGYAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0003-2426-6404",
        "linkedin": ";",
        "or_profile": "~Damin_K\u00fchn1;~Michael_T_Schaub1",
        "aff": "Rheinisch Westf\u00e4lische Technische Hochschule Aachen;RWTH Aachen University",
        "aff_domain": "rwth-aachen.de;rwth-aachen.de",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nkuhn2025global,\ntitle={Global Ground Metric Learning with Applications to sc{RNA} data},\nauthor={Damin K{\\\"u}hn and Michael T Schaub},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3E9HooMHRV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3E9HooMHRV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3K7rKkU7Ri",
        "title": "A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "A key property of neural networks is their capacity of adapting to data during training. Yet, our current mathematical understanding of feature learning and its relationship to generalization remain limited. In this work, we provide a random matrix analysis of how fully-connected two-layer neural networks adapt to the target function after a single, but aggressive, gradient descent step. We rigorously establish the equivalence between the updated features and an isotropic spiked random feature model, in the limit of large batch size. For the latter model, we derive a deterministic equivalent description of the feature empirical covariance matrix in terms of certain low-dimensional operators. This allows us to sharply characterize the impact of training in the asymptotic feature spectrum, and in particular, provides a theoretical grounding for how the tails of the feature spectrum modify with training. The deterministic equivalent further yields the exact asymptotic generalization error, shedding light on the mechanisms behind its improvement in the presence of feature learning. Our result goes beyond standard random matrix ensembles, and therefore we believe it is of independent technical interest. Different from previous work, our result holds in the challenging maximal learning rate regime, is fully rigorous and allows for finitely supported second layer initialization, which turns out to be crucial for studying the functional expressivity of the learned features. This provides a sharp description of the impact of feature learning in the generalization of two-layer neural networks, beyond the random features and lazy training regimes.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yatin Dandi;Luca Pesce;Hugo Cui;Florent Krzakala;Yue Lu;Bruno Loureiro",
        "authorids": "~Yatin_Dandi1;~Luca_Pesce1;~Hugo_Cui1;~Florent_Krzakala1;~Yue_Lu1;~Bruno_Loureiro1",
        "gender": "M;M;;;M;M",
        "homepage": "https://yatindandi.github.io/;https://lucpoisson.github.io;;http://Krzakala.org;https://lu.seas.harvard.edu;https://brloureiro.github.io/",
        "dblp": "255/6032;321/1650;;25/1282;39/6975;207/1834",
        "google_scholar": "UiEzYkMAAAAJ;praGYvoAAAAJ;;https://scholar.google.fr/citations?user=3jDeUlMAAAAJ;wc0FCZUAAAAJ;DXl3ir8AAAAJ",
        "orcid": ";;;0000-0003-2313-2578;;0000-0002-6327-4688",
        "linkedin": ";;;;;bruno-loureiro-43183b14a/",
        "or_profile": "~Yatin_Dandi1;~Luca_Pesce1;~Hugo_Cui1;~Florent_Krzakala1;~Yue_Lu1;~Bruno_Loureiro1",
        "aff": "EPFL - EPF Lausanne;EPFL - EPF Lausanne;;Swiss Federal Institute of Technology Lausanne;School of Engineering and Applied Sciences, Harvard University;Ecole Normale Sup\u00e9rieure, Ecole Normale Sup\u00e9rieure de Paris",
        "aff_domain": "epfl.ch;epfl.ch;;epfl.ch;seas.harvard.edu;di.ens.fr",
        "position": "PhD student;PhD student;;Full Professor;Professor;Researcher",
        "bibtex": "@inproceedings{\ndandi2025a,\ntitle={A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities},\nauthor={Yatin Dandi and Luca Pesce and Hugo Cui and Florent Krzakala and Yue Lu and Bruno Loureiro},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3K7rKkU7Ri}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3K7rKkU7Ri",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3LpX1vcRHj",
        "title": "Theoretically Grounded Pruning of Large Ground Sets for Constrained, Discrete Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern instances of combinatorial optimization problems often exhibit billion-scale ground sets, which have many uninformative or redundant elements. In this work, we develop light-weight pruning algorithms to quickly discard elements that are unlikely to be part of an optimal solution. Under mild assumptions on the instance, we prove theoretical guarantees on the fraction of the optimal value retained and the size of the resulting pruned ground set. Through extensive experiments on real-world datasets for various applications, we demonstrate that our algorithm, QuickPrune, efficiently prunes over 90% of the ground set and outperforms state-of-the-art classical and machine learning heuristics for pruning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ankur Nath;Alan Kuhnle",
        "authorids": "~Ankur_Nath1;~Alan_Kuhnle1",
        "gender": "M;M",
        "homepage": "https://ankurnath.github.io/;https://www.alankuhnle.com",
        "dblp": ";153/2879",
        "google_scholar": ";https://scholar.google.com/citations?hl=en",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ankur_Nath1;~Alan_Kuhnle1",
        "aff": "Texas A&M University - College Station;Texas A&M University - College Station",
        "aff_domain": "tamu.edu;tamu.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nnath2025theoretically,\ntitle={Theoretically Grounded Pruning of Large Ground Sets for Constrained, Discrete Optimization},\nauthor={Ankur Nath and Alan Kuhnle},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3LpX1vcRHj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3LpX1vcRHj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3R5K51d9kr",
        "title": "Achieving $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ Regret in Average-Reward POMDPs with Known Observation Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We tackle average-reward infinite-horizon POMDPs with an unknown transition model but a known observation model, a setting that has been previously addressed in two limiting ways: (i) frequentist methods relying on suboptimal stochastic policies having a minimum probability of choosing each action, and (ii) Bayesian approaches employing the optimal policy class but requiring strong assumptions about the consistency of employed estimators. Our work removes these limitations by proving convenient estimation guarantees for the transition model and introducing an optimistic algorithm that leverages the optimal class of deterministic belief-based policies.  \nWe introduce modifications to existing estimation techniques providing theoretical guarantees separately for each estimated action transition matrix. Unlike existing estimation methods that are unable to use samples from different policies, we present a novel and simple estimator that overcomes this barrier. This new data-efficient technique, combined with the proposed $\\textit{Action-wise OAS-UCRL}$ algorithm and a tighter theoretical analysis, leads to the first approach enjoying a regret guarantee of order $\\mathcal{O}(\\sqrt{T \\log T})$ when compared against the optimal policy, thus improving over state of the art techniques. Finally, theoretical results are validated through numerical simulations showing the efficacy of our method against baseline methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alessio Russo;Alberto Maria Metelli;Marcello Restelli",
        "authorids": "~Alessio_Russo2;~Alberto_Maria_Metelli2;~Marcello_Restelli1",
        "gender": "M;M;M",
        "homepage": "https://github.com/alesnow97;https://albertometelli.github.io/;http://home.deib.polimi.it/restelli/",
        "dblp": ";209/4941;64/1011",
        "google_scholar": "1D3mdpIAAAAJ;R31IsPwAAAAJ;https://scholar.google.com.tw/citations?user=xdgxRiEAAAAJ",
        "orcid": "0009-0008-3275-7653;0000-0002-3424-5212;0000-0002-6322-1076",
        "linkedin": "alessio-russo-b98546193/;;",
        "or_profile": "~Alessio_Russo2;~Alberto_Maria_Metelli2;~Marcello_Restelli1",
        "aff": "Polytechnic Institute of Milan;Politecnico di Milano;Politecnico di Milano",
        "aff_domain": "polimi.it;polimi.it;polimi.it",
        "position": "PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nrusso2025achieving,\ntitle={Achieving \\${\\textbackslash}widetilde\\{{\\textbackslash}mathcal\\{O\\}\\}({\\textbackslash}sqrt\\{T\\})\\$ Regret in Average-Reward {POMDP}s with Known Observation Models},\nauthor={Alessio Russo and Alberto Maria Metelli and Marcello Restelli},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3R5K51d9kr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3R5K51d9kr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3j3NtXcc95",
        "title": "Stochastic Rounding for LLM Training: Theory and Practice",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As the parameters of Large Language Models (LLMs) have scaled to hundreds of billions, the demand for efficient training methods\u2014balancing faster computation and reduced memory usage without sacrificing accuracy\u2014has become more critical than ever. In recent years, various mixed precision strategies, which involve different precision levels for optimization components, have been proposed to increase training speed with minimal accuracy degradation. However, these strategies often require manual adjustments and lack theoretical justification. In this work, we leverage stochastic rounding (SR) to address numerical errors of training with low-precision representation. We provide theoretical analyses of implicit regularization and convergence under the Adam optimizer when SR is utilized. With the insights from these analyses, we extend previous BF16 + SR strategy to be used in distributed settings, enhancing the stability and performance for large scale training. Empirical results from pre-training models with up to 6.7B parameters, for the first time, demonstrate that our BF16 with SR strategy outperforms (BF16, FP32) mixed precision strategies, achieving better validation perplexity, up to 1.54$\\times$ higher throughput, and 30\\% lower memory usage.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaan Ozkara;Tao Yu;Youngsuk Park",
        "authorids": "~Kaan_Ozkara1;~Tao_Yu1;~Youngsuk_Park1",
        "gender": ";M;M",
        "homepage": ";https://ydtydr.github.io/;https://youngsuk0723.github.io/",
        "dblp": ";;88/11095",
        "google_scholar": "W-JoHj0AAAAJ;lbi95bUAAAAJ;jWROvQ0AAAAJ",
        "orcid": ";;0000-0002-0970-9214",
        "linkedin": ";tao-yu-220720182/;y-park",
        "or_profile": "~Kaan_Ozkara1;~Tao_Yu1;~Youngsuk_Park1",
        "aff": "University of California, Los Angeles;Amazon;Amazon, AWS AI Labs",
        "aff_domain": "ucla.edu;amazon.com;amazon.com",
        "position": "PhD student;Researcher;Research",
        "bibtex": "@inproceedings{\nozkara2025stochastic,\ntitle={Stochastic Rounding for {LLM} Training: Theory and Practice},\nauthor={Kaan Ozkara and Tao Yu and Youngsuk Park},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3j3NtXcc95}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3j3NtXcc95",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3kRGSyp309",
        "title": "Amortized Probabilistic Conditioning for Optimization, Simulation and Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Amortized meta-learning methods based on pre-training have propelled fields like natural language processing and vision. Transformer-based neural processes and their variants are leading models for probabilistic meta-learning with a tractable objective. Often trained on synthetic data, these models implicitly capture essential latent information in the data-generation process. However, existing methods do not allow users to flexibly inject (condition on) and extract (predict) this probabilistic latent information at runtime, which is key to many tasks.\nWe introduce the Amortized Conditioning Engine (ACE), a new transformer-based meta-learning model that explicitly represents latent variables of interest. ACE affords conditioning on both observed data and interpretable latent variables, the inclusion of priors at runtime, and outputs predictive distributions for discrete and continuous data and latents.  We show ACE's practical utility  across diverse tasks such as image completion and classification, Bayesian optimization, and simulation-based inference, demonstrating how a general conditioning framework can replace task-specific solutions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Paul Edmund Chang;Nasrulloh Ratu Bagus Satrio Loka;Daolang Huang;Ulpu Remes;Samuel Kaski;Luigi Acerbi",
        "authorids": "~Paul_Edmund_Chang1;~Nasrulloh_Ratu_Bagus_Satrio_Loka1;~Daolang_Huang1;~Ulpu_Remes1;~Samuel_Kaski1;~Luigi_Acerbi1",
        "gender": "M;M;M;;M;M",
        "homepage": "https://research.aalto.fi/en/persons/paul-chang;;https://www.huangdaolang.com;;https://people.aalto.fi/samuel.kaski;http://luigiacerbi.com/",
        "dblp": "270/0387;248/5730.html;277/8410;16/8717;64/5826;72/1450",
        "google_scholar": "CLzK5SkAAAAJ;bA9vjiAAAAAJ;2togGHoAAAAJ;;https://scholar.google.com/citations?hl=en;https://scholar.google.co.uk/citations?user=QYBZoGwAAAAJ",
        "orcid": ";0000-0002-4761-2368;;0000-0003-1435-0207;0000-0003-1925-9154;0000-0001-7471-7336",
        "linkedin": ";;daolanghuang/?originalSubdomain=fi;;samuel-kaski-27790/;luigi-acerbi-719b492/",
        "or_profile": "~Paul_Edmund_Chang1;~Nasrulloh_Ratu_Bagus_Satrio_Loka1;~Daolang_Huang1;~Ulpu_Remes1;~Samuel_Kaski1;~Luigi_Acerbi1",
        "aff": "University of Helsinki;University of Helsinki;Aalto University;University of Helsinki;University of Manchester+Helsinki Institute for Information Technology+Aalto University;University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi;aalto.fi;helsinki.fi;manchester.ac.uk+hiit.fi+aalto.fi;helsinki.fi",
        "position": "Postdoc;Postdoc;PhD student;Postdoc;Full Professor+Several positions+Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nchang2025amortized,\ntitle={Amortized Probabilistic Conditioning for Optimization, Simulation and Inference},\nauthor={Paul Edmund Chang and Nasrulloh Ratu Bagus Satrio Loka and Daolang Huang and Ulpu Remes and Samuel Kaski and Luigi Acerbi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3kRGSyp309}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3kRGSyp309",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3t33grIKkc",
        "title": "Loss Gradient Gaussian Width based Generalization and Optimization Guarantees",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Generalization and optimization guarantees on the population loss often rely on uniform convergence based analysis, typically based on the Rademacher complexity of the predictors. The rich representation power of modern models has led to concerns about this approach. In this paper, we present generalization and optimization guarantees in terms of the complexity of the gradients, as measured by the Loss Gradient Gaussian Width (LGGW). First, we introduce generalization guarantees directly in terms of the LGGW under a flexible gradient domination condition. Second, we show that sample reuse in ERM does not make the empirical gradients deviate from the population gradients as long as the LGGW is small. Third, focusing on deep networks, we bound their single-sample LGGW in terms of the Gaussian width of the featurizer, i.e., the output of the last-but-one layer. To our knowledge, our generalization and optimization guarantees in terms of LGGW are the first results of its kind, and hold considerable promise towards quantitatively tight bounds for deep models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arindam Banerjee;Qiaobo Li;Yingxue Zhou",
        "authorids": "~Arindam_Banerjee4;~Qiaobo_Li1;~Yingxue_Zhou1",
        "gender": ";M;F",
        "homepage": "https://arindam.cs.illinois.edu/;;https://sites.google.com/umn.edu/zhou0877/home",
        "dblp": "82/4807.html;;",
        "google_scholar": "RY7cuPAAAAAJ;;EEm_z9YAAAAJ",
        "orcid": ";;",
        "linkedin": ";qiaobo-li-581815251/;",
        "or_profile": "~Arindam_Banerjee4;~Qiaobo_Li1;~Yingxue_Zhou1",
        "aff": "University of Illinois, Urbana Champaign;Department of Computer Science, University of Illinois at Urbana-Champaign;University of Minnesota, Minneapolis",
        "aff_domain": "illinois.edu;cs.illinois.edu;umn.edu",
        "position": "Professor;PhD student;PhD student",
        "bibtex": "@inproceedings{\nbanerjee2025loss,\ntitle={Loss Gradient Gaussian Width based Generalization and Optimization Guarantees},\nauthor={Arindam Banerjee and Qiaobo Li and Yingxue Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3t33grIKkc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3t33grIKkc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3tV5AtAXk0",
        "title": "Causal Discovery-Driven Change Point Detection in Time Series",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Change point detection in time series aims to identify moments when the probability distribution of time series changes. It is widely applied in many areas, such as human activity sensing and medical science. In the context of multivariate time series, this typically involves examining the joint distribution of multiple variables: If the distribution of any one variable changes, the entire time series undergoes a distribution shift. However, in practical applications, we may be interested only in certain components of the time series, exploring abrupt changes in their distributions while accounting for the presence of other components. Here, assuming an underlying structural causal model that governs the time-series data generation, we address this task by proposing a two-stage non-parametric algorithm that first learns parts of the causal structure through constraint-based discovery methods, and then employs conditional relative Pearson divergence estimation to identify the change points. The conditional relative Pearson divergence quantifies the distribution difference between consecutive segments in the time series, while the causal discovery method allows a focus on the causal mechanism, facilitating access to independent and identically distributed (IID) samples. Theoretically, the typical assumption of samples being IID in conventional change point detection methods can be relaxed based on the Causal Markov Condition. Through experiments on both synthetic and real-world datasets, we validate the correctness and utility of our approach.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shanyun Gao;Raghavendra Addanki;Tong Yu;Ryan A. Rossi;Murat Kocaoglu",
        "authorids": "~Shanyun_Gao2;~Raghavendra_Addanki1;~Tong_Yu3;~Ryan_A._Rossi2;~Murat_Kocaoglu1",
        "gender": "F;M;;;M",
        "homepage": ";https://raddanki.github.io/;https://www.linkedin.com/in/tong-yu-42790744;;https://www.muratkocaoglu.com",
        "dblp": ";218/5579;32/1593-1;;74/11343",
        "google_scholar": ";SUPaOhgAAAAJ;https://scholar.google.com/citations?hl=en;;7N7bzdwAAAAJ",
        "orcid": ";;0000-0002-5991-2050;;",
        "linkedin": "https://www.linkedin.com/jobs/?src=go-pa&trk=sem-ga_campid.18853522261_asid.146084015209_crid.633923221414_kw.linkedin_d.c_tid.kwd-296170574619_n.g_mt.e_geo.9016722&mcid=6994434350142418944&cid=&gclid=Cj0KCQiAi8KfBhCuARIsADp-A565vO-bdHEB4of97YFPZUvB5FwxZx0Aphoa3GoJSDpVVvjy6XFooQQaAskBEALw_wcB&gclsrc=aw.ds;;tong-yu-42790744;;mkocaoglu/",
        "or_profile": "~Shanyun_Gao2;~Raghavendra_Addanki1;~Tong_Yu3;~Ryan_A._Rossi2;~Murat_Kocaoglu1",
        "aff": "Purdue University;Adobe Systems;Adobe Research;;Purdue University",
        "aff_domain": "purdue.edu;adobe.com;adobe.com;;purdue.edu",
        "position": "PhD student;Research Scientist;Senior Research Scientist;;Assistant Professor",
        "bibtex": "@inproceedings{\ngao2025causal,\ntitle={Causal Discovery-Driven Change Point Detection in Time Series},\nauthor={Shanyun Gao and Raghavendra Addanki and Tong Yu and Ryan A. Rossi and Murat Kocaoglu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=3tV5AtAXk0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3tV5AtAXk0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "446GZolzar",
        "title": "Deep Optimal Sensor Placement for Black Box Stochastic Simulations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Selecting cost-effective optimal sensor configurations for subsequent inference of parameters in black-box stochastic systems faces significant computational barriers. We propose a novel and robust approach,  modelling the joint distribution over input parameters and solution with a joint energy-based model, trained on simulation data. Unlike existing simulation-based inference approaches, which must be tied to a specific set of point evaluations, we learn a functional representation of parameters and solution. This is used as a resolution-independent plug-and-play surrogate for the joint distribution, which can be conditioned over any set of points, permitting an efficient approach to sensor placement. We demonstrate the validity of our framework on a variety of stochastic problems, showing that our method provides highly informative sensor locations at a lower computational cost compared to conventional approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Paula Cordero Encinar;Tobias Schr\u00f6der;Peter Yatsyshin;Andrew B. Duncan",
        "authorids": "~Paula_Cordero_Encinar1;~Tobias_Schr\u00f6der2;~Peter_Yatsyshin1;~Andrew_B._Duncan1",
        "gender": "F;;Not Specified;",
        "homepage": ";;https://www.yatsyshin.com;",
        "dblp": ";;;",
        "google_scholar": "R27puNcAAAAJ;;https://scholar.google.co.uk/citations?hl=en;",
        "orcid": ";;0000-0002-8844-281X;",
        "linkedin": "paula-cordero-encinar-9b6b58210/;;peter-yatsyshin-phd/;",
        "or_profile": "~Paula_Cordero_Encinar1;~Tobias_Schr\u00f6der2;~Peter_Yatsyshin1;~Andrew_B._Duncan1",
        "aff": "Imperial College London;;Alan Turing Institute+Alan Turing Institute;",
        "aff_domain": "imperial.ac.uk;;turing.ac.uk+turing.ac.uk;",
        "position": "PhD student;;Postdoc+Fellow;",
        "bibtex": "@inproceedings{\nencinar2025deep,\ntitle={Deep Optimal Sensor Placement for Black Box Stochastic Simulations},\nauthor={Paula Cordero Encinar and Tobias Schr{\\\"o}der and Peter Yatsyshin and Andrew B. Duncan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=446GZolzar}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=446GZolzar",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "44w2O8tJ9v",
        "title": "Truncated Inverse-L\u00e9vy Measure Representation of the Beta Process",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The beta process is a widely used nonparametric prior in Bayesian machine learning. While various inference schemes have been developed for the beta process and related models, the current state-of-the-art method relies heavily on the stick-breaking representation with decreasing atom weights, which is available only for a special hyperparameter. In this paper, we introduce the truncated inverse-L\u00e9vy measure representation (TILe-Rep) that extends the decreasing atom weights representation of the beta process to general hyperparameters. The TILe-Rep fills the gap between the two stick-breaking representations in Teh et al. (2007) and Paisley et al. (2010). Moreover, it has a lower truncation error compared to other sequential representations of the beta process and potentially leads to the posterior consistency property of the Bayesian factor models. We demonstrate the usage of the TILe-Rep in the celebrated beta process factor analysis model and beta process sparse factor model.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junyi Zhang;Angelos Dassios;Zhong Chong;Qiufei Yao",
        "authorids": "~Junyi_Zhang4;~Angelos_Dassios1;~Zhong_Chong1;~Qiufei_Yao1",
        "gender": ";M;M;F",
        "homepage": ";https://stats.lse.ac.uk/angelos;https://www.researchgate.net/profile/Chong-Zhong-6;https://yqf1999.github.io/qiufeiyao.github.io/",
        "dblp": ";;;",
        "google_scholar": ";;;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Junyi_Zhang4;~Angelos_Dassios1;~Zhong_Chong1;~Qiufei_Yao1",
        "aff": ";London School of Economics and Political Science, University of London;The Hong Kong Polytechnic University;Bocconi University",
        "aff_domain": ";lse.ac.uk;polyu.edu.hk;unibocconi.eu",
        "position": ";Full Professor;Postdoc;PhD student",
        "bibtex": "@inproceedings{\nzhang2025truncated,\ntitle={Truncated Inverse-L\\'evy Measure Representation of the Beta Process},\nauthor={Junyi Zhang and Angelos Dassios and Zhong Chong and Qiufei Yao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=44w2O8tJ9v}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=44w2O8tJ9v",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4C1aRz2gRq",
        "title": "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the model's ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies have demonstrated that the Transformer architecture used in LLMs can implement a single-step gradient descent update by processing in-context examples in a single forward pass. Recent work has further shown that, during in-context learning, a looped Transformer can implement multi-step gradient descent updates in forward passes. However, their theoretical results require an exponential number of in-context examples, $n = \\exp(\\Omega(T))$, where $T$ is the number of loops or passes, to achieve a reasonably low error. In this paper, we study linear looped  Transformers in-context learning on linear vector generation tasks. We show that linear looped Transformers can implement multi-step gradient descent efficiently for in-context learning. Our results demonstrate that as long as the input data has a constant condition number, e.g., $n = O(d)$, the linear looped Transformers can achieve a small error by multi-step gradient descent during in-context learning. Furthermore, our preliminary experiments validate our theoretical analysis. Our findings reveal that the Transformer architecture possesses a stronger in-context learning capability than previously understood, offering new insights into the mechanisms behind LLMs and potentially guiding the better design of efficient inference algorithms for LLMs.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bo Chen;Xiaoyu Li;Yingyu Liang;Zhenmei Shi;Zhao Song",
        "authorids": "~Bo_Chen22;~Xiaoyu_Li12;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3",
        "gender": "M;;;M;M",
        "homepage": "https://scholar.google.com/citations?user=BWfZIIYAAAAJ;;;http://zhmeishi.github.io/;https://www.youtube.com/@simapofang",
        "dblp": "89/5615-32.html;;;246/5216;76/4051-2",
        "google_scholar": "BWfZIIYAAAAJ;;;0oeNnzMAAAAJ;yDZct7UAAAAJ",
        "orcid": ";;;0009-0007-6741-7598;0000-0003-4589-5234",
        "linkedin": ";;;zhenmei-shi-56408a113/;",
        "or_profile": "~Bo_Chen22;~Xiaoyu_Li12;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3",
        "aff": "University of Hong Kong+Middle Tennessee State University;;;MongoDB+Voyage AI;University of California, Berkeley",
        "aff_domain": "hku.hk+mtsu.edu;;;mongodb.com+voyageai.com;berkeley.edu",
        "position": "MPhil student+Undergrad student;;;Researcher+Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nchen2025bypassing,\ntitle={Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent},\nauthor={Bo Chen and Xiaoyu Li and Yingyu Liang and Zhenmei Shi and Zhao Song},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4C1aRz2gRq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4C1aRz2gRq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4CMgrpXrt1",
        "title": "Bayesian Circular Regression with von Mises Quasi-Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The need for regression models to predict circular values arises in many scientific fields. In this work we explore a family of expressive and interpretable distributions over circle-valued random functions related to Gaussian processes targeting two Euclidean dimensions conditioned on the unit circle. The  probability model has connections with continuous spin models in statistical physics. Moreover, its density is very simple and has maximum-entropy, unlike previous Gaussian process-based approaches, which use wrapping or radial marginalization. For posterior inference, we introduce a new Stratonovich-like augmentation that lends itself to fast Gibbs sampling. We argue that transductive learning in these models favors a Bayesian approach to the parameters and apply our sampling scheme to the Double Metropolis-Hastings algorithm. We present experiments applying this model to the prediction of (i) wind directions and (ii) the percentage of the running gait cycle as a function of joint angles.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yarden Cohen;Alexandre Khae Wu Navarro;Jes Frellsen;Richard E. Turner;Raziel Riemer;Ari Pakman",
        "authorids": "~Yarden_Cohen1;~Alexandre_Khae_Wu_Navarro1;~Jes_Frellsen1;~Richard_E_Turner1;~Raziel_Riemer2;~Ari_Pakman1",
        "gender": "F;M;M;M;M;M",
        "homepage": ";https://akwn.org;https://frellsen.org;https://rich-turner-group.github.io/;https://in.bgu.ac.il/engn/iem/BRL/Pages/Research.aspx;https://aripakman.github.io/",
        "dblp": ";;83/8247;40/5352;;139/1387",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=H14tlssAAAAJ;Yj2sBWkAAAAJ;https://scholar.google.co.uk/citations?user=DgLEyZgAAAAJ;https://scholar.google.co.il/citations?user=ACZdmywAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0001-9224-1271;;0000-0002-9358-6287;0000-0001-8047-2240",
        "linkedin": "yarden-cohen2?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;https://linkedin.com/in/akwn;frellsen/;;raziel-riemer-48ab72173/;",
        "or_profile": "~Yarden_Cohen1;~Alexandre_Khae_Wu_Navarro1;~Jes_Frellsen1;~Richard_E_Turner1;~Raziel_Riemer2;~Ari_Pakman1",
        "aff": "Ben Gurion University of the Negev;Babylon health;Technical University of Denmark;Alan Turing Institute+University of Cambridge;Ben Gurion University of the Negev;Ben-Gurion University of the Negev",
        "aff_domain": "post.bgu.ac.il;babylonhealth.com;dtu.dk;turing.ac.uk+cam.ac.uk;bgu.ac.il;bgu.ac.il",
        "position": "MS student;Research Scientist;Associate Professor;Researcher+Professor;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncohen2025bayesian,\ntitle={Bayesian Circular Regression with von Mises Quasi-Processes},\nauthor={Yarden Cohen and Alexandre Khae Wu Navarro and Jes Frellsen and Richard E. Turner and Raziel Riemer and Ari Pakman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4CMgrpXrt1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4CMgrpXrt1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4DXZqyG0p3",
        "title": "On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The holy grail of machine learning is to enable Continual Federated Learning (CFL) to enhance the efficiency, privacy, and scalability of AI systems while learning from streaming data. The primary challenge of a CFL system is to overcome global catastrophic forgetting, wherein the accuracy of the global model trained on new tasks declines on the old tasks. In this work, we propose \\emph{Continual Federated Learning with Aggregated Gradients} (C-FLAG), a novel replay-memory based federated strategy consisting of edge-based gradient updates on memory and aggregated gradients on the current data. We provide convergence analysis of the C-FLAG approach which addresses forgetting and bias while converging at a rate of $O(1/\\sqrt{T})$ over $T$ communication rounds. We formulate an optimization sub-problem that minimizes catastrophic forgetting, translating CFL into an iterative algorithm with adaptive learning rates that ensure seamless learning across tasks. We empirically show that C-FLAG outperforms several state-of-the-art baselines on both task and class-incremental settings with respect to metrics such as accuracy and forgetting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Satish Kumar Keshri;Nazreen Shah;Ranjitha Prasad",
        "authorids": "~Satish_Kumar_Keshri1;~Nazreen_Shah1;~Ranjitha_Prasad1",
        "gender": "M;F;F",
        "homepage": ";https://nazreenshah.github.io/;https://www.iiitd.ac.in/ranjitha",
        "dblp": ";;17/6671",
        "google_scholar": ";7HCUafUAAAAJ;https://scholar.google.com.sg/citations?user=ADJe1AsAAAAJ",
        "orcid": ";;0000-0002-2649-7882",
        "linkedin": "satish-kumar-keshri/;nazreenshah1997/;ranjitha-prasad-7ab84b30/",
        "or_profile": "~Satish_Kumar_Keshri1;~Nazreen_Shah1;~Ranjitha_Prasad1",
        "aff": "Indraprastha Institute of Information Technology, Delhi;Indraprastha Institute of Information Technology, Delhi;Indraprastha Institute of Information Technology, Delhi, Dhirubhai Ambani Institute Of Information and Communication Technology",
        "aff_domain": "iiitd.ac.in;iiitd.ac.in;iiitd.ac.in",
        "position": "Researcher;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nkeshri2025on,\ntitle={On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients},\nauthor={Satish Kumar Keshri and Nazreen Shah and Ranjitha Prasad},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4DXZqyG0p3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4DXZqyG0p3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4dE0JLhIsC",
        "title": "Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have recently demonstrated significant potential in the field of time series forecasting, offering impressive capabilities in handling complex temporal data. However, their robustness and reliability in real-world applications remain under-explored, particularly concerning their susceptibility to adversarial attacks. In this paper, we introduce a targeted adversarial attack framework for LLM-based time series forecasting. By employing both gradient-free and black-box optimization methods, we generate minimal yet highly effective perturbations that significantly degrade the forecasting accuracy across multiple datasets and LLM architectures. Our experiments, which include models like LLMTime with GPT-3.5, GPT-4, LLaMa, and Mistral, TimeGPT, and TimeLLM show that adversarial attacks lead to much more severe performance degradation than random noise, and demonstrate the broad effectiveness of our attacks across different LLMs. The results underscore the critical vulnerabilities of LLMs in time series forecasting, highlighting the need for robust defense mechanisms to ensure their reliable deployment in practical applications. The code repository can be found at https://github.com/JohnsonJiang1996/AdvAttack_LLM4TS.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fuqiang Liu;Sicong Jiang;Luis Miranda-Moreno;Seongjin Choi;Lijun Sun",
        "authorids": "~Fuqiang_Liu2;~Sicong_Jiang1;~Luis_Miranda-Moreno1;~Seongjin_Choi1;~Lijun_Sun1",
        "gender": "M;M;M;M;",
        "homepage": ";https://johnsonjiang1996.github.io/;;https://choi-seongjin.github.io/;",
        "dblp": ";224/4668;;;",
        "google_scholar": "_8DtvZ4AAAAJ;;https://scholar.google.com/;tyLWFk4AAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";sicong-johnson-j-a85a71175/;;seongjin-choi-852b2b198/;",
        "or_profile": "~Fuqiang_Liu2;~Sicong_Jiang1;~Luis_Miranda-Moreno1;~Seongjin_Choi1;~Lijun_Sun1",
        "aff": "McGill University;McGill University;McGill University;University of Minnesota - Twin Cities;",
        "aff_domain": "mail.mcgill.ca;mail.mcgill.ca;mcgill.ca;umn.edu;",
        "position": "PhD student;PhD student;Associate Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nliu2025adversarial,\ntitle={Adversarial Vulnerabilities in Large Language Models for Time Series Forecasting},\nauthor={Fuqiang Liu and Sicong Jiang and Luis Miranda-Moreno and Seongjin Choi and Lijun Sun},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4dE0JLhIsC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4dE0JLhIsC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4fhFayxBO5",
        "title": "Locally Private Estimation with Public Features",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We initiate the study of locally differentially private (LDP) learning with public features. We define semi-feature LDP, where some features are publicly available while the remaining ones, along with the label, require protection under local differential privacy. Under semi-feature LDP, we demonstrate that the mini-max convergence rate for non-parametric regression is significantly reduced compared to that of classical LDP. Then we propose HistOfTree, an estimator that fully leverages the information contained in both public and private features. Theoretically, HistOfTree reaches the mini-max optimal convergence rate. Empirically, HistOfTree achieves superior performance on both synthetic and real data. We also explore scenarios where users have the flexibility to select features for protection manually. In such cases, we propose an estimator and a data-driven parameter tuning strategy, leading to analogous theoretical and empirical results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuheng Ma;Ke Jia;Hanfang Yang",
        "authorids": "~Yuheng_Ma1;~Ke_Jia1;~Hanfang_Yang2",
        "gender": "M;F;M",
        "homepage": "https://karlmyh.github.io/;;http://stat.ruc.edu.cn/en/teacher_more.php?cid=89248&id=40",
        "dblp": "258/0645-1.html;;",
        "google_scholar": "JvMlW0gAAAAJ;;EsSjDdAAAAAJ",
        "orcid": ";0009-0008-1110-8670;",
        "linkedin": ";;",
        "or_profile": "~Yuheng_Ma1;~Ke_Jia1;~Hanfang_Yang2",
        "aff": "Renmin University of China+Mohamed bin Zayed University of Artificial Intelligence;Renmin University of China;Renmin University of China",
        "aff_domain": "ruc.edu.cn+mbzuai.ac.ae;ruc.edu.cn;ruc.edu.cn",
        "position": "PhD student+PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nma2025locally,\ntitle={Locally Private Estimation with Public Features},\nauthor={Yuheng Ma and Ke Jia and Hanfang Yang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4fhFayxBO5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4fhFayxBO5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "4yQg5rixo0",
        "title": "Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends\u2014whether explicitly or implicitly\u2014upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Deep Chakraborty;Yann LeCun;Tim G. J. Rudner;Erik Learned-Miller",
        "authorids": "~Deep_Chakraborty1;~Yann_LeCun1;~Tim_G._J._Rudner2;~Erik_Learned-Miller2",
        "gender": ";M;;",
        "homepage": "https://deepc94.github.io;http://yann.lecun.com;;",
        "dblp": "194/5620;l/YannLeCun;;",
        "google_scholar": "Ld6-470AAAAJ;WLN3QrAAAAAJ;;",
        "orcid": "0000-0003-0950-4320;;;",
        "linkedin": "deepc94/;;;",
        "or_profile": "~Deep_Chakraborty1;~Yann_LeCun1;~Tim_G._J._Rudner2;~Erik_Learned-Miller2",
        "aff": "University of Massachusetts at Amherst;Meta+New York University;;",
        "aff_domain": "umass.edu;meta.com+nyu.edu;;",
        "position": "PhD student;Chief AI Scientist +Full Professor;;",
        "bibtex": "@inproceedings{\nchakraborty2025improving,\ntitle={Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization},\nauthor={Deep Chakraborty and Yann LeCun and Tim G. J. Rudner and Erik Learned-Miller},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=4yQg5rixo0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4yQg5rixo0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "52tHFOuGlg",
        "title": "Local Stochastic Sensitivity Analysis For Dynamical Systems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We derive local sensitivities of statistical quantities of interest with respect to model parameters in dynamical systems.\nOur main contribution is the extension of adjoint-based a posteriori analysis for differential operators of generic dynamical systems acting on states to the Liouville operator acting on probability densities of the states. This results in theoretically rigorous estimates of sensitivity and error for a broad class of computed quantities of interest while propagating uncertainty through dynamical systems.\nWe also derive Monte-Carlo type estimators to make these estimates computationally tractable using spatio-temporal normalizing flows and exploiting the hyperbolic nature of the Liouville equation.  \nThree examples demonstrate our method.\nFirst, for verification of the theoretical results, we use a 2D linear dynamical system with an initial multivariate Gaussian density. \nThen, we apply our method to the challenging task of propagating uncertainty in a double attractor system to illustrate sensitivities in bimodal distributions. \nFinally, we show that our method can provide sensitivities with respect to the parameters of Neural Ordinary Differential Equations (here, in the context of classification).",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nishant Panda;Jehanzeb H Chaudhry;Natalie Klein;James Carzon;Troy Butler",
        "authorids": "~Nishant_Panda1;~Jehanzeb_H_Chaudhry1;~Natalie_Klein1;~James_Carzon1;~Troy_Butler2",
        "gender": "M;M;;;",
        "homepage": ";https://math.unm.edu/~jehanzeb/;;https://jamescarzon.github.io/;",
        "dblp": "146/9708;;299/9350;;",
        "google_scholar": "zOV6TUAAAAAJ;EYOgO5MAAAAJ;VlHLqU0AAAAJ;tqjNLO0AAAAJ;aaf6oCAAAAAJ",
        "orcid": "0000-0001-9754-2794;0000-0002-6484-4072;0000-0002-7532-4013;0009-0000-1038-5475;",
        "linkedin": ";;;;",
        "or_profile": "~Nishant_Panda1;~Jehanzeb_H_Chaudhry1;~Natalie_Klein1;~James_Carzon1;~Troy_Butler2",
        "aff": "Los Alamos National Laboratory;University of New Mexico;Los Alamos National Laboratory;Carnegie Mellon University;University of Colorado at Denver",
        "aff_domain": "lanl.gov;unm.edu;lanl.gov;cmu.edu;ucdenver.edu",
        "position": "Researcher;Associate Professor;Researcher;PhD student;Full Professor",
        "bibtex": "@inproceedings{\npanda2025local,\ntitle={Local Stochastic Sensitivity Analysis For Dynamical Systems},\nauthor={Nishant Panda and Jehanzeb H Chaudhry and Natalie Klein and James Carzon and Troy Butler},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=52tHFOuGlg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=52tHFOuGlg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "56aJtyPC4X",
        "title": "Anytime-Valid A/B Testing of Counting Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Motivated by monitoring the arrival of incoming adverse events such as customer support calls or crash events from users exposed to an experimental product change, we consider sequential hypothesis testing of continuous-time counting processes. Specifically, we provide a multivariate confidence process on the cumulative rates $(\\Lambda^A_t, \\Lambda^B_t)$ giving an anytime-valid coverage guarantee $\\mathbb{P}[(\\Lambda^A_t, \\Lambda^B_t) \\in C^\\alpha_t \\, \\forall t >0] \\geq 1-\\alpha$. This provides simultaneous confidence process on $\\Lambda^A_t$, $\\Lambda^B_t$ and their difference $\\Lambda^B_t-\\Lambda^A_t$, allowing each arm of the experiment and the difference between them to be safely monitored throughout the experiment. We extend our results by constructing a closed-form $e$-process for testing the equality of rates with a time-uniform Type-I error guarantee at a nominal $\\alpha$. We characterize the asymptotic growth rate of the proposed $e$-process under the alternative and show that it has power 1 when the average rates of the two process differ in the limit.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Lindon;Nathan Kallus",
        "authorids": "~Michael_Lindon1;~Nathan_Kallus1",
        "gender": "M;",
        "homepage": ";http://nathankallus.com/",
        "dblp": "321/3620;142/2900",
        "google_scholar": "-dW0PugAAAAJ;K2WfIlsAAAAJ",
        "orcid": "0000-0003-3694-5420;0000-0003-1672-0507",
        "linkedin": "michaelslindon/;",
        "or_profile": "~Michael_Lindon1;~Nathan_Kallus1",
        "aff": "NetFlix;Netflix+Cornell University",
        "aff_domain": "netflix.com;netflix.com+cornell.edu",
        "position": "Researcher;Research Director+Associate Professor",
        "bibtex": "@inproceedings{\nlindon2025anytimevalid,\ntitle={Anytime-Valid Continuous-Time Confidence Processes for Inhomogeneous Poisson Processes},\nauthor={Michael Lindon and Nathan Kallus},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=56aJtyPC4X}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=56aJtyPC4X",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5DypCUsMg4",
        "title": "A Unifying Framework for Action-Conditional Self-Predictive Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Learning a good representation is a crucial challenge for reinforcement learning (RL) agents. Self-predictive algorithms jointly learn a latent representation and dynamics model by bootstrapping from future latent representations (BYOL). Recent work has developed theoretical insights into these algorithms by studying a continuous-time ODE model in the case of a fixed policy (BYOL-$\\Pi$); this assumption is at odds with practical implementations, which explicitly condition their predictions on future actions. In this work, we take a step towards bridging the gap between theory and practice by analyzing an action-conditional self-predictive objective (BYOL-AC) using the ODE framework. Interestingly, we uncover that BYOL-$\\Pi$ and BYOL-AC are related through the lens of variance. We unify the study of these objectives through two complementary lenses; a model-based perspective, where each objective is related to low-rank approximation of certain dynamics, and a model-free perspective, which relates the objectives to modified value, Q-value, and Advantage functions. This mismatch with the true value functions leads to the empirical observation (in both linear and deep RL experiments) that BYOL-$\\Pi$ and BYOL-AC are either very similar in performance across many tasks or task-dependent.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Khimya Khetarpal;Zhaohan Daniel Guo;Bernardo Avila Pires;Yunhao Tang;Clare Lyle;Mark Rowland;Nicolas Heess;Diana L Borsa;Arthur Guez;Will Dabney",
        "authorids": "~Khimya_Khetarpal1;~Zhaohan_Daniel_Guo1;~Bernardo_Avila_Pires1;~Yunhao_Tang1;~Clare_Lyle1;~Mark_Rowland1;~Nicolas_Heess1;~Diana_L_Borsa1;~Arthur_Guez1;~Will_Dabney1",
        "gender": "F;M;M;M;;M;;;M;M",
        "homepage": "https://kkhetarpal.github.io/;;;https://robintyh1.github.io;;http://sites.google.com/view/markrowland;;;https://www.gatsby.ucl.ac.uk/~aguez/;",
        "dblp": "186/3048;160/9943;124/8971;210/2229;192/1910;86/4090;76/9181;164/6204;;https://dblp.uni-trier.de/pers/hd/d/Dabney:Will",
        "google_scholar": "https://scholar.google.ca/citations?user=VLOUhF0AAAAJ;fxr_9oQAAAAJ;WpAH4iUAAAAJ;;;https://scholar.google.co.uk/citations?user=-0U84zMAAAAJ;79k7bGEAAAAJ;;https://scholar.google.co.uk/citations?user=iyD9aw8AAAAJ;https://scholar.google.co.uk/citations?user=dR-7QW8AAAAJ",
        "orcid": ";;;;;;;;;",
        "linkedin": ";;;;;;;diana-l-borsa-12834023;;",
        "or_profile": "~Khimya_Khetarpal1;~Zhaohan_Daniel_Guo1;~Bernardo_Avila_Pires1;~Yunhao_Tang1;~Clare_Lyle1;~Mark_Rowland1;~Nicolas_Heess1;~Diana_L_Borsa1;~Arthur_Guez1;~Will_Dabney1",
        "aff": ";Google DeepMind;Google DeepMind;Google DeepMind;Google DeepMind;Google DeepMind;Google DeepMind;DeepMind/Google;Google DeepMind;Google DeepMind",
        "aff_domain": ";deepmind.com;google.com;deepmind.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "position": ";Research Scientist;Research Scientist;Research Scientist;Researcher;Research Scientist;Research Scientist;Research Scientist;Research Scientist;Research Scientist",
        "bibtex": "@inproceedings{\nkhetarpal2025a,\ntitle={A Unifying Framework for Action-Conditional Self-Predictive Reinforcement Learning},\nauthor={Khimya Khetarpal and Zhaohan Daniel Guo and Bernardo Avila Pires and Yunhao Tang and Clare Lyle and Mark Rowland and Nicolas Heess and Diana L Borsa and Arthur Guez and Will Dabney},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5DypCUsMg4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5DypCUsMg4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5SH1LH18uh",
        "title": "Task-Driven Discrete Representation Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In recent years, deep discrete representation learning (DRL) has achieved significant success across various domains. Most DRL frameworks (e.g., the widely used VQ-VAE and its variants) have primarily focused on generative settings, where the quality of a representation is implicitly gauged by the fidelity of its generation. In fact, the goodness of a discrete representation remain ambiguously defined across the literature. In this work, we adopt a practical approach that examines DRL from a task-driven perspective. We propose a unified framework that explores the usefulness of discrete features in relation to downstream tasks, with generation naturally viewed as one possible application. In this context, the properties of discrete representations as well as the way they benefit certain tasks are also relatively understudied. We therefore provide an additional theoretical analysis of the trade-off between representational capacity and sample complexity, shedding light on how discrete representation utilization impacts task performance. Finally, we demonstrate the flexibility and effectiveness of our framework across diverse applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Long Tung Vuong",
        "authorids": "~Long_Tung_Vuong1",
        "gender": "M",
        "homepage": "",
        "dblp": "329/6838",
        "google_scholar": "DCC657sAAAAJ",
        "orcid": "",
        "linkedin": "long-vuong-783477131/",
        "or_profile": "~Long_Tung_Vuong1",
        "aff": "Monash University",
        "aff_domain": "monash.edu",
        "position": "PhD student",
        "bibtex": "@inproceedings{\nvuong2025taskdriven,\ntitle={Task-Driven Discrete Representation Learning},\nauthor={Long Tung Vuong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5SH1LH18uh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5SH1LH18uh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5bm7TH6tWb",
        "title": "Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications. While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics. This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications. We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique. Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled. This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jarren Briscoe;Garrett Kepler;Daryl Robert DeFord;Assefaw Gebremedhin",
        "authorids": "~Jarren_Briscoe1;garrett.kepler@wsu.edu;~Daryl_Robert_DeFord1;~Assefaw_Gebremedhin1",
        "gender": ";;M;M",
        "homepage": ";;http://math.wsu.edu/faculty/ddeford;https://www.eecs.wsu.edu/~assefaw/",
        "dblp": ";;;",
        "google_scholar": ";;nE14CxwAAAAJ;",
        "orcid": "0000-0002-7422-9575;;;",
        "linkedin": ";;;",
        "or_profile": "~Jarren_Briscoe1;garrett.kepler@wsu.edu;~Daryl_Robert_DeFord1;~Assefaw_Gebremedhin1",
        "aff": "Washington State University;;;Washington State University, Pullman",
        "aff_domain": "wsu.edu;;;wsu.edu",
        "position": "PhD student;;;Associate Professor",
        "bibtex": "@inproceedings{\nbriscoe2025samplesizeinduced,\ntitle={Sample-Size-Induced Bias in Confusion-Matrix Metrics},\nauthor={Jarren Briscoe and Garrett Kepler and Daryl Robert DeFord and Assefaw Gebremedhin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5bm7TH6tWb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5bm7TH6tWb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5m0M2dZ40D",
        "title": "Learning Graph Node Embeddings by Smooth Pair Sampling",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Random walk based node embedding algorithms have attracted a lot of attention due to their scalability and ease of implementation. Previous research has focused on different walk strategies, optimization objectives, and embedding learning models.\nInspired by observations on real data, we take a different approach and propose a new regularization technique. More precisely, the frequencies of node pairs generated by the skip-gram model on random walk node sequences follow a highly skewed distribution which causes learning to be dominated by a fraction of the pairs. We address the issue by designing an efficient sampling procedure that generates node pairs according to their smoothed frequency. Theoretical and experimental results demonstrate the advantages of our approach.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Konstantin Kutzkov",
        "authorids": "~Konstantin_Kutzkov1",
        "gender": "M",
        "homepage": "https://konstantinkutzkov.github.io/",
        "dblp": "68/5508",
        "google_scholar": "T6p-52IAAAAJ",
        "orcid": "",
        "linkedin": "",
        "or_profile": "~Konstantin_Kutzkov1",
        "aff": "",
        "aff_domain": "",
        "position": "",
        "bibtex": "@inproceedings{\nkutzkov2025learning,\ntitle={Learning Graph Node Embeddings by Smooth Pair Sampling},\nauthor={Konstantin Kutzkov},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5m0M2dZ40D}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5m0M2dZ40D",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5oZC5NeWsI",
        "title": "Testing Conditional Independence with Deep Neural Network Based Binary Expansion Testing (DeepBET)",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper focuses on testing conditional independence between two random variables ($X$ and $Y$) given a set of high-dimensional confounding variables ($Z$). The high dimensionality of these confounding variables presents a challenge, often resulting in inflated type-I errors or insufficient power in many existing tests. To address this issue, we leverage the power of Deep Neural Networks (DNNs) to handle complex, high-dimensional data while mitigating the curse of dimensionality. We propose a novel test procedure, DeepBET. First, a DNN is used on part of the data to estimate the conditional means of $X$ and $Y$ given $Z$. Then, binary expansion testing (BET) are applied to the predicted errors from the remaining data. Additionally, we implement a multiple-split procedure to further enhance the power of the test. DeepBET is computationally efficient and robust to the tuning parameters in DNNs. Interestingly, the DeepBET statistic converges at a root-$n$ rate despite the nonparametric and high-dimensional nature of the confounding effects. Our numerical results demonstrate that the proposed method controls type-I error under various scenarios and enhances both power and interpretability for conditional dependence when present, making it a robust  alternative for testing conditional independence in high-dimensional settings. When applied to dry eye disease data, DeepBET reveals meaningful nonlinear relationships between the epithelial thickness and the tear production in the central region of eyes, given other regions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yang Yang;Kai Zhang;Ping-Shou Zhong",
        "authorids": "~Yang_Yang93;~Kai_Zhang2;~Ping-Shou_Zhong1",
        "gender": "F;M;M",
        "homepage": ";https://zhangk.web.unc.edu/;https://mscs.uic.edu/profiles/pszhong/",
        "dblp": ";;",
        "google_scholar": ";;regQUi4AAAAJ",
        "orcid": ";;",
        "linkedin": "yang-yang-uic2020/;;",
        "or_profile": "~Yang_Yang93;~Kai_Zhang2;~Ping-Shou_Zhong1",
        "aff": "University of Illinois at Chicago;;University of Illinois at Chicago",
        "aff_domain": "uic.edu;;uic.edu",
        "position": "PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nyang2025verifying,\ntitle={Verifying Conditional Independence with Deep Neural Network Based Binary Expansion Testing (Deep{BET})},\nauthor={Yang Yang and Ping-Shou Zhong and Kai Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5oZC5NeWsI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5oZC5NeWsI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5qYVbLA1lK",
        "title": "$q\\texttt{POTS}$: Efficient Batch Multiobjective Bayesian Optimization via Pareto Optimal Thompson Sampling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Classical evolutionary approaches for multiobjective optimization are quite accurate but incur a lot of queries to the objectives; this can be prohibitive when objectives are expensive oracles. A sample-efficient approach to solving multiobjective optimization is via Gaussian process (GP) surrogates and Bayesian optimization (BO). Multiobjective Bayesian optimization (MOBO) involves the construction of an acquisition function which is optimized to acquire new observation candidates sequentially. This ``inner'' optimization can be hard due to various reasons: acquisition functions being nonconvex, nondifferentiable and/or unavailable in analytical form; batch sampling usually exacerbates these problems and the success of MOBO heavily relies on this inner optimization. This, ultimately, affects their sample efficiency. To overcome these challenges, we propose a Thompson sampling (TS) based approach ($q\\texttt{POTS}$). Whereas TS chooses candidates according to the probability that they are optimal, $q\\texttt{POTS}$ chooses candidates according to the probability that they are Pareto optimal. Instead of a hard acquisition function optimization, $q\\texttt{POTS}$ solves a cheap multiobjective optimization on the GP posteriors with evolutionary approaches. This way we get the best of both worlds: accuracy of evolutionary approaches and sample-efficiency of MOBO. New candidates are chosen on the posterior GP Pareto frontier according to a maximin distance criterion. $q\\texttt{POTS}$ is endowed with theoretical guarantees, a natural exploration-exploitation trade-off, and superior empirical performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ashwin Renganathan;Kade Carlson",
        "authorids": "~Ashwin_Renganathan1;~Kade_Carlson1",
        "gender": ";",
        "homepage": "https://sites.google.com/view/sarenganathan;http://kade-carlson.com",
        "dblp": ";",
        "google_scholar": "bRzhctsAAAAJ;HVJId3UAAAAJ",
        "orcid": ";",
        "linkedin": ";https://linkedin.com/in/kade-carlson",
        "or_profile": "~Ashwin_Renganathan1;~Kade_Carlson1",
        "aff": "Pennsylvania State University;Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu",
        "position": "Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\nrenganathan2025qtextttpots,\ntitle={\\$q{\\textbackslash}texttt\\{{POTS}\\}\\$: Efficient Batch Multiobjective Bayesian Optimization via Pareto Optimal Thompson Sampling},\nauthor={Ashwin Renganathan and Kade Carlson},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5qYVbLA1lK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5qYVbLA1lK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5sELVG7ry3",
        "title": "Consistent Validation for Predictive Methods in Spatial Settings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Spatial prediction tasks are key to weather forecasting, studying air pollution impacts, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "David R. Burt;Yunyi Shen;Tamara Broderick",
        "authorids": "~David_R._Burt1;~Yunyi_Shen1;~Tamara_Broderick2",
        "gender": "M;;",
        "homepage": "https://davidrburt.github.io/;https://yunyishen.github.io;http://tamarabroderick.com/",
        "dblp": "238/1347;368/5101;40/7412",
        "google_scholar": "Kve55S4AAAAJ;;dPX0wQcAAAAJ",
        "orcid": ";;",
        "linkedin": ";;tamara-broderick-b20243139/",
        "or_profile": "~David_R._Burt1;~Yunyi_Shen1;~Tamara_Broderick2",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "position": "Postdoc;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nburt2025consistent,\ntitle={Consistent Validation for Predictive Methods in Spatial Settings},\nauthor={David R. Burt and Yunyi Shen and Tamara Broderick},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5sELVG7ry3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5sELVG7ry3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5wilN4aCW1",
        "title": "Models That Are Interpretable But Not Transparent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Faithful explanations are essential for machine learning models in high-stakes applications. Inherently interpretable models are well-suited for these applications because they naturally provide faithful explanations by revealing their decision logic. However, model designers often need to keep these models proprietary to maintain their value. This creates a tension: we need models that are interpretable\u2014allowing human decision-makers to understand and justify predictions, but not transparent, so that the model's decision boundary is not easily replicated by attackers. Shielding the model's decision boundary is particularly challenging alongside the requirement of completely faithful explanations, since such explanations reveal the true logic of the model for an entire subspace around each query point. This work provides an approach, FaithfulDefense, that creates model explanations for logical models that are completely faithful, yet reveal as little as possible about the decision boundary. FaithfulDefense is based on a maximum set cover formulation, and we provide multiple formulations for it, taking advantage of submodularity.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chudi Zhong;Panyu Chen;Cynthia Rudin",
        "authorids": "~Chudi_Zhong1;~Panyu_Chen1;~Cynthia_Rudin1",
        "gender": "F;;",
        "homepage": "https://chudizhong.github.io/;https://scholars.duke.edu/person/panyu.chen;",
        "dblp": "267/5474;;",
        "google_scholar": "DXKNTLIAAAAJ;;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Chudi_Zhong1;~Panyu_Chen1;~Cynthia_Rudin1",
        "aff": "University of North Carolina at Chapel Hill;Duke University;",
        "aff_domain": "unc.edu;duke.edu;",
        "position": "Assistant Professor;MS student;",
        "bibtex": "@inproceedings{\nzhong2025models,\ntitle={Models That Are Interpretable But Not Transparent},\nauthor={Chudi Zhong and Panyu Chen and Cynthia Rudin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=5wilN4aCW1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5wilN4aCW1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "698r7XuT2a",
        "title": "Harnessing the Power of Vicinity-Informed Analysis for Classification under Covariate Shift",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Transfer learning enhances prediction accuracy on a target distribution by leveraging data from a source distribution, demonstrating significant benefits in various applications. This paper introduces a novel dissimilarity measure that utilizes vicinity information, i.e., the local structure of data points, to analyze the excess error in classification under covariate shift, a transfer learning setting where marginal feature distributions differ but conditional label distributions remain the same. We characterize the excess error using the proposed measure and demonstrate faster or competitive convergence rates compared to previous techniques. Notably, our approach is effective in the support non-containment assumption, which often appears in real-world applications, holds. Our theoretical analysis bridges the gap between current theoretical findings and empirical observations in transfer learning, particularly in scenarios with significant differences between source and target distributions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mitsuhiro Fujikawa;Youhei Akimoto;Jun Sakuma;Kazuto Fukuchi",
        "authorids": "~Mitsuhiro_Fujikawa1;~Youhei_Akimoto1;~Jun_Sakuma1;~Kazuto_Fukuchi2",
        "gender": "M;;M;M",
        "homepage": ";;https://sites.google.com/view/junsakuma/english;https://kfukuchi.me/",
        "dblp": ";71/1035;43/5716.html;133/7753",
        "google_scholar": ";m7OXdsUAAAAJ;v5emswQAAAAJ;https://scholar.google.co.jp/citations?user=496_ICsAAAAJ",
        "orcid": ";0000-0003-2760-8123;;0000-0003-3895-219X",
        "linkedin": "%E5%85%89%E6%B5%A9-%E8%97%A4%E5%B7%9D-400375191/;;;",
        "or_profile": "~Mitsuhiro_Fujikawa1;~Youhei_Akimoto1;~Jun_Sakuma1;~Kazuto_Fukuchi2",
        "aff": ";RIKEN AIP+University of Tsukuba;Institute of Science Tokyo+RIKEN;University of Tsukuba+RIKEN",
        "aff_domain": ";riken.jp+tsukuba.ac.jp;titech.ac.jp+riken.jp;tsukuba.ac.jp+riken.jp",
        "position": ";Researcher+Associate Professor;Full Professor+Principal Researcher;Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\nfujikawa2025harnessing,\ntitle={Harnessing the Power of Vicinity-Informed Analysis for Classification under Covariate Shift},\nauthor={Mitsuhiro Fujikawa and Youhei Akimoto and Jun Sakuma and Kazuto Fukuchi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=698r7XuT2a}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=698r7XuT2a",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "69YorF3kr3",
        "title": "Get rid of your constraints and reparametrize: A study in NNLS and implicit bias",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Over the past years, there has been significant interest in understanding the implicit bias of gradient descent optimization and its connection to the generalization properties of overparametrized neural networks. Several works observed that when training linear diagonal networks on the square loss for regression tasks (which corresponds to overparametrized linear regression) gradient descent converges to special solutions, e.g., non-negative ones. We connect this observation to Riemannian optimization and view overparametrized GD with identical initialization as a Riemannian GD. We use this fact for solving non-negative least squares (NNLS), an important problem behind many techniques, e.g., non-negative matrix factorization. We show that gradient flow on the reparametrized objective converges globally to NNLS solutions, providing convergence rates also for its discretized counterpart. Unlike previous methods, we do not rely on the calculation of exponential maps or geodesics. We further show accelerated convergence using a second-order ODE, lending itself to accelerated descent methods. Finally, we establish the stability against negative perturbations and discuss generalization to other constrained optimization problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hung-Hsu Chou;Johannes Maly;Claudio Mayrink Verdun;Bernardo Freitas Paulo da Costa;Heudson Mirandola",
        "authorids": "~Hung-Hsu_Chou1;~Johannes_Maly1;~Claudio_Mayrink_Verdun1;~Bernardo_Freitas_Paulo_da_Costa1;~Heudson_Mirandola1",
        "gender": "M;M;M;;M",
        "homepage": "https://sites.google.com/nyu.edu/hung-hsuchou/home;https://johannes-maly.github.io/;;;http://lattes.cnpq.br/3030808069800164",
        "dblp": "267/5615;220/3056;;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;e5tABOYAAAAJ;lsOne4AAAAAJ;;XO5pakEAAAAJ",
        "orcid": "0000-0001-7023-4177;0000-0001-7134-2495;;;",
        "linkedin": ";;;;",
        "or_profile": "~Hung-Hsu_Chou1;~Johannes_Maly1;~Claudio_Mayrink_Verdun1;~Bernardo_Freitas_Paulo_da_Costa1;~Heudson_Mirandola1",
        "aff": "University of Pittsburgh;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;Harvard University;;Universidade Federal do Rio de Janeiro",
        "aff_domain": "pitt.edu;lmu.de;harvard.edu;;ufrj.br",
        "position": "Assistant Professor;Assistant Professor;Postdoc;;Associate Professor",
        "bibtex": "@inproceedings{\nchou2025get,\ntitle={Get rid of your constraints and reparametrize: A study in {NNLS} and implicit bias},\nauthor={Hung-Hsu Chou and Johannes Maly and Claudio Mayrink Verdun and Bernardo Freitas Paulo da Costa and Heudson Mirandola},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=69YorF3kr3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=69YorF3kr3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6GyX0YRw8P",
        "title": "Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Diffusion models may be formulated as a time-indexed sequence of energy-based models, where the score corresponds to the negative gradient of an energy function. As opposed to learning the score directly, an energy parameterization is attractive as the energy itself can be used to control generation via Monte Carlo samplers. Architectural constraints and training instability in energy parameterized models have so far yielded inferior performance compared to directly approximating the score or denoiser. We address these deficiencies by introducing a novel training regime for the energy function through distillation of pre-trained diffusion models, resembling a Helmholtz decomposition of the score vector field.\nWe further showcase the synergies between energy and score by casting the diffusion sampling procedure as a Feynman Kac Model where sampling is controlled using potentials from the learnt energy functions. \nThe Feynman Kac model formalism enables composition and low temperature sampling through sequential Monte Carlo.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "James Thornton;Louis B\u00e9thune;Ruixiang ZHANG;Arwen Bradley;Preetum Nakkiran;Shuangfei Zhai",
        "authorids": "~James_Thornton1;~Louis_B\u00e9thune1;~Ruixiang_ZHANG1;~Arwen_Bradley1;~Preetum_Nakkiran1;~Shuangfei_Zhai3",
        "gender": ";M;M;F;;M",
        "homepage": "https://jtt94.github.io/;https://louis-bethune.fr/;http://ruixiangz.me/;;http://preetum.nakkiran.org;http://cs.binghamton.edu/~szhai2",
        "dblp": ";270/0797;20/9860;278/8216;151/6343;",
        "google_scholar": "oFZHOwgAAAAJ;1zvpCDcAAAAJ;https://scholar.google.ca/citations?user=VQYdApgAAAAJ;cxi6phoAAAAJ;zithBbUAAAAJ;G6vdBYsAAAAJ",
        "orcid": ";0000-0003-1498-8251;;0000-0002-4086-217X;;",
        "linkedin": ";;;arwen-bradley-2084ba2b/;;",
        "or_profile": "~James_Thornton1;~Louis_B\u00e9thune1;~Ruixiang_ZHANG1;~Arwen_Bradley1;~Preetum_Nakkiran1;~Shuangfei_Zhai3",
        "aff": "Google DeepMind+Apple;Apple ;Mila, UdeM;Apple;Apple;Apple",
        "aff_domain": "google.com+apple.com;apple.com;mila.qubec;apple.com;apple.com;apple.com",
        "position": "Researcher+Researcher;Researcher;PhD student;Researcher;Principal Researcher;Research Scientist",
        "bibtex": "@inproceedings{\nthornton2025controlled,\ntitle={Controlled Generation with Distilled Diffusion Energy Models and Sequential Monte Carlo},\nauthor={James Thornton and Louis B{\\'e}thune and Ruixiang ZHANG and Arwen Bradley and Preetum Nakkiran and Shuangfei Zhai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6GyX0YRw8P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6GyX0YRw8P",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6HWDypPncm",
        "title": "Permutation Invariant Functions: Statistical Testing, Density Estimation, and Metric Entropy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Permutation invariance is among the most common symmetries that can be exploited to simplify complex problems in machine learning. There has been a tremendous surge of research activities in building permutation invariant machine learning architectures. However, less attention is given to: (1) how to statistically test for the assumption of permutation invariance of coordinates in a random vector where the dimension is allowed to grow with the sample size; (2) how to estimate permutation invariant density functions; (3) how much ``smaller'' is the class of smooth functions with permutation invariance compared to that without permutation invariance. In this paper, we take a step back and examine these fundamental questions. In particular, our testing method is based on a sorting trick, and our estimation method is based on an averaging trick. These tricks substantially simplify the exploitation of permutation invariance. We also analyze the metric entropy of permutation invariant function classes and compare them with their counterparts without imposing permutation invariance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wee Chaimanowong;Ying Zhu",
        "authorids": "~Wee_Chaimanowong1;~Ying_Zhu6",
        "gender": "M;Not Specified",
        "homepage": "https://sites.google.com/view/chaimanowongw;https://sites.google.com/view/ying-zhu",
        "dblp": ";",
        "google_scholar": "https://scholar.google.com.au/citations?user=wWlc2fUAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Wee_Chaimanowong1;~Ying_Zhu6",
        "aff": "The Chinese University of Hong Kong;University of California, San Diego",
        "aff_domain": "cuhk.edu.hk;ucsd.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nchaimanowong2025permutation,\ntitle={Permutation Invariant Multivariate Distribution Functions: Statistical Testing and Density Estimation},\nauthor={Wee Chaimanowong and Ying Zhu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6HWDypPncm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6HWDypPncm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6Za8OuaTtL",
        "title": "Planning and Learning in Risk-Aware Restless Multi-Arm Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In restless multi-arm bandits, a central agent is tasked with optimally distributing limited resources across several bandits (arms), with each arm being a Markov decision process. In this work, we generalize the traditional restless multi-arm bandit problem with a risk-neutral objective by incorporating risk-awareness. We establish indexability conditions for the case of a risk-aware objective and provide a solution based on Whittle index. In addition, we address the learning problem when the true transition probabilities are unknown by proposing a Thompson sampling approach and show that it achieves bounded regret that scales sublinearly with the number of episodes and quadratically with the number of arms. The efficacy of our method in reducing risk exposure in restless multi-arm bandits is illustrated through a set of numerical experiments in the contexts of machine replacement and patient scheduling applications under both planning and learning setups.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nima Akbarzadeh;Yossiri Adulyasak;Erick Delage",
        "authorids": "~Nima_Akbarzadeh1;~Yossiri_Adulyasak1;~Erick_Delage2",
        "gender": ";;M",
        "homepage": ";http://yossiri.info/;http://web.hec.ca/pages/erick.delage/",
        "dblp": "180/5631;139/1308;26/1546",
        "google_scholar": "https://scholar.google.com.tr/citations?user=ZVewNNQAAAAJ;https://scholar.google.ca/citations?user=Ru9Zco8AAAAJ;https://scholar.google.ca/citations?user=ciH2ROgAAAAJ",
        "orcid": ";;0000-0002-6740-3600",
        "linkedin": ";;erick-delage-2105361/?originalSubdomain=ca",
        "or_profile": "~Nima_Akbarzadeh1;~Yossiri_Adulyasak1;~Erick_Delage2",
        "aff": ";\u00c9cole des Hautes \u00c9tudes Commerciales+Department of Computer Science and Operations Research, Universit\u00e9 de Montr\u00e9al;HEC Montreal+Computer Science Department",
        "aff_domain": ";hec.ca+diro.umontreal.ca;hec.ca+cs.stanford.edu",
        "position": ";Associate Professor+Adjunct Professor;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nakbarzadeh2025planning,\ntitle={Planning and Learning in Risk-Aware Restless Multi-Arm Bandits},\nauthor={Nima Akbarzadeh and Yossiri Adulyasak and Erick Delage},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6Za8OuaTtL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6Za8OuaTtL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6gKgmdMlkj",
        "title": "Perfect Recovery for Random Geometric Graph Matching with Shallow Graph Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the graph matching problem in the presence of vertex feature information using shallow graph neural networks. Specifically, given two graphs that are independent perturbations of a single random geometric graph with sparse binary features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed two-layer graph neural network can, with high probability, recover the correct mapping between the vertices with the help of the graph structure. Additionally, we prove that our condition on the noise parameter is tight up to logarithmic factors. Finally, we compare the performance of the graph neural network to directly solving an assignment problem using the noisy vertex features and demonstrate that when the noise level is at least constant, this direct matching fails to achieve perfect recovery, whereas the graph neural network can tolerate noise levels growing as fast as a power of the size of the graph. Our theoretical findings are further supported by numerical studies as well as real-world data experiments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Suqi Liu;Morgane Austern",
        "authorids": "~Suqi_Liu1;~Morgane_Austern1",
        "gender": ";F",
        "homepage": ";https://sites.google.com/view/morganeaustern/home",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Suqi_Liu1;~Morgane_Austern1",
        "aff": ";Harvard University",
        "aff_domain": ";harvard.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025perfect,\ntitle={Perfect Recovery for Random Geometric Graph Matching with Shallow Graph Neural Networks},\nauthor={Suqi Liu and Morgane Austern},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6gKgmdMlkj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6gKgmdMlkj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6h1KG7f0yn",
        "title": "On the Inherent Privacy of Zeroth-Order Projected Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Differentially private zeroth-order optimization methods have recently gained popularity in private fine tuning of machine learning models due to their reduced memory requirements. Current approaches for privatizing zeroth-order methods rely on adding Gaussian noise to the estimated zeroth-order gradients. However, since the search direction in the zeroth-order methods is inherently random, researchers including Tang et al. (2024) and Zhang et al. (2024a) have raised an important question: is the inherent noise in zeroth-order estimators sufficient to ensure the overall differential privacy of the algorithm? This work settles this question for a class of oracle-based optimization algorithms where the oracle returns zeroth-order gradient estimates. In particular, we show that for a fixed initialization, there exist strongly convex objective functions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD) is not differentially private. Furthermore, we show that even with random initialization and without revealing intermediate iterates, the privacy loss in ZO-GD can grow superlinearly with the number of iterations when minimizing convex objective functions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Devansh Gupta;Meisam Razaviyayn;Vatsal Sharan",
        "authorids": "~Devansh_Gupta2;~Meisam_Razaviyayn1;~Vatsal_Sharan1",
        "gender": "M;M;M",
        "homepage": "https://devanshgupta160.github.io;https://sites.usc.edu/razaviyayn/;https://vatsalsharan.github.io/",
        "dblp": ";43/8577;126/2543",
        "google_scholar": "C0nqTjsAAAAJ;https://scholar.google.com/citations?hl=en;Ize17HEAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Devansh_Gupta2;~Meisam_Razaviyayn1;~Vatsal_Sharan1",
        "aff": "University of Southern California;University of Southern California+Google;University of Southern California",
        "aff_domain": "usc.edu;usc.edu+google.com;usc.edu",
        "position": "PhD student;Associate Professor+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\ngupta2025on,\ntitle={On the Inherent Privacy of Zeroth-Order Projected Gradient Descent},\nauthor={Devansh Gupta and Meisam Razaviyayn and Vatsal Sharan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6h1KG7f0yn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6h1KG7f0yn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6nmRoDYVpY",
        "title": "Relating Piecewise Linear Kolmogorov Arnold Networks to ReLU Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Kolmogorov-Arnold Networks are a new family of neural network architectures which holds promise for overcoming the curse of dimensionality and has interpretability benefits (Liu et al., 2024). In this paper, we explore the connection between Kolmogorov Arnold Networks (KANs) with piecewise linear (univariate real) functions and ReLU networks. We provide completely explicit constructions to convert a piecewise linear KAN into a ReLU network and vice versa.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nandi Schoots;Mattia Jacopo Villani;Niels uit de Bos",
        "authorids": "~Nandi_Schoots1;~Mattia_Jacopo_Villani1;~Niels_uit_de_Bos1",
        "gender": "F;M;M",
        "homepage": "https://safeandtrustedai.org/person/nandi-schoots/;https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiyt_OS28f9AhUCTEEAHYkrBTQQFnoECAQQAQ&url=https%3A%2F%2Fuk.linkedin.com%2Fin%2Fmattiajacopovillani&usg=AOvVaw1BZpdC5zOmUb8fss3EAcV-;",
        "dblp": "285/0412;;",
        "google_scholar": ";V_AC090AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;",
        "linkedin": ";https://www.linkedin.com/mattiajacopovillani;",
        "or_profile": "~Nandi_Schoots1;~Mattia_Jacopo_Villani1;~Niels_uit_de_Bos1",
        "aff": "University of Oxford;J.P. Morgan Chase+King's College London, University of London;ML Alignment & Theory Scholars",
        "aff_domain": "ox.ac.uk;jpmchase.com+kcl.ac.uk;serimats.org",
        "position": "Postdoc;Researcher+PhD student;Researcher",
        "bibtex": "@inproceedings{\nschoots2025relating,\ntitle={Relating Piecewise Linear Kolmogorov Arnold Networks to Re{LU} Networks},\nauthor={Nandi Schoots and Mattia Jacopo Villani and Niels uit de Bos},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6nmRoDYVpY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6nmRoDYVpY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "6tZxEVlpnL",
        "title": "SINE: Scalable MPE Inference for Probabilistic Graphical Models using Advanced Neural Embeddings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Our paper builds on the recent trend of using neural networks trained with self-supervised or supervised learning to solve the Most Probable Explanation (MPE) task in discrete graphical models. At inference time, these networks take an evidence assignment as input and generate the most likely assignment for the remaining variables via a single forward pass. We address two key limitations of existing approaches: (1) the inability to fully exploit the graphical model's structure and parameters, and (2) the suboptimal discretization of continuous neural network outputs. Our approach embeds model structure and parameters into a more expressive feature representation, significantly improving  performance. Existing methods rely on standard thresholding, which often yields suboptimal results due to the non-convexity of the loss function. We introduce two methods to overcome discretization challenges: (1) an external oracle-based approach that infers uncertain variables using additional evidence from confidently predicted ones, and (2) a technique that identifies and selects the highest-scoring discrete solutions near the continuous output. Experimental results on various probabilistic models demonstrate the effectiveness and scalability of our approach, highlighting its practical impact.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shivvrat Arya;Tahrima Rahman;Vibhav Giridhar Gogate",
        "authorids": "~Shivvrat_Arya1;~Tahrima_Rahman1;~Vibhav_Giridhar_Gogate1",
        "gender": "M;F;",
        "homepage": "https://shivvrat.github.io;http://www.utdallas.edu/~txr110830/;",
        "dblp": "275/7819;150/2674;",
        "google_scholar": "eM1co-kAAAAJ;VDBfmocAAAAJ;",
        "orcid": "0000-0002-9727-2533;;",
        "linkedin": "shivvrat/;;",
        "or_profile": "~Shivvrat_Arya1;~Tahrima_Rahman1;~Vibhav_Giridhar_Gogate1",
        "aff": "New Jersey Institute of Technology+The University of Texas at Dallas;University of Texas, Dallas;",
        "aff_domain": "njit.edu+cs.utdallas.edu;utdallas.edu;",
        "position": "Assistant Professor+PhD;Research Scientist;",
        "bibtex": "@inproceedings{\narya2025sine,\ntitle={{SINE}: Scalable {MPE} Inference for Probabilistic Graphical Models using Advanced Neural Embeddings},\nauthor={Shivvrat Arya and Tahrima Rahman and Vibhav Giridhar Gogate},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=6tZxEVlpnL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6tZxEVlpnL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "724vEMy8vq",
        "title": "Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "For regression tasks, standard Gaussian processes (GPs) provide natural uncertainty quantification (UQ), while deep neural networks (DNNs) excel at representation learning. Deterministic UQ methods for neural networks have successfully combined the two and require only a single pass through the neural network. However, current methods necessitate changes to network training to address feature collapse, where unique inputs map to identical feature vectors. We propose an alternative solution, the deep Vecchia ensemble (DVE), which allows deterministic UQ to work in the presence of feature collapse, negating the need for network retraining. DVE comprises an ensemble of GPs built on hidden-layer outputs of a DNN, achieving scalability via Vecchia approximations that leverage nearest-neighbor conditional independence. DVE is compatible with pretrained networks and incurs low computational overhead. We demonstrate DVE's utility on several datasets and carry out experiments to understand the inner workings of the proposed method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Felix Jimenez;Matthias Katzfuss",
        "authorids": "~Felix_Jimenez1;~Matthias_Katzfuss1",
        "gender": "M;",
        "homepage": "https://felix-jimenez.com/;",
        "dblp": "131/8598-2;",
        "google_scholar": "UHY-7YkAAAAJ;",
        "orcid": "0000-0002-2241-711X;",
        "linkedin": ";",
        "or_profile": "~Felix_Jimenez1;~Matthias_Katzfuss1",
        "aff": "Amazon+University of Wisconsin - Madison;",
        "aff_domain": "amazon.com+wisc.edu;",
        "position": "Researcher+PhD student;",
        "bibtex": "@inproceedings{\njimenez2025vecchia,\ntitle={Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks},\nauthor={Felix Jimenez and Matthias Katzfuss},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=724vEMy8vq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=724vEMy8vq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "76DLXLgWwP",
        "title": "Q-function Decomposition with Intervention Semantics for Factored Action Spaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many practical reinforcement learning environments have a discrete factored action space that induces a large combinatorial set of actions, thereby posing significant challenges.  Existing approaches leverage the regular structure of the action space and resort to a linear decomposition of Q-functions, which avoids enumerating all combinations of factored actions. \nIn this paper,  we consider Q-functions defined over a lower dimensional projected subspace of the original action space, and study the condition for the unbiasedness of decomposed Q-functions using causal effect estimation from the no unobserved confounder setting in causal statistics.  This leads to a general scheme which we call action decomposed reinforcement learning that uses the projected Q-functions to approximate the Q-function in standard model-free reinforcement learning algorithms. The proposed approach is shown to improve sample complexity in a model-based reinforcement learning setting. We demonstrate improvements in sample efficiency compared to state-of-the-art baselines in online continuous control environments and a real-world offline sepsis treatment environment.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junkyu Lee;Tian Gao;Elliot Nelson;Miao Liu;Debarun Bhattacharjya;Songtao Lu",
        "authorids": "~Junkyu_Lee1;~Tian_Gao1;~Elliot_Nelson1;~Miao_Liu1;~Debarun_Bhattacharjya1;~Songtao_Lu1",
        "gender": ";;;M;M;M",
        "homepage": "https://www.linkedin.com/in/junkyul/;https://sites.google.com/view/tiangao/home;;https://sites.google.com/view/miaoliuhome;https://researcher.watson.ibm.com/researcher/view.php?person=us-debarunb;https://songtaogithub.github.io/",
        "dblp": "65/6241-1;;323/9332;;98/5604;05/2887",
        "google_scholar": "kigtlXEAAAAJ;5rweipAAAAAJ;YBvdBOkAAAAJ;7QHvAEYAAAAJ;pwfVt-MAAAAJ;LRsjX7kAAAAJ",
        "orcid": "0000-0002-6636-2886;0000-0002-0337-6682;;;;",
        "linkedin": "junkyul/;;elliot-nelson-18295377/;miao-liu-3273a32b;;",
        "or_profile": "~Junkyu_Lee1;~Tian_Gao1;~Elliot_Nelson1;~Miao_Liu1;~Debarun_Bhattacharjya1;~Songtao_Lu1",
        "aff": "International Business Machines;International Business Machines+Rensselaer Polytechnic Institute;;International Business Machines;International Business Machines;Department of Computer Science and Engineering, The Chinese University of Hong Kong",
        "aff_domain": "ibm.com;ibm.com+rpi.edu;;ibm.com;ibm.com;cse.cuhk.edu.hk",
        "position": "Researcher;Reseach Staff Member+PhD student;;Research Staff Member;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025qfunction,\ntitle={Q-function Decomposition with Intervention Semantics for Factored Action Spaces},\nauthor={Junkyu Lee and Tian Gao and Elliot Nelson and Miao Liu and Debarun Bhattacharjya and Songtao Lu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=76DLXLgWwP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=76DLXLgWwP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "78or9GLOFX",
        "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (Preference data MIA). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qizhang Feng;Siva Rajesh Kasa;SANTHOSH KUMAR KASA;Hyokun Yun;Choon Hui Teo;Sravan Babu Bodapati",
        "authorids": "~Qizhang_Feng1;~Siva_Rajesh_Kasa1;~SANTHOSH_KUMAR_KASA1;~Hyokun_Yun1;~Choon_Hui_Teo1;~Sravan_Babu_Bodapati1",
        "gender": "M;;M;M;;M",
        "homepage": ";;https://github.com/santhosh-kasa;http://bikestra.github.io/;;",
        "dblp": "323/5667.html;258/2739;;45/9671;58/3494;233/6338",
        "google_scholar": ";qCs5b2IAAAAJ;;W4oOmZEAAAAJ;h8W0ppcAAAAJ;UjxTzEsAAAAJ",
        "orcid": "0000-0002-2574-0270;;;;;",
        "linkedin": "qizhang-feng-355478197/;;;hyokun-yun-b4439b7/;;justsravan/",
        "or_profile": "~Qizhang_Feng1;~Siva_Rajesh_Kasa1;~SANTHOSH_KUMAR_KASA1;~Hyokun_Yun1;~Choon_Hui_Teo1;~Sravan_Babu_Bodapati1",
        "aff": "Texas A&M;Amazon;;Amazon;Amazon;Amazon",
        "aff_domain": "tamu.edu;amazon.com;;amazon.com;amazon.com;amazon.com",
        "position": "PhD student;Researcher;;Machine Learning Scientist;Applied Scientist;Principal Researcher",
        "bibtex": "@inproceedings{\nfeng2025exposing,\ntitle={Exposing Privacy Gaps: Membership Inference Attack on Preference Data for {LLM} Alignment},\nauthor={Qizhang Feng and Siva Rajesh Kasa and SANTHOSH KUMAR KASA and Hyokun Yun and Choon Hui Teo and Sravan Babu Bodapati},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=78or9GLOFX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=78or9GLOFX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7QZ65OECUt",
        "title": "Cross-Modal Imputation and Uncertainty Estimation for Spatial Transcriptomics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "High-resolution spatial transcriptomics (ST) technologies can capture gene expression at the cellular level along with spatial information, but are limited in the number of genes that can be profiled.\nIn contrast, single-cell RNA sequencing (SC) provides more comprehensive gene expression profiles but lacks spatial context.\nTo bridge these gaps, existing methods typically focus on single-modality prediction tasks, leveraging complementary information from the other modality. \nHere, we propose an attention-based cross-modal framework that simultaneously imputes gene expression for ST and recovers spatial locations for SC, while also providing uncertainty estimates for the expression of the imputed genes.\nOur approach was evaluated on three real-world datasets, where it consistently outperformed state-of-the-art methods in spatial gene profile imputation. \nMoreover, our framework enhances latent embedding integration between the two modalities, resulting in more accurate spatial position estimates.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiangyu Guo;Ricardo Henao",
        "authorids": "~Xiangyu_Guo2;~Ricardo_Henao1",
        "gender": "M;M",
        "homepage": ";http://rhenaog.github.io",
        "dblp": ";27/3207",
        "google_scholar": "aBsOZdAAAAAJ;p_mm4-YAAAAJ",
        "orcid": ";0000-0003-4980-845X",
        "linkedin": ";",
        "or_profile": "~Xiangyu_Guo2;~Ricardo_Henao1",
        "aff": "Duke University;Duke University",
        "aff_domain": "duke.edu;duke.edu",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nguo2025crossmodal,\ntitle={Cross-modal imputation and uncertainty estimation for spatial transcriptomics},\nauthor={Xiangyu Guo and Ricardo Henao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7QZ65OECUt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7QZ65OECUt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7TEw9dp10c",
        "title": "Evaluating Prediction-based Interventions with Human Decision Makers In Mind",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Automated decision systems (ADS) are broadly deployed to inform or support human decision- making across a wide range of consequential contexts. However, various context-specific details complicate the goal of establishing meaningful experimental evaluations for prediction-based interventions. Notably, specific experimental design decisions may induce cognitive biases in human decision makers, which could then significantly alter the observed effect sizes of the prediction intervention. In this paper, we formalize and investigate various models of human decision-making in the presence of a predictive model aid. We show that each of these behavioral models produces dependencies across decision subjects and results in the violation of existing assumptions, with consequences for treatment effect estimation. This work aims to further advance the scientific validity of intervention-based evaluation schemes for the assessment of ADS deployments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Inioluwa Deborah Raji;Lydia T. Liu",
        "authorids": "~Inioluwa_Deborah_Raji1;~Lydia_T._Liu2",
        "gender": "F;",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": "https://scholar.google.com/citations?hl=en;",
        "orcid": ";",
        "linkedin": "deborah-raji-065751b2/;",
        "or_profile": "~Inioluwa_Deborah_Raji1;~Lydia_T._Liu2",
        "aff": "University of California, Berkeley;",
        "aff_domain": "berkeley.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nraji2025designing,\ntitle={Designing Experimental Evaluations  of Algorithmic Interventions  with Human Decision Makers In Mind},\nauthor={Inioluwa Deborah Raji and Lydia T. Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7TEw9dp10c}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7TEw9dp10c",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7Xc2asGD0p",
        "title": "Differentially Private Range Queries with Correlated Input Perturbation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work proposes a class of differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our theoretical and empirical analysis demonstrates that we achieve near-optimal utility, effectively compete with other methods, and retain all the favorable statistical properties discussed earlier.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Prathamesh Dharangutte;Jie Gao;Ruobin Gong;Guanyang Wang",
        "authorids": "~Prathamesh_Dharangutte1;~Jie_Gao6;~Ruobin_Gong1;~Guanyang_Wang2",
        "gender": "M;;F;M",
        "homepage": "https://prathameshtd.github.io/;https://sites.rutgers.edu/jie-gao/;https://ruobingong.github.io/;https://guanyangwang.github.io",
        "dblp": "267/9631;g/JieGao;192/2810;224/0093.html",
        "google_scholar": "WnkSzHUAAAAJ;P1CMmgEAAAAJ;8W4eh2UAAAAJ;",
        "orcid": ";0000-0001-5083-6082;0000-0003-2965-9266;",
        "linkedin": ";;;",
        "or_profile": "~Prathamesh_Dharangutte1;~Jie_Gao6;~Ruobin_Gong1;~Guanyang_Wang2",
        "aff": "Rutgers University;Rutgers University;Rutgers University;Rutgers University",
        "aff_domain": "rutgers.edu;rutgers.edu;rutgers.edu;rutgers.edu",
        "position": "PhD student;Full Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ndharangutte2025differentially,\ntitle={Differentially Private Range Queries with Correlated Input Perturbation},\nauthor={Prathamesh Dharangutte and Jie Gao and Ruobin Gong and Guanyang Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7Xc2asGD0p}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7Xc2asGD0p",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7gausJk3jB",
        "title": "Parabolic Continual Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Regularizing continual learning techniques is important for anticipating algorithmic behavior under new realizations of data. \nWe introduce a new approach to continual learning by imposing the properties of a parabolic partial differential equation (PDE) to regularize the expected behavior of the loss over time.\nThis class of parabolic PDEs has a number of favorable properties that allow us to analyze the error incurred through forgetting and the error induced through generalization.\nSpecifically, we do this through imposing boundary conditions where the boundary is given by a memory buffer.\nBy using the memory buffer as a boundary, we can enforce long term dependencies by bounding the expected error by the boundary loss.\nFinally, we illustrate the empirical performance of the method on a series of continual learning tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haoming Yang;Ali Hasan;Vahid Tarokh",
        "authorids": "~Haoming_Yang1;~Ali_Hasan1;~Vahid_Tarokh1",
        "gender": "M;;",
        "homepage": "https://imkeithyang.github.io;https://alluly.github.io;",
        "dblp": ";200/8502.html;",
        "google_scholar": "uz7goREAAAAJ;4De_LnYAAAAJ;",
        "orcid": ";;",
        "linkedin": "haoming-yang-2a8a9612b/;;",
        "or_profile": "~Haoming_Yang1;~Ali_Hasan1;~Vahid_Tarokh1",
        "aff": "Duke University;Morgan Stanley;",
        "aff_domain": "duke.edu;morganstanley.com;",
        "position": "PhD student;Researcher;",
        "bibtex": "@inproceedings{\nyang2025parabolic,\ntitle={Parabolic Continual Learning},\nauthor={Haoming Yang and Ali Hasan and Vahid Tarokh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7gausJk3jB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7gausJk3jB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7lccoXFMVh",
        "title": "The Sample Complexity of Stackelberg Games",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Stackelberg games (SGs) constitute the most fundamental and acclaimed models of strategic interactions involving some form of commitment. Moreover, they form the basis of more elaborate models of this kind, such as, e.g., Bayesian persuasion and principal-agent problems. Addressing learning tasks in SGs and related models is crucial to operationalize them in practice, where model parameters are usually unknown. In this paper, we revise the sample complexity of learning an optimal strategy to commit to in SGs. We provide a novel algorithm that (i) does not require any of the limiting assumptions made by state-of-the-art approaches and (ii) deals with a trade-off between sample complexity and termination probability arising when leader\u2019s strategies representation has finite precision. Such a trade-off has been completely neglected by existing algorithms and, if not properly managed, it may result in them using exponentially-many samples. Our algorithm requires novel techniques, which also pave the way to addressing learning problems in other models with commitment ubiquitous in the real world.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Francesco Bacchiocchi;Matteo Bollini;Matteo Castiglioni;Alberto Marchesi;Nicola Gatti",
        "authorids": "~Francesco_Bacchiocchi1;~Matteo_Bollini1;~Matteo_Castiglioni1;~Alberto_Marchesi1;~Nicola_Gatti1",
        "gender": "M;M;;M;M",
        "homepage": ";;https://castiglionimatteo.github.io;https://albymarke.github.io;https://www4.ceda.polimi.it/manifesti/manifesti/controller/ricerche/RicercaPerDocentiPublic.do?k_doc=75785&lang=EN&EVN_PRODOTTI=evento&__pj0=0&__pj1=d918ee8916afbd0005f5c0bc3c0ff350",
        "dblp": "312/4794.html;377/9397;225/7720;204/1718;g/NicolaGatti",
        "google_scholar": "https://scholar.google.com.vn/citations?user=UKGWeAoAAAAJ;;https://scholar.google.it/citations?user=NPE3HAYAAAAJ;vXDtCzoAAAAJ;https://scholar.google.com.tw/citations?user=j-HrYREAAAAJ",
        "orcid": ";;0000-0002-1070-6766;;0000-0001-7349-3932",
        "linkedin": ";matteo-bollini-4542072b2/;;;nicola-gatti-1284b21",
        "or_profile": "~Francesco_Bacchiocchi1;~Matteo_Bollini1;~Matteo_Castiglioni1;~Alberto_Marchesi1;~Nicola_Gatti1",
        "aff": "Polytechnic Institute of Milan;Polytechnic Institute of Milan;Politecnico di Milano;Politecnico di Milano;Polytechnic Institute of Milan",
        "aff_domain": "polimi.it;polimi.it;polimi.it;polimi.it;polimi.it",
        "position": "PhD student;PhD student;Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nbacchiocchi2025the,\ntitle={The Sample Complexity of Stackelberg Games},\nauthor={Francesco Bacchiocchi and Matteo Bollini and Matteo Castiglioni and Alberto Marchesi and Nicola Gatti},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7lccoXFMVh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7lccoXFMVh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7lqwDUNjdG",
        "title": "The cost of local and global fairness in Federated Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "With the emerging application of Federated Learning (FL) in finance, hiring and healthcare, FL models are regulated to be fair, preventing disparities with respect to legally protected attributes such as race or gender.  Two concepts of fairness are important in FL: global and local fairness.  Global fairness addresses the disparity across the entire population and local fairness is concerned with the disparity within each client.  Prior fair FL frameworks have improved either global or local fairness without considering both.  Furthermore, while the majority of studies on fair FL focuses on binary settings, many real-world applications are multi-class problems. This paper proposes a framework that investigates  the minimum accuracy lost for enforcing a specified level of global and local fairness in  multi-class FL settings. Our framework leads to a simple post-processing algorithm that derives fair outcome predictors from the Bayesian optimal score functions. Experimental results show that our algorithm outperforms the current state of the art (SOTA) with regard to the accuracy-fairness tradoffs, computational and communication costs. Codes  are available at: https://github.com/papersubmission678/The-cost-of-local-and-global-fairness-in-FL.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuying Duan;Gelei Xu;Yiyu Shi;Michael Lemmon",
        "authorids": "~Yuying_Duan1;~Gelei_Xu1;~Yiyu_Shi1;~Michael_Lemmon1",
        "gender": "F;F;M;M",
        "homepage": ";https://gracellgg.github.io/;;https://www3.nd.edu/~lemmon",
        "dblp": ";;94/5536;09/6355",
        "google_scholar": ";;;",
        "orcid": ";;;",
        "linkedin": "yuying-duan-45b13821b/;;;",
        "or_profile": "~Yuying_Duan1;~Gelei_Xu1;~Yiyu_Shi1;~Michael_Lemmon1",
        "aff": "University of Notre Dame;University of Notre Dame;University of Notre Dame;University of Notre Dame",
        "aff_domain": "nd.edu;nd.edu;nd.edu;nd.edu",
        "position": "PhD student;PhD student;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nduan2025the,\ntitle={The cost of local and global fairness in Federated Learning},\nauthor={Yuying Duan and Gelei Xu and Yiyu Shi and Michael Lemmon},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7lqwDUNjdG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7lqwDUNjdG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7vhymFCZM5",
        "title": "Tamed Langevin sampling under weaker conditions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity. In terms of structure, we only assume that the target distribution satisfies either a Log-Sobolev  or a Poincare inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity. This set of assumptions greatly exceeds the operational limits of the \"vanilla\" ULA, making sampling from such distributions a highly involved affair. To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the KL divergence, total variation, and Wasserstein distance to the target distribution.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Iosif Lytras;Panayotis Mertikopoulos",
        "authorids": "~Iosif_Lytras1;~Panayotis_Mertikopoulos1",
        "gender": "M;M",
        "homepage": "https://scholar.google.com/citations?user=qi5_6vUAAAAJ&hl=en&oi=ao;http://polaris.imag.fr/panayotis.mertikopoulos/",
        "dblp": ";49/6721",
        "google_scholar": "qi5_6vUAAAAJ;xsusqPYAAAAJ",
        "orcid": ";0000-0003-2026-9616",
        "linkedin": ";",
        "or_profile": "~Iosif_Lytras1;~Panayotis_Mertikopoulos1",
        "aff": "IMIS - \"Athena\" Research Center;French National Center for Scientific Research",
        "aff_domain": "imis.athena-innovation.gr;imag.fr",
        "position": "Postdoc;Principal Researcher",
        "bibtex": "@inproceedings{\nlytras2025tamed,\ntitle={Tamed Langevin sampling under weaker conditions},\nauthor={Iosif Lytras and Panayotis Mertikopoulos},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7vhymFCZM5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7vhymFCZM5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7voMr77XSs",
        "title": "Differentiable Calibration of Inexact Stochastic Simulation Models via Kernel Score Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Stochastic simulation models are generative models that mimic complex systems to help with decision-making. The reliability of these models heavily depends on well-calibrated input model parameters. However, in many practical scenarios, only output-level data are available to learn the input model parameters, which is challenging due to the often intractable likelihood of the stochastic simulation model. Moreover, stochastic simulation models are frequently inexact, with discrepancies between the model and the target system. No existing methods can effectively learn and quantify the uncertainties of input parameters using only output-level data. In this paper, we propose to learn differentiable input parameters of stochastic simulation models using output-level data via kernel score minimization with stochastic gradient descent. We quantify the uncertainties of the learned input parameters using a frequentist confidence set procedure based on a new asymptotic normality result that accounts for model inexactness. The proposed method is evaluated on exact and inexact G/G/1 queueing models as well as a stochastic volatility model.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziwei Su;Diego Klabjan",
        "authorids": "~Ziwei_Su1;~Diego_Klabjan1",
        "gender": "M;M",
        "homepage": "https://ziweisu.github.io/;http://dynresmanagement.com/index.html",
        "dblp": ";17/105",
        "google_scholar": "S6CIuycAAAAJ;TaQZ_VUAAAAJ",
        "orcid": ";0000-0003-4213-9281",
        "linkedin": "https://www.linkedin.com/mwlite/in/ziwei-su-502a58201;diegoklabjan",
        "or_profile": "~Ziwei_Su1;~Diego_Klabjan1",
        "aff": "Northwestern University;Northwestern University",
        "aff_domain": "northwestern.edu;u.northwestern.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nsu2025differentiable,\ntitle={Differentiable Calibration of Inexact Stochastic Simulation Models via Kernel Score Minimization},\nauthor={Ziwei Su and Diego Klabjan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7voMr77XSs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7voMr77XSs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7xXqB7G8qA",
        "title": "Black-Box Uniform Stability for Non-Euclidean Empirical Risk Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study first-order algorithms that are uniformly stable for empirical risk minimization (ERM) problems that are convex and smooth with respect to $p$-norms, $p \\geq 1$. We propose a black-box reduction method that, by  employing properties of uniformly convex regularizers, turns an optimization algorithm for H\u00f6lder smooth convex losses into a uniformly stable learning algorithm with optimal statistical risk bounds on the excess risk, up to a constant factor depending on $p$. Achieving a black-box reduction for uniform stability was posed as an open question by Attia and Koren (2022), which had solved the Euclidean case $p=2$. We explore applications that leverage non-Euclidean geometry in addressing binary classification problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Simon Vary;David Mart\u00ednez-Rubio;Patrick Rebeschini",
        "authorids": "~Simon_Vary1;~David_Mart\u00ednez-Rubio2;~Patrick_Rebeschini1",
        "gender": "M;;M",
        "homepage": "https://simonvary.github.io;;http://www.stats.ox.ac.uk/~rebeschi/",
        "dblp": "230/4630;;164/7439",
        "google_scholar": "V6OqU-cAAAAJ;;",
        "orcid": ";;0000-0001-7772-4160",
        "linkedin": ";;patrick-rebeschini/",
        "or_profile": "~Simon_Vary1;~David_Mart\u00ednez-Rubio2;~Patrick_Rebeschini1",
        "aff": "University of Oxford;;University of Oxford",
        "aff_domain": "stats.ox.ac.uk;;oxford.ac.uk",
        "position": "Postdoc;;Full Professor",
        "bibtex": "@inproceedings{\nvary2025blackbox,\ntitle={Black-Box Uniform Stability for Non-Euclidean Empirical Risk Minimization},\nauthor={Simon Vary and David Mart{\\'\\i}nez-Rubio and Patrick Rebeschini},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7xXqB7G8qA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7xXqB7G8qA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7yTQehHDYa",
        "title": "A Differential Inclusion Approach for Learning Heterogeneous Sparsity in Neuroimaging Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In voxel-based neuroimaging disease prediction, it was recently found that in addition to lesion features, there exists another type of feature called \"Procedural Bias\", which is introduced during preprocessing and can further improve the prediction power. However, traditional sparse learning methods fail to simultaneously capture both types of features due to their heterogeneity in sparsity types. Specifically, the lesion features are spatially coherent and suffer from volumetric degeneration, while the procedural bias refers to enlarged voxels that are dispersedly distributed. In this paper, we propose a new method based on differential inclusion, which generates a sparse regularized solution path on multiple parameters that are enforced with heterogeneous sparsity to capture lesion features and the procedural bias separately. Specifically, we employ Total Variation with a non-negative constraint for the parameter associated with degenerated and spatially coherent lesions; on the other hand, we impose $\\ell_1$ sparsity with a non-positive constraint on the parameter related to enlarged and scatterly distributed procedural bias. We theoretically show that our method enjoys model selection consistency and $\\ell_2$ consistency in estimation. The utility of our method is demonstrated by improved prediction power and interpretability in the early prediction of Alzheimer's Disease.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenjing Han;Yueming Wu;Xinwei Sun;Lingjing Hu;Yizhou Wang",
        "authorids": "~Wenjing_Han4;~Yueming_Wu2;~Xinwei_Sun1;~Lingjing_Hu1;~Yizhou_Wang1",
        "gender": "F;;M;F;M",
        "homepage": "https://daisy-hwj.github.io/;;https://sunxinwei0625.github.io/sunxw.github.io/;https://www.researchgate.net/profile/Lingjing_Hu;https://cfcs.pku.edu.cn/wangyizhou/",
        "dblp": ";;145/6592-1;;71/3387-1",
        "google_scholar": ";;;;831z_VcAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";https://www.linkedin.cn/incareer/in/ACoAAENYahcBPD5D6CvfDh4EmcBxhZpZyxVnoAs;;;",
        "or_profile": "~Wenjing_Han4;~Yueming_Wu2;~Xinwei_Sun1;~Lingjing_Hu1;~Yizhou_Wang1",
        "aff": "Communication University of China+Capital Medical University;Institute of Medical Technology, Peking University Health Science Center;  Aerospace Center Hospital, Peking University Aerospace Clinical College;Fudan University;Medical Imaging Technoloy Department, Capital Medical University, China;Peking University",
        "aff_domain": "cuc.edu.cn+ccmu.edu.cn;bjmu.edu.cn;fudan.edu.cn;ccmu.edu.cn;pku.edu.cn",
        "position": "PhD student+Lecturer;MS student;Assistant Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nhan2025a,\ntitle={A Differential Inclusion Approach for Learning Heterogeneous Sparsity in Neuroimaging Analysis},\nauthor={Wenjing Han and Yueming Wu and Xinwei Sun and Lingjing Hu and Yizhou Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7yTQehHDYa}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7yTQehHDYa",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7zYfr9bDUg",
        "title": "Distributional Adversarial Loss",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We initiate the study of a new notion of adversarial loss which we call distributional adversarial loss. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions, and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss. We show sample complexity bounds in the PAC-learning setting for our notion of adversarial loss. Our notion of adversarial loss contrasts the prior work on robust learning that considers a set of points, not distributions, as the perturbation set of each clean example. As an application of our approach, we show how to unify the two lines of work on randomized smoothing and robust learning in the PAC-learning setting and derive sample complexity bounds for randomized smoothing methods.\n\nFurthermore, we investigate the role of randomness in achieving robustness against adversarial attacks. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks and show its effectiveness empirically.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Saba Ahmadi;Siddharth Bhandari;Avrim Blum;Chen Dan;Prabhav Jain",
        "authorids": "~Saba_Ahmadi1;~Siddharth_Bhandari1;~Avrim_Blum1;~Chen_Dan1;~Prabhav_Jain1",
        "gender": "F;;M;M;M",
        "homepage": "https://sabaahmadi.github.io/;https://sites.google.com/view/siddharth-bhandari;https://home.ttic.edu/~avrim/;https://chendancmu.github.io/;",
        "dblp": "200/9551;215/5219.html;b/AvrimBlum;156/6710;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;s3isPhsAAAAJ;https://scholar.google.com.tw/citations?user=Jlv4MR4AAAAJ;hQQFfuwAAAAJ;",
        "orcid": ";0000-0003-3481-6078;;;",
        "linkedin": ";;;;prabhav-jain-89a04420a/",
        "or_profile": "~Saba_Ahmadi1;~Siddharth_Bhandari1;~Avrim_Blum1;~Chen_Dan1;~Prabhav_Jain1",
        "aff": "Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;Toyota Technological Institute at Chicago;",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu;ttic.edu;",
        "position": "Postdoc;Researcher;Full Professor;Postdoc;",
        "bibtex": "@inproceedings{\nahmadi2025distributional,\ntitle={Distributional Adversarial Loss},\nauthor={Saba Ahmadi and Siddharth Bhandari and Avrim Blum and Chen Dan and Prabhav Jain},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=7zYfr9bDUg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7zYfr9bDUg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8E3DXJGZG7",
        "title": "A Generalized Theory of Mixup for Structure-Preserving Synthetic Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mixup is a widely adopted data augmentation technique known for enhancing the generalization of machine learning models by interpolating between data points. Despite its success and popularity, limited attention has been given to understanding the statistical properties of the synthetic data it generates. In this paper, we delve into the theoretical underpinnings of mixup, specifically its effects on the statistical structure of synthesized data. We demonstrate that while mixup improves model performance, it can distort key statistical properties such as variance, potentially leading to unintended consequences in data synthesis. To address this, we propose a novel mixup method that incorporates a generalized and flexible weighting scheme, better preserving the original data\u2019s structure. Through theoretical developments, we provide conditions under which our proposed method maintains the (co)variance and distributional properties of the original dataset. Numerical experiments confirm that the new approach not only preserves the statistical characteristics of the original data but also sustains model performance across repeated synthesis, alleviating concerns of model collapse identified in previous research.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chungpa Lee;Jongho Im;Joseph H.T. Kim",
        "authorids": "~Chungpa_Lee1;~Jongho_Im1;~Joseph_H.T._Kim1",
        "gender": "M;Not Specified;M",
        "homepage": "https://www.chungpa.com/;https://sites.google.com/yonsei.ac.kr/yd3sl/home;",
        "dblp": ";;",
        "google_scholar": "N2S3jFcAAAAJ;kYdt1kwAAAAJ;",
        "orcid": "0009-0009-9915-3173;;0000-0002-9902-8235",
        "linkedin": ";;",
        "or_profile": "~Chungpa_Lee1;~Jongho_Im1;~Joseph_H.T._Kim1",
        "aff": "Yonsei University;Yonsei University;",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;",
        "position": "PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\nlee2025a,\ntitle={A Generalized Theory of Mixup for Structure-Preserving Synthetic Data},\nauthor={Chungpa Lee and Jongho Im and Joseph H.T. Kim},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8E3DXJGZG7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8E3DXJGZG7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8GJmWyhD4b",
        "title": "Conditional Prediction ROC Bands for Graph Classification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graph classification in medical imaging and drug discovery requires accuracy and robust uncertainty quantification. To address this need, we introduce Conditional Prediction ROC (CP-ROC) bands, offering uncertainty quantification for ROC curves and robustness to distributional shifts in test data. Although developed for Tensorized Graph Neural Networks (TGNNs), CP-ROC is adaptable to general Graph Neural Networks (GNNs) and other machine learning models. We establish statistically guaranteed coverage for CP-ROC under a local exchangeability condition. This addresses uncertainty challenges for ROC curves under non-iid setting, ensuring reliability when test graph distributions differ from training data. Empirically, to establish local exchangeability for TGNNs, we introduce a data-driven approach to construct local calibration sets for graphs. Comprehensive evaluations show that CP-ROC significantly improves prediction reliability across diverse tasks. This method enhances uncertainty quantification efficiency and reliability for ROC curves, proving valuable for real-world applications with non-iid objects.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yujia Wu;Bo Yang;Elynn Chen;Yuzhou Chen;Zheshi Zheng",
        "authorids": "~Yujia_Wu2;~Bo_Yang28;~Elynn_Chen1;~Yuzhou_Chen1;~Zheshi_Zheng1",
        "gender": "M;M;F;;F",
        "homepage": "https://www.linkedin.com/in/yujia-wu-290823289;;https://elynncc.github.io/;;https://public.websites.umich.edu/~songlab/people.html",
        "dblp": ";;;;",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;;",
        "orcid": ";;;;",
        "linkedin": "yujia-wu-290823289;megumiybb;;;",
        "or_profile": "~Yujia_Wu2;~Bo_Yang28;~Elynn_Chen1;~Yuzhou_Chen1;~Zheshi_Zheng1",
        "aff": "New York University;University of Michigan - Ann Arbor;New York University;;University of Michigan - Ann Arbor",
        "aff_domain": "nyu.edu;umich.edu;nyu.edu;;umich.edu",
        "position": "MS student;PhD student;Assistant Professor;;Postdoc",
        "bibtex": "@inproceedings{\nwu2025conditional,\ntitle={Conditional Prediction {ROC} Bands for Graph Classification},\nauthor={Yujia Wu and Bo Yang and Elynn Chen and Yuzhou Chen and Zheshi Zheng},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8GJmWyhD4b}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8GJmWyhD4b",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8JwuE2HhY8",
        "title": "Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples.\nSupervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the \u201cNode-Edge approach\u201d, in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to concurrently and independently predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class (MEC) theory, both the skeleton and the v-structures are *identifiable* causal structures under the canonical MEC setting, so predictions about skeleton and v-structures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiaru Zhang;Rui Ding;Qiang Fu;Huang Bojun;zizhen Deng;Yang Hua;Haibing Guan;Shi Han;Dongmei Zhang",
        "authorids": "~Jiaru_Zhang1;~Rui_Ding1;~Qiang_Fu7;~Huang_Bojun1;~zizhen_Deng1;~Yang_Hua2;~Haibing_Guan1;~Shi_Han1;~Dongmei_Zhang2",
        "gender": "M;M;M;M;M;M;M;M;",
        "homepage": "https://jiaruzhang.github.io;https://www.microsoft.com/en-us/research/people/juding/;;;https://zizhendeng.github.io/;https://pure.qub.ac.uk/en/persons/yang-hua;http://www.cs.sjtu.edu.cn/~hbguan/;https://www.microsoft.com/en-us/research/people/shihan/;https://www.microsoft.com/en-us/research/people/dongmeiz/",
        "dblp": "300/5670;55/5564-1;;54/9376;;;96/5680.html;23/3395;87/461-1",
        "google_scholar": "d6q4zkMAAAAJ;ggVQ4dIAAAAJ;bwTLZSIAAAAJ;ljKDN0QAAAAJ;;N0tFi8MAAAAJ;;wLabxmYAAAAJ;jLlBBl4AAAAJ",
        "orcid": "0000-0002-9273-9093;0000-0003-3990-7403;0000-0002-5821-7267;;;0000-0001-5536-503X;;0000-0002-0360-6089;0000-0002-9230-2799",
        "linkedin": ";;qiang-fu-08301285/;;;;;shi-han-86888526/;dongmei-zhang-38a86317/",
        "or_profile": "~Jiaru_Zhang1;~Rui_Ding1;~Qiang_Fu7;~Huang_Bojun1;~zizhen_Deng1;~Yang_Hua2;~Haibing_Guan1;~Shi_Han1;~Dongmei_Zhang2",
        "aff": "Purdue University;Microsoft;Microsoft;Rakuten Institute of Technology;;Queen's University Belfast;Shanghai Jiaotong University;Microsoft;Microsoft",
        "aff_domain": "purdue.edu;microsoft.com;microsoft.com;rakuten.com;;qub.ac.uk;sjtu.edu.cn;microsoft.com;microsoft.com",
        "position": "Postdoc;Researcher;Researcher;Principal Researcher;;Assistant Professor;Full Professor;Researcher;Distinguished Scientist",
        "bibtex": "@inproceedings{\nzhang2025learning,\ntitle={Learning Identifiable Structures Avoids Bias in {DNN}-based Supervised Causal Learning},\nauthor={Jiaru Zhang and Rui Ding and Qiang Fu and Huang Bojun and zizhen Deng and Yang Hua and Haibing Guan and Shi Han and Dongmei Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8JwuE2HhY8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8JwuE2HhY8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8LRZ62HWp3",
        "title": "M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A probabilistic graphical model is proposed, modeling the joint model parameter and multiplier evolution, with a hypervolume based likelihood, promoting multi-objective descent in structural risk minimization. \nWe address multi-objective model parameter optimization via a surrogate single objective penalty loss with time-varying multipliers, equivalent to online scheduling of loss landscape. \nThe multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems with shrinking bounds according to Pareto dominance. The bound serves as setpoint for the low-level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method forms closed loop of model parameter dynamic, circumvents excessive memory requirements and extra computational burden of existing multi-objective deep learning methods, and is robust against controller hyperparameter variation, demonstrated on domain generalization tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xudong Sun;Nutan Chen;Alexej Gossmann;Yu Xing;Matteo Wohlrapp;Emilio Dorigatti;Carla Feistner;Felix Drost;Daniele Scarcella;Lisa Helen Beer;Carsten Marr",
        "authorids": "~Xudong_Sun1;~Nutan_Chen2;~Alexej_Gossmann1;~Yu_Xing3;~Matteo_Wohlrapp1;~Emilio_Dorigatti1;~Carla_Feistner1;~Felix_Drost1;~Daniele_Scarcella1;~Lisa_Helen_Beer1;~Carsten_Marr1",
        "gender": ";;;;M;;F;M;M;;M",
        "homepage": "https://smilesun.github.io/;;;;;;https://github.com/Car-la-F;;;https://www.linkedin.com/in/lisa-beer-422182227/;https://www.helmholtz-muenchen.de/icb/research/groups/marr-lab/overview/index.html",
        "dblp": "61/9265-2;;;;;;;;;;",
        "google_scholar": "https://scholar.google.de/citations?user=FnWKUqYAAAAJ;;;MZRqhuAAAAAJ;QMM-viYAAAAJ;;;ghlxCH4AAAAJ;;;https://scholar.google.de/citations?user=Wg9zjqEAAAAJ",
        "orcid": "0000-0001-9234-4932;;;;0009-0008-7537-0096;;;;;;0000-0003-2154-4552",
        "linkedin": ";;;;matteo-wohlrapp-989b87190/;;;felixdrost/;daniele-scarcella-467925158/;;",
        "or_profile": "~Xudong_Sun1;~Nutan_Chen2;~Alexej_Gossmann1;~Yu_Xing3;~Matteo_Wohlrapp1;~Emilio_Dorigatti1;~Carla_Feistner1;~Felix_Drost1;~Daniele_Scarcella1;~Lisa_Helen_Beer1;~Carsten_Marr1",
        "aff": "Helmholtz Munich;;;;Technische Universit\u00e4t M\u00fcnchen;;Friedrich-Alexander Universit\u00e4t Erlangen-N\u00fcrnberg;;Helmholtz Munich ;;Helmholtz Munich",
        "aff_domain": "helmholtz-munich.de;;;;tum.de;;fau.de;;helmholtz.munich.de;;helmholtz-muenchen.de",
        "position": "Postdoc;;;;MS student;;PhD student;;PhD student;;Principal Researcher",
        "bibtex": "@inproceedings{\nsun2025multiobjective,\ntitle={Multi-objective Hierarchical Feedback Optimization of Penalty Multiplier for Domain Invariant Auto-encoding},\nauthor={Xudong Sun and Nutan Chen and Matteo Wohlrapp and Alexej Gossmann and Yu Xing and Emilio Dorigatti and Carla Feistner and Felix Drost and Daniele Scarcella and Lisa Helen Beer and Carsten Marr},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8LRZ62HWp3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8LRZ62HWp3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8Nerj9ziRZ",
        "title": "Beyond Discretization: Learning the Optimal Solution Path",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many applications require minimizing a family of optimization problems indexed by some hyperparameter $\\lambda \\in \\Lambda$ to obtain an entire solution path. Traditional approaches proceed by discretizing $\\Lambda$ and solving a series of optimization problems. We propose an alternative approach that parameterizes the solution path with a set of basis functions and solves a \\emph{single} stochastic optimization problem to learn the entire solution path.  Our method offers substantial complexity improvements over discretization. When using constant-step size SGD, the uniform error of our learned solution path relative to the true path exhibits linear convergence to a constant related to the expressiveness of the basis. When the true solution path lies in the span of the basis, this constant is zero. We also prove stronger results for special cases common in machine learning: When $\\lambda \\in [-1, 1]$ and the solution path is $\\nu$-times differentiable, constant step-size SGD learns a path with $\\epsilon$ uniform error after at most $O(\\epsilon^{\\frac{1}{1-\\nu}} \\log(1/\\epsilon))$ iterations, and when the solution path is analytic, it only requires $O\\left(\\log^2(1/\\epsilon)\\log\\log(1/\\epsilon)\\right)$.  By comparison, the best-known discretization schemes in these settings require at least $O(\\epsilon^{-1/2})$ discretization points (and even more gradient calls). Finally, we propose an adaptive variant of our method that sequentially adds basis functions and demonstrate strong numerical performance through experiments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qiran Dong;Paul Grigas;Vishal Gupta",
        "authorids": "~Qiran_Dong1;~Paul_Grigas1;~Vishal_Gupta1",
        "gender": ";M;M",
        "homepage": "https://dongqiran1995.wixsite.com/homepage;https://grigas.ieor.berkeley.edu/;https://faculty.marshall.usc.edu/Vishal-Gupta/",
        "dblp": ";132/9308;66/6170-4",
        "google_scholar": ";Kw1eyxAAAAAJ;KZcnoikAAAAJ",
        "orcid": ";;0000-0003-4371-9114",
        "linkedin": ";;",
        "or_profile": "~Qiran_Dong1;~Paul_Grigas1;~Vishal_Gupta1",
        "aff": "University of California, Berkeley;University of California, Berkeley;University of Southern California",
        "aff_domain": "berkeley.edu;berkeley.edu;usc.edu",
        "position": "PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\ndong2025beyond,\ntitle={Beyond Discretization: Learning the Optimal Solution Path},\nauthor={Qiran Dong and Paul Grigas and Vishal Gupta},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8Nerj9ziRZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8Nerj9ziRZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8RkuhtbH0H",
        "title": "Steering No-Regret Agents in MFGs under Model Uncertainty",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics towards desired outcomes by providing additional payments beyond intrinsic rewards. However, most existing works focus on a finite, small set of agents or assume complete knowledge of the game, limiting their applicability to real-world scenarios involving large populations and model uncertainty. To address this gap, we study the design of steering rewards in Mean-Field Games (MFGs) with density-independent transitions, where both the transition dynamics and intrinsic reward functions are unknown. This setting presents non-trivial challenges, as the mediator must incentivize the agents to explore for its model learning under uncertainty, while simultaneously steer them to converge to desired behaviors without incurring excessive incentive payments. Assuming agents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic exploration algorithms. Theoretically, we establish sub-linear regret guarantees for the cumulative gaps between the agents' behaviors and the desired ones. In terms of the steering cost, we demonstrate that our total incentive payments incur only sub-linear excess, competing with a baseline steering strategy that stabilizes the target policy as an equilibrium. Our work presents an effective framework for steering agents behaviors in large-population systems under uncertainty.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Leo Widmer;Jiawei Huang;Niao He",
        "authorids": "~Leo_Widmer1;~Jiawei_Huang3;~Niao_He3",
        "gender": ";;",
        "homepage": "https://github.com/sowieichheisse;https://jiaweihhuang.github.io;",
        "dblp": ";13/4208;",
        "google_scholar": ";6IcfJiIAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Leo_Widmer1;~Jiawei_Huang3;~Niao_He3",
        "aff": ";Department of Computer Science, ETHZ - ETH Zurich;",
        "aff_domain": ";inf.ethz.ch;",
        "position": ";PhD student;",
        "bibtex": "@inproceedings{\nwidmer2025steering,\ntitle={Steering No-Regret Agents in {MFG}s under Model Uncertainty},\nauthor={Leo Widmer and Jiawei Huang and Niao He},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8RkuhtbH0H}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8RkuhtbH0H",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8W42uW8Eki",
        "title": "Analysis of Two-Stage Rollout Designs with Clustering for Causal Inference under Network Interference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Estimating causal effects under interference is pertinent to many real-world settings. Recent work with low-order potential outcomes models uses a rollout design to obtain unbiased estimators that require no interference network information. However, the required extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experiment that selects a sub-population in the first stage and restricts treatment rollout to this sub-population in the second stage. \nWe explore the role of clustering in the first stage by analyzing the bias and variance of a polynomial interpolation-style estimator under this experimental design. Bias increases with the number of edges cut in the clustering of the interference network, but variance depends on qualities of the clustering that relate to homophily and covariate balance. There is a tension between clustering objectives that minimize the number of cut edges versus those that maximize covariate balance across clusters. \nThrough simulations, we explore {a bias-variance} trade-off and compare the performance of the estimator under different clustering strategies.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mayleen Cortez-Rodriguez;Matthew Eichhorn;Christina Yu",
        "authorids": "~Mayleen_Cortez-Rodriguez1;~Matthew_Eichhorn2;~Christina_Yu1",
        "gender": ";M;",
        "homepage": ";https://maeichho.github.io/;https://cleeyu.orie.cornell.edu/",
        "dblp": ";;246/4764",
        "google_scholar": ";13yqSuIAAAAJ;GyzNZMcAAAAJ",
        "orcid": ";0009-0001-3841-8686;",
        "linkedin": ";;",
        "or_profile": "~Mayleen_Cortez-Rodriguez1;~Matthew_Eichhorn2;~Christina_Yu1",
        "aff": ";Cornell University;Amazon+Cornell University",
        "aff_domain": ";cornell.edu;amazon.com+cornell.edu",
        "position": ";Lecturer;Amazon Visiting Academic+Assistant Professor",
        "bibtex": "@inproceedings{\ncortez-rodriguez2025analysis,\ntitle={Analysis of Two-Stage Rollout Designs with Clustering for Causal Inference under Network Interference},\nauthor={Mayleen Cortez-Rodriguez and Christina Yu and Matthew Eichhorn},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8W42uW8Eki}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8W42uW8Eki",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8YFRJdr3CS",
        "title": "Energy-consistent Neural Operators for Hamiltonian and Dissipative Partial Differential Equations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the functional derivative is calculated making full use of automatic differentiation, allowing one to bias the outputs of the DNN-based solution operators to obey appropriate energetic behavior without explicit PDEs. Experiments on multiple systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yusuke Tanaka;Takaharu Yaguchi;Tomoharu Iwata;Naonori Ueda",
        "authorids": "~Yusuke_Tanaka1;~Takaharu_Yaguchi1;~Tomoharu_Iwata1;~Naonori_Ueda1",
        "gender": "M;M;M;M",
        "homepage": "https://sites.google.com/view/yusuketanaka/english;http://www.math.kobe-u.ac.jp/HOME/yaguchi/indexe.htm;http://www.kecl.ntt.co.jp/as/members/iwata/;https://www.kecl.ntt.co.jp/as/members/ueda/index-e.html",
        "dblp": "34/2327-2;40/8408;29/5953;87/2491.html",
        "google_scholar": "https://scholar.google.co.jp/citations?user=leMnxA4AAAAJ;5pYoTyYAAAAJ;S1F-gScAAAAJ;lelCr80AAAAJ",
        "orcid": "0000-0002-7316-1425;0000-0001-9025-6015;;0000-0001-5701-9333",
        "linkedin": ";;tomoharu-iwata-025a493;",
        "or_profile": "~Yusuke_Tanaka1;~Takaharu_Yaguchi1;~Tomoharu_Iwata1;~Naonori_Ueda1",
        "aff": "NTT;Kobe University;NTT;RIKEN AIP+NTT Communication Science Laboratories",
        "aff_domain": "ntt.com;kobe-u.ac.jp;hco.ntt.co.jp;riken.jp+hco.ntt.co.jp",
        "position": "Researcher;Full Professor;Researcher;Principal Researcher+Researcher",
        "bibtex": "@inproceedings{\ntanaka2025energyconsistent,\ntitle={Energy-consistent Neural Operators for Hamiltonian and Dissipative Partial Differential Equations},\nauthor={Yusuke Tanaka and Takaharu Yaguchi and Tomoharu Iwata and Naonori Ueda},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8YFRJdr3CS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8YFRJdr3CS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8e9XlbzHbx",
        "title": "Data-Driven Upper Confidence Bounds with Near-Optimal Regret for Heavy-Tailed Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Stochastic multi-armed bandits (MABs) provide a fundamental reinforcement learning model to study sequential decision making in uncertain environments. The upper confidence bounds (UCB) algorithm gave birth to the renaissance of bandit algorithms, as it achieves near-optimal regret rates under various moment assumptions. Up until recently most UCB methods relied on concentration inequalities leading to confidence bounds which depend on moment parameters, such as the variance proxy, that are usually unknown in practice. In this paper, we propose a new distribution-free, data-driven UCB algorithm for symmetric reward distributions, which needs no moment information. The key idea is to combine a refined, one-sided version of the recently developed resampled median-of-means (RMM) method with UCB. We prove a near-optimal regret bound for the proposed anytime, parameter-free RMM-UCB method, even for heavy-tailed distributions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ambrus Tam\u00e1s;Szabolcs Szentp\u00e9teri;Bal\u00e1zs Cs\u00e1ji",
        "authorids": "~Ambrus_Tam\u00e1s1;~Szabolcs_Szentp\u00e9teri1;~Bal\u00e1zs_Cs\u00e1ji1",
        "gender": "M;M;",
        "homepage": "https://sztaki.hun-ren.hu/tamas-ambrus;;",
        "dblp": ";;",
        "google_scholar": "n1X0870AAAAJ;https://scholar.google.hu/citations?user=z4Z3K3sAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Ambrus_Tam\u00e1s1;~Szabolcs_Szentp\u00e9teri1;~Bal\u00e1zs_Cs\u00e1ji1",
        "aff": ";SZTAKI;",
        "aff_domain": ";sztaki.hun-ren.hu;",
        "position": ";PhD student;",
        "bibtex": "@inproceedings{\ntamas2025datadriven,\ntitle={Data-Driven Upper Confidence Bounds with Near-Optimal Regret for Heavy-Tailed Bandits},\nauthor={Ambrus Tam{\\'a}s and Szabolcs Szentp{\\'e}teri and Bal{\\'a}zs Cs{\\'a}ji},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8e9XlbzHbx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8e9XlbzHbx",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8iHAMw3DQM",
        "title": "Bilevel Reinforcement Learning via the Development of Hyper-gradient without Lower-Level Convexity",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bilevel reinforcement learning (RL), which features intertwined two-level problems, has attracted growing interest recently. The inherent non-convexity of the lower-level RL problem is, however, to be an impediment to developing bilevel optimization methods. By employing the fixed point equation associated with the regularized RL, we characterize the hyper-gradient via fully first-order information, thus circumventing the assumption of lower-level convexity. This, remarkably, distinguishes our development of hyper-gradient from the general AID-based bilevel frameworks since we take advantage of the specific structure of RL problems. Moreover, we design both model-based and model-free bilevel reinforcement learning algorithms, facilitated by access to the fully first-order hyper-gradient. Both algorithms enjoy the convergence rate $\\mathcal{O}\\left(\\epsilon^{-1}\\right)$. To extend the applicability, a stochastic version of the model-free algorithm is proposed, along with results on its convergence rate and sampling complexity. In addition, numerical experiments demonstrate that the hyper-gradient indeed serves as an integration of exploitation and exploration.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yan Yang;Bin Gao;Ya-xiang Yuan",
        "authorids": "~Yan_Yang9;~Bin_Gao6;~Ya-xiang_Yuan1",
        "gender": "M;M;",
        "homepage": "https://github.com/UCAS-YanYang?tab=repositories;https://www.gaobin.cc/;http://lsec.cc.ac.cn/~yyx",
        "dblp": ";181/2330-7;",
        "google_scholar": ";Q9uKXacAAAAJ;",
        "orcid": ";0000-0001-5290-4675;",
        "linkedin": ";;",
        "or_profile": "~Yan_Yang9;~Bin_Gao6;~Ya-xiang_Yuan1",
        "aff": "Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences;Academy of Mathematics and Systems Science;Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences",
        "aff_domain": "amss.ac.cn;lsec.cc.ac.cn;amss.ac.cn",
        "position": "PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nyang2025bilevel,\ntitle={Bilevel Reinforcement Learning via the Development of Hyper-gradient without Lower-Level Convexity},\nauthor={Yan Yang and Bin Gao and Ya-xiang Yuan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8iHAMw3DQM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8iHAMw3DQM",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8irxkKQZUD",
        "title": "Adversarial Training in High-Dimensional Regression: Generated Data and Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In recent years, studies such as Carmon et al., (2019); Gowal et al., (2021)  demonstrate that incorporating additional real or generated data with pseudo-labels can enhance adversarial training through a two-stage training framework. In this paper, we perform a theoretical analysis of the asymptotic behavior of this method in high-dimensional regression problem when using two-layer neural networks. We first derive the asymptotics of the two-stage training framework using linear regression as a preliminary. Then, we analyze the convergence of two-layer neural networks in the two-stage framework. The analysis considers two different regimes: in the first stage of the framework, it is a high-dimensional regime, and in the second stage, the sample size is much larger than the data dimension. To analyze adversarial training, we track the change of the adversarial attack, and reveal that training with two-layer neural networks gives a prediction performance similar to training a linear model with some particular $\\mathcal{L}_2$ regularization corresponding to different regimes. To highlight our technical contribution, we are the first to investigate adversarial training in two-layer neural networks under moderate attack strength, which is different from most existing literature in vanishing attack strength.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yue Xing",
        "authorids": "~Yue_Xing1",
        "gender": "",
        "homepage": "https://sites.google.com/site/xingyuecuhk/",
        "dblp": "185/5744-2.html",
        "google_scholar": "",
        "orcid": "",
        "linkedin": "",
        "or_profile": "~Yue_Xing1",
        "aff": "Michigan State University",
        "aff_domain": "msu.edu",
        "position": "Assistant Professor",
        "bibtex": "@inproceedings{\nxing2025adversarial,\ntitle={Adversarial Training in High-Dimensional Regression: Generated Data and Neural Networks},\nauthor={Yue Xing},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8irxkKQZUD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8irxkKQZUD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8rVkjNajF1",
        "title": "Poisoning Bayesian Inference via Data Deletion and Replication",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Research in adversarial machine learning (AML) has shown that statistical models are vulnerable to maliciously altered data. However, despite advances in Bayesian machine learning models, most AML research remains concentrated on classical techniques. Therefore, we focus on extending the white-box model poisoning paradigm to attack generic Bayesian inference, highlighting its vulnerability in adversarial contexts. A suite of attacks are developed that allow an attacker to steer the Bayesian posterior toward a target distribution through the strategic deletion and replication of true observations, even when only sampling access to the posterior is available.\nAnalytic properties of these algorithms are proven and their performance is empirically examined in both synthetic and real-world scenarios. With relatively little effort, the attacker is able to substantively alter the Bayesian's beliefs and, by accepting more risk, they can mold these beliefs to their will. By carefully constructing the adversarial posterior, surgical poisoning is achieved such that only targeted inferences are corrupted and others are minimally disturbed.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Matthieu Carreau;Roi Naveiro;William N. Caballero",
        "authorids": "matthieu.carreau@telecom-paris.fr;~Roi_Naveiro1;william.caballero@afit.edu",
        "gender": ";M;",
        "homepage": ";https://roinaveiro.github.io/;",
        "dblp": ";227/3049;",
        "google_scholar": ";77tPQfEAAAAJ;",
        "orcid": ";0000-0001-9032-2465;",
        "linkedin": ";;",
        "or_profile": "matthieu.carreau@telecom-paris.fr;~Roi_Naveiro1;william.caballero@afit.edu",
        "aff": ";CUNEF Universidad;",
        "aff_domain": ";cunef.edu;",
        "position": ";Assistant Professor;",
        "bibtex": "@inproceedings{\ncarreau2025poisoning,\ntitle={Poisoning Bayesian Inference via Data Deletion and Replication},\nauthor={Matthieu Carreau and Roi Naveiro and William N. Caballero},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=8rVkjNajF1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8rVkjNajF1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "98VGlF621z",
        "title": "Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "When dealing with right-censored data, where some outcomes are missing due to a limited observation period, survival analysis \u2014known as *time-to-event analysis*\u2014 focuses on predicting the time until an event of interest occurs. Multiple classes of outcomes lead to a classification variant: predicting the most likely event, a less explored area known as *competing risks*. Classic competing risks models couple architecture and loss, limiting scalability. \n\nTo address these issues, we design a strictly proper censoring-adjusted separable scoring rule, allowing optimization on a subset of the data as each observation is evaluated independently. The loss estimates outcome probabilities and enables stochastic optimization for competing risks, which we use for efficient gradient boosting trees. **SurvivalBoost** not only outperforms 12 state-of-the-art models across several metrics on 4 real-life datasets, both in competing risks and survival settings, but also provides great calibration, the ability to predict across any time horizon, and computation times faster than existing methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Julie Alberge;Vincent Maladiere;Olivier Grisel;Judith Ab\u00e9cassis;Gael Varoquaux",
        "authorids": "~Julie_Alberge1;~Vincent_Maladiere1;~Olivier_Grisel1;~Judith_Ab\u00e9cassis1;~Gael_Varoquaux1",
        "gender": "F;;M;F;M",
        "homepage": ";https://vincent-maladiere.github.io/;http://ogrisel.com;https://judithabk6.github.io/;http://gael-varoquaux.info",
        "dblp": ";;86/10824;;36/7585",
        "google_scholar": ";;duoYY64AAAAJ;https://scholar.google.fr/citations?user=M055Oo8AAAAJ;https://scholar.google.fr/citations?user=OGGu384AAAAJ",
        "orcid": ";;;0000-0002-5818-8304;",
        "linkedin": "julie-alberge/;;oliviergrisel/;judith-abecassis-0760a26b/;",
        "or_profile": "~Julie_Alberge1;~Vincent_Maladiere1;~Olivier_Grisel1;~Judith_Ab\u00e9cassis1;~Gael_Varoquaux1",
        "aff": "INRIA;;Inria;INRIA;INRIA",
        "aff_domain": "inria.fr;;inria.fr;inria.fr;inria.fr",
        "position": "PhD student;;Research Engineer;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nalberge2025survival,\ntitle={Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks},\nauthor={Julie Alberge and Vincent Maladiere and Olivier Grisel and Judith Ab{\\'e}cassis and Gael Varoquaux},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=98VGlF621z}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=98VGlF621z",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "99oYWUq3yx",
        "title": "Causal Temporal Regime Structure Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding causal relationships in multivariate time series is essential for predicting and controlling dynamic systems in fields like economics, neuroscience, and climate science. However, existing causal discovery methods often assume stationarity, limiting their effectiveness when time series consist of sequential regimes, consecutive temporal segments with unknown boundaries and changing causal structures. In this work, we firstly introduce a framework to describe and model such time series. Then, we present CASTOR, a novel method that concurrently learns the Directed Acyclic Graph (DAG) for each regime while determining the number of regimes and their sequential arrangement. CASTOR optimizes the data log-likelihood using an expectation-maximization algorithm, alternating between assigning regime indices (expectation step) and inferring causal relationships in each regime (maximization step). We establish the identifiability of the regimes and DAGs within our framework. Extensive experiments show that CASTOR consistently outperforms existing causal discovery models in detecting different regimes and learning their DAGs across various settings, including linear and nonlinear causal relationships, on both synthetic and real world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abdellah Rahmani;Pascal Frossard",
        "authorids": "~Abdellah_Rahmani1;~Pascal_Frossard1",
        "gender": "M;",
        "homepage": "https://people.epfl.ch/abdellah.rahmani;",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Abdellah_Rahmani1;~Pascal_Frossard1",
        "aff": "EPFL - EPF Lausanne;",
        "aff_domain": "epfl.ch;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nrahmani2025causal,\ntitle={Causal Temporal Regime Structure Learning},\nauthor={Abdellah Rahmani and Pascal Frossard},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=99oYWUq3yx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=99oYWUq3yx",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9L0Qz7d4Pb",
        "title": "Quantifying the Optimization and Generalization Advantages of Graph Neural Networks Over Multilayer Perceptrons",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graph neural networks (GNNs) have demonstrated remarkable capabilities in learning from graph-structured data, often outperforming traditional Multilayer Perceptrons (MLPs) in numerous graph-based tasks. Although existing works have demonstrated the benefits of graph convolution through Laplacian smoothing, expressivity or separability, there remains a lack of quantitative analysis comparing GNNs and MLPs from an optimization and generalization perspective. This study aims to address this gap by examining the role of graph convolution through feature learning theory. Using a signal-noise data model, we conduct a comparative analysis of the optimization and generalization between two-layer graph convolutional networks (GCNs) and their MLP counterparts. Our approach tracks the trajectory of signal learning and noise memorization in GNNs, characterizing their post-training generalization. We reveal that GNNs significantly prioritize signal learning, thus enhancing the regime of {low test error} over MLPs by $D^{q-2}$ times, where $D$ denotes a node's expected degree and $q$ is the power of ReLU activation function with $q>2$. This finding highlights a substantial and quantitative discrepancy between GNNs and MLPs in terms of optimization and generalization, a conclusion further supported by our empirical simulations on both synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wei Huang;Yuan Cao;Haonan Wang;Xin Cao;Taiji Suzuki",
        "authorids": "~Wei_Huang6;~Yuan_Cao1;~Haonan_Wang1;~Xin_Cao2;~Taiji_Suzuki1",
        "gender": "M;M;M;M;M",
        "homepage": "https://weihuang05.github.io/;https://yuancaohku.github.io/;http://charles-haonan-wang.me/;https://xincao-unsw.github.io/;http://ibis.t.u-tokyo.ac.jp/suzuki/",
        "dblp": "81/6685-34;;;77/2970-1;08/312",
        "google_scholar": "RZfDh4MAAAAJ;-VGnHI4AAAAJ;cLziVZMAAAAJ;https://scholar.google.com.sg/citations?user=kJIkUagAAAAJ;x8osrBsAAAAJ",
        "orcid": "0000-0001-5674-7021;;0009-0006-6963-8987;0000-0002-3519-7013;",
        "linkedin": ";;;;",
        "or_profile": "~Wei_Huang6;~Yuan_Cao1;~Haonan_Wang1;~Xin_Cao2;~Taiji_Suzuki1",
        "aff": "RIKEN AIP;University of Hong Kong;national university of singaore, National University of Singapore;University of New South Wales;The University of Tokyo",
        "aff_domain": "riken.jp;hku.hk;u.nus.edu;unsw.edu.au;u-tokyo.ac.jp",
        "position": "Research Scientist;Assistant Professor;PhD student;Senior Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nhuang2025quantifying,\ntitle={Quantifying the Optimization and Generalization Advantages of Graph Neural Networks Over Multilayer Perceptrons},\nauthor={Wei Huang and Yuan Cao and Haonan Wang and Xin Cao and Taiji Suzuki},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9L0Qz7d4Pb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9L0Qz7d4Pb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9QqSTI4ZD5",
        "title": "UNHaP: Unmixing Noise from Hawkes Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Physiological signal analysis often involves identifying events crucial to understanding biological dynamics. \nMany methods have been proposed to detect them, from handcrafted and supervised approaches to unsupervised techniques.\nAll these methods tend to produce spurious events, mainly as they detect each event independently.\nThis work introduces UNHaP (Unmix Noise from Hawkes Processes), a novel approach addressing the joint learning of temporal structures in events and the removal of spurious detections.\nBy treating the event detection output as a mixture of structured Hawkes and unstructured Poisson events, UNHaP efficiently unmixes these processes and estimates their parameters.\nThis approach significantly enhances event distribution characterization while minimizing false detection rates on simulated and real data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Virginie Loison;Guillaume Staerman;Thomas Moreau",
        "authorids": "~Virginie_Loison1;~Guillaume_Staerman1;~Thomas_Moreau2",
        "gender": ";Not Specified;",
        "homepage": ";https://guillaumestaermanml.github.io/;",
        "dblp": ";;",
        "google_scholar": "i0PDGl0AAAAJ;Zb2ax0wAAAAJ;",
        "orcid": "0009-0001-2999-7831;;",
        "linkedin": "virginie-loison/;;",
        "or_profile": "~Virginie_Loison1;~Guillaume_Staerman1;~Thomas_Moreau2",
        "aff": "INRIA;;",
        "aff_domain": "inria.fr;;",
        "position": "PhD student;;",
        "bibtex": "@inproceedings{\nloison2025unhap,\ntitle={{UNH}aP: Unmixing Noise from Hawkes Process to Model Physiological Events},\nauthor={Virginie Loison and Guillaume Staerman and Thomas Moreau},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9QqSTI4ZD5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9QqSTI4ZD5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9bB1FJSKKS",
        "title": "Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of differentially private stochastic convex optimization (DP-SCO) in a distributed setting with $M$ clients, where each of them has a local dataset of $N$ i.i.d. data samples from an underlying data distribution. The objective is to design an algorithm to minimize a convex population loss using a collaborative effort across $M$ clients, while ensuring the privacy of the local datasets. In this work, we investigate the accuracy-communication-privacy trade-off for this problem. We establish matching converse and achievability results using a novel lower bound and a new algorithm for distributed DP-SCO based on Vaidya\u2019s plane cutting method. Thus, our results provide a complete characterization of the accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sudeep Salgia;Nikola Pavlovic;Yuejie Chi;Qing Zhao",
        "authorids": "~Sudeep_Salgia1;~Nikola_Pavlovic1;~Yuejie_Chi1;~Qing_Zhao1",
        "gender": "M;M;;F",
        "homepage": "https://sudeepsalgia.github.io/;;;https://zhao.ece.cornell.edu/",
        "dblp": "207/8460;;;",
        "google_scholar": "Y5d5L84AAAAJ;;;ymsLVFsAAAAJ",
        "orcid": ";0009-0009-8532-6504;;",
        "linkedin": ";;;",
        "or_profile": "~Sudeep_Salgia1;~Nikola_Pavlovic1;~Yuejie_Chi1;~Qing_Zhao1",
        "aff": "Carnegie Mellon University;Cornell University;;Cornell University",
        "aff_domain": "cmu.edu;cornell.edu;;cornell.edu",
        "position": "Postdoc;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nsalgia2025characterizing,\ntitle={Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization},\nauthor={Sudeep Salgia and Nikola Pavlovic and Yuejie Chi and Qing Zhao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9bB1FJSKKS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9bB1FJSKKS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9dUT2AMjEu",
        "title": "On the Power of Adaptive Weighted Aggregation in Heterogeneous Federated Learning and Beyond",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Federated averaging (FedAvg) is the most fundamental algorithm in Federated learning (FL). Previous theoretical results assert that FedAvg convergence and generalization degenerate under heterogeneous clients. \nHowever, recent empirical results show that FedAvg can perform well in many real-world heterogeneous tasks.\nThese results reveal an inconsistency between FL theory and practice that is not fully explained. \nIn this paper, we show that common heterogeneity measures contribute to this inconsistency based on rigorous convergence analysis.\nFurthermore, we introduce a new measure \\textit{client consensus dynamics} and prove that \\textit{FedAvg can effectively handle client heterogeneity when an appropriate aggregation strategy is used}. \nBuilding on this theoretical insight, we present a simple and effective FedAvg variant termed FedAWARE. \nExtensive experiments on three datasets and two modern neural network architectures demonstrate that FedAWARE ensures faster convergence and better generalization in heterogeneous client settings. Moreover, our results show that FedAWARE can significantly enhance the generalization performance of advanced FL algorithms when used as a plug-in module.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dun Zeng;Zenglin Xu;SHIYU LIU;Yu Pan;Qifan Wang;Xiaoying Tang",
        "authorids": "~Dun_Zeng1;~Zenglin_Xu2;~SHIYU_LIU2;~Yu_Pan1;~Qifan_Wang2;~Xiaoying_Tang2",
        "gender": ";;M;M;M;F",
        "homepage": "https://github.com/Zengdun-cs;;https://github.com/shyuliu;https://yupan.me;https://wqfcr.github.io/;https://sse.cuhk.edu.cn/en/faculty/tangxiaoying",
        "dblp": "298/1134;;242/8959.html;;;134/9714-2",
        "google_scholar": "CuNFd3EAAAAJ;;;NuxEyPAAAAAJ;LrSyLosAAAAJ;https://scholar.google.com/citations?hl=zh-TW",
        "orcid": ";;;0000-0001-7515-8492;0000-0002-7570-5756;0000-0003-3955-1195",
        "linkedin": ";;;;;",
        "or_profile": "~Dun_Zeng1;~Zenglin_Xu2;~SHIYU_LIU2;~Yu_Pan1;~Qifan_Wang2;~Xiaoying_Tang2",
        "aff": "University of Electronic Science and Technology of China;;Southwest University of Finance and Economics;Huawei Technologies Ltd.;Meta AI;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "uestc.edu.cn;;swufe.edu.cn;huawei.com;fb.com;cuhk.edu.cn",
        "position": "PhD student;;Assistant Professor;Researcher;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nzeng2025on,\ntitle={On the Power of Adaptive Weighted Aggregation in Heterogeneous Federated Learning and Beyond},\nauthor={Dun Zeng and Zenglin Xu and SHIYU LIU and Yu Pan and Qifan Wang and Xiaoying Tang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9dUT2AMjEu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9dUT2AMjEu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9e0yjZevPe",
        "title": "Parallel Backpropagation for Inverse of a Convolution with Application to Normalizing Flows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The inverse of an invertible convolution is an important operation that comes up in Normalizing Flows, Image Deblurring, etc. The naive algorithm for backpropagation of this operation using Gaussian elimination has running time $O(n^3)$ where $n$ is the number of pixels in the image. We give a fast parallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square image and provide a GPU implementation of the same.\n\nInverse of Convolutions are usually used in Normalizing Flows in the sampling pass, making them slow. We propose to use Inverse of  Convolutions in the forward (image to latent vector) pass of the Normalizing flow. Since the sampling pass is the inverse of the forward pass, it will use convolutions only, resulting in efficient sampling times. We use our parallel backpropagation algorithm for optimizing the inverse of convolution layer, resulting in fast training times also. We implement this approach in various Normalizing Flow backbones, resulting in our Inverse-Flow models. We benchmark Inverse-Flow on standard datasets and show significantly improved sampling times with similar bits per dimension compared to previous models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sandeep Nagar;Girish Varma",
        "authorids": "~Sandeep_Nagar1;~Girish_Varma1",
        "gender": "M;",
        "homepage": "https://researchweb.iiit.ac.in/~sandeep.nagar/;",
        "dblp": ";",
        "google_scholar": "MQQy_T4AAAAJ;",
        "orcid": "0000-0002-4360-8011;",
        "linkedin": "sandeepnaagar/;",
        "or_profile": "~Sandeep_Nagar1;~Girish_Varma1",
        "aff": "International Institute of Information Technology, Hyderabad, India;",
        "aff_domain": "iiit.ac.in;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nnagar2025parallel,\ntitle={Parallel Backpropagation for Inverse of a Convolution with Application to Normalizing Flows},\nauthor={Sandeep Nagar and Girish Varma},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9e0yjZevPe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9e0yjZevPe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9gaGhPzo5w",
        "title": "Narrowing the Gap between Adversarial and Stochastic MDPs via Policy Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of learning in adversarial Markov decision processes [MDPs] with an oblivious adversary in a full-information setting. The agent interacts with an environment during $T$ episodes, each of which consists of $H$ stages, and each episode is evaluated with respect to a reward function that will be revealed only at the end of the episode. We propose an algorithm, called APO-MVP, that achieves a regret bound of order $\\tilde{\\mathcal{O}}(\\mathrm{poly}(H)\\sqrt{SAT})$, where $S$ and $A$ are sizes of the state and action spaces, respectively. This result improves upon the best-known regret bound by a factor of $\\sqrt{S}$, bridging the gap between adversarial and stochastic MDPs, and matching the minimax lower bound $\\Omega(\\sqrt{H^3SAT})$ as far as the dependencies in $S,A,T$ are concerned. The proposed algorithm and analysis completely avoid the typical tool given by occupancy measures; instead, it performs policy optimization based only on dynamic programming and on a black-box online linear optimization strategy run over estimated advantage functions, making it easy to implement. The analysis leverages two recent techniques: policy optimization based on online linear optimization strategies (Jonckheere et al., 2023) and a refined martingale analysis of the impact on values of estimating transitions kernels (Zhang et al., 2023).",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniil Tiapkin;Evgenii Chzhen;Gilles Stoltz",
        "authorids": "~Daniil_Tiapkin1;~Evgenii_Chzhen1;~Gilles_Stoltz1",
        "gender": "M;;M",
        "homepage": "https://d-tiapkin.github.io/;;https://www.imo.universite-paris-saclay.fr/fr/perso/gilles-stoltz/",
        "dblp": "267/5445;;18/3915",
        "google_scholar": "https://scholar.google.ru/citations?user=AB23PXQAAAAJ;;",
        "orcid": "0000-0002-8832-7926;;",
        "linkedin": "daniil-tiapkin-049714240/;;",
        "or_profile": "~Daniil_Tiapkin1;~Evgenii_Chzhen1;~Gilles_Stoltz1",
        "aff": "Ecole Polytechnique+Universit\u00e9 Paris-Saclay;;Universit\u00e9 Paris Saclay+HEC Paris+CNRS",
        "aff_domain": "polytechnique.edu+universite-paris-saclay.fr;;universite-paris-saclay.fr+hec.fr+cnrs.fr",
        "position": "PhD student+PhD student;;Researcher+Associate Professor+Researcher",
        "bibtex": "@inproceedings{\ntiapkin2025narrowing,\ntitle={Narrowing the Gap between Adversarial and Stochastic {MDP}s via Policy Optimization},\nauthor={Daniil Tiapkin and Evgenii Chzhen and Gilles Stoltz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9gaGhPzo5w}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9gaGhPzo5w",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9pjJXQWYXc",
        "title": "Information-Theoretic Causal Discovery in Topological Order",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Identifying causal relationships is a cornerstone task in science, but most data-driven methods offer ambiguous results or require restrictive assumptions. Recent work on the basis of information theory shows promising results across many domains, but leaves open how to provably identify causal graphs. Here, we develop a general information-theoretic framework called TOPIC for causal discovery in topological order. TOPIC is based on the universal measure of Kolmogorov complexity and is fully identifiable. We show that TOPIC's guarantees extend to both the i.i.d. and non-i.i.d. continuous settings. Our evaluations on continuous, time series, and interventional data show that TOPIC, using domain-specific approximations of Kolmogorov complexity, learns faithful topological orderings and frequently outperforms specialized methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sascha Xu;Sarah Mameche;Jilles Vreeken",
        "authorids": "~Sascha_Xu1;~Sarah_Mameche1;~Jilles_Vreeken2",
        "gender": "M;F;M",
        "homepage": ";https://cispa.de/en/people/c01sama;https://vreeken.eu",
        "dblp": "247/3300;326/4243;94/6462",
        "google_scholar": "https://scholar.google.de/citations?user=82xDR9IAAAAJ;uRajBQEAAAAJ;p5HEQfIAAAAJ",
        "orcid": "0009-0008-5191-0342;;0000-0002-2310-2806",
        "linkedin": "sascha-xu-36073216a/;;jilles-vreeken-b3b05b58/",
        "or_profile": "~Sascha_Xu1;~Sarah_Mameche1;~Jilles_Vreeken2",
        "aff": "CISPA, saarland university, saarland informatics campus;CISPA, saarland university, saarland informatics campus;CISPA Helmholtz Center for Information Security",
        "aff_domain": "cispa.saarland;cispa.saarland;cispa.de",
        "position": "PhD student;PhD student;Tenured Faculty",
        "bibtex": "@inproceedings{\nxu2025informationtheoretic,\ntitle={Information-Theoretic Causal Discovery in Topological Order},\nauthor={Sascha Xu and Sarah Mameche and Jilles Vreeken},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9pjJXQWYXc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9pjJXQWYXc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9zrByMY8DY",
        "title": "Disentangling impact of capacity, objective, batchsize, estimators, and step-size on flow VI",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Normalizing flow-based variational inference (flow VI) is a promising approximate inference approach, but its performance remains inconsistent across studies. Numerous algorithmic choices influence flow VI's performance. We conduct a step-by-step analysis to disentangle the impact of some of the key factors: capacity, objectives, gradient estimators, number of gradient estimates (batchsize), and step-sizes. Each step examines one factor while neutralizing others using insights from the previous steps and/or using extensive parallel computation. To facilitate high-fidelity evaluation, we curate a benchmark of synthetic targets that represent common posterior pathologies and allow for exact sampling. We provide specific recommendations for different factors and propose a flow VI recipe that matches or surpasses leading turnkey Hamiltonian Monte Carlo (HMC) methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abhinav Agrawal;Justin Domke",
        "authorids": "~Abhinav_Agrawal1;~Justin_Domke1",
        "gender": "M;Unspecified",
        "homepage": "https://abhiagwl.github.io/;https://people.cs.umass.edu/~domke/",
        "dblp": "73/3655-1;39/5186",
        "google_scholar": "ufb1EMYAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Abhinav_Agrawal1;~Justin_Domke1",
        "aff": "University of Massachusetts Amherst;University of Massachusetts at Amherst",
        "aff_domain": "cs.umass.edu;umass.edu",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nagrawal2025disentangling,\ntitle={Disentangling impact of capacity, objective, batchsize, estimators, and step-size on flow-based {VI}},\nauthor={Abhinav Agrawal and Justin Domke},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=9zrByMY8DY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9zrByMY8DY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "A91Za3oDo1",
        "title": "Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Missing data in supervised learning is well-studied, but the specific issue of missing labels during model evaluation has been overlooked. Ignoring samples with missing values, a common solution, can introduce bias, especially when data is Missing Not At Random (MNAR). We propose a multiple imputation technique for evaluating classifiers using metrics such as precision, recall, and ROC-AUC. This method not only offers point estimates but also a predictive distribution for these quantities when labels are missing. We empirically show that the predictive distribution's location and shape are generally correct, even in the MNAR regime. Moreover, we establish that this distribution is approximately Gaussian and provide finite-sample convergence bounds. Additionally, a robustness proof is presented, confirming the validity of the approximation under a realistic error model.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Danial Dervovic;Michael Cashmore",
        "authorids": "~Danial_Dervovic1;~Michael_Cashmore1",
        "gender": "M;",
        "homepage": "https://www.danialdervovic.com;",
        "dblp": "203/8299.html;118/2640",
        "google_scholar": "ttWrIOcAAAAJ;",
        "orcid": "0000-0002-6135-561X;",
        "linkedin": "https://uk.linkedin.com/in/danial-dervovic;",
        "or_profile": "~Danial_Dervovic1;~Michael_Cashmore1",
        "aff": "J.P. Morgan Chase;J.P. Morgan Chase",
        "aff_domain": "jpmorgan.com;jpmorgan.com",
        "position": "Researcher;AI Research Director",
        "bibtex": "@inproceedings{\ndervovic2025model,\ntitle={Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels},\nauthor={Danial Dervovic and Michael Cashmore},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=A91Za3oDo1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=A91Za3oDo1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ADsGXBwpck",
        "title": "Performative Reinforcement Learning with Linear Markov Decision Process",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the setting of \\emph{performative reinforcement learning} where the deployed policy affects both the reward, and the transition of the underlying Markov decision process. Prior work (Mandal et al., 2023)  has addressed this problem under the tabular setting and established last-iterate convergence of repeated retraining with iteration complexity explicitly depending on the number of states. In this work, we generalize the results to \\emph{linear Markov decision processes} which is the primary theoretical model of large-scale MDPs. The main challenge with linear MDP is that the regularized objective is no longer strongly convex and we want a bound that scales with the dimension of the features, rather than states which can be infinite. Our first result shows that repeatedly optimizing a regularized objective converges to a \\emph{performatively stable policy}. In the absence of strong convexity, our analysis leverages a new recurrence relation that uses a specific linear combination of optimal dual solutions for proving convergence. We then tackle the finite sample setting where the learner has access to a set of trajectories drawn from the current policy. We consider a reparametrized version of the primal problem, and construct an empirical Lagrangian which is to be optimized from the samples. We show that, under a \\emph{bounded coverage} condition, repeatedly solving a saddle point of this empirical Lagrangian converges to a performatively stable solution, and also construct a primal-dual algorithm that solves the empirical Lagrangian efficiently. Finally, we show several applications of the general framework of performative RL including multi-agent systems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Debmalya Mandal;Goran Radanovic",
        "authorids": "~Debmalya_Mandal2;~Goran_Radanovic1",
        "gender": "M;",
        "homepage": "https://debmandal.github.io;",
        "dblp": "151/3685;133/1771",
        "google_scholar": "OquWQpEAAAAJ;KBG_JlAAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Debmalya_Mandal2;~Goran_Radanovic1",
        "aff": "University of Warwick;MPI-SWS",
        "aff_domain": "warwick.ac.uk;mpi-sws.org",
        "position": "Assistant Professor;Research group leader",
        "bibtex": "@inproceedings{\nmandal2025performative,\ntitle={Performative Reinforcement Learning with Linear Markov Decision Processes},\nauthor={Debmalya Mandal and Goran Radanovic},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ADsGXBwpck}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ADsGXBwpck",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "AM5VTtoexY",
        "title": "Corruption Robust Offline Reinforcement Learning with Human Feedback",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an \n$\\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustifies an offline RLHF framework by first learning a reward model along with confidence sets and then learning a pessimistic optimal policy over the confidence set. Our key insight is that learning optimal policy can be done by leveraging an offline corruption-robust RL oracle in different ways (e.g., zero-order oracle or first-order oracle), depending on the data coverage assumptions. To our knowledge, ours is the first work that provides provable corruption robust offline RLHF methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Debmalya Mandal;Andi Nika;Parameswaran Kamalaruban;Adish Singla;Goran Radanovic",
        "authorids": "~Debmalya_Mandal2;~Andi_Nika1;~Parameswaran_Kamalaruban2;~Adish_Singla2;~Goran_Radanovic1",
        "gender": "M;M;M;;",
        "homepage": "https://debmandal.github.io;https://andinika.github.io/;https://markovkernel.net/;;",
        "dblp": "151/3685;268/2761;164/7413;;133/1771",
        "google_scholar": "OquWQpEAAAAJ;oTIFCrEAAAAJ;0ioRCikAAAAJ;;KBG_JlAAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Debmalya_Mandal2;~Andi_Nika1;~Parameswaran_Kamalaruban2;~Adish_Singla2;~Goran_Radanovic1",
        "aff": "University of Warwick;MPI-SWS;VISA+Featurespace;;MPI-SWS",
        "aff_domain": "warwick.ac.uk;mpi-sws.org;visa.com+featurespace.co.uk;;mpi-sws.org",
        "position": "Assistant Professor;PhD student;Researcher+Researcher;;Research group leader",
        "bibtex": "@inproceedings{\nmandal2025corruption,\ntitle={Corruption Robust Offline Reinforcement Learning with Human Feedback},\nauthor={Debmalya Mandal and Andi Nika and Parameswaran Kamalaruban and Adish Singla and Goran Radanovic},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=AM5VTtoexY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AM5VTtoexY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "AZ6T7HdCRt",
        "title": "Bridging the Theoretical Gap in Randomized Smoothing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Randomized smoothing has become a leading approach for certifying adversarial robustness in machine learning models. However, a persistent gap remains between theoretical certified robustness and empirical robustness accuracy. This paper introduces a new framework that bridges this gap by leveraging Lipschitz continuity for certification and proposing a novel, less conservative method for computing confidence intervals in randomized smoothing. Our approach tightens the bounds of certified robustness, offering a more accurate reflection of model robustness in practice. Through rigorous experimentation we show that our method improves the robust accuracy, compressing the gap between empirical findings and previous theoretical results.\n  We argue that investigating local Lipschitz constants and designing ad-hoc confidence intervals can further enhance the performance of randomized smoothing. These results pave the way for a deeper understanding of the relationship between Lipschitz continuity and certified robustness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Blaise Delattre;Paul Caillon;Quentin Barth\u00e9lemy;Erwan Fagnou;Alexandre Allauzen",
        "authorids": "~Blaise_Delattre1;~Paul_Caillon1;~Quentin_Barth\u00e9lemy1;~Erwan_Fagnou1;~Alexandre_Allauzen1",
        "gender": "M;;;M;M",
        "homepage": ";;https://github.com/qbarthelemy/;https://erwanfagnou.github.io;http://allauzen.github.io/",
        "dblp": ";;;;54/8163",
        "google_scholar": "0SNA45sAAAAJ;;97OQs9IAAAAJ;;https://scholar.google.fr/citations?user=B2-gXkkAAAAJ",
        "orcid": ";;;;0000-0002-8627-1965",
        "linkedin": ";;;;",
        "or_profile": "~Blaise_Delattre1;~Paul_Caillon1;~Quentin_Barth\u00e9lemy1;~Erwan_Fagnou1;~Alexandre_Allauzen1",
        "aff": "Universit\u00e9 Paris-Dauphine (Paris IX);;;Universit\u00e9 Paris-Dauphine - PSL;Ecole Sup\u00e9rieure de Physique et de Chimie Industrielles+Universit\u00e9 Paris-Dauphine",
        "aff_domain": "lamsade.dauphine.fr;;;dauphine.psl.eu;espci.fr+dauphine.fr",
        "position": "PhD student;;;PhD student;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\ndelattre2025bridging,\ntitle={Bridging the Theoretical Gap in Randomized Smoothing},\nauthor={Blaise Delattre and Paul Caillon and Erwan Fagnou and Quentin Barth{\\'e}lemy and Alexandre Allauzen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=AZ6T7HdCRt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AZ6T7HdCRt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "AcX96FA6Ck",
        "title": "Fairness Risks for Group-Conditionally Missing Demographics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual\u2019s fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaiqi Jiang;Wenzhe Fan;Mao Li;Xinhua Zhang",
        "authorids": "~Kaiqi_Jiang1;~Wenzhe_Fan1;~Mao_Li1;~Xinhua_Zhang3",
        "gender": "M;F;M;M",
        "homepage": ";;;https://www.cs.uic.edu/~zhangx/",
        "dblp": ";;https://dblp.org/rec/conf/nips/LiMZ20;45/6863",
        "google_scholar": ";;;https://scholar.google.com.tw/citations?user=jrkrn3sAAAAJ",
        "orcid": ";;;",
        "linkedin": "kaiqi-jiang;wenzhe-fan-46a8b728a/;;",
        "or_profile": "~Kaiqi_Jiang1;~Wenzhe_Fan1;~Mao_Li1;~Xinhua_Zhang3",
        "aff": "University of Illinois, Chicago;University of Illinois at Chicago;Amazon;University of Illinois, Chicago",
        "aff_domain": "uic.edu;uic.edu;amazon.com;uic.edu",
        "position": "PhD student;PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\njiang2025fairness,\ntitle={Fairness Risks for Group-Conditionally Missing Demographics},\nauthor={Kaiqi Jiang and Wenzhe Fan and Mao Li and Xinhua Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=AcX96FA6Ck}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AcX96FA6Ck",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "AdXSZNm3SL",
        "title": "Unbiased Quantization of the $L_1$ Ball for Communication-Efficient Distributed Mean Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of unbiased minimum mean squared error quantization of the $L_1$ ball, with applications to distributed mean estimation and federated learning. Inspired by quantization of probability distributions using types, we design a novel computationally efficient unbiased quantization scheme for vectors that lie within the $L_1$ ball. We also derive upper bounds on the worst-case mean squared error achieved by our scheme and show that this is order optimal. We then use this to design polynomial (in the dimension of the input vectors)-time schemes for communication-efficient distributed mean estimation and distributed/federated learning, and demonstrate its effectiveness using simulations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nithish Suresh Babu;Ritesh Kumar;Shashank Vatedka",
        "authorids": "nithish576s@gmail.com;~Ritesh_Kumar2;~Shashank_Vatedka1",
        "gender": ";M;M",
        "homepage": ";;https://people.iith.ac.in/shashankvatedka/html/home.html",
        "dblp": ";;",
        "google_scholar": ";https://scholar.google.co.il/citations?user=r7ssBPQAAAAJ;https://scholar.google.co.in/citations?user=f4dNpLQAAAAJ",
        "orcid": ";0000-0001-9151-5322;",
        "linkedin": ";ritesh-kumar-1b4230194/;",
        "or_profile": "nithish576s@gmail.com;~Ritesh_Kumar2;~Shashank_Vatedka1",
        "aff": ";Indian Institute of Technology, Hyderabad;Indian Institute of Technology, Hyderabad",
        "aff_domain": ";iith.ac.in;iith.ac.in",
        "position": ";PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nbabu2025unbiased,\ntitle={Unbiased Quantization of the \\$L\\_1\\$ Ball for Communication-Efficient Distributed Mean Estimation},\nauthor={Nithish Suresh Babu and Ritesh Kumar and Shashank Vatedka},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=AdXSZNm3SL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AdXSZNm3SL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "B50OF0Fc6O",
        "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-Context Learning (ICL) ability has been found efficient across a wide range of applications, where the Large Language Models (LLM) learn to complete the tasks from the examples in the prompt without tuning the parameters. In this work, we conduct a comprehensive study to understand ICL from a statistical perspective. First, we show that the perfectly pretrained LLMs perform Bayesian Model Averaging (BMA) for ICL under a dynamic model of examples in the prompt. The average error analysis for ICL is then built for the perfectly pretrained LLMs with the analysis of BMA. Second, we demonstrate how the attention structure boosts the BMA implementation. With sufficient examples in the prompt, attention is proven to perform BMA under the Gaussian linear ICL model, which also motivates the explicit construction of the hidden concepts from the attention heads values. Finally, we analyze the pretraining behavior of LLMs. The pretraining error is decomposed as the generalization error and the approximation error. The generalization error is upper bounded via PAC-Bayes framework. Then the ICL average error of the pretrained LLMs is shown to be the sum of $O(T^{-1})$ and the pretraining error. In addition, we analyze the ICL performance of the pretrained LLMs with misspecified examples.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yufeng Zhang;Fengzhuo Zhang;Zhuoran Yang;Zhaoran Wang",
        "authorids": "~Yufeng_Zhang2;~Fengzhuo_Zhang1;~Zhuoran_Yang1;~Zhaoran_Wang1",
        "gender": "M;M;M;Not Specified",
        "homepage": ";;https://zhuoranyang.github.io/;https://zhaoranwang.github.io/",
        "dblp": "17/1651;254/1627;;117/2756",
        "google_scholar": ";;;https://scholar.google.com.tw/citations?user=HSx0BgQAAAAJ",
        "orcid": ";;;",
        "linkedin": "yufeng-zhang-485844119/;%E4%B8%B0%E5%8D%93-%E5%BC%A0-4576a5135/;;",
        "or_profile": "~Yufeng_Zhang2;~Fengzhuo_Zhang1;~Zhuoran_Yang1;~Zhaoran_Wang1",
        "aff": "ByteDance Inc.;National University of Singapore;Yale University;Northwestern University",
        "aff_domain": "bytedance.com;nus.edu;yale.edu;northwestern.edu",
        "position": "Researcher;PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025what,\ntitle={What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization},\nauthor={Yufeng Zhang and Fengzhuo Zhang and Zhuoran Yang and Zhaoran Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=B50OF0Fc6O}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=B50OF0Fc6O",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "B86T1rjmff",
        "title": "Copula Based Trainable Calibration Error Estimator of Multi-Label Classification with Label Interdependencies",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A key challenge in calibrating Multi-Label Classification(MLC) problems is to consider the interdependencies among labels. To address this, in this research we propose an unbiased, differentiable, trainable calibration error estimator for MLC problems by using Copula. Unlike other methods for calibrating MLC tasks that focus on marginal calibration, this novel estimator takes label interdependencies into account and enables us to tackle the strictest notion of calibration that is canonical calibration. To design the estimator, we begin by leveraging the kernel trick to construct a continuous distribution from the discrete label space. Then we take a semiparametric approach to construct the estimator where the marginals are modeled non-parametrically and the Copula is modeled parametrically. Theoretically we show that our estimator is unbiased and converges to true $L^p$ calibration error. We also use our estimator as a regularizer at the time of training and observe that it reduces calibration error on test datasets significantly. Experiments on a well established dataset endorses our claims.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arkapal Panda;Utpal Garain",
        "authorids": "~Arkapal_Panda1;~Utpal_Garain1",
        "gender": "M;M",
        "homepage": ";https://www.isical.ac.in/~utpal/",
        "dblp": "351/8239;81/4230",
        "google_scholar": ";https://scholar.google.co.in/citations?user=4Jlqf30AAAAJ",
        "orcid": ";0000-0001-7207-5018",
        "linkedin": "arkapal-panda-1166621ab;utpal-garain-7b365943/?originalSubdomain=in",
        "or_profile": "~Arkapal_Panda1;~Utpal_Garain1",
        "aff": "Indian Statistical Institute;Indian Statistical Institute",
        "aff_domain": "isical.ac.in;isical.ac.in",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\npanda2025copula,\ntitle={Copula Based Trainable Calibration Error Estimator of Multi-Label Classification with Label Interdependencies},\nauthor={Arkapal Panda and Utpal Garain},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=B86T1rjmff}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=B86T1rjmff",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BDUb3feGuq",
        "title": "Sampling from the Random Linear Model via Stochastic Localization Up to the AMP Threshold",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recently, Approximate Message Passing (AMP) has been integrated with stochastic localization (diffusion model) by providing a computationally efficient estimator of the posterior mean. Existing (rigorous) analysis typically proves the success of sampling for sufficiently small noise, but determining the exact threshold involves several challenges. In this paper, we focus on sampling from the posterior in the linear inverse problem, with an i.i.d. random design matrix, and show that the threshold for sampling coincide with that of posterior mean estimation. We give a proof for the convergence in smoothed KL divergence whenever the noise variance $\\Delta$ is below $\\Delta_{\\rm AMP}$, which is the computation threshold for mean estimation introduced in (Barbier et al., 2020). We also show convergence in the Wasserstein distance under the same threshold assuming a dimension-free bound on the operator norm of the posterior covariance matrix, a condition strongly suggested by recent breakthroughs on operator norm bounds in similar replica symmetric systems. A key observation in our analysis is that phase transition does not occur along the sampling and interpolation paths assuming $\\Delta<\\Delta_{\\rm AMP}$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Han Cui;Zhiyuan Yu;Jingbo Liu",
        "authorids": "~Han_Cui3;~Zhiyuan_Yu8;~Jingbo_Liu3",
        "gender": "M;M;M",
        "homepage": ";https://stat.illinois.edu/directory/profile/yu124;https://stat.illinois.edu/directory/profile/jingbol",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": "han-cui-827135249;;",
        "or_profile": "~Han_Cui3;~Zhiyuan_Yu8;~Jingbo_Liu3",
        "aff": "University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\ncui2025sampling,\ntitle={Sampling from the Random Linear Model via Stochastic Localization Up to the {AMP} Threshold},\nauthor={Han Cui and Zhiyuan Yu and Jingbo Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BDUb3feGuq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BDUb3feGuq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BErNKnkpDn",
        "title": "Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-objective Bayesian optimization has been widely adopted in scientific experiment design, including drug discovery and hyperparameter optimization. In practice, regulatory or safety concerns often impose additional thresholds on certain attributes of the experimental outcomes. Previous work has primarily focused on constrained single-objective optimization tasks or active search under constraints. The existing constrained multi-objective algorithms address the issue with heuristics and approximations, posing challenges to the analysis of the sample efficiency.\nWe propose a novel constrained multi-objective Bayesian optimization algorithm **COMBOO** that balances active learning of the level-set defined on multiple unknowns with multi-objective optimization within the feasible region. We provide both theoretical analysis and empirical evidence, demonstrating the efficacy of our approach on various synthetic benchmarks and real-world applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Diantong Li;Fengxue Zhang;Chong Liu;Yuxin Chen",
        "authorids": "~Diantong_Li2;~Fengxue_Zhang1;~Chong_Liu1;~Yuxin_Chen1",
        "gender": ";M;M;",
        "homepage": ";;https://chong-l.github.io/;http://yuxinchen.org/",
        "dblp": ";;47/2929-7.html;11/5123-1",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;-k1N7HAAAAAJ",
        "orcid": ";;0000-0002-7028-7508;",
        "linkedin": ";fengxue-zhang-18b205146/;danielcliu/;",
        "or_profile": "~Diantong_Li2;~Fengxue_Zhang1;~Chong_Liu1;~Yuxin_Chen1",
        "aff": ";University of Chicago;University at Albany, State University of New York;University of Chicago",
        "aff_domain": ";uchicago.edu;albany.edu;uchicago.edu",
        "position": ";Ph.D. student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025constrained,\ntitle={Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation},\nauthor={Diantong Li and Fengxue Zhang and Chong Liu and Yuxin Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BErNKnkpDn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BErNKnkpDn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BVQ8rIFuYa",
        "title": "Near-Polynomially Competitive Active Logistic Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We address the problem of active logistic regression in the realizable setting. It is well known that active learning can require exponentially fewer label queries compared to passive learning, in some cases using $\\log \\frac{1}{\\varepsilon}$ rather than $\\mathrm{poly}(1/\\varepsilon)$ samples to get error $\\varepsilon$ larger than the optimum.\n\nWe present the first algorithm that is polynomially competitive with the optimal algorithm on every input instance, up to factors polylogarithmic in the error and domain size.  In particular, if any algorithm achieves label complexity polylogarithmic in $\\varepsilon$, so does ours.  Our algorithm is based on efficient sampling and can be extended to learn more general class of functions. We further support our theoretical results with experiments demonstrating performance gains for logistic regression compared to existing active learning algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yihan Zhou;Eric Price;Trung Nguyen",
        "authorids": "~Yihan_Zhou3;~Eric_Price1;~Trung_Nguyen2",
        "gender": "M;;",
        "homepage": "https://joeyandbluewhale.github.io/;;",
        "dblp": "199/6805;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yihan_Zhou3;~Eric_Price1;~Trung_Nguyen2",
        "aff": "University of Texas at Austin;;",
        "aff_domain": "cs.utexas.edu;;",
        "position": "PhD student;;",
        "bibtex": "@inproceedings{\nzhou2025nearpolynomially,\ntitle={Near-Polynomially Competitive Active Logistic Regression},\nauthor={Yihan Zhou and Eric Price and Trung Nguyen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BVQ8rIFuYa}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BVQ8rIFuYa",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BhmLkJiahO",
        "title": "The Strong Product Model for Network Inference without Independence Assumptions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-axis graphical modelling techniques allow us to perform network inference without making independence assumptions.  This is done by replacing the independence assumption with a weaker assumption about the interaction between the axes; there are several choices for which assumption to use.  In single-cell RNA sequencing data, genes may interact differently depending on whether they are expressed in the same cell, or in different cells.  Unfortunately, current methods are not able to make this distinction.  In this paper, we address this problem by introducing the strong product model for Gaussian graphical modelling.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bailey Andrew;David Robert Westhead;Luisa Cutillo",
        "authorids": "~Bailey_Andrew1;~David_Robert_Westhead1;~Luisa_Cutillo1",
        "gender": "M;M;F",
        "homepage": ";https://biologicalsciences.leeds.ac.uk/molecular-and-cellular-biology/staff/154/professor-david-r-westhead;https://eps.leeds.ac.uk/maths/staff/5526/dr-luisa-cutillo",
        "dblp": ";52/4810;32/6082",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;;",
        "orcid": ";0000-0002-0519-3820;",
        "linkedin": ";david-westhead-04178b30/;",
        "or_profile": "~Bailey_Andrew1;~David_Robert_Westhead1;~Luisa_Cutillo1",
        "aff": "University of Leeds;University of Leeds;University of Leeds",
        "aff_domain": "leeds.ac.uk;leeds.ac.uk;leeds.ac.uk",
        "position": "PhD student;Full Professor;Lecturer",
        "bibtex": "@inproceedings{\nandrew2025the,\ntitle={The Strong Product Model for Network Inference without Independence Assumptions},\nauthor={Bailey Andrew and David Robert Westhead and Luisa Cutillo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BhmLkJiahO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BhmLkJiahO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BrYraqtSKs",
        "title": "Hyperboloid GPLVM for Discovering Continuous Hierarchies via Nonparametric Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Dimensionality reduction (DR) offers interpretable representations of complex high-dimensional data, and recent DR methods have leveraged hyperbolic geometry to obtain faithful low-dimensional embeddings of high-dimensional hierarchical relationships. However, existing methods are dependent on neighbor embedding, which frequently ruins the continuous nature of the hierarchical structures. This paper proposes hyperboloid Gaussian process latent variable models (hGP-LVMs) to embed high-dimensional hierarchical data while preserving the implicit continuity via nonparametric estimation. We adopt generative modeling using the GP, which provides effective hierarchical embedding and executes ill-posed hyperparameter tuning. This paper presents three variants of the proposed models that employ original point, sparse point, and Bayesian estimations, and we establish their learning algorithms by incorporating the Riemannian optimization and active approximation scheme of the GP-LVM. In addition, we employ the reparameterization trick for scalable learning of the latent variables in the Bayesian estimation method. The proposed hGP-LVMs were applied to several datasets, and the results demonstrate their ability to represent high-dimensional hierarchies in low-dimensional spaces.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Koshi Watanabe;Keisuke Maeda;Takahiro Ogawa;Miki Haseyama",
        "authorids": "~Koshi_Watanabe1;~Keisuke_Maeda1;~Takahiro_Ogawa1;~Miki_Haseyama1",
        "gender": "M;M;M;F",
        "homepage": ";https://www-lmd.ist.hokudai.ac.jp/member/keisuke-maeda/;https://www-lmd.ist.hokudai.ac.jp/member/takahiro-ogawa/;https://www-lmd.ist.hokudai.ac.jp/member/miki-haseyama/",
        "dblp": ";134/3015;45/6528;29/5984",
        "google_scholar": "AVTMuyIAAAAJ;5sZKrK0AAAAJ;https://scholar.google.co.jp/citations?user=vPixFIsAAAAJ;https://scholar.google.co.jp/citations?hl=en",
        "orcid": "0000-0002-0458-802X;0000-0001-8039-3462;0000-0001-5332-8112;0000-0003-1496-1761",
        "linkedin": ";;;",
        "or_profile": "~Koshi_Watanabe1;~Keisuke_Maeda1;~Takahiro_Ogawa1;~Miki_Haseyama1",
        "aff": "Hokkaido University;Hokkaido University+Hokkaido University;Hokkaido University;Hokkaido University ",
        "aff_domain": "hokudai.ac.jp;hokudai.ac.jp+hokudai.ac.jp;hokudai.ac.jp;hokudai.ac.jp",
        "position": "PhD student;Associate Professor+Associate Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwatanabe2025hyperboloid,\ntitle={Hyperboloid {GPLVM} for Discovering Continuous Hierarchies via Nonparametric Estimation},\nauthor={Koshi Watanabe and Keisuke Maeda and Takahiro Ogawa and Miki Haseyama},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BrYraqtSKs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BrYraqtSKs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BrgLOu0ezN",
        "title": "Statistical Test for Auto Feature Engineering by Selective Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Auto Feature Engineering (AFE) plays a crucial role in developing practical machine learning pipelines by automating the transformation of raw data into meaningful features that enhance model performance. By generating features in a data-driven manner, AFE enables the discovery of important features that may not be apparent through human experience or intuition. On the other hand, since AFE generates features based on data, there is a risk that these features may be overly adapted to the data, making it essential to assess their reliability appropriately. Unfortunately, because most AFE problems are formulated as combinatorial search problems and solved by heuristic algorithms, it has been challenging to theoretically quantify the reliability of generated features. To address this issue, we propose a new statistical test for generated features by AFE algorithms based on a framework called selective inference. As a proof of concept, we consider a simple class of tree search-based heuristic AFE algorithms, and consider the problem of testing the generated features when they are used in a linear model. The proposed test can quantify the statistical significance of the generated features in the form of $p$-values, enabling theoretically guaranteed control of the risk of false findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tatsuya Matsukawa;Tomohiro Shiraishi;Shuichi Nishino;Teruyuki Katsuoka;Ichiro Takeuchi",
        "authorids": "~Tatsuya_Matsukawa1;~Tomohiro_Shiraishi1;~Shuichi_Nishino1;~Teruyuki_Katsuoka1;~Ichiro_Takeuchi1",
        "gender": ";M;M;M;M",
        "homepage": "https://github.com/matsuimatsuoka;https://github.com/shirara1016;https://github.com/ni-shu;https://github.com/teruyukikatsuoka;https://www.mlds.mae.nagoya-u.ac.jp/takeuchi/index.en.html",
        "dblp": ";;;;36/4181",
        "google_scholar": ";;;;IwBHa3gAAAAJ",
        "orcid": ";;;;0009-0005-1905-2366",
        "linkedin": ";;;;",
        "or_profile": "~Tatsuya_Matsukawa1;~Tomohiro_Shiraishi1;~Shuichi_Nishino1;~Teruyuki_Katsuoka1;~Ichiro_Takeuchi1",
        "aff": "Nagoya University;Nagoya University;Nagoya University+RIKEN+Hitachi, Ltd.;Nagoya University;Nagoya University+RIKEN",
        "aff_domain": "nagoya-u.ac.jp;nagoya-u.ac.jp;nagoya-u.ac.jp+riken.jp+hitachi.com;nagoya-u.ac.jp;nagoya-u.ac.jp+riken.jp",
        "position": "MS student;PhD student;PhD student+PhD student+Researcher;Undergrad student;Full Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nmatsukawa2025statistical,\ntitle={Statistical Test for Auto Feature Engineering by Selective Inference},\nauthor={Tatsuya Matsukawa and Tomohiro Shiraishi and Shuichi Nishino and Teruyuki Katsuoka and Ichiro Takeuchi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BrgLOu0ezN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BrgLOu0ezN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BuhgHqJQV7",
        "title": "Weighted Euclidean Distance Matrices over Mixed Continuous and Categorical Inputs for Gaussian Process Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Gaussian Process (GP) models are widely utilized as surrogate models in scientific and engineering fields. However, standard GP models are limited to continuous variables due to the difficulties in establishing correlation structures for categorical variables. To overcome this limitation, we introduce **WE**ighted Euclidean distance matrices **G**aussian **P**rocess (WEGP). WEGP constructs the kernel function for each categorical input by estimating the Euclidean distance matrix (EDM) among all categorical choices of this input. The EDM is represented as a linear combination of several predefined base EDMs, each scaled by a positive weight. The weights, along with other kernel hyperparameters, are inferred using a fully Bayesian framework. We analyze the predictive performance of WEGP theoretically. Numerical experiments validate the accuracy of our GP model, and by WEGP, into Bayesian Optimization (BO), we achieve superior performance on both synthetic and real-world optimization problems. The code is available at: https://github.com/pmy0124nus/WEGP.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mingyu Pu;Wang Songhao;Haowei Wang;Szu Hui Ng",
        "authorids": "~Mingyu_Pu1;~Wang_Songhao1;~Haowei_Wang2;~Szu_Hui_Ng1",
        "gender": "F;M;M;",
        "homepage": ";;;https://cde.nus.edu.sg/isem/staff/ng-szu-hui/",
        "dblp": ";;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;kitjbCsAAAAJ;FJKQJywAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Mingyu_Pu1;~Wang_Songhao1;~Haowei_Wang2;~Szu_Hui_Ng1",
        "aff": "National University of Singapore;Southern University of Science and Technology;National University of Singapore;National University of Singapore",
        "aff_domain": "nus.edu;sustech.edu.cn;nus.edu.sg;nus.edu.sg",
        "position": "PhD student;Assistant Professor;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\npu2025weighted,\ntitle={Weighted Euclidean Distance Matrices over Mixed Continuous and Categorical Inputs for Gaussian Process Models},\nauthor={Mingyu Pu and Wang Songhao and Haowei Wang and Szu Hui Ng},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BuhgHqJQV7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BuhgHqJQV7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BvN5MWyh3k",
        "title": "From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "With the strengths of both deep learning and kernel methods like Gaussian Processes (GPs), Deep Kernel Learning (DKL) has gained considerable attention in recent years. From the computational perspective, however, DKL becomes challenging when the input dimension of the GP layer is high. To address this challenge, we propose the Deep Additive Kernel (DAK) model, which incorporates i) an additive structure for the last-layer GP; and ii) induced prior approximation for each GP unit. This naturally leads to a last-layer Bayesian neural network (BNN) architecture. The proposed method enjoys the interpretability of DKL as well as the computational advantages of BNN. Empirical results show that the proposed approach outperforms state-of-the-art DKL methods in both regression and classification tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenyuan Zhao;Haoyuan Chen;Tie Liu;Rui Tuo;Chao Tian",
        "authorids": "~Wenyuan_Zhao1;~Haoyuan_Chen1;~Tie_Liu1;~Rui_Tuo1;~Chao_Tian2",
        "gender": "M;;M;M;",
        "homepage": "https://wyzhao23.github.io;;;https://sites.google.com/site/ruituo2017/home?authuser=0;",
        "dblp": ";;;184/0554;",
        "google_scholar": "MoYK76gAAAAJ;maRSH-AAAAAJ;ryt2nuoAAAAJ;J_D0pSUAAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Wenyuan_Zhao1;~Haoyuan_Chen1;~Tie_Liu1;~Rui_Tuo1;~Chao_Tian2",
        "aff": "Texas A&M University - College Station;Texas A&M University - College Station;Texas A&M University;Texas A&M University - College Station;",
        "aff_domain": "tamu.edu;tamu.edu;tamu.edu;tamu.edu;",
        "position": "PhD student;PhD student;Full Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nzhao2025from,\ntitle={From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation},\nauthor={Wenyuan Zhao and Haoyuan Chen and Tie Liu and Rui Tuo and Chao Tian},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BvN5MWyh3k}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BvN5MWyh3k",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BxniFq6TRd",
        "title": "Disentangling Interactions and Dependencies in Feature Attributions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In explainable machine learning, global feature importance methods try to determine how much each individual feature contributes to predicting the target variable, resulting in one importance score for each feature. But often, predicting the target variable requires interactions between several features (such as in the XOR function), and features might have complex statistical dependencies that allow to partially replace one feature with another one. In commonly used feature importance scores, these cooperative effects are conflated with the features' individual contributions, making them prone to misinterpretations. \nIn this work, we derive DIP, a new mathematical decomposition of individual feature importance scores that disentangles three components: the standalone contribution and the contributions stemming from interactions and dependencies. We prove that the DIP decomposition is unique and show how it can be estimated in practice. Based on these results, we propose a new visualization of feature importance scores that clearly illustrates the different contributions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gunnar K\u00f6nig;Eric G\u00fcnther;Ulrike von Luxburg",
        "authorids": "~Gunnar_K\u00f6nig1;~Eric_G\u00fcnther1;~Ulrike_von_Luxburg1",
        "gender": "Not Specified;M;F",
        "homepage": "https://gunnarkoenig.com/;https://www.tml.cs.uni-tuebingen.de/team/guenther/index.php;",
        "dblp": "267/1299.html;;06/1082",
        "google_scholar": "THTbZ5EAAAAJ;;mMifMdoAAAAJ",
        "orcid": "0000-0001-6141-4942;;",
        "linkedin": ";;",
        "or_profile": "~Gunnar_K\u00f6nig1;~Eric_G\u00fcnther1;~Ulrike_von_Luxburg1",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen+T\u00fcbingen AI Center;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;University of Tuebingen",
        "aff_domain": "uni-tuebingen.de+tuebingen.ai;uni-tuebingen.de;uni-tuebingen.de",
        "position": "Postdoc+Postdoc;PhD student;Professor",
        "bibtex": "@inproceedings{\nkonig2025disentangling,\ntitle={Disentangling Interactions and Dependencies in Feature Attributions},\nauthor={Gunnar K{\\\"o}nig and Eric G{\\\"u}nther and Ulrike von Luxburg},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=BxniFq6TRd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BxniFq6TRd",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "C2E5cpaqZ5",
        "title": "New User Event Prediction Through the Lens of Causal Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modeling and analysis for event series generated by users of heterogeneous behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. \nThe most commonly adopted approach to this task is to assign users to behavior-based categories and analyze each of them separately.\nHowever, this requires extensive data to fully understand the user behavior, presenting challenges in modeling newcomers without significant historical knowledge.\nIn this work, we propose a novel discrete event prediction framework for new users with limited history, without needing to know the user's category. We treat the user event history as the \"treatment\" for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, where each event is re-weighted by its inverse propensity score.\nWe demonstrate the improved performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Henry Yuchi;Shixiang Zhu;Li Dong;Yigit M. Arisoy;Matthew C. Spencer",
        "authorids": "~Henry_Yuchi1;~Shixiang_Zhu1;~Li_Dong3;~Yigit_M._Arisoy1;~Matthew_C._Spencer1",
        "gender": "M;M;;M;M",
        "homepage": ";https://sites.google.com/view/woodyzhu;;;",
        "dblp": ";133/3853;;;",
        "google_scholar": "Tgg201wAAAAJ;v6_Gv6IAAAAJ;;J1f7GJgAAAAJ;Jt6-hJUAAAAJ",
        "orcid": ";0000-0002-2241-6096;;0000-0001-9296-4226;",
        "linkedin": ";shixiang-zhu-26b956a0/;;yigit-m-arisoy/;matthewcspencer/",
        "or_profile": "~Henry_Yuchi1;~Shixiang_Zhu1;~Li_Dong3;~Yigit_M._Arisoy1;~Matthew_C._Spencer1",
        "aff": "Los Alamos National Laboratory;Carnegie Mellon University;;Amazon;Amazon",
        "aff_domain": "lanl.gov;cmu.edu;;amazon.com;amazon.com",
        "position": "Researcher;Assistant Professor;;Researcher;Researcher",
        "bibtex": "@inproceedings{\nyuchi2025new,\ntitle={New User Event Prediction Through the Lens of Causal Inference},\nauthor={Henry Yuchi and Shixiang Zhu and Li Dong and Yigit M. Arisoy and Matthew C. Spencer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=C2E5cpaqZ5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=C2E5cpaqZ5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "C4oYj4q4rC",
        "title": "Differentiable Causal Structure Learning with Identifiability by NOTIME",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The introduction of the NOTEARS algorithm resulted in a wave of research on differentiable Directed Acyclic Graph (DAG) learning. Differentiable DAG learning transforms the combinatorial problem of identifying the DAG underlying a Structural Causal Model (SCM) into a constrained continuous optimization problem. Being differentiable, these problems can be solved using gradient-based tools which allow integration into other differentiable objectives. However, in contrast to classical constrained-based algorithms, the identifiability properties of differentiable algorithms are poorly understood. We illustrate that even in the well-known Linear Non-Gaussian Additive Model (LiNGAM), the current state-of-the-art methods do not identify the true underlying DAG. To address the issue, we propose NOTIME  (*Non-combinatorial Optimization of Trace exponential and Independence MEasures*), the first differentiable DAG learning algorithm with *provable* identifiability guarantees under the LiNGAM by building on a measure of (joint) independence. With its identifiability guarantees, NOTIME remains invariant to normalization of the data on a population level, a property lacking in existing methods. NOTIME compares favourably against NOTEARS and other (scale-invariant) differentiable DAG learners, across different noise distributions and normalization procedures. Introducing the first identifiability guarantees to general LiNGAM is an important step towards practical adoption of differentiable DAG learners.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jeroen Berrevoets;Jakob Raymaekers;Mihaela van der Schaar;Tim Verdonck;Ruicong Yao",
        "authorids": "~Jeroen_Berrevoets1;~Jakob_Raymaekers1;~Mihaela_van_der_Schaar2;~Tim_Verdonck1;~Ruicong_Yao1",
        "gender": ";;F;M;M",
        "homepage": "https://jeroenbe.github.io;;https://www.vanderschaar-lab.com;https://www.uantwerpen.be/en/staff/tim-verdonck/;",
        "dblp": "236/4591;201/6075;;95/1823;339/8800",
        "google_scholar": "https://scholar.google.be/citations?user=Bq1dFNQAAAAJ;AG-W7eUAAAAJ;DZ3S--MAAAAJ;hJ4MY3gAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0003-1105-2028;0000-0001-7581-1797",
        "linkedin": ";;;tim-verdonck-205a8a1/;",
        "or_profile": "~Jeroen_Berrevoets1;~Jakob_Raymaekers1;~Mihaela_van_der_Schaar2;~Tim_Verdonck1;~Ruicong_Yao1",
        "aff": "Ataraxis.ai;Universiteit Antwerpen;University of Cambridge+University of California, Los Angeles;Universiteit Antwerpen;KU Leuven",
        "aff_domain": "ataraxis.ai;uantwerpen.be;cam.ac.uk+ucla.edu;uantwerpen.be;kuleuven.be",
        "position": "Researcher;Assistant Professor;Full Professor+Full Professor;Full Professor;PhD student",
        "bibtex": "@inproceedings{\nberrevoets2025differentiable,\ntitle={Differentiable Causal Structure Learning with Identifiability},\nauthor={Jeroen Berrevoets and Jakob Raymaekers and Mihaela van der Schaar and Tim Verdonck and Ruicong Yao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=C4oYj4q4rC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=C4oYj4q4rC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "C4ultdMG8R",
        "title": "Differential Privacy in Distributed Learning: Beyond Uniformly Bounded Stochastic Gradients",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper explores locally differentially private distributed algorithms that solve non-convex empirical risk minimization problems. \nTraditional approaches often assume uniformly bounded stochastic gradients, which may not hold in practice. \nTo address this issue, we propose differentially **Pri**vate **S**tochastic recursive **M**omentum with gr**A**dient clipping (PriSMA) that judiciously integrates clipping and momentum to enhance utility while guaranteeing privacy. Without assuming uniformly bounded stochastic gradients, given privacy requirement $(\\epsilon,\\delta)$, PriSMA achieves a learning error of $\\tilde{\\mathcal{O}}\\big((\\frac{\\sqrt{d}}{\\sqrt{M}N\\epsilon})^\\frac{2}{5}\\big)$, where $M$ is the number of clients, $N$ is the number of data samples on each client and $d$ is the model dimension. This learning error bound is better than the state-of-the-art $\\tilde{\\mathcal{O}}\\big((\\frac{\\sqrt{d}}{{\\sqrt{M}N\\epsilon}})^\\frac{1}{3}\\big)$ in terms of the dependence on $M$ and $N$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yue Huang;Jiaojiao Zhang;Qing Ling",
        "authorids": "~Yue_Huang14;~Jiaojiao_Zhang3;~Qing_Ling2",
        "gender": ";F;",
        "homepage": ";https://jiaojiaozhang-jjz.github.io//;http://home.ustc.edu.cn/~qingling/publications.html",
        "dblp": ";;52/3617-1",
        "google_scholar": ";Y9TIpzAAAAAJ;u70vRDYAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yue_Huang14;~Jiaojiao_Zhang3;~Qing_Ling2",
        "aff": ";Great Bay University+KTH Royal Institute of Technology;SUN YAT-SEN UNIVERSITY",
        "aff_domain": ";gbu.edu.cn+kth.se;sysu.edu.cn",
        "position": ";Associate Professor+Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nhuang2025differential,\ntitle={Differential Privacy in Distributed Learning: Beyond Uniformly Bounded Stochastic Gradients},\nauthor={Yue Huang and Jiaojiao Zhang and Qing Ling},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=C4ultdMG8R}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=C4ultdMG8R",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CCmUC6oG9P",
        "title": "ClusterSC: Advancing Synthetic Control with Donor Selection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In causal inference with observational studies, synthetic control (SC) has emerged as a prominent tool. SC has traditionally been applied to aggregate-level datasets, but more recent work has extended its use to individual-level data. As they contain a greater number of observed units, this shift introduces the curse of dimensionality to SC. To address this, we propose Cluster Synthetic Control (ClusterSC), based on the idea that groups of individuals may exist where behavior aligns internally but diverges between groups. ClusterSC incorporates a clustering step to select only the relevant donors for the target. We provide theoretical guarantees on the improvements induced by ClusterSC, supported by empirical demonstrations on synthetic and real-world datasets. The results indicate that ClusterSC consistently outperforms classical SC approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Saeyoung Rho;Andrew Tang;Noah Bergam;Rachel Cummings;Vishal Misra",
        "authorids": "~Saeyoung_Rho1;~Andrew_Tang1;~Noah_Bergam1;~Rachel_Cummings1;~Vishal_Misra1",
        "gender": ";M;M;;M",
        "homepage": ";https://andrewtcr.github.io/;https://njbergam.github.io/;https://rachelcummings.com/;https://www.cs.columbia.edu/~misra/",
        "dblp": ";;;56/9841;",
        "google_scholar": ";;VQfpXAoAAAAJ;;",
        "orcid": ";;;;",
        "linkedin": ";;noah-bergam-83723a160/;;",
        "or_profile": "~Saeyoung_Rho1;~Andrew_Tang1;~Noah_Bergam1;~Rachel_Cummings1;~Vishal_Misra1",
        "aff": ";Columbia University;Columbia University;Columbia University;, Columbia University",
        "aff_domain": ";columbia.edu;columbia.edu;columbia.edu;cs.columbia.edu",
        "position": ";PhD student;PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nrho2025clustersc,\ntitle={cluster{SC}: Advancing Synthetic Control with Donor Clustering for Disaggregate-level Data},\nauthor={Saeyoung Rho and Andrew Tang and Noah Bergam and Rachel Cummings and Vishal Misra},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CCmUC6oG9P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CCmUC6oG9P",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CE6E3ZgAUd",
        "title": "Adapting to Online Distribution Shifts in Deep Learning: A Black-Box Approach",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the well-motivated problem of online distribution shift in which the data arrive in batches and the distribution of each batch can change arbitrarily over time. Since the shifts can be large or small, abrupt or gradual, the length of the relevant historical data to learn from may vary over time, which poses a major challenge in designing algorithms that can automatically adapt to the best \"attention span'' while remaining computationally efficient. We propose a meta-algorithm that takes any network architecture and any Online Learner (OL) algorithm as input and produces a new algorithm which provably enhances the performance of the given OL under non-stationarity. Our algorithm is efficient (it requires maintaining only  $O(\\log T)$ OL instances) and adaptive (it automatically chooses OL instances with the ideal \"attention'' length at every timestamp). Experiments on various real-world datasets across text and image modalities show that our method consistently improves the accuracy of user specified OL algorithms for classification tasks. Key novel algorithmic ingredients include a multi-resolution instance design inspired by wavelet theory and a cross-validation-through-time technique. Both could be of independent interest.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dheeraj Baby;Boran Han;Shuai Zhang;Cuixiong Hu;Bernie Wang;Yu-Xiang Wang",
        "authorids": "~Dheeraj_Baby1;~Boran_Han1;~Shuai_Zhang7;~Cuixiong_Hu1;~Bernie_Wang1;~Yu-Xiang_Wang1",
        "gender": ";;;M;M;",
        "homepage": "https://dheeraj-b.github.io/home/;;;;http://web.mit.edu/~ywang02/www/;http://www.cs.ucsb.edu/~yuxiangw/publications.html",
        "dblp": ";;;72/9587;43/8355-1;62/1637-3.html",
        "google_scholar": "L3YF8nIAAAAJ;;;Y3oqP5gAAAAJ;IKUm624AAAAJ;HGNZ1fkAAAAJ",
        "orcid": ";;;;0000-0002-0291-7184;",
        "linkedin": ";;;cuixiong-tony-hu-ba355a14/;;",
        "or_profile": "~Dheeraj_Baby1;~Boran_Han1;~Shuai_Zhang7;~Cuixiong_Hu1;~Bernie_Wang1;~Yu-Xiang_Wang1",
        "aff": "Amazon;;;Amazon;Amazon;University of California, San Diego",
        "aff_domain": "amazon.com;;;amazon.com;amazon.com;ucsd.edu",
        "position": "Researcher;;;Principal Engineer;Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nbaby2025adapting,\ntitle={Adapting to Online Distribution Shifts in Deep Learning: A Black-Box Approach},\nauthor={Dheeraj Baby and Boran Han and Shuai Zhang and Cuixiong Hu and Bernie Wang and Yu-Xiang Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CE6E3ZgAUd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CE6E3ZgAUd",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CTYgrczjj2",
        "title": "Importance-weighted Positive-unlabeled Learning for Distribution Shift Adaptation",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Positive and unlabeled (PU) learning is a fundamental task in many applications, which trains a binary classifier from only PU data. Existing PU learning methods typically assume that training and test distributions are identical. However, this assumption is often violated due to distribution shifts, and identifying shift types such as covariate and concept shifts is generally difficult. In this paper, we propose a distribution shift adaptation method for PU learning without assuming shift types by using a few PU data in the test distribution and PU data in the training distribution. Our method is based on the importance weighting, which learns the classifier in a principled manner by minimizing the importance-weighted training risk that approximates the test risk. Although existing methods require positive and negative data in both distributions for the importance weighting without assuming shift types, we theoretically show that it can be performed with only PU data in both distributions. Based on this finding, our neural network-based classifiers can be effectively trained by iterating the importance weight estimation and classifier learning. We show that our method outperforms various existing methods with seven real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Atsutoshi Kumagai;Tomoharu Iwata;Hiroshi Takahashi;Taishi Nishiyama;Yasuhiro Fujiwara",
        "authorids": "~Atsutoshi_Kumagai2;~Tomoharu_Iwata1;~Hiroshi_Takahashi1;~Taishi_Nishiyama1;~Yasuhiro_Fujiwara1",
        "gender": "M;M;M;M;M",
        "homepage": "https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ&hl=ja;http://www.kecl.ntt.co.jp/as/members/iwata/;https://takahashihiroshi.github.io/;;http://www.linkedin.com/in/yasuhiro-fujiwara-8960b0180",
        "dblp": "178/8630;29/5953;54/2994;;02/2520",
        "google_scholar": "https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ;S1F-gScAAAAJ;https://scholar.google.co.jp/citations?user=ncTryO4AAAAJ;https://scholar.google.jp/citations?user=xsRmpOIAAAAJ;https://scholar.google.co.jp/citations?user=kCaZaaMAAAAJ",
        "orcid": "0000-0002-2915-4615;;0000-0001-5102-2830;;0000-0001-9578-1118",
        "linkedin": ";tomoharu-iwata-025a493;;;",
        "or_profile": "~Atsutoshi_Kumagai2;~Tomoharu_Iwata1;~Hiroshi_Takahashi1;~Taishi_Nishiyama1;~Yasuhiro_Fujiwara1",
        "aff": "NTT;NTT;NTT;NTT;NTT",
        "aff_domain": "ntt.co.jp;hco.ntt.co.jp;ntt.co.jp;ntt.co.jp;ntt.co.jp",
        "position": "Researcher;Researcher;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nkumagai2025importanceweighted,\ntitle={Importance-weighted Positive-unlabeled Learning for Distribution Shift Adaptation},\nauthor={Atsutoshi Kumagai and Tomoharu Iwata and Hiroshi Takahashi and Taishi Nishiyama and Yasuhiro Fujiwara},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CTYgrczjj2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CTYgrczjj2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CTlnhuuiaP",
        "title": "Infinite Width Limits of Self Supervised Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The NTK is a widely used tool in the theoretical analysis of deep learning, allowing us to look at supervised deep neural networks through the lenses of kernel regression. Recently, several works have investigated kernel models for self-supervised learning, hypothesizing that these also shed light on the behavior of wide neural networks by virtue of the NTK. However, it remains an open question to what extent this connection is mathematically sound --- it is a commonly encountered misbelief that the kernel behavior of wide neural networks emerges irrespective of the loss function it is trained on. In this paper, we bridge the gap between the NTK and self-supervised learning, focusing on two-layer neural networks trained under the Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant as the width of the network approaches infinity. Our analysis technique is a bit different from previous works on the NTK and may be of independent interest. Overall, our work provides a first justification for the use of classic kernel theory to understand self-supervised learning of wide neural networks. Building on this result, we derive generalization error bounds for kernelized Barlow Twins and connect them to neural networks of finite width.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Maximilian Fleissner;Gautham Govind Anil;Debarghya Ghoshdastidar",
        "authorids": "~Maximilian_Fleissner1;~Gautham_Govind_Anil1;~Debarghya_Ghoshdastidar1",
        "gender": ";M;M",
        "homepage": ";;https://www.cit.tum.de/tfai/people/debarghya-ghoshdastidar/",
        "dblp": ";372/2524;63/10964",
        "google_scholar": ";;Kp-enVQAAAAJ",
        "orcid": ";;0000-0003-0202-7007",
        "linkedin": ";gautham-govind-a-8357211a7;",
        "or_profile": "~Maximilian_Fleissner1;~Gautham_Govind_Anil1;~Debarghya_Ghoshdastidar1",
        "aff": ";Google;Technical University Munich",
        "aff_domain": ";google.com;tum.de",
        "position": ";Intern;Assistant Professor",
        "bibtex": "@inproceedings{\nfleissner2025infinite,\ntitle={Infinite Width Limits of Self Supervised Neural Networks},\nauthor={Maximilian Fleissner and Gautham Govind Anil and Debarghya Ghoshdastidar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CTlnhuuiaP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CTlnhuuiaP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CUXfaqPDmY",
        "title": "Two-Timescale Linear Stochastic Approximation: Constant Stepsizes Go a Long Way",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Previous studies on two-timescale stochastic approximation (SA) mainly focused on bounding mean-squared errors under diminishing stepsize schemes. In this work, we investigate {\\it constant} stpesize schemes through the lens of Markov processes, proving that the iterates of both timescales converge to a unique joint stationary distribution in Wasserstein metric. We derive explicit geometric and non-asymptotic convergence rates, as well as the variance and bias introduced by constant stepsizes in the presence of Markovian noise. Specifically, with two constant stepsizes $\\alpha < \\beta$, we show that the biases scale linearly with both stepsizes as  $\\Theta(\\alpha)+\\Theta(\\beta)$ up to higher-order terms, while the variance of the slower iterate (resp., faster iterate) scales only with its own stepsize as $O(\\alpha)$ (resp., $O(\\beta)$). Unlike previous work, our results require no additional assumptions such as $\\beta^2 \\ll \\alpha$ nor extra dependence on dimensions. These fine-grained characterizations allow tail-averaging and extrapolation techniques to reduce variance and bias, improving mean-squared error bound to $O(\\beta^4 + \\frac{1}{t})$ for both iterates.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jeongyeol Kwon;Luke Dotson;Yudong Chen;Qiaomin Xie",
        "authorids": "~Jeongyeol_Kwon1;~Luke_Dotson1;~Yudong_Chen1;~Qiaomin_Xie1",
        "gender": "M;Non-Binary;M;F",
        "homepage": "https://kwonchungli.github.io/;;https://pages.cs.wisc.edu/~yudongchen/;https://qiaominxie.github.io/",
        "dblp": "https://dblp.uni-trier.de/pid/228/9224;;15/1975-1;37/10269",
        "google_scholar": "cnyMCYMAAAAJ;;ze5rCdwAAAAJ;RVNcy4EAAAAJ",
        "orcid": ";0009-0006-1984-4299;0000-0002-6416-5635;",
        "linkedin": ";;;",
        "or_profile": "~Jeongyeol_Kwon1;~Luke_Dotson1;~Yudong_Chen1;~Qiaomin_Xie1",
        "aff": "University of Wisconsin - Madison;University of Wisconsin - Madison;Department of Computer Sciences, University of Wisconsin - Madison;University of Wisconsin - Madison",
        "aff_domain": "wisc.edu;wisc.edu;cs.wisc.edu;wisc.edu",
        "position": "Postdoc;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkwon2025twotimescale,\ntitle={Two-Timescale Linear Stochastic Approximation: Constant Stepsizes Go a Long Way},\nauthor={Jeongyeol Kwon and Luke Dotson and Yudong Chen and Qiaomin Xie},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CUXfaqPDmY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CUXfaqPDmY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Cg1QUTpTIp",
        "title": "A Multi-Armed Bandit Approach to Online Selection and Evaluation of Generative Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing frameworks for evaluating and comparing generative models consider an offline setting, where the evaluator has access to large batches of data produced by the models. However, in practical scenarios, the goal is often to identify and select the best model using the fewest possible generated samples to minimize the costs of querying data from the sub-optimal models. In this work, we propose an online evaluation and selection framework to find the generative model that maximizes a standard assessment score among a group of available models. We view the task as a multi-armed bandit (MAB) and propose upper confidence bound (UCB) bandit algorithms to identify the model producing data with the best evaluation score that quantifies the quality and diversity of generated data. Specifically, we develop the MAB-based selection of generative models considering the Fr\u00e9chet Distance (FD) and Inception Score (IS) metrics, resulting in the FD-UCB and IS-UCB algorithms. We prove regret bounds for these algorithms and present numerical results on standard image datasets. Our empirical results suggest the efficacy of MAB approaches for the sample-efficient evaluation and selection of deep generative models. The project code is available at [https://github.com/yannxiaoyanhu/dgm-online-eval](https://github.com/yannxiaoyanhu/dgm-online-eval).",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaoyan Hu;Ho-fung Leung;Farzan Farnia",
        "authorids": "~Xiaoyan_Hu2;~Ho-fung_Leung1;~Farzan_Farnia1",
        "gender": "M;M;M",
        "homepage": "https://yannxiaoyanhu.github.io;http://www.cse.cuhk.edu.hk/~lhf/;https://www.cse.cuhk.edu.hk/~farnia/",
        "dblp": ";l/HofungLeung;132/7757",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com.hk/citations?user=JDErdKcAAAAJ;GYPCqcYAAAAJ",
        "orcid": "0000-0002-5766-1059;0000-0003-4914-2934;0000-0002-6049-9232",
        "linkedin": "xiaoyan-hu-9a26661b9/;ho-fung-leung-1a73135/;farzan-farnia-00798335",
        "or_profile": "~Xiaoyan_Hu2;~Ho-fung_Leung1;~Farzan_Farnia1",
        "aff": "Carnegie Mellon University+The Chinese University of Hong Kong;The Chinese University of Hong Kong+ ;The Chinese University of Hong Kong",
        "aff_domain": "andrew.cmu.edu+cse.cuhk.edu.hk;cuhk.edu.hk+outlook.com;cuhk.edu.hk",
        "position": "Postdoc+PhD student;Emeritus Professor+Independent Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nhu2025an,\ntitle={An Optimism-based Approach to Online Evaluation of Generative Models},\nauthor={Xiaoyan Hu and Ho-fung Leung and Farzan Farnia},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Cg1QUTpTIp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Cg1QUTpTIp",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CgJiTbwwrY",
        "title": "Gated Recurrent Neural Networks with Weighted Time-Delay Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we present a novel approach to modeling long-term dependencies in sequential data by introducing a gated recurrent unit (GRU) with a weighted time-delay feedback mechanism. Our proposed model, named $\\tau$-GRU, is a discretized version of a continuous-time formulation of a recurrent unit, where the dynamics are governed by delay differential equations (DDEs). We prove the existence and uniqueness of solutions for the continuous-time model and show that the proposed feedback mechanism can significantly improve the modeling of long-term dependencies. Our empirical results indicate that $\\tau$-GRU outperforms state-of-the-art recurrent units and gated recurrent architectures on a range of tasks, achieving faster convergence and better generalization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "N. Benjamin Erichson;Soon Hoe Lim;Michael W. Mahoney",
        "authorids": "~N._Benjamin_Erichson1;~Soon_Hoe_Lim1;~Michael_W._Mahoney1",
        "gender": "M;M;",
        "homepage": "https://www.benerichson.com/;https://shoelim.github.io/;",
        "dblp": "173/5153;268/0660;",
        "google_scholar": "https://scholar.google.co.uk/citations?user=8ViYcioAAAAJ;ufTqvyoAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~N._Benjamin_Erichson1;~Soon_Hoe_Lim1;~Michael_W._Mahoney1",
        "aff": "Lawrence Berkeley National Lab;KTH Royal Institute of Technology;",
        "aff_domain": "lbl.gov;kth.se;",
        "position": "Researcher;Assistant Professor;",
        "bibtex": "@inproceedings{\nerichson2025gated,\ntitle={Gated Recurrent Neural Networks with Weighted Time-Delay Feedback},\nauthor={N. Benjamin Erichson and Soon Hoe Lim and Michael W. Mahoney},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CgJiTbwwrY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CgJiTbwwrY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CzsjbD34bY",
        "title": "Scalable Implicit Graphon Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graphons are continuous models that represent the structure of graphs and allow the generation of graphs of varying sizes. We propose Scalable Implicit Graphon Learning (SIGL), a scalable method that combines implicit neural representations (INRs) and graph neural networks (GNNs) to estimate a graphon from observed graphs. Unlike existing methods, which face important limitations like fixed resolution and scalability issues, SIGL learns a continuous graphon at arbitrary resolutions. GNNs are used to determine the correct node ordering, improving graph alignment. Furthermore, we characterize the asymptotic consistency of our estimator, showing that more expressive INRs and GNNs lead to consistent estimators. We evaluate SIGL in synthetic and real-world graphs, showing that it outperforms existing methods and scales effectively to larger graphs, making it ideal for tasks like graph data augmentation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ali Azizpour;Nicolas Zilberstein;Santiago Segarra",
        "authorids": "~Ali_Azizpour1;~Nicolas_Zilberstein1;~Santiago_Segarra1",
        "gender": "M;M;M",
        "homepage": ";https://sites.google.com/view/nzilberstein/;http://segarra.rice.edu/",
        "dblp": ";;125/2340",
        "google_scholar": "kq9ay68AAAAJ;nXRpGgkAAAAJ;O1aSMXQAAAAJ",
        "orcid": ";;",
        "linkedin": "ali-azizpour-935315190/;;",
        "or_profile": "~Ali_Azizpour1;~Nicolas_Zilberstein1;~Santiago_Segarra1",
        "aff": "Rice University;Rice University;Rice University",
        "aff_domain": "rice.edu;rice.edu;rice.edu",
        "position": "PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nazizpour2025scalable,\ntitle={Scalable Implicit Graphon Learning},\nauthor={Ali Azizpour and Nicolas Zilberstein and Santiago Segarra},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=CzsjbD34bY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CzsjbD34bY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "D4HqXujpKA",
        "title": "On Subjective Uncertainty Quantification and Calibration in Natural Language Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Applications of large language models often involve the generation of free-form responses, in which case uncertainty quantification becomes challenging. This is due to the need to identify task-specific uncertainties (e.g., about the semantics) which appears difficult to define in general cases. This work addresses these challenges from a perspective of Bayesian decision theory, starting from the assumption that our utility is characterized by a similarity measure that compares a generated response with a hypothetical true response. We discuss how this assumption enables principled quantification of the model's subjective uncertainty and its calibration. We further derive a measure for epistemic uncertainty, based on a missing data perspective and its characterization as an excess risk. The proposed methods can be applied to black-box language models. We illustrate the methods on question answering and machine translation tasks. Our experiments provide a principled evaluation of task-specific calibration, and demonstrate that epistemic uncertainty offers a promising deferral strategy for efficient data acquisition in in-context learning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziyu Wang;Christopher C. Holmes",
        "authorids": "~Ziyu_Wang2;~Christopher_C._Holmes1",
        "gender": "Unspecified;M",
        "homepage": "http://ziyu-wang.info;",
        "dblp": "73/4689-6;08/6129",
        "google_scholar": "zMAlv2kAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ziyu_Wang2;~Christopher_C._Holmes1",
        "aff": "University of Oxford;University of Oxford",
        "aff_domain": "ox.ac.uk;ox.ac.uk",
        "position": "Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nwang2025on,\ntitle={On Subjective Uncertainty Quantification and Calibration in Natural Language Generation},\nauthor={Ziyu Wang and Christopher C. Holmes},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=D4HqXujpKA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=D4HqXujpKA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "D5FL5qjDX3",
        "title": "Is Gibbs sampling faster than Hamiltonian Monte Carlo on GLMs?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The Hamiltonian Monte Carlo (HMC) algorithm is often lauded for its ability to effectively sample from high-dimensional distributions. In this paper we challenge the presumed domination of HMC for the Bayesian analysis of GLMs. By utilizing the structure of the compute graph rather than the graphical model, we show a reduction of the time per sweep of a full-scan Gibbs sampler from $O(d^2)$ to $O(d)$, where $d$ is the number of GLM parameters. A simple change to the implementation of the Gibbs sampler allow us to perform Bayesian inference on high-dimensional GLMs that are practically infeasible with traditional Gibbs sampler implementations. We empirically demonstrate a substantial increase in effective sample size per time when comparing our Gibbs algorithms to state-of-the-art HMC algorithms. While Gibbs is superior in terms of dimension scaling, neither Gibbs nor HMC dominate the other: we provide numerical and theoretical evidence that HMC retains an edge in certain circumstances thanks to its advantageous condition number scaling. Interestingly, for GLMs of fixed data size, we observe that increasing dimensionality can stabilize or even decrease condition number, shedding light on the empirical advantage of our efficient Gibbs sampler.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Son Luu;Zuheng Xu;Nikola Surjanovic;Miguel Biron-Lattes;Trevor Campbell;Alexandre Bouchard-Cote",
        "authorids": "~Son_Luu1;~Zuheng_Xu1;~Nikola_Surjanovic1;~Miguel_Biron-Lattes1;~Trevor_Campbell1;~Alexandre_Bouchard-Cote1",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://luuquanghaison.github.io/Personal-website/;https://zuhengxu.github.io/;https://nikola-sur.netlify.app/;https://miguelbiron.github.io/;https://trevorcampbell.me;https://www.stat.ubc.ca/~bouchard/papers.html",
        "dblp": ";278/8104;346/0912;;130/3822;52/3912",
        "google_scholar": ";lkMkblkAAAAJ;wjkTE9MAAAAJ;pmEj_48AAAAJ;;",
        "orcid": ";;;0000-0003-1001-1216;;",
        "linkedin": "s%C6%A1n-l%C6%B0u-quang-h%E1%BA%A3i-1830a01a6/;zuheng-david-xu-29825624b/;;miguelbiron/;;",
        "or_profile": "~Son_Luu1;~Zuheng_Xu1;~Nikola_Surjanovic1;~Miguel_Biron-Lattes1;~Trevor_Campbell1;~Alexandre_Bouchard-Cote1",
        "aff": "University of British Columbia;University of British Columbia;University of British Columbia;Simon Fraser University;University of British Columbia;University of British Columbia",
        "aff_domain": "ubc.ca;ubc.ca;ubc.ca;sfu.ca;ubc.ca;ubc.ca",
        "position": "PhD student;PhD student;PhD student;Postdoc;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nluu2025is,\ntitle={Is Gibbs sampling faster than Hamiltonian Monte Carlo on {GLM}s?},\nauthor={Son Luu and Zuheng Xu and Nikola Surjanovic and Miguel Biron-Lattes and Trevor Campbell and Alexandre Bouchard-Cote},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=D5FL5qjDX3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=D5FL5qjDX3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DCt4v3Siky",
        "title": "Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LocalSGD and SCAFFOLD are widely used\nmethods in distributed stochastic optimization, \nwith numerous applications in machine\nlearning, large-scale data processing, and federated \nlearning. However, rigorously establishing \ntheir theoretical advantages over simpler\nmethods, such as minibatch SGD (MbSGD),\nhas proven challenging, as existing analyses\noften rely on strong assumptions, unrealistic\npremises, or overly restrictive scenarios.\n\nIn this work, we revisit the convergence properties \nof LocalSGD and SCAFFOLD under a\nvariety of existing or weaker conditions, including gradient similarity, \nHessian similarity, weak convexity, and\nLipschitz continuity of the Hessian. Our analysis \nshows that (i) LocalSGD achieves faster\nconvergence compared to MbSGD for weakly\nconvex functions without requiring stronger\ngradient similarity assumptions; (ii) LocalSGD\nbenefits significantly from higher-order similarity \nand smoothness; and (iii) SCAFFOLD\ndemonstrates faster convergence than MbSGD\nfor a broader class of non-quadratic functions.\nThese theoretical insights provide a clearer\nunderstanding of the conditions under which\nLocalSGD and SCAFFOLD outperform MbSGD.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruichen Luo;Sebastian U Stich;Samuel Horv\u00e1th;Martin Tak\u00e1\u010d",
        "authorids": "~Ruichen_Luo3;~Sebastian_U_Stich1;~Samuel_Horv\u00e1th1;~Martin_Tak\u00e1\u010d1",
        "gender": ";M;M;",
        "homepage": "https://riekenluo.github.io;https://www.sstich.ch;https://sites.google.com/view/samuelhorvath;",
        "dblp": "234/0367;04/10549;234/8604;",
        "google_scholar": ";https://scholar.google.ch/citations?user=8l-mDfQAAAAJ;k252J7kAAAAJ;",
        "orcid": ";;0000-0003-0619-9260;",
        "linkedin": ";;samuel-horvath/;",
        "or_profile": "~Ruichen_Luo3;~Sebastian_U_Stich1;~Samuel_Horv\u00e1th1;~Martin_Tak\u00e1\u010d1",
        "aff": "Institute of Science and Technology Austria;CISPA Helmholtz Center+CISPA Helmholtz Center;MBZUAI;",
        "aff_domain": "ista.ac.at;cispa.de+cispa.de;mbzuai.ac.ae;",
        "position": "PhD student;Faculty+Tenure Track Faculty;Assistant Professor;",
        "bibtex": "@inproceedings{\nluo2025revisiting,\ntitle={Revisiting Local{SGD} and {SCAFFOLD}: Improved Rates and Missing Analysis},\nauthor={Ruichen Luo and Sebastian U Stich and Samuel Horv{\\'a}th and Martin Tak{\\'a}{\\v{c}}},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DCt4v3Siky}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DCt4v3Siky",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DDeyFOIG1E",
        "title": "Tighter Confidence Bounds for Sequential Kernel Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. They are a core component in many sequential learning and decision-making algorithms, with tighter confidence bounds giving rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail inequalities to establish new confidence bounds for sequential kernel regression. Our confidence bounds can be computed by solving a conic program, although this bare version quickly becomes impractical, because the number of variables grows with the sample size. However, we show that the dual of this conic program allows us to efficiently compute tight confidence bounds. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to kernel bandit problems, and we find that when our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and comparable computational cost.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hamish Flynn;David Reeb",
        "authorids": "~Hamish_Flynn1;~David_Reeb2",
        "gender": ";M",
        "homepage": ";https://www.bosch-ai.com/about-us/our-people/",
        "dblp": ";129/1561",
        "google_scholar": ";https://scholar.google.com/citations?hl=en",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Hamish_Flynn1;~David_Reeb2",
        "aff": ";Robert Bosch GmbH, Bosch",
        "aff_domain": ";de.bosch.com",
        "position": ";Research Scientist",
        "bibtex": "@inproceedings{\nflynn2025tighter,\ntitle={Tighter Confidence Bounds for Sequential Kernel Regression},\nauthor={Hamish Flynn and David Reeb},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DDeyFOIG1E}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DDeyFOIG1E",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DEV7FwZrOt",
        "title": "AxlePro: Momentum-Accelerated Batched Training of Kernel Machines",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper we derive a novel iterative algorithm for learning kernel machines.\n    Our algorithm, $\\textsf{AxlePro}$, extends the $\\textsf{EigenPro}$ family of algorithms via momentum-based acceleration. $\\textsf{AxlePro}$ can be applied to train kernel machines with arbitrary positive semidefinite kernels. \n    We provide a convergence guarantee for the algorithm and demonstrate the speed-up of $\\textsf{AxlePro}$ over competing algorithms via numerical experiments. \n    Furthermore, we also derive a version of $\\textsf{AxlePro}$ to train large kernel models over arbitrarily large datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yiming Zhang;Parthe Pandit",
        "authorids": "~Yiming_Zhang19;~Parthe_Pandit1",
        "gender": "M;M",
        "homepage": "https://www.math.ucsd.edu/node/3130;https://parthe.github.io",
        "dblp": ";166/6545",
        "google_scholar": ";gp_Gdr8AAAAJ",
        "orcid": ";0000-0002-2524-8817",
        "linkedin": "yiming-zhang-909689221/;",
        "or_profile": "~Yiming_Zhang19;~Parthe_Pandit1",
        "aff": "University of California, San Diego;Indian Institute of Technology, Bombay",
        "aff_domain": "ucsd.edu;iitb.ac.in",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025momentum,\ntitle={Momentum Accelerated Training of Kernel Machines},\nauthor={Yiming Zhang and Parthe Pandit},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DEV7FwZrOt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DEV7FwZrOt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DJZlkYL9Jb",
        "title": "Proximal Sampler with Adaptive Step Size",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of sampling from a target unnormalized distribution $\\exp(-f(x))$ defined on $\\mathbb{R}^d$ where $f(x)$ is smooth, but the smoothness parameter is unknown. As a key design parameter of Markov chain Monte Carlo (MCMC) algorithms, the step size is crucial for the convergence guarantee. Existing non-asymptotic analysis on MCMC with fixed step sizes indicates that the step size heavily relies on global smoothness. However, this choice does not utilize the local information and fails when the smoothness coefficient is hard to estimate. A tuning-free algorithm that can adaptively update stepsize is highly desirable. In this work, we propose an \\textbf{adaptive} proximal sampler that can utilize the local geometry to adjust step sizes and is guaranteed to converge to the target distribution. Experiments demonstrate the comparable or superior performance of our algorithm over various baselines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bo Yuan;Jiaojiao Fan;Jiaming Liang;Yongxin Chen",
        "authorids": "~Bo_Yuan8;~Jiaojiao_Fan1;~Jiaming_Liang1;~Yongxin_Chen1",
        "gender": "M;F;M;M",
        "homepage": "https://flair.ae.gatech.edu/;https://sbyebss.github.io;https://jiaming-liang.github.io;https://yongxin.ae.gatech.edu/",
        "dblp": ";78/10176;;",
        "google_scholar": ";zse9JEwAAAAJ;fUBCj-0AAAAJ;X8BYiV4AAAAJ",
        "orcid": ";;;",
        "linkedin": ";jiaojiao-fan-9a1a14162/?locale=en_US;;",
        "or_profile": "~Bo_Yuan8;~Jiaojiao_Fan1;~Jiaming_Liang1;~Yongxin_Chen1",
        "aff": "Georgia Institute of Technology;NVIDIA;University of Rochester;NVIDIA+Georgia Institute of Technology",
        "aff_domain": "gatech.edu;nvidia.com;rochester.edu;nvidia.com+gatech.edu",
        "position": "PhD student;Researcher;Assistant Professor;Principal Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nyuan2025proximal,\ntitle={Proximal Sampler with Adaptive Step Size},\nauthor={Bo Yuan and Jiaojiao Fan and Jiaming Liang and Yongxin Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DJZlkYL9Jb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DJZlkYL9Jb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DOFjKvgsvE",
        "title": "Robust Kernel Hypothesis Testing under Data Corruption",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We propose a general method for constructing robust permutation tests under data corruption. The proposed tests effectively control the non-asymptotic type I error under data corruption, and we prove their consistency in power under minimal conditions. This contributes to the practical deployment of hypothesis tests for real-world applications with potential adversarial attacks. For the two-sample and independence settings, we show that our kernel robust tests are minimax optimal, in the sense that they are guaranteed to be non-asymptotically powerful against alternatives uniformly separated from the null in the kernel MMD and HSIC metrics at some optimal rate (tight with matching lower bound). We point out that existing differentially private tests can be adapted to be robust to data corruption, and we demonstrate in experiments that our proposed tests achieve much higher power than these private tests. Finally, we provide publicly available implementations and empirically illustrate the practicality of our robust tests.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Antonin Schrab;Ilmun Kim",
        "authorids": "~Antonin_Schrab1;~Ilmun_Kim1",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": ";",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\nschrab2025robust,\ntitle={Robust Kernel Hypothesis Testing under Data Corruption},\nauthor={Antonin Schrab and Ilmun Kim},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DOFjKvgsvE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DOFjKvgsvE",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DTpFxijEbo",
        "title": "Robust Offline Policy Learning with Observational Data from Multiple Sources",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of using observational bandit feedback data from multiple heterogeneous data sources to learn a personalized decision policy that robustly generalizes across diverse target settings. To achieve this, we propose a minimax regret optimization objective to ensure uniformly low regret under general mixtures of the source distributions. We develop a policy learning algorithm tailored to this objective, combining doubly robust offline policy evaluation techniques and no-regret learning algorithms for minimax optimization. Our regret analysis shows that this approach achieves the minimal worst-case mixture regret up to a moderated vanishing rate of the total data across all sources. Our analysis, extensions, and experimental results demonstrate the benefits of this approach for learning robust decision policies from multiple data sources.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aldo Gael Carranza;Susan Athey",
        "authorids": "~Aldo_Gael_Carranza1;~Susan_Athey1",
        "gender": ";F",
        "homepage": ";https://athey.people.stanford.edu/",
        "dblp": ";59/6032",
        "google_scholar": ";UdaJi94AAAAJ",
        "orcid": ";0000-0001-6934-562X",
        "linkedin": ";",
        "or_profile": "~Aldo_Gael_Carranza1;~Susan_Athey1",
        "aff": ";Stanford University",
        "aff_domain": ";stanford.edu",
        "position": ";Full Professor",
        "bibtex": "@inproceedings{\ncarranza2025robust,\ntitle={Robust Offline Policy Learning with Observational Data from Multiple Sources},\nauthor={Aldo Gael Carranza and Susan Athey},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DTpFxijEbo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DTpFxijEbo",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DZwHPyPeZO",
        "title": "Learning to Negotiate via Voluntary Commitment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The partial alignment and conflict of autonomous agents lead to mixed-motive scenarios in many real-world applications. However, agents may fail to cooperate in practice even when cooperation yields a better outcome. One well known reason for this failure comes from non-credible commitments. To facilitate commitments among agents for better cooperation, we define Markov Commitment Games (MCGs), a variant of commitment games, where agents can voluntarily commit to their proposed future plans. Based on MCGs, we propose a learnable commitment protocol via policy gradients. We further propose incentive-compatible learning to accelerate convergence to equilibria with better social welfare. Experimental results in challenging mixed-motive tasks demonstrate faster empirical convergence and higher returns for our method compared with its counterparts. Our code is available at https://github.com/shuhui-zhu/DCL.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuhui Zhu;Baoxiang Wang;Sriram Ganapathi Subramanian;Pascal Poupart",
        "authorids": "~Shuhui_Zhu1;~Baoxiang_Wang1;~Sriram_Ganapathi_Subramanian1;~Pascal_Poupart2",
        "gender": "F;;M;M",
        "homepage": ";;https://sriramsubramanian.com;https://cs.uwaterloo.ca/~ppoupart",
        "dblp": ";;217/9729;26/2122",
        "google_scholar": "https://scholar.google.ca/citations?user=mKti-YAAAAAJ;;O2jvQAYAAAAJ;https://scholar.google.ca/citations?user=KhAJWroAAAAJ",
        "orcid": "0009-0004-0154-0065;;;",
        "linkedin": "shuhui-zhu-b41873204/;;sriram-ganapathi-subramanian-7518a9a2/;",
        "or_profile": "~Shuhui_Zhu1;~Baoxiang_Wang1;~Sriram_Ganapathi_Subramanian1;~Pascal_Poupart2",
        "aff": "University of Waterloo;;Carleton University+Vector Institute;University of Waterloo",
        "aff_domain": "uwaterloo.ca;;carleton.ca+vectorinstitute.ai;uwaterloo.ca",
        "position": "PhD student;;Assistant Professor+Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nzhu2025learning,\ntitle={Learning to Negotiate via Voluntary Commitment},\nauthor={Shuhui Zhu and Baoxiang Wang and Sriram Ganapathi Subramanian and Pascal Poupart},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DZwHPyPeZO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DZwHPyPeZO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DgOFFfr2Na",
        "title": "Conditional Generative Learning from Invariant Representations in Multi-Source: Robustness and Efficiency",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-source generative models have gained significant attention due to their ability to capture complex data distributions across diverse domains. However, existing approaches often struggle with limitations such as negative transfer and an over-reliance on large pre-trained models. To address these challenges, we propose a novel method that effectively handles scenarios with outlier source domains, while making weaker assumptions about the data, thus ensuring broader applicability. Our approach enhances robustness and efficiency, supported by rigorous theoretical analysis, including non-asymptotic error bounds and asymptotic guarantees. In the experiments, we validate our methods through numerical simulations and realworld data experiments, showcasing their practical effectiveness and adaptability.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Guojun Zhu;Sanguo Zhang;Mingyang Ren",
        "authorids": "~Guojun_Zhu2;~Sanguo_Zhang1;~Mingyang_Ren2",
        "gender": "M;M;M",
        "homepage": "https://zgj19stat.github.io/;https://teacher.ucas.ac.cn/~sgzhang?language=en;https://ren-mingyang.github.io/",
        "dblp": ";;",
        "google_scholar": "xZA5QYMAAAAJ;;",
        "orcid": ";;0000-0002-8061-9940",
        "linkedin": ";;",
        "or_profile": "~Guojun_Zhu2;~Sanguo_Zhang1;~Mingyang_Ren2",
        "aff": "University of Chinese Academy of Sciences;;Shanghai Jiaotong University",
        "aff_domain": "ucas.ac.cn;;sjtu.edu.cn",
        "position": "MS student;;Assistant Professor",
        "bibtex": "@inproceedings{\nzhu2025conditional,\ntitle={Conditional Generative Learning from Invariant Representations in Multi-Source: Robustness and Efficiency},\nauthor={Guojun Zhu and Sanguo Zhang and Mingyang Ren},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DgOFFfr2Na}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DgOFFfr2Na",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DgbY2CuyhW",
        "title": "Near-Optimal Sample Complexity in Reward-Free Kernel-based Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement Learning (RL) problems are being considered under increasingly more complex structures. While tabular and linear models have been thoroughly explored, the analytical study of RL under non-linear function approximation,\n  especially kernel-based models, has recently gained traction for their strong representational capacity and theoretical tractability. In this context, we examine the question of statistical efficiency in kernel-based RL within the reward-free RL framework, specifically asking: how many samples are required to design a near-optimal policy?\n    Existing work addresses this question under restrictive assumptions about the class of kernel functions. We first explore this question assuming a generative model, then relax this assumption at the cost of increasing the sample complexity by a factor of $H$, the episode length.  We tackle this fundamental problem using a broad class of kernels and a simpler algorithm compared to prior work. Our approach derives new confidence intervals for kernel ridge regression, specific to our RL setting, that may be of broader applicability. We further validate our theoretical findings through simulations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aya Kayal;Sattar Vakili;Laura Toni;Alberto Bernacchia",
        "authorids": "~Aya_Kayal1;~Sattar_Vakili1;~Laura_Toni1;~Alberto_Bernacchia1",
        "gender": "F;;;",
        "homepage": ";https://sattar-vakili.github.io/;https://laspucl2016.com/team/laura-toni/;",
        "dblp": ";140/5473;81/7871;68/9669",
        "google_scholar": "2f2nQOQAAAAJ;N9xs8w0AAAAJ;fQ-oWKUAAAAJ;n48pFqcAAAAJ",
        "orcid": ";;0000-0002-8441-8791;",
        "linkedin": "https://linkedin.com/in/aya-kayal-793533162;sattar-vakili-221a1b63/;;",
        "or_profile": "~Aya_Kayal1;~Sattar_Vakili1;~Laura_Toni1;~Alberto_Bernacchia1",
        "aff": "University College London;MediaTek Research;University College London;MedaiTek Research",
        "aff_domain": "ucl.ac.uk;mtkresearch.com;ucl.ac.uk;mtkresearch.com",
        "position": "PhD student;Principal AI Research Manager;Associate Professor;Team Lead",
        "bibtex": "@inproceedings{\nkayal2025nearoptimal,\ntitle={Near-Optimal Sample Complexity in Reward-Free Reinforcement Learning},\nauthor={Aya Kayal and Sattar Vakili and Laura Toni and Alberto Bernacchia},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DgbY2CuyhW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DgbY2CuyhW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DobXzInjnV",
        "title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the training dynamics of deep classifiers by examining how hierarchical relationships between classes evolve during training. Through extensive experiments, we argue that the learning process in classification problems can be understood through the lens of label clustering. Specifically, we observe that networks tend to distinguish higher-level (hypernym) categories in the early stages of training, and learn more specific (hyponym) categories later. We introduce a novel framework to track the evolution of the feature manifold during training, revealing how the hierarchy of class relations emerges and refines across the network layers. Our analysis demonstrates that the learned representations closely align with the semantic structure of the dataset, providing a quantitative description of the clustering process. Notably, we show that in the hypernym label space, certain properties of neuronal collapse appear earlier than in the hyponym label space, helping to bridge the gap between the initial and terminal phases of learning. We believe our findings offer new insights into the mechanisms driving hierarchical learning in deep networks, paving the way for future advancements in understanding deep learning dynamics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Roman Malashin;Yachnaya Valeria;Alexandr V. Mullin",
        "authorids": "~Roman_Malashin1;~Yachnaya_Valeria1;~Alexandr_V._Mullin1",
        "gender": "M;;",
        "homepage": ";https://ieeexplore.ieee.org/author/37086832812;",
        "dblp": ";;",
        "google_scholar": ";;Itx828sAAAAJ",
        "orcid": "0000-0002-2493-839X;0000-0002-7239-2096;",
        "linkedin": ";;",
        "or_profile": "~Roman_Malashin1;~Yachnaya_Valeria1;~Alexandr_V._Mullin1",
        "aff": "St. Petersburg State University of Aerospace Instrumentation+Pavlov Institute of Physiology of RAS, St. Petersburg;Pavlov Institute of Physiology of RAS, St. Petersburg+St. Petersburg State University of Aerospace Instrumentation;",
        "aff_domain": "suai.ru+infran.ru;infran.ru+suai.ru;",
        "position": "Associate Professor+Researcher;Researcher+PhD student;",
        "bibtex": "@inproceedings{\nmalashin2025hypernym,\ntitle={Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy},\nauthor={Roman Malashin and Yachnaya Valeria and Alexandr V. Mullin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DobXzInjnV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DobXzInjnV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Dttmp0TPVY",
        "title": "Tensor Network Based Feature Learning Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many approximations were suggested to circumvent the cubic complexity of kernel-based algorithms, allowing their application to large-scale datasets. One strategy is to consider the primal formulation of the learning problem by mapping the data to a higher-dimensional space using tensor-product structured polynomial and Fourier features. The curse of dimensionality due to these tensor-product features was effectively solved by a tensor network reparameterization of the model parameters. \nHowever, another important aspect of model training \u2014 identifying optimal feature hyperparameters \u2014 has not been addressed and is typically handled using the standard cross-validation approach.\nIn this paper, we introduce the Feature Learning (FL) model, which addresses this issue by representing tensor-product features as a learnable Canonical Polyadic Decomposition (CPD). By leveraging this CPD structure, we efficiently learn the hyperparameters associated with different features alongside the model parameters using an Alternating Least Squares (ALS) optimization method.\nWe prove the effectiveness of the FL model through experiments on real data of various dimensionality and scale. The results show that the FL model can be consistently trained 3-5 times faster than and have the prediction quality on par with a standard cross-validated model.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Albert Saiapin;kim batselier",
        "authorids": "~Albert_Saiapin1;~kim_batselier1",
        "gender": "M;",
        "homepage": ";",
        "dblp": ";128/5511",
        "google_scholar": "https://scholar.google.ru/citations?user=k3t9iugAAAAJ;",
        "orcid": ";0000-0001-7381-2630",
        "linkedin": "albert-sayapin/;",
        "or_profile": "~Albert_Saiapin1;~kim_batselier1",
        "aff": "Delft University of Technology;Delft University of Technology",
        "aff_domain": "tudelft.nl;tudelft.nl",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nsaiapin2025tensor,\ntitle={Tensor Network Based Feature Learning Model},\nauthor={Albert Saiapin and kim batselier},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Dttmp0TPVY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Dttmp0TPVY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DuUFmnvbND",
        "title": "Credibility-Aware Multimodal Fusion Using Probabilistic Circuits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of late multimodal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multimodal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while being competitive with the state-of-the-art.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sahil Sidheekh;Pranuthi Tenali;Saurabh Mathur;Erik Blasch;Kristian Kersting;Sriraam Natarajan",
        "authorids": "~Sahil_Sidheekh3;~Pranuthi_Tenali1;~Saurabh_Mathur1;~Erik_Blasch1;~Kristian_Kersting1;~Sriraam_Natarajan1",
        "gender": "M;F;M;M;M;M",
        "homepage": "https://sahilsid.github.io/;;https://saurabhmathur96.github.io/;https://sites.google.com/site/erikblasch/;http://www.ml.informatik.tu-darmstadt.de/;http://homes.soic.indiana.edu/natarasr/",
        "dblp": "276/7995;;00/3312-2;01/4960;40/3793;19/1038",
        "google_scholar": "TlaqOIMAAAAJ;;zl-6JPMAAAAJ;Po7s1TsAAAAJ;QY-earAAAAAJ;llF8XbMAAAAJ",
        "orcid": ";;0000-0002-8604-3890;0000-0001-6894-6108;0000-0002-2873-9152;",
        "linkedin": ";pranuthitenali/;saurabhmathur96/;erik-blasch-76a0429/;;",
        "or_profile": "~Sahil_Sidheekh3;~Pranuthi_Tenali1;~Saurabh_Mathur1;~Erik_Blasch1;~Kristian_Kersting1;~Sriraam_Natarajan1",
        "aff": "The University of Texas at Dallas;University of Texas at Dallas;University of Texas at Dallas;Air Force Research Laboratory;German Research Center for AI+The Hessian Center for AI+TU Darmstadt;University of Texas at Dallas",
        "aff_domain": "cs.utdallas.edu;utdallas.edu;utdallas.edu;us.af.mil;dfki.de+hessian.ai+tu-darmstadt.de;utdallas.edu",
        "position": "PhD student;PhD student;PhD student;Principal Researcher;Full Professor+Full Professor+Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nsidheekh2025credibilityaware,\ntitle={Credibility-Aware Multimodal Fusion Using Probabilistic Circuits},\nauthor={Sahil Sidheekh and Pranuthi Tenali and Saurabh Mathur and Erik Blasch and Kristian Kersting and Sriraam Natarajan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=DuUFmnvbND}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DuUFmnvbND",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "E4IFPgNLby",
        "title": "Variation Due to Regularization Tractably Recovers Bayesian Deep Learning Uncertainty",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Uncertainty quantification in deep learning is crucial for safe and reliable decision-making in downstream tasks. Existing methods quantify uncertainty at the last layer or other approximations of the network which may miss some sources of uncertainty in the model. To address this gap, we propose an uncertainty quantification method for large networks based on variation due to regularization. Essentially, predictions that are more (less) sensitive to the regularization of network parameters are less (more, respectively) certain. This principle can be implemented by deterministically tweaking the training loss during the fine-tuning phase and reflects confidence in the output as a function of all layers of the network. We show that regularization variation (RegVar) provides rigorous uncertainty estimates that, in the infinitesimal limit, exactly recover the Laplace approximation in Bayesian deep learning. We demonstrate its success in several deep learning architectures, showing it can scale tractably with the network size while maintaining or improving uncertainty quantification quality. Our experiments across multiple datasets show that RegVar not only identifies uncertain predictions effectively but also provides insights into the stability of learned representations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "James McInerney;Nathan Kallus",
        "authorids": "~James_McInerney2;~Nathan_Kallus1",
        "gender": ";",
        "homepage": "http://jamesmc.com;http://nathankallus.com/",
        "dblp": "128/4650;142/2900",
        "google_scholar": "0rXgFbsAAAAJ;K2WfIlsAAAAJ",
        "orcid": "0009-0004-6025-5555;0000-0003-1672-0507",
        "linkedin": "jemcinerney/;",
        "or_profile": "~James_McInerney2;~Nathan_Kallus1",
        "aff": "Netflix;Netflix+Cornell University",
        "aff_domain": "netflix.com;netflix.com+cornell.edu",
        "position": "Snr Research Scientist;Research Director+Associate Professor",
        "bibtex": "@inproceedings{\nmcinerney2025variation,\ntitle={Variation Due to Regularization Tractably Recovers Bayesian Deep Learning Uncertainty},\nauthor={James McInerney and Nathan Kallus},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=E4IFPgNLby}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=E4IFPgNLby",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "EJq51evClI",
        "title": "Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Local Differential Privacy (LDP) offers strong privacy guarantees without requiring users to trust external parties. However, LDP applies uniform protection to all data features, including less sensitive ones, which degrades performance of downstream tasks. To overcome this limitation, we propose a Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that enables feature-specific privacy quantification. This more nuanced approach complements LDP by adjusting privacy protection according to the sensitivity of each feature, enabling improved performance of downstream tasks without compromising privacy. We characterize the properties of BCDP and articulate its connections with  standard non-Bayesian privacy frameworks. We further apply our BCDP framework to the problems of private mean estimation and ordinary least-squares regression. The BCDP-based approach obtains improved accuracy compared to a purely LDP-based approach, without compromising on privacy.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Maryam Aliakbarpour;Syomantak Chaudhuri;Thomas Courtade;Alireza Fallah;Michael Jordan",
        "authorids": "~Maryam_Aliakbarpour1;~Syomantak_Chaudhuri1;~Thomas_Courtade1;~Alireza_Fallah1;~Michael_Jordan1",
        "gender": "F;;M;M;M",
        "homepage": "https://maryamaliakbarpour.com;;https://people.eecs.berkeley.edu/~courtade/;https://afallah.lids.mit.edu;http://www.cs.berkeley.edu/~jordan/",
        "dblp": "175/1689;;23/7883.html;182/2522-1;j/MichaelIJordan",
        "google_scholar": "Q0crxvwAAAAJ;;https://scholar.google.com.tw/citations?user=xRmmtzIAAAAJ;2qkqvm4AAAAJ;https://scholar.google.com.tw/citations?user=yxUduqMAAAAJ",
        "orcid": "0000-0001-5064-3221;;;;0000-0001-8935-817X",
        "linkedin": ";;;;",
        "or_profile": "~Maryam_Aliakbarpour1;~Syomantak_Chaudhuri1;~Thomas_Courtade1;~Alireza_Fallah1;~Michael_Jordan1",
        "aff": "Rice University;;University of California, Berkeley;Rice University+University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "rice.edu;;berkeley.edu;rice.edu+berkeley.edu;berkeley.edu",
        "position": "Assistant Professor;;Associate Professor;Assistant Professor+Postdoc;Full Professor",
        "bibtex": "@inproceedings{\naliakbarpour2025enhancing,\ntitle={Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy},\nauthor={Maryam Aliakbarpour and Syomantak Chaudhuri and Thomas Courtade and Alireza Fallah and Michael Jordan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=EJq51evClI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EJq51evClI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "EL1l4TCUqP",
        "title": "A Family of Distributions of Random Subsets for Controlling Positive and Negative Dependence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Positive and negative dependence are fundamental concepts that characterize the attractive and repulsive behavior of random subsets. Although some probabilistic models are known to exhibit positive or negative dependence, it is challenging to seamlessly bridge them with a practicable probabilistic model. In this study, we introduce a new family of distributions, named the discrete kernel point process (DKPP), which includes determinantal point processes and parts of Boltzmann machines. We also develop some computational methods for probabilistic operations and inference with DKPPs, such as calculating marginal and conditional probabilities and learning the parameters. Our numerical experiments demonstrate the controllability of positive and negative dependence and the effectiveness of the computational methods for DKPPs.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Takahiro Kawashima;Hideitsu Hino",
        "authorids": "~Takahiro_Kawashima1;~Hideitsu_Hino2",
        "gender": "M;M",
        "homepage": "https://wasyro.github.io/;",
        "dblp": ";49/5462",
        "google_scholar": "https://scholar.google.co.jp/citations?user=eo5JcAUAAAAJ;",
        "orcid": "0000-0003-4736-3725;0000-0002-6405-4361",
        "linkedin": ";",
        "or_profile": "~Takahiro_Kawashima1;~Hideitsu_Hino2",
        "aff": "ZOZO Research;The Institute of Statistical Mathematics, Japan",
        "aff_domain": "zozo.com;ism.ac.jp",
        "position": "Researcher;Professor",
        "bibtex": "@inproceedings{\nkawashima2025a,\ntitle={A Family of Distributions of Random Subsets for Controlling Positive and Negative Dependence},\nauthor={Takahiro Kawashima and Hideitsu Hino},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=EL1l4TCUqP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EL1l4TCUqP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ENscW7Y0lr",
        "title": "Semiparametric conformal prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many risk-sensitive applications require well-calibrated prediction sets over multiple, potentially correlated target variables, for which the prediction algorithm may report correlated errors. In this work, we aim to construct the conformal prediction set accounting for the joint correlation structure of the vector-valued non-conformity scores. Drawing from the rich literature on multivariate quantiles and semiparametric statistics, we propose an algorithm to estimate the $1-\\alpha$ quantile of the scores, where $\\alpha$ is the user-specified miscoverage rate. In particular, we flexibly estimate the joint cumulative distribution function (CDF) of the scores using nonparametric vine copulas and improve the asymptotic efficiency of the quantile estimate using its influence function. The vine decomposition allows our method to scale well to a large number of targets. As well as guaranteeing asymptotically exact coverage, our method yields desired coverage and competitive efficiency on a range of real-world regression problems, including those with missing-at-random labels in the calibration set.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ji Won Park;Kyunghyun Cho",
        "authorids": "~Ji_Won_Park1;~Kyunghyun_Cho1",
        "gender": "F;M",
        "homepage": ";http://kyunghyuncho.me",
        "dblp": "83/10554;41/9736",
        "google_scholar": "URG3MMYAAAAJ;https://scholar.google.fi/citations?user=0RAmmIAAAAAJ",
        "orcid": "0000-0002-0692-1092;",
        "linkedin": ";",
        "or_profile": "~Ji_Won_Park1;~Kyunghyun_Cho1",
        "aff": "Genentech;Genentech+New York University",
        "aff_domain": "gene.com;gene.com+nyu.edu",
        "position": "Researcher;Executive Director of Frontier Research+Professor",
        "bibtex": "@inproceedings{\npark2025semiparametric,\ntitle={Semiparametric conformal prediction},\nauthor={Ji Won Park and Kyunghyun Cho},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ENscW7Y0lr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ENscW7Y0lr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "EYhGcfYpFS",
        "title": "Differentially Private Continual Release of Histograms and Related Queries",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study privately releasing column sums of a $d$-dimensional table with entries from a universe $\\chi$ undergoing $T$ row updates, called histogram under continual release. Our mechanisms give better additive $\\ell_\\infty$-error than existing mechanisms for a large class of queries and input streams.\n\nOur first contribution is an output-sensitive mechanism in the insertions-only model ($\\chi = \\\\{0,1\\\\}$) for maintaining (i) the histogram or (ii) queries that do not require maintaining the entire histogram, such as the maximum or minimum column sum, the median, or any quantiles.\nThe mechanism has an additive error of $O(d\\log^2 (dq^*)+\\log T)$ whp, where $q^*$ is the maximum output value over all time steps on this dataset. The mechanism does not require $q^*$ as input. This breaks the $\\Omega(d \\log T)$ bound of prior work when $q^* \\ll T$.\n\nOur second contribution is a mechanism for the turnstile model that admits negative entry updates ($\\chi = \\\\{-1, 0,1\\\\}$). This mechanism has an additive error of $O(d \\log^2 (dK) + \\log T)$ whp, where $K$ is the number of times two consecutive data rows differ, and the mechanism does not require $K$ as input. This is useful when monitoring inputs that only vary under unusual circumstances. For $d=1$ this gives the first private mechanism with error $O(\\log^2 K + \\log T)$ for continual counting in the turnstile model, improving on the $O(\\log^2 n + \\log T)$ error bound by Dwork, Naor, Reingold, Rothblum (ASIACRYPT 2015), where $n$ is the number of ones in the stream, as well as allowing negative entries, while Dwork et al. (2015) can only handle nonnegative entries ($\\chi=\\\\{0,1\\\\}$).",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Monika Henzinger;A. R. Sricharan;Teresa Anna Steiner",
        "authorids": "~Monika_Henzinger1;~A._R._Sricharan1;~Teresa_Anna_Steiner1",
        "gender": ";M;F",
        "homepage": ";https://arsricharan.in/;http://people.compute.dtu.dk/terst/",
        "dblp": ";281/6794;235/2633",
        "google_scholar": "NXbggxYAAAAJ;;",
        "orcid": ";;0000-0003-1078-4075",
        "linkedin": ";;",
        "or_profile": "~Monika_Henzinger1;~A._R._Sricharan1;~Teresa_Anna_Steiner1",
        "aff": "Institute of Science and Technology;Universit\u00e4t Vienna;",
        "aff_domain": "ist.ac.at;univie.ac.at;",
        "position": "Full Professor;PhD student;",
        "bibtex": "@inproceedings{\nhenzinger2025differentially,\ntitle={Differentially Private Continual Release of Histograms and Related Queries},\nauthor={Monika Henzinger and A. R. Sricharan and Teresa Anna Steiner},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=EYhGcfYpFS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EYhGcfYpFS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Efffg6uSeF",
        "title": "Type Information-Assisted Self-Supervised Knowledge Graph Denoising",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Knowledge graphs serve as critical resources supporting intelligent systems, but they can be noisy due to imperfect automatic generation processes. Existing approaches to noise detection often rely on external facts, logical rule constraints, or structural embeddings. These methods are often challenged by imperfect entity alignment, flexible knowledge graph construction, and overfitting on structures. In this paper, we propose to exploit the consistency between entity and relation type information for noise detection, resulting a novel self-supervised knowledge graph denoising method that avoids those problems. We formalize \\textit{type inconsistency} noise as triples that deviate from the majority with respect to type-dependent reasoning along the topological structure. Specifically, we first extract a compact representation of a given knowledge graph via an encoder that models the type dependencies of triples. Then, the decoder reconstructs the original input knowledge graph based on the compact representation. It is worth noting that, our proposal has the potential to address the problems of knowledge graph compression and completion, although this is not our focus. For the specific task of noise detection, the discrepancy between the reconstruction results and the input knowledge graph provides an opportunity for denoising, which is facilitated by the type consistency embedded in our method. Experimental validation demonstrates the effectiveness of our approach in detecting potential noise in real-world data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiaqi Sun;Yujia Zheng;Xinshuai Dong;Haoyue Dai;Kun Zhang",
        "authorids": "~Jiaqi_Sun1;~Yujia_Zheng1;~Xinshuai_Dong1;~Haoyue_Dai1;~Kun_Zhang1",
        "gender": ";M;M;;M",
        "homepage": ";https://yjzheng.com;https://dongxinshuai.github.io/;https://hyda.cc;http://www.andrew.cmu.edu/user/kunz1/",
        "dblp": ";245/6109-1.html;279/6151.html;277/1316;96/3115-1",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=ioiW248AAAAJ;A7JyL1sAAAAJ;f4tCtoMAAAAJ;RGoypN4AAAAJ",
        "orcid": ";0009-0003-5225-6366;;;",
        "linkedin": ";;;;",
        "or_profile": "~Jiaqi_Sun1;~Yujia_Zheng1;~Xinshuai_Dong1;~Haoyue_Dai1;~Kun_Zhang1",
        "aff": ";Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Mohamed bin Zayed University of Artificial Intelligence+Carnegie Mellon University",
        "aff_domain": ";cmu.edu;cmu.edu;cmu.edu;mbzuai.ac.ae+cmu.edu",
        "position": ";PhD student;PhD student;PhD student;Professor+Associate Professor",
        "bibtex": "@inproceedings{\nsun2025type,\ntitle={Type Information-Assisted Self-Supervised Knowledge Graph Denoising},\nauthor={Jiaqi Sun and Yujia Zheng and Xinshuai Dong and Haoyue Dai and Kun Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Efffg6uSeF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Efffg6uSeF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "EkO8rb3liX",
        "title": "Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications. While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited. In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates and smooth log-concave data distributions. We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers. Our proof technique relies on spelling out explicitly the contraction rate for the continuous-time ODE and analyzing the discretization and score-matching errors by using synchronous coupling; the challenge in our analysis mainly arises from the inherent non-autonomy of the probability flow ODE and the specific exponential integrator that we study.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xuefeng Gao;Lingjiong Zhu",
        "authorids": "~Xuefeng_Gao1;~Lingjiong_Zhu1",
        "gender": ";M",
        "homepage": "http://www1.se.cuhk.edu.hk/~xfgao/;",
        "dblp": ";178/6958",
        "google_scholar": ";Z9JkFaoAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Xuefeng_Gao1;~Lingjiong_Zhu1",
        "aff": "The Chinese University of Hong Kong;Florida State University",
        "aff_domain": "cuhk.edu.hk;fsu.edu",
        "position": "Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\ngao2025convergence,\ntitle={Convergence Analysis for General Probability Flow {ODE}s of Diffusion Models in Wasserstein Distances},\nauthor={Xuefeng Gao and Lingjiong Zhu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=EkO8rb3liX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EkO8rb3liX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ElvhiUFA02",
        "title": "A Theoretical Framework for Preventing Class Collapse in Supervised Contrastive Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Supervised contrastive learning (SupCL) has emerged as a prominent approach in representation learning, leveraging both supervised and self-supervised losses. However, achieving an optimal balance between these losses is challenging; failing to do so can lead to class collapse, reducing discrimination among individual embeddings in the same class. In this paper, we present theoretically grounded guidelines for SupCL to prevent class collapse in learned representations. Specifically, we introduce the Simplex-to-Simplex Embedding Model (SSEM), a theoretical framework that models various embedding structures, including all embeddings that minimize the supervised contrastive loss. Through SSEM, we analyze how hyperparameters affect learned representations, offering practical guidelines for hyperparameter selection to mitigate the risk of class collapse. Our theoretical findings are supported by empirical results across synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chungpa Lee;Jeongheon Oh;Kibok Lee;Jy-yong Sohn",
        "authorids": "~Chungpa_Lee1;~Jeongheon_Oh1;~Kibok_Lee1;~Jy-yong_Sohn1",
        "gender": "M;;M;M",
        "homepage": "https://www.chungpa.com/;https://github.com/alan2013-github;https://ml.yonsei.ac.kr/;https://itml.yonsei.ac.kr/professor",
        "dblp": ";;157/3147-3.html;188/6303",
        "google_scholar": "N2S3jFcAAAAJ;;6wwWRdEAAAAJ;https://scholar.google.co.kr/citations?user=Cs75s1MAAAAJ",
        "orcid": "0009-0009-9915-3173;;0000-0001-6995-7327;",
        "linkedin": ";;;",
        "or_profile": "~Chungpa_Lee1;~Jeongheon_Oh1;~Kibok_Lee1;~Jy-yong_Sohn1",
        "aff": "Yonsei University;;Yonsei University;Yonsei University",
        "aff_domain": "yonsei.ac.kr;;yonsei.ac.kr;yonsei.ac.kr",
        "position": "PhD student;;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025a,\ntitle={A Theoretical Framework for Preventing Class Collapse in Supervised Contrastive Learning},\nauthor={Chungpa Lee and Jeongheon Oh and Kibok Lee and Jy-yong Sohn},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ElvhiUFA02}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ElvhiUFA02",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Epaf0FSr9F",
        "title": "Mean-Field Microcanonical Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of stationary time series and 2D textures.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Marcus H\u00e4ggbom;Morten Karlsmark;Joakim And\u00e9n",
        "authorids": "~Marcus_H\u00e4ggbom1;~Morten_Karlsmark1;~Joakim_And\u00e9n1",
        "gender": ";M;M",
        "homepage": "https://www.kth.se/profile/haggbo;https://seb.se;",
        "dblp": ";;98/10570",
        "google_scholar": ";;https://scholar.google.com/citations?hl=sv",
        "orcid": ";;0000-0002-3377-813X",
        "linkedin": ";;",
        "or_profile": "~Marcus_H\u00e4ggbom1;~Morten_Karlsmark1;~Joakim_And\u00e9n1",
        "aff": "KTH Royal Institute of Technology;SEB;KTH Royal Institute of Technology, Stockholm, Sweden",
        "aff_domain": "kth.se;seb.se;kth.se",
        "position": "PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nhaggbom2025meanfield,\ntitle={Mean-Field Microcanonical Gradient Descent},\nauthor={Marcus H{\\\"a}ggbom and Morten Karlsmark and Joakim And{\\'e}n},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Epaf0FSr9F}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Epaf0FSr9F",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FBgjx0Ix2U",
        "title": "Behavior-Inspired Neural Networks for Relational Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "From pedestrians to Kuramoto oscillators, interactions between agents govern how dynamical systems evolve in space and time. Discovering how these agents relate to each other has the potential to improve our understanding of the often complex dynamics that underlie these systems. Recent works learn to categorize relationships between agents based on observations of their physical behavior. These approaches model relationship categories as outcomes of a categorical distribution which is limiting and contrary to real-world systems, where relationship categories often intermingle and interact. In this work, we introduce a level of abstraction between the observable behavior of agents and the latent categories that determine their behavior. To do this, we learn a mapping from agent observations to agent preferences for a set of latent categories. The learned preferences and inter-agent proximity are integrated in a nonlinear opinion dynamics model, which allows us to naturally identify mutually exclusive categories, predict an agent's evolution in time, and control an agent's behavior. Through extensive experiments, we demonstrate the utility of our model for learning interpretable categories, and the efficacy of our model for long-horizon trajectory prediction.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yulong Yang;Bowen Feng;Keqin Wang;Naomi Leonard;Adji Bousso Dieng;Christine Allen-Blanchette",
        "authorids": "~Yulong_Yang2;~Bowen_Feng1;~Keqin_Wang1;~Naomi_Leonard1;~Adji_Bousso_Dieng1;~Christine_Allen-Blanchette1",
        "gender": "M;M;M;F;F;",
        "homepage": ";https://fengb2-coder.github.io/;;https://www.princeton.edu/~naomi/;https://vertaix.princeton.edu/;",
        "dblp": "33/1008-3;;;;188/6478;164/8339",
        "google_scholar": "cfOgydoAAAAJ;;;;ZCniP_MAAAAJ;0BbFHcEAAAAJ",
        "orcid": "0000-0002-5075-6768;;;;0000-0001-5687-3554;0000-0002-5396-7974",
        "linkedin": "yulong-yang-b273b219a/;;wang-keqin-1b284618a/;;diengadji45;christine-allen-blanchette-a3467b12/",
        "or_profile": "~Yulong_Yang2;~Bowen_Feng1;~Keqin_Wang1;~Naomi_Leonard1;~Adji_Bousso_Dieng1;~Christine_Allen-Blanchette1",
        "aff": "Princeton University;Princeton University;Princeton University;Princeton University;Princeton University;Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "position": "PhD student;PhD student;PhD student;Full Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025behaviorinspired,\ntitle={Behavior-Inspired Neural Networks for Relational Inference},\nauthor={Yulong Yang and Bowen Feng and Keqin Wang and Naomi Leonard and Adji Bousso Dieng and Christine Allen-Blanchette},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FBgjx0Ix2U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FBgjx0Ix2U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FCczPIFPD6",
        "title": "Scalable Inference for Bayesian Multinomial Logistic-Normal Dynamic Linear Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many scientific fields collect longitudinal count compositional data. Each observation is a multivariate count vector, where the total counts are arbitrary, and the information lies in the relative frequency of the counts. Multiple authors have proposed Bayesian Multinomial Logistic-Normal Dynamic Linear Models (MLN-DLMs) as a flexible approach to modeling these data. However,  adoption of these methods has been limited by computational challenges. This article develops an efficient and accurate approach to posterior state estimation, called Fenrir. Our approach relies on a novel algorithm for MAP estimation and an accurate approximation to a key posterior marginal of the model. As there are no equivalent methods against which we can compare, we also develop an optimized Stan implementation of MLN-DLMs. Our experiments suggest that Fenrir can be three orders of magnitude more efficient than Stan and can even be incorporated into larger sampling schemes for joint inference of model hyperparameters. Our methods are made available to the community as a user-friendly software library written in C++ with an R interface.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Manan Saxena;Tinghua Chen;Justin D Silverman",
        "authorids": "~Manan_Saxena1;~Tinghua_Chen1;~Justin_D_Silverman1",
        "gender": "M;F;M",
        "homepage": "https://manansaxena.github.io/;https://www.tinghua-chen.github.io;http://www.justin-silverman.com/",
        "dblp": ";;",
        "google_scholar": "2fzUb4cAAAAJ;;V8RmiSAAAAAJ%26hl",
        "orcid": ";;0000-0002-3063-2098",
        "linkedin": "manansaxena05/;tinghua-chen-3b944816b/;",
        "or_profile": "~Manan_Saxena1;~Tinghua_Chen1;~Justin_D_Silverman1",
        "aff": "Pennsylvania State University;Pennsylvania State University;Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;psu.edu",
        "position": "Researcher;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nsaxena2025scalable,\ntitle={Scalable Inference for Bayesian Multinomial Logistic-Normal Dynamic Linear Models},\nauthor={Manan Saxena and Tinghua Chen and Justin D Silverman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FCczPIFPD6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FCczPIFPD6",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FDUfAcAVjO",
        "title": "The Size of Teachers as a Measure of Data Complexity: PAC-Bayes Excess Risk Bounds and Scaling Laws",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the generalization properties of neural networks through the lens of data complexity.  Recent work by Buzaglo et al. (2024) shows that random (nearly) interpolating networks generalize, provided there is a small \"teacher\" network that achieves small excess risk. \nWe give a short single-sample PAC-Bayes proof of this result and an analogous \"fast-rate\" result for random samples from Gibbs posteriors. The resulting oracle inequality motivates a new notion of data complexity, based on the minimal size of a teacher network required to achieve any given level of excess risk. We show that polynomial data complexity gives rise to power laws connecting risk to the number of training samples, like in empirical neural scaling laws. By comparing the \"scaling laws\" resulting from our bounds to those observed in empirical studies, we provide evidence for lower bounds on the data complexity of standard benchmarks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gintare Karolina Dziugaite;Daniel M. Roy",
        "authorids": "~Gintare_Karolina_Dziugaite1;~Daniel_M._Roy1",
        "gender": "F;",
        "homepage": "http://gkdz.org/;",
        "dblp": "163/1774;",
        "google_scholar": "5K1QB_8AAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Gintare_Karolina_Dziugaite1;~Daniel_M._Roy1",
        "aff": "McGill University+Google DeepMind+Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal;",
        "aff_domain": "mcgill.ca+google.com+mila.umontreal.ca;",
        "position": "Adjunct Professor+Senior Researcher+Member;",
        "bibtex": "@inproceedings{\ndziugaite2025the,\ntitle={The Size of Teachers as a Measure of Data Complexity: {PAC}-Bayes Excess Risk Bounds and Scaling Laws},\nauthor={Gintare Karolina Dziugaite and Daniel M. Roy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FDUfAcAVjO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FDUfAcAVjO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FFZ60lEExR",
        "title": "Robust Score Matching",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Proposed in Hyv\u00e4rinen (2005), score matching is a statistical estimation procedure that does not require computation of distributional normalizing constants. In this work we utilize the geometric median of means to develop a robust score matching procedure that yields consistent parameter estimates in settings where the observed data has been contaminated. A special appeal of the proposed method is that it retains convexity in exponential family models. The new method is therefore particularly attractive for non-Gaussian, exponential family graphical models where evaluation of normalizing constants is intractable. Support recovery guarantees for such models when contamination is present are provided. Additionally, support recovery is studied in numerical experiments and on a precipitation dataset. We demonstrate that the proposed robust score matching estimator performs comparably to the standard score matching estimator when no contamination is present but greatly outperforms this estimator in a setting with contamination.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Richard Schwank;Andrew McCormack;Mathias Drton",
        "authorids": "~Richard_Schwank1;~Andrew_McCormack1;~Mathias_Drton2",
        "gender": ";;M",
        "homepage": ";https://andymcmck.github.io/;https://www.math.cit.tum.de/en/math/people/professors/drton-mathias/",
        "dblp": ";;78/3067",
        "google_scholar": ";qu3sRLoAAAAJ;CjRMyA4AAAAJ",
        "orcid": "0009-0004-0956-3106;;0000-0001-5614-3025",
        "linkedin": ";;",
        "or_profile": "~Richard_Schwank1;~Andrew_McCormack1;~Mathias_Drton2",
        "aff": "Technische Universit\u00e4t M\u00fcnchen;University of Alberta;Technische Universit\u00e4t M\u00fcnchen",
        "aff_domain": "tum.de;ualberta.ca;tum.de",
        "position": "PhD student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nschwank2025robust,\ntitle={Robust Score Matching},\nauthor={Richard Schwank and Andrew McCormack and Mathias Drton},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FFZ60lEExR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FFZ60lEExR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FI1Q9Bi4rH",
        "title": "Cubic regularized subspace Newton for non-convex optimization",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "This paper addresses the optimization problem of minimizing non-convex continuous functions,\na problem highly relevant in high-dimensional machine learning scenarios, particularly those involving over-parameterization. We analyze a randomized coordinate second-order method named SSCN, which can be interpreted as applying the cubic regularization of Newton's method in random subspaces. This approach effectively reduces the computational complexity associated with utilizing second-order information, making it applicable in higher-dimensional scenarios.\nTheoretically, we establish strong global convergence guarantees for non-convex functions to a stationary point, with interpolating rates for arbitrary subspace sizes and\nallowing inexact curvature estimation, starting from an arbitrary initialization.\nWhen increasing the subspace size, our complexity matches the $\\mathcal{O}(\\epsilon^{-3/2})$ \nrate of the full Newton's method with cubic regularization.\nAdditionally, we propose an adaptive sampling scheme ensuring the exact convergence rate of $\\mathcal{O}(\\epsilon^{-3/2}, \\epsilon^{-3})$ to a second-order stationary point, without requiring to sample all coordinates.\nExperimental results demonstrate substantial speed-ups achieved by SSCN \ncompared to conventional first-order methods and other second-order subspace methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jim Zhao;Nikita Doikov;Aurelien Lucchi",
        "authorids": "~Jim_Zhao2;~Nikita_Doikov1;~Aurelien_Lucchi1",
        "gender": "M;;M",
        "homepage": "https://jydzh.altervista.org;https://doikov.com;http://people.inf.ethz.ch/alucchi/",
        "dblp": ";222/9897;14/5780",
        "google_scholar": "GAxXyUUAAAAJ;YNBhhjUAAAAJ;https://scholar.google.ch/citations?user=V1ONSgIAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Jim_Zhao2;~Nikita_Doikov1;~Aurelien_Lucchi1",
        "aff": "University of Basel;EPFL - EPF Lausanne;University of Basel",
        "aff_domain": "unibas.ch;epfl.ch;unibas.ch",
        "position": "PhD student;Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nzhao2025cubic,\ntitle={Cubic regularized subspace Newton for non-convex optimization},\nauthor={Jim Zhao and Nikita Doikov and Aurelien Lucchi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FI1Q9Bi4rH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FI1Q9Bi4rH",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FdYA56Gcsj",
        "title": "On the Geometry and Optimization of Polynomial Convolutional Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study convolutional neural networks with monomial activation functions. Specifically, we prove that their parameterization map is regular and is an isomorphism almost everywhere, up to rescaling the filters. By leveraging on tools from algebraic geometry, we explore the geometric properties of the image in function space of this map -- typically referred to as neuromanifold. In particular, we compute the dimension and the degree of the neuromanifold, which measure the expressivity of the model, and describe its singularities. Moreover, for a generic large dataset, we derive an explicit formula that quantifies the number of critical points arising in the optimization of a regression loss.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vahid Shahverdi;Giovanni Luca Marchetti;Kathl\u00e9n Kohn",
        "authorids": "~Vahid_Shahverdi1;~Giovanni_Luca_Marchetti1;~Kathl\u00e9n_Kohn1",
        "gender": ";M;F",
        "homepage": "https://sites.google.com/view/vahidshahverdi/;https://www.kth.se/profile/glma;https://kathlenkohn.github.io",
        "dblp": ";310/4949;167/4249",
        "google_scholar": "VxL55WMAAAAJ;ePYa2qAAAAAJ;jk2kwWIAAAAJ",
        "orcid": "0009-0005-2619-9198;;0000-0002-4627-8812",
        "linkedin": "vahid-shahverdi-25277436/?originalSubdomain=se;;",
        "or_profile": "~Vahid_Shahverdi1;~Giovanni_Luca_Marchetti1;~Kathl\u00e9n_Kohn1",
        "aff": "KTH Royal Institute of Technology;KTH Royal Institute of Technology;KTH Royal Institute of Technology",
        "aff_domain": "kth.se;kth.se;kth.se",
        "position": "PhD student;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nshahverdi2025on,\ntitle={On the Geometry and Optimization of Polynomial Convolutional Networks},\nauthor={Vahid Shahverdi and Giovanni Luca Marchetti and Kathl{\\'e}n Kohn},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FdYA56Gcsj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FdYA56Gcsj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Ffb2Gt6p3U",
        "title": "Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Latent variable models are powerful tools for learning low-dimensional manifolds from high-dimensional data. However, when dealing with constrained data such as unit-norm vectors or symmetric positive-definite matrices, existing approaches ignore the underlying geometric constraints or fail to provide meaningful metrics in the latent space. To address these limitations, we propose to learn Riemannian latent representations of such geometric data. To do so, we estimate the pullback metric induced by a Wrapped Gaussian Process Latent Variable Model, which explicitly accounts for the data geometry. This enables us to define geometry-aware notions of distance and shortest paths in the latent space, while ensuring that our model only assigns probability mass to the data manifold. This generalizes previous work and allows us to handle complex tasks in various domains, including robot motion synthesis and analysis of brain connectomes.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Leonel Rozo;Miguel Gonz\u00e1lez-Duque;No\u00e9mie Jaquier;S\u00f8ren Hauberg",
        "authorids": "~Leonel_Rozo1;~Miguel_Gonz\u00e1lez-Duque3;~No\u00e9mie_Jaquier1;~S\u00f8ren_Hauberg1",
        "gender": ";;;M",
        "homepage": ";;;http://www2.compute.dtu.dk/~sohau/",
        "dblp": ";;;39/7226",
        "google_scholar": ";;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Leonel_Rozo1;~Miguel_Gonz\u00e1lez-Duque3;~No\u00e9mie_Jaquier1;~S\u00f8ren_Hauberg1",
        "aff": ";;;Technical University of Denmark",
        "aff_domain": ";;;dtu.dk",
        "position": ";;;Professor",
        "bibtex": "@inproceedings{\nrozo2025riemann,\ntitle={Riemann\\${\\textasciicircum}2\\$: Learning Riemannian Submanifolds from Riemannian Data},\nauthor={Leonel Rozo and Miguel Gonz{\\'a}lez-Duque and No{\\'e}mie Jaquier and S{\\o}ren Hauberg},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Ffb2Gt6p3U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ffb2Gt6p3U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Fht2sx1h2t",
        "title": "Signal Recovery from Random Dot-Product Graphs under Local Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of recovering latent information from graphs under $\\varepsilon$-edge local differential privacy where the presence of relationships/edges between two users/vertices remains confidential, even from the data curator. For the class of generalized random dot-product graphs, we show that a standard local differential privacy mechanism induces a specific geometric distortion in the latent positions. Leveraging this insight, we show that consistent recovery of the latent positions is achievable by appropriately adjusting the statistical inference procedure for the privatized graph. Furthermore, we prove that our procedure is nearly minimax-optimal under local edge differential privacy constraints. Lastly, we show that this framework allows for consistent recovery of geometric and topological information underlying the latent positions, as encoded in their persistence diagrams. Our results extend previous work from the private community detection literature to a substantially richer class of models and inferential tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siddharth Vishwanath;Jonathan Hehir",
        "authorids": "~Siddharth_Vishwanath1;research@jonhehir.com",
        "gender": ";",
        "homepage": "https://sidvishwanath.com;",
        "dblp": ";",
        "google_scholar": "7TQaHEEAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Siddharth_Vishwanath1;research@jonhehir.com",
        "aff": "University of California, San Diego;",
        "aff_domain": "ucsd.edu;",
        "position": "Postdoc;",
        "bibtex": "@inproceedings{\nvishwanath2025signal,\ntitle={Signal Recovery from Random Dot-Product Graphs under Local Differential Privacy},\nauthor={Siddharth Vishwanath and Jonathan Hehir},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Fht2sx1h2t}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Fht2sx1h2t",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FprYRzUziT",
        "title": "M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "With the widespread availability of sensor data across industrial and operational systems, we frequently encounter heterogeneous time series from multiple systems. Anomaly detection is crucial for such systems to facilitate predictive maintenance. However, most existing anomaly detection methods are designed for either univariate or single-system multivariate data, making them insufficient for these complex scenarios. To address this, we introduce M$^2$AD, a framework for unsupervised anomaly detection in multivariate time series data from multiple systems. M$^2$AD employs deep models to capture expected behavior under normal conditions, using the residuals as indicators of potential anomalies. These residuals are then aggregated into a global anomaly score through a Gaussian Mixture Model and Gamma calibration. We theoretically demonstrate that this framework can effectively address heterogeneity and dependencies across sensors and systems. Empirically, M$^2$AD outperforms existing methods in extensive evaluations by 21% on average, and its effectiveness is demonstrated on a large-scale real-world case study on 130 assets in Amazon Fulfillment Centers. Our code and results are available at https://github.com/sarahmish/M2AD.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sarah Alnegheimish;Zelin He;Matthew Reimherr;Akash Chandrayan;Abhinav Pradhan;Luca D'Angelo",
        "authorids": "~Sarah_Alnegheimish1;~Zelin_He2;~Matthew_Reimherr1;~Akash_Chandrayan1;~Abhinav_Pradhan1;~Luca_D'Angelo1",
        "gender": ";Not Specified;;M;M;M",
        "homepage": "https://sarahmish.github.io;;https://www.personal.psu.edu/~mlr36;;;",
        "dblp": "274/6988;;187/4282;;;",
        "google_scholar": "wleaufEAAAAJ;cqhRdPoAAAAJ;UZcbx9gAAAAJ;https://scholar.google.com/citations?hl=en;;",
        "orcid": "0000-0003-4516-5856;;0000-0002-7149-0591;;;",
        "linkedin": ";zelin-he/;;;abhinav-pradhan-442a9414/;lucadangelo83",
        "or_profile": "~Sarah_Alnegheimish1;~Zelin_He2;~Matthew_Reimherr1;~Akash_Chandrayan1;~Abhinav_Pradhan1;~Luca_D'Angelo1",
        "aff": "Massachusetts Institute of Technology;Pennsylvania State University;Pennsylvania State University+Amazon;Amazon;Amazon;",
        "aff_domain": "mit.edu;psu.edu;psu.edu+amazon.com;amazon.com;amazon.com;",
        "position": "PhD student;PhD student;Researcher+Principal Researcher;Researcher;Researcher;",
        "bibtex": "@inproceedings{\nalnegheimish2025mad,\ntitle={M\\${\\textasciicircum}2\\${AD}: Detecting Anomalies in Heterogeneous Multivariate Time Series from Multiple Systems},\nauthor={Sarah Alnegheimish and Zelin He and Matthew Reimherr and Akash Chandrayan and Abhinav Pradhan and Luca D'Angelo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=FprYRzUziT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FprYRzUziT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "G1cIU7JqMC",
        "title": "MEDUSA: Medical Data Under Shadow Attacks via Hybrid Model Inversion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce MEDUSA (Medical Data Under Shadow Attacks), a novel hybrid model inversion framework that leverages gradient-based optimization and TCNNs to reconstruct high-fidelity medical images from model outputs in a gray-box setting. Unlike traditional attacks requiring full model details, MEDUSA uses surrogate shadow models trained on publicly available data, simulating limited-information scenarios often encountered in practice. Our approach shows that even with restricted access, quality image reconstructions are possible, raising serious privacy concerns for patient data. Contributions include demonstrating that a combination of gradient-based methods and TCNNs yields potent reconstructions, even with limited model access, and providing a detailed analysis of how different input configurations impact reconstruction quality. We also evaluate the reconstructions as viable training data, finding that they can approximate real images well enough to use for model training. Finally, we propose robust defensive mechanisms such as output vector truncation, Gaussian noise, and a new k-NN smearing technique to tackle privacy risks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Asfandyar Azhar;Paul Thielen;Curtis Langlotz",
        "authorids": "~Asfandyar_Azhar1;~Paul_Thielen2;~Curtis_Langlotz1",
        "gender": ";M;M",
        "homepage": ";https://forcept007.github.io/portfolio/;https://profiles.stanford.edu/curtis-langlotz",
        "dblp": ";;12/1751",
        "google_scholar": ";;WQkBYwQAAAAJ",
        "orcid": ";;0000-0002-8972-8051",
        "linkedin": "asfandyarazhar/;https://linkedin.com/in/paul-thielen;langlotz/",
        "or_profile": "~Asfandyar_Azhar1;~Paul_Thielen2;~Curtis_Langlotz1",
        "aff": "Stanford University;;Stanford University",
        "aff_domain": "stanford.edu;;stanford.edu",
        "position": "Visiting Researcher;;Full Professor",
        "bibtex": "@inproceedings{\nazhar2025medusa,\ntitle={{MEDUSA}: Medical Data Under Shadow Attacks via Hybrid Model Inversion},\nauthor={Asfandyar Azhar and Paul Thielen and Curtis Langlotz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=G1cIU7JqMC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=G1cIU7JqMC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GB9na8s20p",
        "title": "Bayesian Off-Policy Evaluation and Learning for Large Action Spaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Imad Aouali;Victor-Emmanuel Brunel;David Rohde;Anna Korba",
        "authorids": "~Imad_Aouali2;~Victor-Emmanuel_Brunel1;~David_Rohde1;~Anna_Korba2",
        "gender": ";M;M;",
        "homepage": ";https://vebrunel.com/;;",
        "dblp": ";203/4175;;182/8959.html",
        "google_scholar": ";;9SBYirYAAAAJ;https://scholar.google.fr/citations?user=dbH6E3kAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Imad_Aouali2;~Victor-Emmanuel_Brunel1;~David_Rohde1;~Anna_Korba2",
        "aff": ";Ensae ParisTech;;Ensae ParisTech",
        "aff_domain": ";ensae.fr;;ensae.fr",
        "position": ";Assistant Professor;;Assistant Professor",
        "bibtex": "@inproceedings{\naouali2025bayesian,\ntitle={Bayesian Off-Policy Evaluation and Learning for Large Action Spaces},\nauthor={Imad Aouali and Victor-Emmanuel Brunel and David Rohde and Anna Korba},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GB9na8s20p}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GB9na8s20p",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GBtFYDmjCK",
        "title": "Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate how to select the number of communities for weighted networks without a full likelihood modeling. First, we propose a novel weighted degree-corrected stochastic block model (DCSBM), where the mean adjacency matrix is modeled in the same way as in the standard DCSBM, while the variance profile matrix is assumed to be related to the mean adjacency matrix through a given variance function. Our method of selecting the number of communities is based on a sequential testing framework. In each step, the weighted DCSBM is fitted via some spectral clustering method. A key component of our method is matrix scaling on the estimated variance profile matrix. The resulting scaling factors can be used to normalize the adjacency matrix, from which the test statistic is then obtained. Under mild conditions on the weighted DCSBM, our proposed procedure is shown to be consistent in estimating the true number of communities. Numerical experiments on both simulated and real-world network data demonstrate the desirable empirical properties of our method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yucheng Liu;Xiaodong Li",
        "authorids": "~Yucheng_Liu6;~Xiaodong_Li2",
        "gender": "M;M",
        "homepage": ";https://www.stat.ucdavis.edu/~xdgli/index.html",
        "dblp": ";",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;bW6qGV0AAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Yucheng_Liu6;~Xiaodong_Li2",
        "aff": "Alibaba Group;",
        "aff_domain": "alibaba-inc.com;",
        "position": "Machine Learning Engineer;",
        "bibtex": "@inproceedings{\nliu2025selecting,\ntitle={Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models},\nauthor={Yucheng Liu and Xiaodong Li},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GBtFYDmjCK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GBtFYDmjCK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GFl42W9tYC",
        "title": "Generalization Lower Bounds for GD and SGD in Smooth Stochastic Convex Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work studies the generalization error of gradient methods. More specifically, we focus on how training steps $T$ and step-size $\\eta$ might affect generalization in smooth stochastic convex optimization (SCO) problems. Recent works show that in some cases longer training can hurt generalization. Our work reexamines this for smooth SCO and find that the conclusion can be case-dependent. In particular, we first study SCO problems when the loss is \\emph{realizable}, i.e. an optimal solution minimizes all the data points.  Our work provides excess risk lower bounds for Gradient Descent (GD) and Stochastic Gradient Descent (SGD) and finds that longer training may not hurt generalization. In the short training scenario $\\eta T = O(n)$ ($n$ is sample size), our lower bounds tightly match and certify the respective upper bounds. However, for the long training scenario where $\\eta T =O(n)$, our analysis reveals a gap between the lower and upper bounds, indicating that longer training does hurt generalization for realizable objectives.  A conjecture is proposed that the gap can be closed by improving upper bounds, supported by analyses in two special instances. Moreover, besides the realizable setup, we also provide first tight excess risk lower bounds for GD and SGD under the general non-realizable smooth SCO setting, suggesting that existing stability analyses are tight in step-size and iteration dependence, and that overfitting provably happens when there is no interpolating minimum.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peiyuan Zhang;Jiaye Teng;Jingzhao Zhang",
        "authorids": "~Peiyuan_Zhang1;~Jiaye_Teng1;~Jingzhao_Zhang2",
        "gender": ";;M",
        "homepage": ";;https://sites.google.com/view/jingzhao/home",
        "dblp": ";;220/5559",
        "google_scholar": ";;8NudxYsAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Peiyuan_Zhang1;~Jiaye_Teng1;~Jingzhao_Zhang2",
        "aff": ";;Tsinghua University",
        "aff_domain": ";;mail.tsinghua.edu.cn",
        "position": ";;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025generalization,\ntitle={Generalization Bounds for {GD} and {SGD} in Smooth Stochastic Convex Optimization},\nauthor={Peiyuan Zhang and Jiaye Teng and Jingzhao Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GFl42W9tYC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GFl42W9tYC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GHyBMTpiJg",
        "title": "What Ails Generative Structure-based Drug Design: Expressivity is Too Little or Too Much?",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Several generative models with elaborate training and sampling procedures have been proposed to accelerate structure-based drug design (SBDD); however, their empirical performance turns out to be suboptimal. We seek to better understand this phenomenon from both theoretical and empirical perspectives. Since most of these models apply graph neural networks (GNNs), one may suspect that they inherit the representational limitations of GNNs. We analyze this aspect, establishing the first such results for protein-ligand complexes. A plausible counterview may attribute the underperformance of these models to their excessive parameterizations, inducing expressivity at the expense of generalization. We investigate this possibility with a simple metric-aware approach that learns an economical surrogate for affinity to infer an unlabelled molecular graph and optimizes for labels conditioned on this graph and molecular properties. The resulting model achieves state-of-the-art results using 100x fewer trainable parameters and affords up to 1000x speedup. Collectively, our findings underscore the need to reassess and redirect the existing paradigm and efforts for SBDD. Code is available at https://github.com/rafalkarczewski/SimpleSBDD.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rafal Karczewski;Samuel Kaski;Markus Heinonen;Vikas K Garg",
        "authorids": "~Rafal_Karczewski1;~Samuel_Kaski1;~Markus_Heinonen1;~Vikas_K_Garg1",
        "gender": ";M;M;",
        "homepage": ";https://people.aalto.fi/samuel.kaski;https://users.aalto.fi/~heinom10/;",
        "dblp": "228/6790;64/5826;22/7709;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;hFtfHZoAAAAJ;",
        "orcid": ";0000-0003-1925-9154;;",
        "linkedin": "rafal-karczewski-906ab010a;samuel-kaski-27790/;;",
        "or_profile": "~Rafal_Karczewski1;~Samuel_Kaski1;~Markus_Heinonen1;~Vikas_K_Garg1",
        "aff": "Aalto University;University of Manchester+Helsinki Institute for Information Technology+Aalto University;Aalto University;",
        "aff_domain": "aalto.fi;manchester.ac.uk+hiit.fi+aalto.fi;aalto.fi;",
        "position": "PhD student;Full Professor+Several positions+Full Professor;Researcher;",
        "bibtex": "@inproceedings{\nkarczewski2025what,\ntitle={What Ails Generative Structure-based Drug Design: Expressivity is Too Little or Too Much?},\nauthor={Rafal Karczewski and Samuel Kaski and Markus Heinonen and Vikas K Garg},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GHyBMTpiJg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GHyBMTpiJg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GIqgx7iGpi",
        "title": "Posterior Mean Matching: Generative Modeling through Online Bayesian Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces posterior mean matching (PMM), a new method for generative modeling that is grounded in Bayesian inference. PMM uses conjugate pairs of distributions to model complex data of various modalities like images and text, offering a flexible alternative to existing methods like diffusion models. PMM models iteratively refine noisy approximations of the target distribution using updates from online Bayesian inference. PMM is flexible because its mechanics are based on general Bayesian models. We demonstrate this flexibility by developing specialized examples: a generative PMM model of real-valued data using the Normal-Normal model, a generative PMM model of count data using a Gamma-Poisson model, and a generative PMM model of discrete data using a Dirichlet-Categorical model. For the Normal-Normal PMM model, we establish a direct connection to diffusion models by showing that its continuous-time formulation converges to a stochastic differential equation (SDE). Additionally, for the Gamma-Poisson PMM, we derive a novel SDE driven by a Cox process, which is a significant departure from traditional Brownian motion-based generative models.  PMMs achieve performance that is competitive with generative models for language modeling and image generation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sebastian Salazar;Michal Kucer;Yixin Wang;Emily Casleton;David Blei",
        "authorids": "~Sebastian_Salazar1;~Michal_Kucer1;~Yixin_Wang1;~Emily_Casleton1;~David_Blei2",
        "gender": "M;;;F;M",
        "homepage": ";;;;http://www.cs.columbia.edu/~blei/",
        "dblp": ";;;;86/1910",
        "google_scholar": ";;gFLW9qcAAAAJ;aMk7pIUAAAAJ;https://scholar.google.com.tw/citations?user=8OYE6iEAAAAJ",
        "orcid": ";;0000-0002-6617-4842;;",
        "linkedin": "sebastian-salazar-866782181/;;;;",
        "or_profile": "~Sebastian_Salazar1;~Michal_Kucer1;~Yixin_Wang1;~Emily_Casleton1;~David_Blei2",
        "aff": "Columbia University;;University of Michigan - Ann Arbor;;Columbia University",
        "aff_domain": "columbia.edu;;umich.edu;;columbia.edu",
        "position": "PhD student;;Assistant Professor;;Full Professor",
        "bibtex": "@inproceedings{\nsalazar2025posterior,\ntitle={Posterior Mean Matching: Generative Modeling through Online Bayesian Inference},\nauthor={Sebastian Salazar and Michal Kucer and Yixin Wang and Emily Casleton and David Blei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GIqgx7iGpi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GIqgx7iGpi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GOoxdIkniP",
        "title": "RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrosynthesis poses a key challenge in biopharmaceuticals, aiding chemists in finding appropriate reactant molecules for given product molecules. With reactants and products represented as 2D graphs, retrosynthesis constitutes a conditional graph-to-graph (G2G) generative task. Inspired by advancements in discrete diffusion models for graph generation, we aim to design a diffusion-based method to address this problem. However, integrating a diffusion-based G2G framework while retaining essential chemical reaction template information presents a notable challenge. Our key innovation involves a multi-stage diffusion process. We decompose the retrosynthesis procedure to first sample external groups from the dummy distribution given products, then generate external bonds to connect products and generated groups. Interestingly, this generation process mirrors the reverse of the widely adapted semi-template retrosynthesis workflow, i.e., from reaction center identification to synthon completion. Based on these designs, we introduce Retrosynthesis Diffusion (RetroDiff), a novel diffusion-based method for the retrosynthesis task. Experimental results demonstrate that RetroDiff surpasses all semi-template methods in accuracy, and outperforms template-based and template-free methods in large-scale scenarios and molecular validity, respectively.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yiming Wang;Yuxuan Song;Yiqun Wang;Minkai Xu;Rui Wang;Hao Zhou;Wei-Ying Ma",
        "authorids": "~Yiming_Wang13;~Yuxuan_Song2;~Yiqun_Wang3;~Minkai_Xu1;~Rui_Wang10;~Hao_Zhou5;~Wei-Ying_Ma2",
        "gender": ";M;M;Not Specified;;M;M",
        "homepage": "https://alsace08.github.io/yiming.wang.cv/;https://yuxuansong.com;https://raymond-yiqunwang.github.io/;https://minkaixu.com;;https://zhouh.github.io/;https://air.tsinghua.edu.cn/en/info/1046/1189.htm",
        "dblp": ";;;257/3355;;63/778-12;m/WYMa.html",
        "google_scholar": "2C1VDq8AAAAJ;xlnZ1OIAAAAJ;Z_9piXQAAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com/citations?hl=zh-CN;SToCbu8AAAAJ",
        "orcid": "0000-0001-5821-8895;;0000-0002-1457-0085;0009-0007-9735-3767;;;",
        "linkedin": ";;yiqun-raymond-wang-dot-science;;;;wei-ying-ma-16a0171/",
        "or_profile": "~Yiming_Wang13;~Yuxuan_Song2;~Yiqun_Wang3;~Minkai_Xu1;~Rui_Wang10;~Hao_Zhou5;~Wei-Ying_Ma2",
        "aff": "Shanghai Jiaotong University;ByteDance Inc.+Tsinghua University;Absci Corp;Stanford University;;Tsinghua University;Tsinghua University",
        "aff_domain": "sjtu.edu.cn;bytedance.com+tsinghua.edu.cn;absci.com;stanford.edu;;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;Intern+PhD student;Researcher;PhD student;;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nwang2025retrodiff,\ntitle={RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation},\nauthor={Yiming Wang and Yuxuan Song and Yiqun Wang and Minkai Xu and Rui Wang and Hao Zhou and Wei-Ying Ma},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GOoxdIkniP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GOoxdIkniP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GP8nMmE8jo",
        "title": "Efficient Exploitation of Hierarchical Structure in Sparse Reward Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study goal-conditioned Hierarchical Reinforcement Learning (HRL), where a high-level agent instructs sub-goals to a low-level agent.\nUnder the assumption of a sparse reward function and known hierarchical decomposition, we propose a new algorithm to learn optimal hierarchical policies.\nOur algorithm takes a low-level policy as input and is flexible enough to work with a wide range of low-level policies.\nWe show that when the algorithm that computes the low-level policy is optimistic and provably efficient, our HRL algorithm enjoys a regret bound which represents a significant improvement compared to previous results for HRL. Importantly, our regret upper bound highlights key characteristics of the hierarchical decomposition that guarantee that our hierarchical algorithm is more efficient than the best monolithic approach.\nWe support our theoretical findings with experiments that underscore that our method consistently outperforms algorithms that ignore the hierarchical structure.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gianluca Drappo;Arnaud Robert;Marcello Restelli;Aldo A. Faisal;Alberto Maria Metelli;Ciara Pike-Burke",
        "authorids": "~Gianluca_Drappo1;~Arnaud_Robert1;~Marcello_Restelli1;~Aldo_A._Faisal1;~Alberto_Maria_Metelli2;~Ciara_Pike-Burke2",
        "gender": ";M;M;M;M;",
        "homepage": ";https://faisallab.org/members/arnaud-robert;http://home.deib.polimi.it/restelli/;https://www.imperial.ac.uk/people/a.faisal/;https://albertometelli.github.io/;https://www.ma.imperial.ac.uk/~cpikebur/",
        "dblp": "270/1927;;64/1011;54/5027;209/4941;202/1263",
        "google_scholar": "at8hBXsAAAAJ;eV_mq78AAAAJ;https://scholar.google.com.tw/citations?user=xdgxRiEAAAAJ;https://scholar.google.co.uk/citations?user=WjHjbrwAAAAJ;R31IsPwAAAAJ;Hl1vu1MAAAAJ",
        "orcid": ";;0000-0002-6322-1076;0000-0003-0813-7207;0000-0002-3424-5212;",
        "linkedin": "gianlucadrappo/;;;a-aldo-faisal-057b704b/?originalSubdomain=uk;;",
        "or_profile": "~Gianluca_Drappo1;~Arnaud_Robert1;~Marcello_Restelli1;~Aldo_A._Faisal1;~Alberto_Maria_Metelli2;~Ciara_Pike-Burke2",
        "aff": ";;Politecnico di Milano;UKRI Turing AI Fellow+Imperial College London+UKRI Centre in AI for Healthcare+Universit\u00e4t Bayreuth;Politecnico di Milano;Imperial College London",
        "aff_domain": ";;polimi.it;imperial.ac.uk+imperial.ac.uk+imperial.ac.uk+uni-bayreuth.de;polimi.it;imperial.ac.uk",
        "position": ";;Associate Professor;Principal Researcher+Full Professor+Principal Researcher+Full Professor;Assistant Professor;Lecturer",
        "bibtex": "@inproceedings{\ndrappo2025efficient,\ntitle={Efficient Exploitation of Hierarchical Structure in Sparse Reward Reinforcement Learning},\nauthor={Gianluca Drappo and Arnaud Robert and Marcello Restelli and Aldo A. Faisal and Alberto Maria Metelli and Ciara Pike-Burke},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GP8nMmE8jo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GP8nMmE8jo",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GXRIeFcQCr",
        "title": "Robust Fair Clustering with Group Membership Uncertainty Sets",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the canonical fair clustering problem where each cluster is constrained to have close to population-level representation of each group. Despite significant attention, the salient issue of having incomplete knowledge about the group membership of each point has been superficially addressed. In this paper, we consider a setting where the assigned group memberships are noisy. We introduce a simple noise model that requires a small number of parameters to be given by the decision maker. We then present an algorithm for fair clustering with provable \\emph{robustness} guarantees. Our framework enables the decision maker to trade off between the robustness and the clustering quality. Unlike previous work, our algorithms are backed by worst-case theoretical guarantees. Finally, we empirically verify the performance of our algorithm on real world datasets and show its superior performance over existing baselines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sharmila Duppala;Juan Luque;John P Dickerson;Seyed A. Esmaeili",
        "authorids": "~Sharmila_Duppala1;~Juan_Luque1;~John_P_Dickerson1;~Seyed_A._Esmaeili1",
        "gender": "F;;M;M",
        "homepage": "https://trinity24.github.io/;;https://jpdickerson.com/;https://sa-esmaeili.github.io/",
        "dblp": "239/6063.html;;75/8479;128/4703",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=QgDpfCQAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0003-2231-680X;",
        "linkedin": ";;john-dickerson-83a74a7/;",
        "or_profile": "~Sharmila_Duppala1;~Juan_Luque1;~John_P_Dickerson1;~Seyed_A._Esmaeili1",
        "aff": "Department of Computer Science, University of Maryland, College Park;;University of Maryland, College Park+Arthur AI+Optimized Markets, Inc;University of Chicago",
        "aff_domain": "cs.umd.edu;;umd.edu+arthur.ai+optimizedmarkets.com;uchicago.edu",
        "position": "PhD student;;Associate Professor+Chief Scientist+Consultant;Postdoc",
        "bibtex": "@inproceedings{\nduppala2025robust,\ntitle={Robust Fair Clustering with Group Membership Uncertainty Sets},\nauthor={Sharmila Duppala and Juan Luque and John P Dickerson and Seyed A. Esmaeili},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GXRIeFcQCr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GXRIeFcQCr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Gka7Ob4Jjz",
        "title": "Locally Private Sampling with Public Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Local differential privacy (LDP) is increasingly employed in privacy-preserving machine learning to protect user data before sharing it with an untrusted aggregator. Most LDP methods   assume that users possess only a single data record, which is a significant limitation since users often gather extensive datasets (e.g., images, text, time-series data) and frequently have access to public datasets. To address this limitation, we propose a locally private sampling framework that leverages both the private and public datasets of each user. Specifically, we assume each user has two distributions: $p$ and $q$ that represent their private and public datasets, respectively. The objective is to design a mechanism that generates a private sample approximating $p$ while simultaneously preserving  $q$. We frame this objective as a minimax optimization problem using $f$-divergence as the utility measure. We fully characterize the minimax optimal mechanisms for general $f$-divergences provided that $p$ and $q$ are discrete distributions. Remarkably, we demonstrate that this optimal mechanism is universal across all $f$-divergences. Experiments validate the effectiveness of our minimax optimal mechanism compared to the state-of-the-art private sampler.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Behnoosh Zamanlooy;Mario Diaz;Shahab Asoodeh",
        "authorids": "~Behnoosh_Zamanlooy2;~Mario_Diaz3;~Shahab_Asoodeh1",
        "gender": "F;;M",
        "homepage": ";;https://www.cas.mcmaster.ca/~asoodehs/",
        "dblp": ";;63/8658",
        "google_scholar": "bns0iwUAAAAJ;;CSxeFMsAAAAJ",
        "orcid": ";;",
        "linkedin": ";;shahabasoodeh/",
        "or_profile": "~Behnoosh_Zamanlooy2;~Mario_Diaz3;~Shahab_Asoodeh1",
        "aff": "McMaster University;;McMaster University",
        "aff_domain": "mcmaster.ca;;mcmaster.ca",
        "position": "PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nzamanlooy2025locally,\ntitle={Locally Private Sampling with Public Data},\nauthor={Behnoosh Zamanlooy and Mario Diaz and Shahab Asoodeh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Gka7Ob4Jjz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Gka7Ob4Jjz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GoT8IaDspl",
        "title": "Density-Dependent Group Testing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Group testing is the problem of identifying a small subset of defectives from a large set using as few binary tests as possible. In most current literature on group testing the binary test outcome is $1$ if the pool contains at least one defective, and $0$ otherwise. In this work we initiate the study of a generalized model of group testing that accommodates the physical effects of dilution of infected samples in large pools. In this model the binary test outcome is $1$ with probability $f(\\rho)$, where $\\rho$ is the density of the defectives in the test, and $f:[0,1]\\rightarrow [0,1]$ is a given  \"test function\" that models this dilution process. For a large class of test functions our results establish near-optimal sample complexity bounds, by providing information-theoretic lower bounds on the number of tests necessary to recover the set of defective items, and providing computationally efficient algorithms with sample complexities that match these lower bounds up to constant or logarithmic factors. Furthermore, using tools from real analysis, we extend our results to any \"sufficiently well-behaved function\" $f:[0,1]\\rightarrow [0,1]$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rahil Morjaria;Saikiran Bulusu;Venkata Gandikota;Sidharth Jaggi",
        "authorids": "~Rahil_Morjaria1;~Saikiran_Bulusu1;~Venkata_Gandikota1;~Sidharth_Jaggi1",
        "gender": "M;M;M;",
        "homepage": ";;https://sites.google.com/view/gvenkata;https://research-information.bris.ac.uk/en/persons/sidharth-sid-jaggi",
        "dblp": ";;169/2082;",
        "google_scholar": ";NrfiUzAAAAAJ;4u-2hcwAAAAJ;AX7276AAAAAJ",
        "orcid": ";0000-0002-4594-4844;0000-0003-2381-7788;",
        "linkedin": "rahilmorjaria/;;;",
        "or_profile": "~Rahil_Morjaria1;~Saikiran_Bulusu1;~Venkata_Gandikota1;~Sidharth_Jaggi1",
        "aff": "University of Bristol;International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology+Ohio State University, Columbus;Syracuse University;University of Bristol",
        "aff_domain": "bris.ac.uk;iiit.ac.in+osu.edu;syr.edu;bristol.ac.uk",
        "position": "PhD student;Assistant Professor+Postdoc;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nmorjaria2025densitydependent,\ntitle={Density-Dependent Group Testing},\nauthor={Rahil Morjaria and Saikiran Bulusu and Venkata Gandikota and Sidharth Jaggi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GoT8IaDspl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GoT8IaDspl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GubPyzAW2l",
        "title": "A Safe Bayesian Learning Algorithm for Constrained MDPs with Bounded Constraint Violation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Constrained Markov decision processes (CMDPs) models are increasingly important in many applications with multiple objectives. When the model is unknown and must be learned online, it is desirable to ensure that the constraint is met, or at least the violation is bounded with time. In recent literature, progress has been made on this very challenging problem but with either unsatisfactory assumptions such as the knowledge of a safe policy, or have high cumulative regret. We propose the Safe-PSRL (posterior sampling-based RL) algorithm that does not need such assumptions and yet performs very well, both in terms of theoretical regret bounds as well as empirically. The algorithm efficiently trades-off exploration and exploitation using posterior sampling-based exploration, and yet provably suffers only bounded constraint violation using carefully-crafted pessimism. We establish a sub-linear $\\tilde{O}(H^{2.5}\\sqrt{|S|^2|A|K})$\n upper bound on the Bayesian objective regret along with a bounded, i.e., $\\tilde{O}(1)$ constraint-violation regret over \n$K$  episodes for an  $|S|$-state, $|A|$-action, and $H$ horizon CMDP which improves over state-of-the-art algorithms for the same setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Krishna C Kalagarla;Rahul Jain;Pierluigi Nuzzo",
        "authorids": "~Krishna_C_Kalagarla1;~Rahul_Jain1;~Pierluigi_Nuzzo1",
        "gender": ";M;",
        "homepage": ";http://www.rahuljain.net;https://descyphy.usc.edu/",
        "dblp": ";42/4430-2.html;75/1989-2",
        "google_scholar": ";NIj18UQAAAAJ;GsDdaaYAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Krishna_C_Kalagarla1;~Rahul_Jain1;~Pierluigi_Nuzzo1",
        "aff": ";Google DeepMind+University of Southern California;University of Southern California",
        "aff_domain": ";research.google.com+usc.edu;usc.edu",
        "position": ";Researcher+Professor;Associate Professor",
        "bibtex": "@inproceedings{\nkalagarla2025a,\ntitle={A Safe Bayesian Learning Algorithm for Constrained {MDP}s with Bounded Constraint Violation},\nauthor={Krishna C Kalagarla and Rahul Jain and Pierluigi Nuzzo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=GubPyzAW2l}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GubPyzAW2l",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Gz4mw5xWUi",
        "title": "Near-optimal algorithms for private estimation and sequential testing of collision probability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present new algorithms for estimating and testing \\emph{collision probability}, a fundamental measure of the spread of a discrete distribution that is widely used in many scientific fields. We describe an algorithm that satisfies $(\\alpha, \\beta)$-local differential privacy and estimates collision probability with error at most $\\epsilon$ using $\\tilde{O}\\left(\\frac{\\log(1/\\beta)}{\\alpha^2 \\epsilon^2}\\right)$ samples for $\\alpha \\le 1$, which improves over previous work by a factor of $\\frac{1}{\\alpha^2}$. We also present a sequential testing algorithm for collision probability, which can distinguish between collision probability values that are separated by $\\epsilon$ using $\\tilde{O}(\\frac{1}{\\epsilon^2})$ samples, even when $\\epsilon$ is unknown. Our algorithms have nearly the optimal sample complexity, and in experiments we show that they require significantly fewer samples than previous methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Robert Istvan Busa-Fekete;Umar Syed",
        "authorids": "~Robert_Istvan_Busa-Fekete1;~Umar_Syed1",
        "gender": "M;M",
        "homepage": ";https://umarsyed.com",
        "dblp": "69/4876;75/1894",
        "google_scholar": "UNtKl1MAAAAJ;zKORw8wAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Robert_Istvan_Busa-Fekete1;~Umar_Syed1",
        "aff": "Google Research;Google",
        "aff_domain": "google.com;google.com",
        "position": "Researcher;Researcher",
        "bibtex": "@inproceedings{\nbusa-fekete2025nearoptimal,\ntitle={Near-optimal algorithms for private estimation and sequential testing of collision probability},\nauthor={Robert Istvan Busa-Fekete and Umar Syed},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Gz4mw5xWUi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Gz4mw5xWUi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H0ZBGst3Vh",
        "title": "InnerThoughts: Disentangling Representations and Predictions in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) contain substantial factual knowledge which is commonly elicited by multiple-choice question-answering prompts. Internally, such models process the prompt through multiple transformer layers, building varying representations of the problem within its hidden states. Ultimately, however, only the hidden state corresponding to the final layer and token position is used to predict the answer label. In this work, we propose instead to learn a small separate neural network predictor module on a collection of training questions, that take the hidden states from all the layers at the last temporal position as input and outputs predictions. In effect, such a framework disentangles the representational abilities of LLMs from their predictive abilities. On a collection of hard benchmarks, our method achieves considerable improvements in performance, sometimes comparable to supervised fine-tuning procedures, but at a fraction of the computational cost.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Didier Ch\u00e9telat;Joseph Cotnareanu;Rylee Thompson;Yingxue Zhang;Mark Coates",
        "authorids": "~Didier_Ch\u00e9telat1;~Joseph_Cotnareanu1;~Rylee_Thompson1;~Yingxue_Zhang1;~Mark_Coates1",
        "gender": "M;M;M;F;M",
        "homepage": "https://www.didierchetelat.com;https://github.com/joseph-cotnareanu;;;http://www.ece.mcgill.ca/~mcoate/",
        "dblp": ";357/5120;;174/0010-1.html;c/MarkCoates",
        "google_scholar": "https://scholar.google.ca/citations?user=IkTwAY0AAAAJ;;https://scholar.google.ca/citations?user=pRy6BiAAAAAJ;4bsYpogAAAAJ;https://scholar.google.ca/citations?user=qxWORNoAAAAJ",
        "orcid": ";0009-0004-4876-2765;;;0000-0001-5030-1379",
        "linkedin": ";;rylee-thompson/;yingxue-zhang-03971b112/;",
        "or_profile": "~Didier_Ch\u00e9telat1;~Joseph_Cotnareanu1;~Rylee_Thompson1;~Yingxue_Zhang1;~Mark_Coates1",
        "aff": "Huawei Noah's Ark Lab;;Huawei Technologies Ltd.;Huawei Canada, Huawei Noah's Ark Lab;McGill University",
        "aff_domain": "huawei.com;;huawei.com;huawei.com;mcgill.ca",
        "position": "Researcher;;Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nchetelat2025innerthoughts,\ntitle={InnerThoughts: Disentangling Representations and Predictions in Large Language Models},\nauthor={Didier Ch{\\'e}telat and Joseph Cotnareanu and Rylee Thompson and Yingxue Zhang and Mark Coates},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=H0ZBGst3Vh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H0ZBGst3Vh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H1lH2YaR3O",
        "title": "Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nikola Pavlovic;Sudeep Salgia;Qing Zhao",
        "authorids": "~Nikola_Pavlovic1;~Sudeep_Salgia1;~Qing_Zhao1",
        "gender": "M;M;F",
        "homepage": ";https://sudeepsalgia.github.io/;https://zhao.ece.cornell.edu/",
        "dblp": ";207/8460;",
        "google_scholar": ";Y5d5L84AAAAJ;ymsLVFsAAAAJ",
        "orcid": "0009-0009-8532-6504;;",
        "linkedin": ";;",
        "or_profile": "~Nikola_Pavlovic1;~Sudeep_Salgia1;~Qing_Zhao1",
        "aff": "Cornell University;Carnegie Mellon University;Cornell University",
        "aff_domain": "cornell.edu;cmu.edu;cornell.edu",
        "position": "PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\npavlovic2025orderoptimal,\ntitle={Order-Optimal Regret in Distributed Kernel Bandits},\nauthor={Nikola Pavlovic and Sudeep Salgia and Qing Zhao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=H1lH2YaR3O}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H1lH2YaR3O",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H623AObRxU",
        "title": "QuACK: A Multipurpose Queuing Algorithm for Cooperative $k$-Armed Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper studies the cooperative stochastic $k$-armed bandit problem, where $m$ agents collaborate to identify the optimal action. Rather than adapting a specific single-agent algorithm, we propose a general-purpose black-box reduction that extends any single-agent algorithm to the multi-agent setting. Under mild assumptions, we prove that our black-box approach preserves the regret guarantees of the chosen algorithm, and is capable of achieving minimax-optimality up to an additive graph-dependent term. Our method applies to various bandit settings, including heavy-tailed and duelling bandits, and those with local differential privacy. Empirically, it is competitive with or outperforms specialized multi-agent algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Benjamin Howson;Sarah Lucie Filippi;Ciara Pike-Burke",
        "authorids": "~Benjamin_Howson1;~Sarah_Lucie_Filippi1;~Ciara_Pike-Burke2",
        "gender": "M;F;",
        "homepage": ";;https://www.ma.imperial.ac.uk/~cpikebur/",
        "dblp": ";;202/1263",
        "google_scholar": ";HhMJevQAAAAJ;Hl1vu1MAAAAJ",
        "orcid": ";;",
        "linkedin": "benjaminhowson/;;",
        "or_profile": "~Benjamin_Howson1;~Sarah_Lucie_Filippi1;~Ciara_Pike-Burke2",
        "aff": ";Imperial College London;Imperial College London",
        "aff_domain": ";imperial.ac.uk;imperial.ac.uk",
        "position": ";Associate Professor;Lecturer",
        "bibtex": "@inproceedings{\nhowson2025quack,\ntitle={Qu{ACK}: A Multipurpose Queuing Algorithm for Cooperative \\$k\\$-Armed Bandits},\nauthor={Benjamin Howson and Sarah Lucie Filippi and Ciara Pike-Burke},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=H623AObRxU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H623AObRxU",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H6Co3RbkGB",
        "title": "Variance-Dependent Regret Bounds for Nonstationary Linear Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the \n variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\\text{OFUL}^+$ and Restarted $\\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when the total variance $V_K$ is much smaller than $K$, our algorithms outperform previous state-of-the-art results on non-stationary stochastic linear bandits under different settings. Experimental evaluations further validate the superior performance of our proposed algorithms over existing works.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhiyong Wang;Jize Xie;Yi Chen;John C.S. Lui;Dongruo Zhou",
        "authorids": "~Zhiyong_Wang9;~Jize_Xie1;~Yi_Chen18;~John_C.S._Lui2;~Dongruo_Zhou1",
        "gender": "M;M;M;M;M",
        "homepage": "https://zhiyongwangwzy.github.io/;;https://seng.hkust.edu.hk/about/people/faculty/yi-chen;http://www.cse.cuhk.edu.hk/~cslui/Index.html;",
        "dblp": ";339/2280;;l/JohnCSLui;215/3401",
        "google_scholar": "https://scholar.google.com/citations?hl=en;cX6B3HsAAAAJ;https://scholar.google.ca/citations?user=Vjpq4aYAAAAJ;https://scholar.google.com.tw/citations?user=7LVjQ7MAAAAJ;1780wr0AAAAJ",
        "orcid": ";0000-0001-9702-5025;0000-0003-4283-7485;0000-0001-7466-0384;",
        "linkedin": "zhiyong-wang-a44aaa1a3/;;;;",
        "or_profile": "~Zhiyong_Wang9;~Jize_Xie1;~Yi_Chen18;~John_C.S._Lui2;~Dongruo_Zhou1",
        "aff": "Cornell University+Department of Computer Science and Engineering, The Chinese University of Hong Kong;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;The Chinese University of Hong Kong;Indiana University",
        "aff_domain": "cornell.edu+cse.cuhk.edu.hk;connect.ust.hk;ust.hk;cse.cuhk.edu.hk;iu.edu",
        "position": "Visiting PhD student+PhD student;PhD student;Assistant Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025variancedependent,\ntitle={Variance-Dependent Regret Bounds for Nonstationary Linear Bandits},\nauthor={Zhiyong Wang and Jize Xie and Yi Chen and John C.S. Lui and Dongruo Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=H6Co3RbkGB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H6Co3RbkGB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H9sKOfNgQj",
        "title": "Revisiting Online Learning Approach to Inverse Linear Optimization: A Fenchel\u2013Young Loss Perspective and Gap-Dependent Regret Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper revisits the online learning approach to inverse linear optimization studied by B\u00e4rmann et al. (2017), where the goal is to infer an unknown linear objective function of an agent from sequential observations of the agent's input-output pairs. First, we provide a simple understanding of the online learning approach through its connection to online convex optimization of *Fenchel\u2013Young losses*. As a byproduct, we present an offline guarantee on the *suboptimality loss*, which measures how well predicted objective vectors explain the agent's choices, without assuming the optimality of the agent's choices. Second, assuming that there is a gap between optimal and suboptimal objective values in the agent's decision problems, we obtain an upper bound independent of the time horizon $T$ on the sum of suboptimality and *estimate losses*, where the latter measures the quality of solutions recommended by predicted objective vectors. Interestingly, our gap-dependent analysis achieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by exploiting structures specific to inverse linear optimization, even though neither the loss functions nor their domains possess desirable properties, such as strong convexity.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shinsaku Sakaue;Han Bao;Taira Tsuchiya",
        "authorids": "~Shinsaku_Sakaue1;~Han_Bao2;~Taira_Tsuchiya1",
        "gender": "M;M;M",
        "homepage": "https://ssakaue.github.io/;https://hermite.jp/;https://tsuchhiii.github.io/",
        "dblp": "183/6350;120/1444-2;226/5536",
        "google_scholar": "https://scholar.google.co.jp/citations?user=9oTbrmEAAAAJ;MqMzjeMAAAAJ;https://scholar.google.co.jp/citations?view_op=list_works",
        "orcid": ";0000-0002-4473-2604;",
        "linkedin": ";;",
        "or_profile": "~Shinsaku_Sakaue1;~Han_Bao2;~Taira_Tsuchiya1",
        "aff": "CyberAgent, Inc.+The University of Tokyo+NTT;The Institute of Statistical Mathematics;RIKEN+The University of Tokyo",
        "aff_domain": "cyberagent.co.jp+u-tokyo.ac.jp+ntt.co.jp;ism.ac.jp;riken.jp+u-tokyo.ac.jp",
        "position": "Researcher+Assistant Professor+Researcher;Associate Professor;Visiting Scientist+Assistant Professor",
        "bibtex": "@inproceedings{\nsakaue2025revisiting,\ntitle={Revisiting Online Learning Approach to Inverse Linear Optimization: A Fenchel{\\textendash}Young Loss Perspective and Gap-Dependent Regret Analysis},\nauthor={Shinsaku Sakaue and Han Bao and Taira Tsuchiya},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=H9sKOfNgQj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H9sKOfNgQj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HEgs74QHRZ",
        "title": "FedBaF: Federated Learning Aggregation Biased by a Foundation Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Foundation models are now a major focus of leading technology organizations due to their ability to generalize across diverse tasks. Existing approaches for adapting foundation models to new applications often rely on Federated Learning (FL) and disclose the foundation model weights to clients when using it to initialize the global model. While these methods ensure client data privacy, they compromise model and information security. In this paper, we introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF), a novel method for dynamically integrating pre-trained foundation model weights during the FL aggregation phase. Unlike conventional methods, FedBaF preserves the confidentiality of the foundation model while still leveraging its power to train more accurate models, especially in non-IID and adversarial scenarios. Our comprehensive experiments use Pre-ResNet and foundation models like Vision Transformer to demonstrate that FedBaF not only matches, but often surpasses the test accuracy of traditional weight initialization methods by up to 11.4\\% in IID and up to 15.8\\% in non-IID settings. Additionally, FedBaF applied to a Transformer-based language model significantly reduced perplexity by up to 39.2\\%.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jong-Ik Park;Srinivasa Pranav;Jose M F Moura;Carlee Joe-Wong",
        "authorids": "~Jong-Ik_Park1;~Srinivasa_Pranav1;~Jose_M_F_Moura1;~Carlee_Joe-Wong1",
        "gender": "M;;;F",
        "homepage": ";https://www.andrew.cmu.edu/user/spranav/;;https://www.andrew.cmu.edu/user/cjoewong/",
        "dblp": ";;;40/9937.html",
        "google_scholar": "lJtZwsMAAAAJ;vNg2sxYAAAAJ;;XEztdZgAAAAJ",
        "orcid": ";0000-0003-4159-4098;;",
        "linkedin": "jongikp/;srinivasa-pranav;;",
        "or_profile": "~Jong-Ik_Park1;~Srinivasa_Pranav1;~Jose_M_F_Moura1;~Carlee_Joe-Wong1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;;cmu.edu",
        "position": "PhD student;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\npark2025fedbaf,\ntitle={FedBaF: Federated Learning Aggregation Biased by a Foundation Model},\nauthor={Jong-Ik Park and Srinivasa Pranav and Jose M F Moura and Carlee Joe-Wong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=HEgs74QHRZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HEgs74QHRZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HHgd65xDov",
        "title": "Mixed-Feature Logistic Regression Robust to Distribution Shifts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Logistic regression models are widely used in the social and behavioral sciences and in high-stakes domains, due to their simplicity and interpretability properties. At the same time, such domains are permeated by distribution shifts, where the distribution generating the data changes between training and deployment. In this paper, we study a distributionally robust logistic regression problem that seeks the model that will perform best against adversarial realizations of the data distribution drawn from a suitably constructed Wasserstein ambiguity set.  Our model and solution approach differ from prior work in that we can capture settings where the likelihood of distribution shifts can vary across features, significantly broadening the applicability of our model relative to the state-of-the-art.  We propose a graph-based solution approach that can be integrated into off-the-shelf optimization solvers. We evaluate the performance of our model and algorithms on numerous publicly available datasets. Our solution achieves a 408x speed-up relative to the state-of-the-art. Additionally, compared to the state-of-the-art, our model reduces average calibration error by up to 36.19% and worst-case calibration error by up to 41.70%, while increasing the average area under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to 48.37%.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qingshi Sun;Nathan Justin;Andres Gomez;Phebe Vayanos",
        "authorids": "~Qingshi_Sun1;~Nathan_Justin1;~Andres_Gomez3;~Phebe_Vayanos1",
        "gender": ";M;;F",
        "homepage": ";;https://sites.google.com/usc.edu/gomez;https://sites.google.com/usc.edu/phebevayanos/home?authuser=0",
        "dblp": ";;;",
        "google_scholar": ";;6EbgGsAAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0001-7800-7235",
        "linkedin": "qingshi-sun-88137b1b5/;nathan-justin/;;phebe-vayanos-50106514/",
        "or_profile": "~Qingshi_Sun1;~Nathan_Justin1;~Andres_Gomez3;~Phebe_Vayanos1",
        "aff": "University of Southern California;University of Southern California;University of Southern California;",
        "aff_domain": "usc.edu;usc.edu;usc.edu;",
        "position": "PhD student;PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nsun2025mixedfeature,\ntitle={Mixed-Feature Logistic Regression Robust to Distribution Shifts},\nauthor={Qingshi Sun and Nathan Justin and Andres Gomez and Phebe Vayanos},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=HHgd65xDov}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HHgd65xDov",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Hbwh76xE8S",
        "title": "Offline RL via Feature-Occupancy Gradient Ascent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study offline Reinforcement Learning in large infinite-horizon discounted Markov Decision Processes (MDPs) when the reward and transition models are linearly realizable under a known feature map. Starting from the classic linear-program formulation of the optimal control problem in MDPs, we develop a new algorithm that performs a form of gradient ascent in the space of feature occupancies, defined as the expected feature vectors that can potentially be generated by executing policies in the environment. We show that the resulting simple algorithm satisfies strong computational and sample complexity guarantees, achieved under the least restrictive data coverage assumptions known in the literature. In particular, we show that the sample complexity of our method scales optimally with the desired accuracy level and depends on a weak notion of coverage that only requires the empirical feature covariance matrix to cover a single direction in the feature space (as opposed to covering a full subspace). Additionally, our method can be implemented efficiently without requiring any computational oracles, and requires no prior knowledge of the coverage ratio (or even an upper bound on it), which altogether make it the strongest known algorithm for this setting to date.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gergely Neu;Nneka Okolo",
        "authorids": "~Gergely_Neu1;~Nneka_Okolo1",
        "gender": "M;F",
        "homepage": "http://cs.bme.hu/~gergo;",
        "dblp": "83/7606;331/5997",
        "google_scholar": "https://scholar.google.ch/citations?user=uz27G84AAAAJ;s8DIX2sAAAAJ",
        "orcid": ";0009-0004-0137-970X",
        "linkedin": ";nneka-okolo-876410134/",
        "or_profile": "~Gergely_Neu1;~Nneka_Okolo1",
        "aff": "Universitat Pompeu Fabra;Universitat Pompeu Fabra",
        "aff_domain": "upf.edu;upf.edu",
        "position": "Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\nneu2025offline,\ntitle={Offline {RL} via Feature-Occupancy Gradient Ascent},\nauthor={Gergely Neu and Nneka Okolo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Hbwh76xE8S}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Hbwh76xE8S",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HgzG1S5G5l",
        "title": "Cost-aware simulation-based inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Simulation-based inference (SBI) is rapidly becoming the preferred framework for estimating parameters of intractable models in science and engineering. A significant challenge in this context is the large computational cost of simulating data from complex models, and the fact that this cost often depends on parameter values. We therefore propose *cost-aware SBI methods* which can significantly reduce the cost of existing sampling-based SBI methods, such as neural SBI and approximate Bayesian computation. This is achieved through a combination of rejection and self-normalised importance sampling, which significantly reduces the number of expensive simulations needed. Our approach is studied extensively on models from epidemiology to telecommunications engineering, where we obtain significant reductions in the overall cost of inference.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ayush Bharti;Daolang Huang;Samuel Kaski;Francois-Xavier Briol",
        "authorids": "~Ayush_Bharti1;~Daolang_Huang1;~Samuel_Kaski1;~Francois-Xavier_Briol1",
        "gender": "M;M;M;M",
        "homepage": "https://bharti-ayush.github.io/;https://www.huangdaolang.com;https://people.aalto.fi/samuel.kaski;https://fxbriol.github.io",
        "dblp": "232/3957;277/8410;64/5826;",
        "google_scholar": "https://scholar.google.dk/citations?user=6_7vkiUAAAAJ;2togGHoAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.co.uk/citations?user=yLBYtAwAAAAJ",
        "orcid": "0000-0002-4577-8049;;0000-0003-1925-9154;0000-0002-0181-2559",
        "linkedin": ";daolanghuang/?originalSubdomain=fi;samuel-kaski-27790/;",
        "or_profile": "~Ayush_Bharti1;~Daolang_Huang1;~Samuel_Kaski1;~Francois-Xavier_Briol1",
        "aff": "Aalto University;Aalto University;University of Manchester+Helsinki Institute for Information Technology+Aalto University;University College London, University of London+University College London, University of London",
        "aff_domain": "aalto.fi;aalto.fi;manchester.ac.uk+hiit.fi+aalto.fi;ucl.ac.uk+ucl.ac.uk",
        "position": "Principal Researcher;PhD student;Full Professor+Several positions+Full Professor;Full Professor+Associate Professor",
        "bibtex": "@inproceedings{\nbharti2025costaware,\ntitle={Cost-aware simulation-based inference},\nauthor={Ayush Bharti and Daolang Huang and Samuel Kaski and Francois-Xavier Briol},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=HgzG1S5G5l}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HgzG1S5G5l",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HhhPwHNk6u",
        "title": "Clustering Context in Off-Policy Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Off-policy evaluation can leverage logged data to estimate the effectiveness of new policies in e-commerce, search engines, media streaming services, or automatic diagnostic tools in healthcare. However, the performance of baseline off-policy estimators like IPS deteriorates when the logging policy significantly differs from the evaluation policy. Recent work proposes sharing information across similar actions to mitigate this problem. In this work, we propose an alternative estimator that shares information across similar contexts using clustering. We study the theoretical properties of the proposed estimator, characterizing its bias and variance under different conditions. We also compare the performance of the proposed estimator and existing approaches in various synthetic problems, as well as a real-world recommendation dataset. Our experimental results confirm that clustering contexts improves estimation accuracy, especially in deficient information settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Guzman Olivares;Philipp Schmidt;Jacek Golebiowski;Artur Bekasov",
        "authorids": "~Daniel_Guzman_Olivares1;~Philipp_Schmidt1;~Jacek_Golebiowski1;~Artur_Bekasov1",
        "gender": "M;M;M;M",
        "homepage": ";https://scholar.google.de/citations?user=ltdoI1UAAAAJ&hl=de;;http://abksv.me",
        "dblp": ";;314/8576;230/8366",
        "google_scholar": "https://scholar.google.es/citations?user=Yz167rQAAAAJ;https://scholar.google.de/citations?user=ltdoI1UAAAAJ;K5ivIucAAAAJ;https://scholar.google.co.uk/citations?user=aYnWj6cAAAAJ",
        "orcid": "0000-0002-9469-807X;;;",
        "linkedin": "danielguzmanolivares/;;jacek-golebiowski/;arturbekasov/",
        "or_profile": "~Daniel_Guzman_Olivares1;~Philipp_Schmidt1;~Jacek_Golebiowski1;~Artur_Bekasov1",
        "aff": "Autonomous University of Madrid;Amazon;Amazon;Amazon",
        "aff_domain": "uam.es;amazon.com;amazon.com;amazon.com",
        "position": "PhD student;Researcher;Researcher;Applied scientist",
        "bibtex": "@inproceedings{\nolivares2025clustering,\ntitle={Clustering Context in Off-Policy Evaluation},\nauthor={Daniel Guzman Olivares and Philipp Schmidt and Jacek Golebiowski and Artur Bekasov},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=HhhPwHNk6u}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HhhPwHNk6u",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Hn3tLvXfnA",
        "title": "Optimal Multi-Objective Best Arm Identification with Fixed Confidence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider a multi-armed bandit setting with finitely many arms, in which each arm yields an $M$-dimensional vector reward upon selection. We assume that the reward of each dimension (a.k.a. {\\em objective}) is generated independently of the others. The best arm of any given objective is the arm with the largest component of mean corresponding to the objective. The end goal is to identify the best arm of {\\em every} objective in the shortest (expected) time subject to an upper bound on the probability of error (i.e., fixed-confidence regime). We establish a problem-dependent lower bound on the limiting growth rate of the expected stopping time, in the limit of vanishing error probabilities. This lower bound, we show, is characterised by a max-min optimisation problem that is computationally expensive to solve at each time step. We propose an algorithm that uses the novel idea of {\\em surrogate proportions} to sample the arms at each time step, eliminating the need to solve the max-min optimisation problem at each step. We demonstrate theoretically that our algorithm is asymptotically optimal. In addition, we provide extensive empirical studies to substantiate the efficiency of our algorithm.  While existing works on pure exploration with multi-objective multi-armed bandits predominantly focus on {\\em Pareto front identification}, our work fills the gap in the literature by conducting a formal investigation of the multi-objective best arm identification problem.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhirui Chen;P. N. Karthik;Yeow Meng Chee;Vincent Y. F. Tan",
        "authorids": "~Zhirui_Chen1;~P._N._Karthik1;~Yeow_Meng_Chee2;~Vincent_Tan1",
        "gender": "M;M;M;M",
        "homepage": "https://zchen42.github.io/;https://karthikpn.com;;https://www.ece.nus.edu.sg/stfpage/vtan/pubs.htm",
        "dblp": "251/9562;194/3135;c/YeowMengChee.html;60/2327",
        "google_scholar": "MMxA2qAAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.sg/citations?user=99AJNXEAAAAJ;dJoAVvAAAAAJ",
        "orcid": "0009-0000-4684-9999;0000-0001-7798-1159;0000-0001-7823-8068;0000-0002-5008-4527",
        "linkedin": "zhirui-chen-270650308/?originalSubdomain=sg;pnkarthik/;;",
        "or_profile": "~Zhirui_Chen1;~P._N._Karthik1;~Yeow_Meng_Chee2;~Vincent_Tan1",
        "aff": "National University of Singapore+National University of Singapore;Indian Institute of Technology, Hyderabad;National University of Singapore;National University of Singapore",
        "aff_domain": "nus.edu.sg+u.nus.edu;iith.ac.in;nus.edu.sg;nus.edu.sg",
        "position": "Postdoc+PhD student;Assistant Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nchen2025optimal,\ntitle={Optimal Multi-Objective Best Arm Identification with Fixed Confidence},\nauthor={Zhirui Chen and P. N. Karthik and Yeow Meng Chee and Vincent Y. F. Tan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Hn3tLvXfnA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Hn3tLvXfnA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "I7xxYKd4Ol",
        "title": "Post-processing for Fair Regression via Explainable SVD",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents a post-processing algorithm for training fair neural network regression models that satisfy statistical parity, utilizing an explainable singular value decomposition (SVD) of the weight matrix. We propose a linear transformation of the weight matrix, whereby the singular values derived from the SVD of the transformed matrix directly correspond to the differences in the first and second moments of the output distributions across two groups. Consequently, we can convert the fairness constraints into constraints on the singular values. We analytically solve the problem of finding the optimal weights under these constraints. Experimental validation on various datasets demonstrates that our method achieves a similar or superior fairness-accuracy trade-off compared to the baselines without using the sensitive attribute at the inference time.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhiqun Zuo;Ding Zhu;Mohammad Mahdi Khalili",
        "authorids": "~Zhiqun_Zuo1;~Ding_Zhu1;~Mohammad_Mahdi_Khalili3",
        "gender": "M;M;M",
        "homepage": "https://github.com/zuozhiqun;https://caprilovel.github.io/;https://Khalilimahdi.github.io",
        "dblp": "258/4850;;159/2163.html",
        "google_scholar": ";;hSgnKecAAAAJ",
        "orcid": ";;0000-0002-4223-3254",
        "linkedin": ";;mohammad-mahdi-khalili-aa4241127",
        "or_profile": "~Zhiqun_Zuo1;~Ding_Zhu1;~Mohammad_Mahdi_Khalili3",
        "aff": "Ohio State University, Columbus;Ohio State University, Columbus;Ohio State University, Columbus+Yahoo! Research",
        "aff_domain": "osu.edu;osu.edu;osu.edu+yahooinc.com",
        "position": "PhD student;PhD student;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nzuo2025postprocessing,\ntitle={Post-processing for Fair Regression via Explainable {SVD}},\nauthor={Zhiqun Zuo and Ding Zhu and Mohammad Mahdi Khalili},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=I7xxYKd4Ol}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=I7xxYKd4Ol",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IEHddwHTIx",
        "title": "No-Regret Bayesian Optimization with Stochastic Observation Failures",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study Bayesian optimization problems where observation of the objective function fails stochastically, e.g., synthesis failures in materials development. For this problem, although several heuristic methods have been proposed, they do not have theoretical guarantees and sometimes deteriorate in practice. We propose two algorithms that have a trade-off relation between regret bounds and practical performance. The first one is the first no-regret algorithm for this problem. The second one shows superior practical performance; however, we need some modification of the algorithm to obtain a no-regret guarantee, which is slightly worse than the first one. We demonstrate the effectiveness of our methods in numerical experiments, including the simulation function motivated by quasi-crystal synthesis.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shogo Iwazaki;Tomohiko Tanabe;Mitsuru Irie;Shion Takeno;Kota Matsui;Yu Inatsu",
        "authorids": "~Shogo_Iwazaki1;~Tomohiko_Tanabe1;~Mitsuru_Irie1;~Shion_Takeno1;~Kota_Matsui1;~Yu_Inatsu1",
        "gender": "M;M;;M;M;",
        "homepage": ";;;https://takeno1995.github.io/myhomepage/;https://sites.google.com/site/matsuikotaswebpage/cv-english-1;",
        "dblp": "251/9091;;;234/8990;;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;https://scholar.google.co.jp/citations?user=oGaC1SgAAAAJ;;VkCw9O4AAAAJ",
        "orcid": ";0000-0003-1990-191X;0000-0002-7366-6239;0009-0000-3638-8658;;",
        "linkedin": "shogo-iwazaki-0692a1185/;;mitsuruirie/;;;",
        "or_profile": "~Shogo_Iwazaki1;~Tomohiko_Tanabe1;~Mitsuru_Irie1;~Shion_Takeno1;~Kota_Matsui1;~Yu_Inatsu1",
        "aff": "MI-6 Ltd.+LY Corporation;MI-6 Ltd.;MI-6 Ltd.;Nagoya University+RIKEN;Kyoto University+Nagoya University;Nagoya Institute of Technology+Nagoya Institute of Technology",
        "aff_domain": "mi-6.co.jp+lycorp.co.jp;mi-6.co.jp;mi-6.co.jp;nagoya-u.ac.jp+riken.jp;ac.jp+ac.jp;nitech.ac.jp+nitech.ac.jp",
        "position": "Researcher+Researcher;Researcher;COO/CSO;Assistant Professor+Visiting Reseacher;Associate Professor+Lecturer;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\niwazaki2025noregret,\ntitle={No-Regret Bayesian Optimization with Stochastic Observation Failures},\nauthor={Shogo Iwazaki and Tomohiko Tanabe and Mitsuru Irie and Shion Takeno and Kota Matsui and Yu Inatsu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IEHddwHTIx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IEHddwHTIx",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IG44SrW8l9",
        "title": "Time-varying Gaussian Process Bandits with Unknown Prior",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying prior on the unknown black-box function---most of the theoretical literature assumes this prior is known. However, it is common to have more than one possible prior for a given black-box function, for example suggested by domain experts with differing opinions. In some cases, the type-II maximum likelihood estimator for selecting prior enjoys the consistency guarantee, but it does not universally apply to all types of priors. If the problem is stationary, one could rely on the Regret Balancing scheme to conduct the optimisation, but in the case of time-varying problems, such a scheme cannot be used.  To address this gap in existing research, we propose a novel algorithm, PE-GP-UCB, which is capable of solving time-varying Bayesian optimisation problems even without the exact knowledge of the function's prior.  The algorithm relies on the fact that either the observed function values are consistent with some of the priors, in which case it is easy to reject the wrong priors, or the observations are consistent with all candidate priors, in which case it does not matter which prior our model relies on. We provide a regret bound on the proposed algorithm. Finally, we empirically evaluate our algorithm on toy and real-world time-varying problems and show that it outperforms the maximum likelihood estimator, fully Bayesian treatment of unknown prior and Regret Balancing.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juliusz Ziomek;Masaki Adachi;Michael A Osborne",
        "authorids": "~Juliusz_Ziomek1;~Masaki_Adachi1;~Michael_A_Osborne1",
        "gender": ";M;",
        "homepage": ";https://www.masaki-adachi.com;",
        "dblp": ";317/2023;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";masaki-adachi-b349311a2/;",
        "or_profile": "~Juliusz_Ziomek1;~Masaki_Adachi1;~Michael_A_Osborne1",
        "aff": ";Toyota Motor Corporation;",
        "aff_domain": ";mail.toyota.co.jp;",
        "position": ";Principal Researcher;",
        "bibtex": "@inproceedings{\nziomek2025timevarying,\ntitle={Time-varying Gaussian Process Bandits with Unknown Prior},\nauthor={Juliusz Ziomek and Masaki Adachi and Michael A Osborne},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IG44SrW8l9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IG44SrW8l9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IHTYnWc2D9",
        "title": "Online Assortment and Price Optimization Under Contextual Choice Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider an assortment selection and pricing problem in which a seller has $N$ different items available for sale. In each round, the seller observes a $d$-dimensional contextual preference information vector for the user, and offers to the user an assortment of $K$ items at prices chosen by the seller. The user selects at most one of the products from the offered assortment according to a multinomial logit choice model whose parameters are unknown. The seller observes which, if any, item is chosen at the end of each round, with the goal of maximizing cumulative revenue over a selling horizon of length $T$. For this problem, we propose an algorithm that learns from user feedback and achieves a revenue regret of order $\\widetilde{\\mathcal{O}}(d \\sqrt{K T} / L_0 )$ where $L_0$ is the minimum price sensitivity parameter. We also obtain a lower bound of order $\\Omega(d \\sqrt{T}/ L_0)$ for the regret achievable by any algorithm.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yigit Efe Erginbas;Thomas Courtade;Kannan Ramchandran",
        "authorids": "~Yigit_Efe_Erginbas1;~Thomas_Courtade1;~Kannan_Ramchandran1",
        "gender": "M;M;M",
        "homepage": "https://erginbas.github.io/;https://people.eecs.berkeley.edu/~courtade/;https://www.eecs.berkeley.edu/~kannanr/",
        "dblp": ";23/7883.html;53/5765",
        "google_scholar": ";https://scholar.google.com.tw/citations?user=xRmmtzIAAAAJ;https://scholar.google.com.tw/citations?user=DcV-5RAAAAAJ",
        "orcid": "0000-0001-5010-9766;;0000-0002-4567-328X",
        "linkedin": ";;",
        "or_profile": "~Yigit_Efe_Erginbas1;~Thomas_Courtade1;~Kannan_Ramchandran1",
        "aff": "University of California, Berkeley+Amazon;University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu+amazon.com;berkeley.edu;berkeley.edu",
        "position": "PhD student+Intern;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nerginbas2025online,\ntitle={Online Assortment and Price Optimization under Contextual Choice Models},\nauthor={Yigit Efe Erginbas and Thomas Courtade and Kannan Ramchandran},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IHTYnWc2D9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IHTYnWc2D9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IHpS8A9c2x",
        "title": "Multi-agent Multi-armed Bandit Regret Complexity and Optimality",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\\log T)$ for instance-dependent bounds and $\\sqrt{T}$ for mean-gap independent bounds which are tight. Assuming adversarial rewards, we establish a lower bound $O(T^{\\frac{2}{3}})$ for connected graphs, thereby bridging the gap between the lower and upper bound in the prior work. We also show a linear regret lower bound when the graph is disconnected. These lower bounds are made possible through our newly constructed instances. In the numerical study, we assess the performance of various algorithms on these hard instances. While previous works have explored these settings with upper bounds, we provide a thorough study on tight lower bounds.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mengfan Xu;Diego Klabjan",
        "authorids": "~Mengfan_Xu1;~Diego_Klabjan1",
        "gender": "F;M",
        "homepage": "https://mengfanxu1997.github.io/;http://dynresmanagement.com/index.html",
        "dblp": "205/7008;17/105",
        "google_scholar": "MR47V4cAAAAJ;TaQZ_VUAAAAJ",
        "orcid": ";0000-0003-4213-9281",
        "linkedin": "mengfan-xu-4ba804250/;diegoklabjan",
        "or_profile": "~Mengfan_Xu1;~Diego_Klabjan1",
        "aff": "University of Massachusetts at Amherst;Northwestern University",
        "aff_domain": "umass.edu;u.northwestern.edu",
        "position": "Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nxu2025multiagent,\ntitle={Multi-agent Multi-armed Bandit Regret Complexity and Optimality},\nauthor={Mengfan Xu and Diego Klabjan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IHpS8A9c2x}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IHpS8A9c2x",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ILmND4b1BK",
        "title": "Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fourier Neural Operator (FNO) is  a powerful and popular operator learning method. However, FNO is mainly used in forward prediction, yet a great many applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) for jointly tackling the forward and inverse problems. We developed a series of invertible Fourier blocks in the latent channel space to share the model parameters, exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to mitigate challenges of illposedness, data shortage, noises that are common in inverse problems. We proposed a three-step process to combine the invertible blocks and the VAE component for effective training. The evaluations on seven benchmark forward and inverse tasks have demonstrated the advantages of our approach. The code is available at https://github.com/BayesianAIGroup/iFNO.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Da Long;Zhitong Xu;Qiwei Yuan;Yin Yang;Shandian Zhe",
        "authorids": "~Da_Long1;~Zhitong_Xu2;~Qiwei_Yuan1;~Yin_Yang4;~Shandian_Zhe1",
        "gender": "M;M;M;M;",
        "homepage": "https://long-da.github.io/;https://github.com/XZT008;https://joshuayy.github.io/;https://yangzzzy.github.io/;",
        "dblp": ";;;56/2998-2;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;tKAoUN4AAAAJ;-z2_nggAAAAJ;",
        "orcid": ";;;0000-0001-7645-5931;",
        "linkedin": "da-long-utah/;;joshua-yuan-great/;;",
        "or_profile": "~Da_Long1;~Zhitong_Xu2;~Qiwei_Yuan1;~Yin_Yang4;~Shandian_Zhe1",
        "aff": "The University of Utah+The University of Utah;University of Utah;University of Utah;University of Utah;",
        "aff_domain": "cs.utah.edu+umail.utah.edu;cs.utah.edu;utah.edu;utah.edu;",
        "position": "PhD student+PhD student;PhD student;PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\nlong2025invertible,\ntitle={Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems},\nauthor={Da Long and Zhitong Xu and Qiwei Yuan and Yin Yang and Shandian Zhe},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ILmND4b1BK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ILmND4b1BK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IM6PZikJuc",
        "title": "Learning Pareto manifolds in high dimensions: How can regularization help?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Simultaneously addressing multiple objectives is becoming increasingly important in modern machine learning. At the same time, data is often high-dimensional and costly to label.\nFor a single objective such as prediction risk, conventional regularization techniques are known to improve generalization when the data exhibits low-dimensional structure like sparsity. However, it is largely unexplored how to leverage this structure in the context of multi-objective learning (MOL) with multiple competing objectives. \nIn this work, we discuss how the application of vanilla regularization approaches can fail, and propose a two-stage MOL framework that can successfully leverage low-dimensional structure. \nWe demonstrate its effectiveness experimentally for multi-distribution learning and fairness-risk trade-offs.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tobias Wegel;Filip Kova\u010devi\u0107;Alexandru Tifrea;Fanny Yang",
        "authorids": "~Tobias_Wegel1;~Filip_Kova\u010devi\u01071;~Alexandru_Tifrea1;~Fanny_Yang1",
        "gender": "M;M;M;",
        "homepage": "https://sml.inf.ethz.ch/group/tobiasw/;https://research-explorer.ista.ac.at/person/d0258e7b-50b8-11ef-ad56-8b9f537b6b1b;;http://www.fanny-yang.de",
        "dblp": ";;183/4666;126/4852",
        "google_scholar": ";;i7T1FUsAAAAJ;BfDKicQAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Tobias_Wegel1;~Filip_Kova\u010devi\u01071;~Alexandru_Tifrea1;~Fanny_Yang1",
        "aff": "ETHZ - ETH Zurich;Institute of Science and Technology Austria;Google DeepMind;Swiss Federal Institute of Technology",
        "aff_domain": "ethz.ch;ista.ac.at;google.com;ethz.ch",
        "position": "PhD student;PhD student;Researcher;Professor",
        "bibtex": "@inproceedings{\nwegel2025learning,\ntitle={Learning Pareto fronts in high dimensions: How can regularization help?},\nauthor={Tobias Wegel and Filip Kova{\\v{c}}evi{\\'c} and Alexandru Tifrea and Fanny Yang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IM6PZikJuc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IM6PZikJuc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IPTe8pOWmV",
        "title": "An Adaptive Method for Weak Supervision with Drifting Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. In contrast, our algorithm does not require any assumptions on the drift, and it adapts based on the input by dynamically varying its window size. In particular, at each step, our algorithm estimates the current accuracies of the weak supervision sources by identifying a window of past observations that guarantees a near-optimal minimization of the trade-off between the error due to the variance of the estimation and the error due to the drift. Experiments on synthetic and real-world labelers show that our approach adapts to the drift.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alessio Mazzetto;Reza Esfandiarpoor;Akash Singirikonda;Eli Upfal;Stephen Bach",
        "authorids": "~Alessio_Mazzetto1;~Reza_Esfandiarpoor1;~Akash_Singirikonda1;~Eli_Upfal1;~Stephen_Bach1",
        "gender": "M;;M;M;M",
        "homepage": "https://cs.brown.edu/~amazzett/;;;;http://stephenbach.net",
        "dblp": "239/8316.html;;;u/EliUpfal;90/1077",
        "google_scholar": "FkZ0hSsAAAAJ;;;;hs6pGXoAAAAJ",
        "orcid": "0009-0006-5893-0915;;;;0000-0003-3857-3560",
        "linkedin": ";;akash-si/;;",
        "or_profile": "~Alessio_Mazzetto1;~Reza_Esfandiarpoor1;~Akash_Singirikonda1;~Eli_Upfal1;~Stephen_Bach1",
        "aff": "Brown University;;Brown University;Brown University;Brown University+Snorkel AI",
        "aff_domain": "brown.edu;;brown.edu;brown.edu;cs.brown.edu+snorkel.ai",
        "position": "PhD student;;Undergrad student;Full Professor;Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\nmazzetto2025an,\ntitle={An Adaptive Method for Weak Supervision with Drifting Data},\nauthor={Alessio Mazzetto and Reza Esfandiarpoor and Akash Singirikonda and Eli Upfal and Stephen Bach},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IPTe8pOWmV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IPTe8pOWmV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ISmJCFxxhi",
        "title": "High Dimensional Bayesian Optimization using Lasso Variable Selection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian optimization (BO) is a leading method for optimizing expensive black-box optimization and has been successfully applied across various scenarios. However, BO suffers from the curse of dimensionality, making it challenging to scale to high-dimensional problems. Existing work has adopted a variable selection strategy to select and optimize only a subset of variables iteratively. Although this approach can mitigate the high-dimensional challenge in BO, it still leads to sample inefficiency. To address this issue, we introduce a novel method that identifies important variables by estimating the length scales of Gaussian process kernels. Next, we construct an effective search region consisting of multiple subspaces and optimize the acquisition function within this region, focusing on only the important variables. We demonstrate that our proposed method achieves cumulative regret with a sublinear growth rate in the worst case while maintaining computational efficiency. Experiments on high-dimensional synthetic functions and real-world problems show that our method achieves state-of-the-art performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vu Viet Hoang;Hung The Tran;Sunil Gupta;Vu Nguyen",
        "authorids": "~Vu_Viet_Hoang1;~Hung_The_Tran1;~Sunil_Gupta2;~Vu_Nguyen1",
        "gender": "M;;;M",
        "homepage": "https://scholar.google.com/citations?hl=en&user=YMt0pEoAAAAJ&view_op=list_works&gmla=AH70aAXEDS1P5dxGutVciY7HFkzXhG3FHxlCLXc4KIOvdeucunv1R64Ok9XADeDC9JXOWKhFsnmdWJEpA66Qi2kv;;;http://ntienvu.github.io",
        "dblp": "395/5828;;;68/11111",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;https://scholar.google.com.au/citations?user=5RQyC9cAAAAJ",
        "orcid": ";;;0000-0002-0294-4561",
        "linkedin": ";;;tienvunguyen/",
        "or_profile": "~Vu_Viet_Hoang1;~Hung_The_Tran1;~Sunil_Gupta2;~Vu_Nguyen1",
        "aff": "CMC ATI+FPT Software AI Center;;;Amazon",
        "aff_domain": "cmc.com+fpt.com;;;amazon.com",
        "position": "Researcher+AI Resident;;;Machine Learning Scientist",
        "bibtex": "@inproceedings{\nhoang2025high,\ntitle={High Dimensional Bayesian Optimization using Lasso Variable Selection},\nauthor={Vu Viet Hoang and Hung The Tran and Sunil Gupta and Vu Nguyen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ISmJCFxxhi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ISmJCFxxhi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IWxPi5Ha7C",
        "title": "Provable Benefits of Task-Specific Prompts for In-context Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The in-context learning capabilities of modern language models have motivated a deeper mathematical understanding of sequence models. A line of recent work has shown that linear attention models can emulate projected gradient descent iterations to implicitly learn the task vector from the data provided in the context window. In this work, we consider a novel setting where the global task distribution can be partitioned into a union of conditional task distributions. We then examine the use of task-specific prompts and prediction heads for learning the prior information associated with the conditional task distribution using a one-layer attention model. Our results on loss landscape show that task-specific prompts facilitate a covariance-mean decoupling where prompt-tuning explains the conditional mean of the distribution whereas the variance is learned/explained through in-context learning. Incorporating task-specific head further aids this process by entirely decoupling estimation of mean and variance components. This covariance-mean perspective similarly explains how jointly training prompt and attention weights can provably help over fine-tuning after pretraining.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiangyu Chang;Yingcong Li;Muti Kara;Samet Oymak;Amit Roy-Chowdhury",
        "authorids": "~Xiangyu_Chang2;~Yingcong_Li1;muti.kara@ug.bilkent.edu.tr;~Samet_Oymak2;~Amit_Roy-Chowdhury2",
        "gender": "M;;;;",
        "homepage": ";https://yingcong-li.github.io/;;;",
        "dblp": ";244/4435;;;",
        "google_scholar": "mQh2GmoAAAAJ;9uWgjIUAAAAJ;;;",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Xiangyu_Chang2;~Yingcong_Li1;muti.kara@ug.bilkent.edu.tr;~Samet_Oymak2;~Amit_Roy-Chowdhury2",
        "aff": "University of California, Riverside;University of Michigan - Ann Arbor;;;",
        "aff_domain": "ucr.edu;umich.edu;;;",
        "position": "PhD student;PhD student;;;",
        "bibtex": "@inproceedings{\nchang2025provable,\ntitle={Provable Benefits of Task-Specific Prompts for In-context Learning},\nauthor={Xiangyu Chang and Yingcong Li and Muti Kara and Samet Oymak and Amit Roy-Chowdhury},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IWxPi5Ha7C}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IWxPi5Ha7C",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IZv4W76V3T",
        "title": "Global Optimization of Gaussian Process Acquisition Functions Using a Piecewise-Linear Kernel Approximation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian optimization relies on iteratively constructing and optimizing an acquisition function. The latter turns out to be a challenging, non-convex optimization problem itself. Despite the relative importance of this step, most algorithms employ sampling- or gradient-based methods, which do not provably converge to global optima. This work investigates mixed-integer programming (MIP) as a paradigm for *global* acquisition function optimization. Specifically, our Piecewise-linear Kernel Mixed Integer Quadratic Programming (PK-MIQP) formulation introduces a piecewise-linear approximation for Gaussian process kernels and admits a corresponding MIQP representation for acquisition functions. The proposed method is applicable to uncertainty-based acquisition functions for any stationary or dot-product kernel. We analyze the theoretical regret bounds of the proposed approximation, and empirically demonstrate the framework on synthetic functions, constrained benchmarks, and a hyperparameter tuning task.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yilin Xie;Shiqiang Zhang;Joel Paulson;Calvin Tsay",
        "authorids": "~Yilin_Xie1;~Shiqiang_Zhang1;~Joel_Paulson1;~Calvin_Tsay1",
        "gender": "F;M;M;",
        "homepage": ";;;https://www.imperial.ac.uk/people/c.tsay",
        "dblp": ";;;204/0777",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;voYYip0AAAAJ;i59BQe0AAAAJ",
        "orcid": ";;;",
        "linkedin": "yilin-xie-940352197/;;;",
        "or_profile": "~Yilin_Xie1;~Shiqiang_Zhang1;~Joel_Paulson1;~Calvin_Tsay1",
        "aff": "Imperial College London;Imperial College London;University of Wisconsin - Madison+Ohio State University, Columbus;Imperial College London",
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;wisc.edu+osu.edu;imperial.ac.uk",
        "position": "PhD student;PhD student;Associate Professor+Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxie2025global,\ntitle={Global Optimization of Gaussian Process Acquisition Functions Using a Piecewise-Linear Kernel Approximation},\nauthor={Yilin Xie and Shiqiang Zhang and Joel Paulson and Calvin Tsay},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=IZv4W76V3T}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IZv4W76V3T",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Ilm6kS0Wf0",
        "title": "ROTI-GCV: Generalized Cross-Validation for right-ROTationally Invariant Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Two key tasks in high-dimensional regularized regression are tuning the regularization strength for accurate predictions and estimating the out-of-sample risk. It is known that the standard approach \u2014 $k$-fold cross-validation \u2014 is inconsistent in modern high-dimensional settings. While leave-one-out and generalized cross-validation remain consistent in some high-dimensional cases, they become inconsistent when samples are dependent or contain heavy-tailed covariates. As a first step towards modeling structured sample dependence and heavy tails, we use right-rotationally invariant covariate distributions \u2014 a crucial concept from compressed sensing. In the proportional asymptotics regime where the number of features and samples grow comparably, which is known to better reflect the empirical behavior in moderately sized datasets, we introduce a new framework, ROTI-GCV, for reliably performing cross-validation under these challenging conditions. Along the way, we propose new estimators for the signal-to-noise ratio and noise variance. We conduct experiments that demonstrate the accuracy of our approach in a variety of synthetic and semi-synthetic settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kevin Luo;Yufan Li;Pragya Sur",
        "authorids": "~Kevin_Luo1;~Yufan_Li2;~Pragya_Sur1",
        "gender": "M;M;",
        "homepage": "https://kevinzluo.github.io/;https://statistics.fas.harvard.edu/people/yufan-li;http://sites.fas.harvard.edu/~prs499/",
        "dblp": ";;",
        "google_scholar": ";rATgLxcAAAAJ;woJuzsUAAAAJ",
        "orcid": ";0000-0001-5412-3397;",
        "linkedin": "kevinluo0/;;",
        "or_profile": "~Kevin_Luo1;~Yufan_Li2;~Pragya_Sur1",
        "aff": "Harvard University;Harvard University;Harvard University",
        "aff_domain": "harvard.edu;g.harvard.edu;harvard.edu",
        "position": "Undergrad student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nluo2025rotigcv,\ntitle={{ROTI}-{GCV}: Generalized Cross-Validation for right-{ROT}ationally Invariant Data},\nauthor={Kevin Luo and Yufan Li and Pragya Sur},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Ilm6kS0Wf0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ilm6kS0Wf0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "J1CJaSnmKg",
        "title": "Synthetic Potential Outcomes and Causal Mixture Identifiability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Heterogeneous data from multiple populations, sub-groups, or sources can be represented as a \"mixture model\" with a single latent class influencing all of the observed covariates. Heterogeneity can be resolved at different levels by grouping populations according to different notions of similarity. This paper proposes grouping with respect to the causal response of an intervention or perturbation on the system. This is distinct from previous notions, such as grouping by similar covariate values (e.g., clustering) or similar correlations between covariates (e.g., Gaussian mixture models). To solve the problem, we \"synthetically sample\" from a counterfactual distribution using higher-order multi-linear moments of the observable data. To understand how these ``causal mixtures'' fit in with more classical notions, we develop a hierarchy of mixture identifiability.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bijan Mazaheri;Chandler Squires;Caroline Uhler",
        "authorids": "~Bijan_Mazaheri1;~Chandler_Squires1;~Caroline_Uhler1",
        "gender": "M;M;F",
        "homepage": "https://bijanmazaheri.com/;https://chandlersquires.com;https://www.carolineuhler.com/",
        "dblp": ";231/7704;66/10813",
        "google_scholar": "DXEdM48AAAAJ;https://scholar.google.com.tr/citations?user=Nh3BtpUAAAAJ;https://scholar.google.com.tw/citations?user=dIJFcaoAAAAJ",
        "orcid": ";;",
        "linkedin": ";chandler-squires-749885a0/;",
        "or_profile": "~Bijan_Mazaheri1;~Chandler_Squires1;~Caroline_Uhler1",
        "aff": "Dartmouth College;Carnegie Mellon University;Electrical Engineering & Computer Science, Massachusetts Institute of Technology",
        "aff_domain": "dartmouth.edu;andrew.cmu.edu;eecs.mit.edu",
        "position": "Assistant Professor;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nmazaheri2025synthetic,\ntitle={Synthetic Potential Outcomes and Causal Mixture Identifiability},\nauthor={Bijan Mazaheri and Chandler Squires and Caroline Uhler},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=J1CJaSnmKg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=J1CJaSnmKg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "J4CXpKhP1T",
        "title": "Adaptive Extragradient Methods for Root-finding Problems under Relaxed Assumptions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We develop a new class of self-tuning algorithms to solve a root-finding problem involving a Lipschitz continuous operator, with applications in convex optimization, minimax saddle point problems and variational inequalities. Our methods are adaptive to the unknown, problem specific parameters, such as the Lipschitz constant and the variance of the stochastic operator. Unlike prior work, our approach does not rely on restrictive assumptions, such as a bounded domain, boundedness of the operator or a light-tailed distribution. We prove a $\\tilde{\\mathcal{O}}(N^{-1/2})$ average-iterate convergence rate of the restricted merit function under an affine noise assumption, matching the optimal rate up to log factors. In addition, we improve the convergence rate to $\\mathcal{O}(N^{-1})$ under a strong growth condition, characterizing the field of cutting-edge machine learning models and matching the optimal rate for the \\textit{deterministic regime}. Finally, we illustrate the effectiveness of the proposed algorithms through numerical experiments on saddle point problems. Our results suggest that the adaptive step sizes automatically take advantage of the structure of the noise and observe improved convergence in certain settings, such as when the strong growth condition holds. To the best of our knowledge, this is the first method  for root-finding problems under mild assumptions that adapts to the structure of the noise to obtain an improved convergence rate.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yang Luo;Michael J O'Neill",
        "authorids": "~Yang_Luo8;~Michael_J_O'Neill2",
        "gender": "F;M",
        "homepage": ";https://michaeljoneill.github.io/",
        "dblp": ";",
        "google_scholar": ";0ipTvMcAAAAJ",
        "orcid": ";0000-0002-7369-4571",
        "linkedin": "yang-luo-197a31230;",
        "or_profile": "~Yang_Luo8;~Michael_J_O'Neill2",
        "aff": ";University of North Carolina at Chapel Hill",
        "aff_domain": ";unc.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nluo2025adaptive,\ntitle={Adaptive Extragradient Methods for Root-finding Problems under Relaxed Assumptions},\nauthor={Yang Luo and Michael J O'Neill},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=J4CXpKhP1T}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=J4CXpKhP1T",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JFDRlIELhm",
        "title": "LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the \\emph{linear contextual bandit problem} with independent and identically distributed (i.i.d.) contexts. In this problem, we aim to develop a \\emph{Best-of-Both-Worlds} (BoBW) algorithm with regret upper bounds in both stochastic and adversarial regimes. We develop an algorithm based on \\emph{Follow-The-Regularized-Leader} (FTRL) with Tsallis entropy, referred to as the $\\alpha$-\\emph{Linear-Contextual (LC)-Tsallis-INF}. We show that its regret is at most $O(\\log(T))$ in the stochastic regime under the assumption that the suboptimality gap is uniformly bounded from below, and at most $O(\\sqrt{T})$ in the adversarial regime. Furthermore, our regret analysis is extended to more general regimes characterized by the \\emph{margin condition} with a parameter $\\beta \\in (1, \\infty]$, which imposes a milder assumption on the suboptimality gap than in previous studies. We show that the proposed algorithm achieves $O\\left(\\log(T)^{\\frac{1+\\beta}{2+\\beta}}T^{\\frac{1}{2+\\beta}}\\right)$ regret under the margin condition.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Masahiro Kato;Shinji Ito",
        "authorids": "~Masahiro_Kato1;~Shinji_Ito1",
        "gender": "M;M",
        "homepage": "https://masakat0.github.io/;https://researchmap.jp/shinji_ito?lang=en",
        "dblp": ";49/852",
        "google_scholar": "https://scholar.google.co.jp/schhp?hl=ja;https://scholar.google.co.jp/citations?user=GX0V06wAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Masahiro_Kato1;~Shinji_Ito1",
        "aff": "Mizuho\u2013DL Financial Technology+The University of Tokyo;The University of Tokyo",
        "aff_domain": "fintec.co.jp+tokyo.ac.jp;u-tokyo.ac.jp",
        "position": "Researcher+PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nkato2025lctsallisinf,\ntitle={{LC}-Tsallis-{INF}: Generalized Best-of-Both-Worlds Linear Contextual Bandits},\nauthor={Masahiro Kato and Shinji Ito},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JFDRlIELhm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JFDRlIELhm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JHK0QBKdYY",
        "title": "Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Inferring the graph structure from observed data is a key task in graph machine learning to capture the intrinsic relationship between data entities. While significant advancements have been made in learning the structure of homogeneous graphs, many real-world graphs exhibit heterogeneous patterns where nodes and edges have multiple types. This paper fills this gap by introducing the first approach for heterogeneous graph structure learning (HGSL). To this end, we first propose a novel statistical model for the data-generating process (DGP) of heterogeneous graph data, namely hidden Markov networks for heterogeneous graphs (H2MN). Then we formalize HGSL as a maximum a-posterior estimation problem parameterized by such DGP and derive an alternating optimization method to obtain a solution together with a theoretical justification of the optimization conditions. Finally, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate that our proposed method excels in learning structure on heterogeneous graphs in terms of edge type identification and edge weight recovery.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Keyue Jiang;Bohan Tang;Xiaowen Dong;Laura Toni",
        "authorids": "~Keyue_Jiang1;~Bohan_Tang1;~Xiaowen_Dong1;~Laura_Toni1",
        "gender": "M;M;;",
        "homepage": ";https://github.com/tbh-98;https://web.media.mit.edu/~xdong/;https://laspucl2016.com/team/laura-toni/",
        "dblp": ";304/8908;91/9827-1;81/7871",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;_8tUq8kAAAAJ;fQ-oWKUAAAAJ",
        "orcid": ";;;0000-0002-8441-8791",
        "linkedin": "keyue-jiang-6946881a0/?originalSubdomain=cn;;;",
        "or_profile": "~Keyue_Jiang1;~Bohan_Tang1;~Xiaowen_Dong1;~Laura_Toni1",
        "aff": "University College London;University of Oxford;University of Oxford;University College London",
        "aff_domain": "ucl.ac.uk;ox.ac.uk;ox.ac.uk;ucl.ac.uk",
        "position": "PhD student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\njiang2025heterogeneous,\ntitle={Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes},\nauthor={Keyue Jiang and Bohan Tang and Xiaowen Dong and Laura Toni},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JHK0QBKdYY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JHK0QBKdYY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JHO7gsekNl",
        "title": "Spectral Representation for Causal Estimation with Hidden Confounders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of causal effect estimation in the presence of unobserved confounders, focusing on two settings: instrumental variable (IV) regression with additional observed confounders, and proxy causal learning. Our approach uses a singular value decomposition of a conditional expectation operator combined with a saddle-point optimization method. In the IV regression setting, this can be viewed as a neural network generalization of the seminal approach due to Darolles et al. (2011). Saddle-point formulations have recently gained attention because they mitigate the double-sampling bias and are compatible with modern function approximation methods. We provide experimental validation across various settings and show that our approach outperforms existing methods on common benchmarks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haotian Sun;Antoine Moulin;Tongzheng Ren;Arthur Gretton;Bo Dai",
        "authorids": "~Haotian_Sun1;~Antoine_Moulin1;~Tongzheng_Ren1;~Arthur_Gretton1;~Bo_Dai1",
        "gender": "M;M;M;M;",
        "homepage": "https://haotiansun.tech/;https://antoine-moulin.github.io;https://www.cs.utexas.edu/~tzren/;http://www.gatsby.ucl.ac.uk/~gretton/;https://bo-dai.github.io/",
        "dblp": "12/8162;341/1293;211/8004;56/2574;64/2903",
        "google_scholar": "lcWkVCQAAAAJ;W6d2vtMAAAAJ;VgNDYeYAAAAJ;OUv7J6QAAAAJ;TIKl_foAAAAJ",
        "orcid": "0000-0001-9013-7016;;;;0009-0002-8070-574X",
        "linkedin": "haotian-sun-159597218/;antoinemoulin/;;;",
        "or_profile": "~Haotian_Sun1;~Antoine_Moulin1;~Tongzheng_Ren1;~Arthur_Gretton1;~Bo_Dai1",
        "aff": "Georgia Institute of Technology;Universitat Pompeu Fabra;Citadel Securities;Google+University College London;Georgia Institute of Technology+Google Brain",
        "aff_domain": "gatech.edu;upf.edu;citadelsecurities.com;deepmind.com+ucl.ac.uk;gatech.edu+google.com",
        "position": "PhD student;PhD student;Researcher;Researcher+Professor;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nren2025spectral,\ntitle={Spectral Representation for Causal Estimation with Hidden Confounders},\nauthor={Tongzheng Ren and Haotian Sun and Antoine Moulin and Arthur Gretton and Bo Dai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JHO7gsekNl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JHO7gsekNl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JNWYDk3AnT",
        "title": "Unconditionally Calibrated Priors for Beta Mixture Density Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mixture Density Networks (MDNs) allow to model arbitrarily complex mappings between inputs and mixture densities, enabling flexible conditional density estimation, at the risk of severe overfitting. A Bayesian approach can alleviate this problem by specifying a prior over the parameters of the neural network. However, these priors can be difficult to specify due to the lack of interpretability.  We propose a novel neural network construction for conditional mixture densities that allows one to specify the prior in the predictive distribution domain. The construction is based on mapping the targets to the unit hypercube via a diffeomorphism, enabling the use of mixtures of Beta distributions. We prove that the prior predictive distributions are calibrated in the sense that they are equal to the unconditional density function defined by the diffeomorphism.  Contrary to Bayesian Gaussian MDNs, which exhibit tied functional and distributional complexity, we show that our construction allows to decouple them. We propose an extension allowing to model correlations in the covariates via Gaussian copulas, potentially reducing the necessary number of mixture components. Our experiments show competitive performance on standard benchmarks with respect to the state of the art.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alix Lh\u00e9ritier;Maurizio Filippone",
        "authorids": "~Alix_Lh\u00e9ritier1;~Maurizio_Filippone1",
        "gender": "M;M",
        "homepage": ";",
        "dblp": "172/7770;35/5597",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=ILUeAloAAAAJ",
        "orcid": "0000-0002-6056-1470;",
        "linkedin": "alixlheritier/;",
        "or_profile": "~Alix_Lh\u00e9ritier1;~Maurizio_Filippone1",
        "aff": "Amadeus IT Group;King Abdullah University of Science and Technology",
        "aff_domain": "amadeus.com;kaust.edu.sa",
        "position": "Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nlheritier2025unconditionally,\ntitle={Unconditionally Calibrated Priors for Beta Mixture Density Networks},\nauthor={Alix Lh{\\'e}ritier and Maurizio Filippone},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JNWYDk3AnT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JNWYDk3AnT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JOAAvVi1pf",
        "title": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advances in aligning large language models with human preferences have corroborated the growing importance of best-of-$N$ distillation (BOND). However, the iterative BOND algorithm is prohibitively expensive in practice due to the sample and computation inefficiency. This paper addresses the problem by revealing a unified game-theoretic connection between iterative BOND and self-play alignment, which unifies seemingly disparate algorithmic paradigms. Based on the connection, we establish a novel framework, \\textbf{WIN} rate \\textbf{D}ominance~(WIND), with a series of efficient algorithms for regularized win rate dominance optimization that approximates iterative BOND in the parameter space. We provides provable sample efficiency guarantee for one of the WIND variant with the square loss objective. The experimental results confirm that our algorithm not only accelerates the computation, but also achieves superior sample efficiency compared to existing methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tong Yang;Jincheng Mei;Hanjun Dai;Zixin Wen;Shicong Cen;Dale Schuurmans;Yuejie Chi;Bo Dai",
        "authorids": "~Tong_Yang4;~Jincheng_Mei1;~Hanjun_Dai1;~Zixin_Wen1;~Shicong_Cen1;~Dale_Schuurmans1;~Yuejie_Chi1;~Bo_Dai1",
        "gender": "F;M;M;M;;;;",
        "homepage": "https://pptmiao.github.io;https://jinchengmei.github.io;https://hanjun-dai.github.io;;https://www.andrew.cmu.edu/user/shicongc/;;;https://bo-dai.github.io/",
        "dblp": ";149/1408;144/7311;259/1269;241/9590;;;64/2903",
        "google_scholar": ";;obpl7GQAAAAJ;;QIRWZf8AAAAJ;;;TIKl_foAAAAJ",
        "orcid": ";;;;;;;0009-0002-8070-574X",
        "linkedin": ";;hanjun-dai;;;;;",
        "or_profile": "~Tong_Yang4;~Jincheng_Mei1;~Hanjun_Dai1;~Zixin_Wen1;~Shicong_Cen1;~Dale_Schuurmans1;~Yuejie_Chi1;~Bo_Dai1",
        "aff": "Carnegie Mellon University;Google DeepMind;Google Research;Carnegie Mellon University;Carnegie Mellon University;;;Georgia Institute of Technology+Google Brain",
        "aff_domain": "cmu.edu;google.com;google.com;cmu.edu;andrew.cmu.edu;;;gatech.edu+google.com",
        "position": "PhD student;Research Scientist;Researcher;PhD student;PhD student;;;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nyang2025faster,\ntitle={Faster {WIND}: Accelerating Iterative Best-of-\\$N\\$ Distillation  for {LLM} Alignment},\nauthor={Tong Yang and Jincheng Mei and Hanjun Dai and Zixin Wen and Shicong Cen and Dale Schuurmans and Yuejie Chi and Bo Dai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JOAAvVi1pf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JOAAvVi1pf",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JOkn8uueMg",
        "title": "Privacy in Metalearning and Multitask Learning: Modeling and Separations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Model personalization allows a set of individuals, each facing a different learning task, to train models that are more accurate for each person than those they could develop individually. The goals of personalization are captured in a variety of formal frameworks, such as multitask learning and metalearning. Combining data for model personalization poses risks for privacy because the output of an individual's model can depend on the data of other individuals.  In this work we undertake a systematic study of differentially private personalized learning. Our first main contribution is to construct a taxonomy of formal frameworks for private personalized learning. This taxonomy captures different formal frameworks for learning as well as different threat models for the attacker. Our second main contribution is to prove separations between the personalized learning problems corresponding to different choices.  In particular, we prove a novel separation between private multitask learning and private metalearning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Maryam Aliakbarpour;Konstantina Bairaktari;Adam Smith;Marika Swanberg;Jonathan Ullman",
        "authorids": "~Maryam_Aliakbarpour1;~Konstantina_Bairaktari1;~Adam_Smith1;~Marika_Swanberg1;~Jonathan_Ullman1",
        "gender": "F;;M;F;M",
        "homepage": "https://maryamaliakbarpour.com;;http://cs-people.bu.edu/ads22;https://cs-people.bu.edu/marikas/;https://jonathan-ullman.github.io/",
        "dblp": "175/1689;;04/5072;237/9667.html;02/8164",
        "google_scholar": "Q0crxvwAAAAJ;;fkGi-JMAAAAJ;;https://scholar.google.com.tw/citations?user=WfS41RAAAAAJ",
        "orcid": "0000-0001-5064-3221;;;;",
        "linkedin": ";;;marika-swanberg-21b21211b;",
        "or_profile": "~Maryam_Aliakbarpour1;~Konstantina_Bairaktari1;~Adam_Smith1;~Marika_Swanberg1;~Jonathan_Ullman1",
        "aff": "Rice University;;Boston University+Google;Google+Boston University;Northeastern University",
        "aff_domain": "rice.edu;;bu.edu+google.com;google.com+bu.edu;northeastern.edu",
        "position": "Assistant Professor;;Full Professor+Researcher;Researcher+PhD student;Associate Professor",
        "bibtex": "@inproceedings{\naliakbarpour2025privacy,\ntitle={Privacy in Metalearning and Multitask Learning: Modeling and Separations},\nauthor={Maryam Aliakbarpour and Konstantina Bairaktari and Adam Smith and Marika Swanberg and Jonathan Ullman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JOkn8uueMg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JOkn8uueMg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JWLkJrtpKp",
        "title": "Conformal Prediction Under Generalized Covariate Shift with Posterior Drift",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the covariate shift with posterior drift setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Baozhen Wang;Xingye Qiao",
        "authorids": "~Baozhen_Wang1;~Xingye_Qiao1",
        "gender": "M;",
        "homepage": "http://www2.math.binghamton.edu/p/people/grads/wangbao/start;http://people.math.binghamton.edu/qiao/",
        "dblp": ";21/10859",
        "google_scholar": ";O8NqeoQAAAAJ",
        "orcid": ";0000-0003-0937-9822",
        "linkedin": "baozhenwang/;",
        "or_profile": "~Baozhen_Wang1;~Xingye_Qiao1",
        "aff": "State University of New York at Binghamton;State University of New York at Binghamton",
        "aff_domain": "binghamton.edu;binghamton.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nwang2025conformal,\ntitle={Conformal Prediction Under Generalized Covariate Shift with Posterior Drift},\nauthor={Baozhen Wang and Xingye Qiao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JWLkJrtpKp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JWLkJrtpKp",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JYM9QAHMi3",
        "title": "Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In the era of foundation models, Out-of-Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pre-trained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at https://github.com/kthrn22/OOD-Linker",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Katherine Tieu;Dongqi Fu;Jun Wu;Jingrui He",
        "authorids": "~Katherine_Tieu1;~Dongqi_Fu1;~Jun_Wu3;~Jingrui_He1",
        "gender": ";M;M;F",
        "homepage": "https://github.com/kthrn22;https://dongqifu.github.io/;https://junwu6.github.io/;https://www.hejingrui.org",
        "dblp": "396/7084;273/0228;20/3894-19.html;34/2685",
        "google_scholar": "https://scholar.google.com/citations?hl=en;WByXZAcAAAAJ;TZXUS-oAAAAJ;hXpZynkAAAAJ",
        "orcid": "0009-0004-4689-8810;0000-0002-8726-9234;0000-0002-1512-524X;0000-0002-6429-6272",
        "linkedin": "katherine-tieu-ab27b4201/;;jun-wu-08a962176/;",
        "or_profile": "~Katherine_Tieu1;~Dongqi_Fu1;~Jun_Wu3;~Jingrui_He1",
        "aff": "University of Illinois Urbana-Champaign;Meta;Michigan State University;University of Illinois, Urbana Champaign",
        "aff_domain": "illinois.edu;meta.com;msu.edu;illinois.edu",
        "position": "PhD student;Researcher;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\ntieu2025invariant,\ntitle={Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem},\nauthor={Katherine Tieu and Dongqi Fu and Jun Wu and Jingrui He},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JYM9QAHMi3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JYM9QAHMi3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JoDTjOu693",
        "title": "RTD-Lite: Scalable Topological Analysis for Comparing Weighted Graphs in Learning Tasks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Topological methods for comparing weighted graphs are valuable in various learning tasks but often suffer from computational inefficiency on large datasets. We introduce RTD-Lite, a scalable algorithm that efficiently compares topological features, specifically connectivity or cluster structures at arbitrary scales, of two weighted graphs with one-to-one correspondence between vertices. By leveraging minimal spanning trees in auxiliary graphs, RTD-Lite captures topological discrepancies with $O(n^2)$ time and memory complexity. This efficiency enables its application in tasks like dimensionality reduction and neural network training. Experiments on synthetic and real-world datasets demonstrate that RTD-Lite effectively identifies topological differences while significantly reducing computation time compared to existing methods. Moreover, integrating RTD-Lite into neural network training as a loss function component enhances the preservation of topological structures in learned representations. Our code is publicly available at https://github.com/ArGintum/RTD-Lite.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Eduard Tulchinskii;Daria Voronkova;Ilya Trofimov;Evgeny Burnaev;Serguei Barannikov",
        "authorids": "~Eduard_Tulchinskii1;~Daria_Voronkova1;~Ilya_Trofimov1;~Evgeny_Burnaev1;~Serguei_Barannikov1",
        "gender": "M;;;M;",
        "homepage": ";;;http://faculty.skoltech.ru/people/evgenyburnaev;",
        "dblp": "320/8026;;130/0370;144/7845;255/5203",
        "google_scholar": "https://scholar.google.com/citations?hl=ru;;https://scholar.google.ru/citations?user=V1c6KjgAAAAJ;https://scholar.google.ru/citations?user=pCRdcOwAAAAJ;https://scholar.google.fr/citations?user=-soT8KcAAAAJ",
        "orcid": "0009-0003-2794-4140;;0000-0002-2961-7368;0000-0001-8424-0690;0000-0002-9323-0651",
        "linkedin": ";;https://ru.linkedin.com/in/ilya-trofimov-ba122748;;",
        "or_profile": "~Eduard_Tulchinskii1;~Daria_Voronkova1;~Ilya_Trofimov1;~Evgeny_Burnaev1;~Serguei_Barannikov1",
        "aff": "Huawei Technologies Ltd.+Skolkovo Institute of Science and Technology;;Skoltech;Skolkovo Institute of Science and Technology;Skolkovo Institute of Science and Technology+CNRS, Institut Mathematiques de Jussieu, Paris Diderot University",
        "aff_domain": "huawei.com+skoltech.ru;;skoltech.ru;skoltech.ru;skoltech.ru+imj-prg.fr",
        "position": "Intern+PhD student;;Research scientist;Full Professor;Leading Research Scientist+Researcher",
        "bibtex": "@inproceedings{\ntulchinskii2025rtdlite,\ntitle={{RTD}-Lite: Scalable Topological Analysis for Comparing Weighted Graphs in Learning Tasks},\nauthor={Eduard Tulchinskii and Daria Voronkova and Ilya Trofimov and Evgeny Burnaev and Serguei Barannikov},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=JoDTjOu693}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JoDTjOu693",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Jz3Oed8F0g",
        "title": "Strong Screening Rules for Group-based SLOPE Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable to the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fabio Feser;Marina Evangelou",
        "authorids": "~Fabio_Feser1;~Marina_Evangelou1",
        "gender": "M;F",
        "homepage": "https://www.imperial.ac.uk/people/fabio.feser20;https://www.imperial.ac.uk/people/m.evangelou",
        "dblp": "378/6702;",
        "google_scholar": ";",
        "orcid": "0009-0007-3088-9727;",
        "linkedin": "fabio-feser/;",
        "or_profile": "~Fabio_Feser1;~Marina_Evangelou1",
        "aff": "Imperial College London;Imperial College London",
        "aff_domain": "ic.ac.uk;imperial.ac.uk",
        "position": "PhD student;Senior Lecturer",
        "bibtex": "@inproceedings{\nfeser2025strong,\ntitle={Strong Screening Rules for Group-based {SLOPE} Models},\nauthor={Fabio Feser and Marina Evangelou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Jz3Oed8F0g}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Jz3Oed8F0g",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "K6yiVZkm8l",
        "title": "Learning Stochastic Nonlinear Dynamics with Embedded Latent Transfer Operators",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider an operator-based latent Markov representation of a stochastic nonlinear dynamical system, where the stochastic evolution of the latent state embedded in a reproducing kernel Hilbert space is described with the corresponding transfer operator, and develop a spectral method to learn this representation based on the theory of stochastic realization. The embedding may be learned simultaneously using reproducing kernels, for example, constructed with feed-forward neural networks. We also address the generalization of sequential state-estimation (Kalman filtering) in stochastic nonlinear systems, and of operator-based eigen-mode decomposition of dynamics, for the representation. Several examples with synthetic and real-world data are shown to illustrate the empirical characteristics of our methods, and to investigate the performance of our model in sequential state-estimation and mode decomposition.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Naichang Ke;Ryogo Tanaka;Yoshinobu Kawahara",
        "authorids": "naichang.ke@ist.osaka-u.ac.jp;ryogo.tanaka@ist.osaka-u.ac.jp;~Yoshinobu_Kawahara1",
        "gender": ";;M",
        "homepage": ";;https://mls.ist.osaka-u.ac.jp/en/~kawahara/",
        "dblp": ";;09/4700",
        "google_scholar": ";;B8sRETUAAAAJ",
        "orcid": ";;0000-0001-7789-4709",
        "linkedin": ";;",
        "or_profile": "naichang.ke@ist.osaka-u.ac.jp;ryogo.tanaka@ist.osaka-u.ac.jp;~Yoshinobu_Kawahara1",
        "aff": ";;The University of Osaka+RIKEN",
        "aff_domain": ";;ist.osaka-u.ac.jp+riken.jp",
        "position": ";;Full Professor+Team Director",
        "bibtex": "@inproceedings{\nke2025learning,\ntitle={Learning Stochastic Nonlinear Dynamics with Embedded Latent Transfer Operators},\nauthor={Naichang Ke and Ryogo Tanaka and Yoshinobu Kawahara},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=K6yiVZkm8l}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=K6yiVZkm8l",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "K8eGHgAHAz",
        "title": "Reinforcement Learning for Adaptive MCMC",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task.  Yet, to-date it has remained unclear how to exploit modern reinforcement learning technologies for adaptive MCMC.  The aim of this paper is to set out a general framework, called *Reinforcement Learning Metropolis\u2014Hastings*, that is theoretically supported and empirically validated.  Our principal focus is on learning fast-mixing Metropolis\u2014Hastings transition kernels, which we cast as deterministic policies and optimise via a policy gradient.  Control of the learning rate provably ensures conditions for ergodicity are satisfied.  The methodology is used to construct a gradient-free sampler that out-performs a popular gradient-free adaptive Metropolis--Hastings algorithm on $\\approx$90% of tasks in the *PosteriorDB* benchmark.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Congye Wang;Wilson Ye Chen;Heishiro Kanagawa;Chris J. Oates",
        "authorids": "~Congye_Wang1;~Wilson_Ye_Chen2;~Heishiro_Kanagawa1;~Chris_J._Oates1",
        "gender": "M;M;M;",
        "homepage": "https://congyewang.github.io/;;;https://oates.work",
        "dblp": "369/8180.html;https://dblp.uni-trier.de/pers/hd/c/Chen:Wilson_Ye;182/8957;118/6076",
        "google_scholar": "nyt9y-UAAAAJ;jX8lC6EAAAAJ;aS_WmUwAAAAJ;W_Ul5jMAAAAJ",
        "orcid": "0009-0001-0167-1362;;;",
        "linkedin": "congyewang;;;",
        "or_profile": "~Congye_Wang1;~Wilson_Ye_Chen2;~Heishiro_Kanagawa1;~Chris_J._Oates1",
        "aff": "University of Newcastle-upon-Tyne;University of Sydney, Australia;University of Newcastle-upon-Tyne;Newcastle University",
        "aff_domain": "ncl.ac.uk;sydney.edu.au;ncl.ac.uk;ncl.ac.uk",
        "position": "PhD student;Assistant Professor;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nwang2025reinforcement,\ntitle={Reinforcement Learning for Adaptive {MCMC}},\nauthor={Congye Wang and Wilson Ye Chen and Heishiro Kanagawa and Chris J. Oates},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=K8eGHgAHAz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=K8eGHgAHAz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KES1r1Z1on",
        "title": "Conditioning diffusion models by explicit forward-backward bridging",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Given an unconditional diffusion model targeting a joint model $\\pi(x, y)$, using it to perform conditional simulation $\\pi(x \\mid y)$ is still largely an open question and is typically achieved by learning conditional drifts to the denoising SDE after the fact. In this work, we express \\emph{exact} conditional simulation within the \\emph{approximate} diffusion model as an inference problem on an augmented space corresponding to a partial SDE bridge. This perspective allows us to implement efficient and principled particle Gibbs and pseudo-marginal samplers marginally targeting the conditional distribution $\\pi(x \\mid y)$. Contrary to existing methodology, our methods do not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error. We showcase the benefits and drawbacks of our approach on a series of synthetic and real data examples.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Adrien Corenflos;Zheng Zhao;Thomas B. Sch\u00f6n;Simo S\u00e4rkk\u00e4;Jens Sj\u00f6lund",
        "authorids": "~Adrien_Corenflos1;~Zheng_Zhao1;~Thomas_B._Sch\u00f6n1;~Simo_S\u00e4rkk\u00e41;~Jens_Sj\u00f6lund1",
        "gender": "M;;M;M;M",
        "homepage": "https://adriencorenflos.github.io/;;http://user.it.uu.se/~thosc112/index.html;https://users.aalto.fi/~ssarkka/;https://jsjol.github.io",
        "dblp": "284/8438;;85/4891;38/4897;155/3118",
        "google_scholar": "https://scholar.google.co.uk/citations?user=sJJ7FKgAAAAJ;;https://scholar.google.se/citations?user=FUqUC2oAAAAJ;;https://scholar.google.se/citations?user=AlF2g-YAAAAJ",
        "orcid": "0000-0002-8374-4659;;0000-0001-5183-234X;;0000-0002-9099-3522",
        "linkedin": ";;thomas-sch%C3%B6n-2b587b1/;;jens-sj%C3%B6lund/",
        "or_profile": "~Adrien_Corenflos1;~Zheng_Zhao1;~Thomas_B._Sch\u00f6n1;~Simo_S\u00e4rkk\u00e41;~Jens_Sj\u00f6lund1",
        "aff": "University of Warwick;;Uppsala University;Aalto University;Uppsala University",
        "aff_domain": "warwick.ac.uk;;uu.se;aalto.fi;uu.se",
        "position": "Postdoc;;Full Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncorenflos2025conditioning,\ntitle={Conditioning diffusion models by explicit forward-backward bridging},\nauthor={Adrien Corenflos and Zheng Zhao and Thomas B. Sch{\\\"o}n and Simo S{\\\"a}rkk{\\\"a} and Jens Sj{\\\"o}lund},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=KES1r1Z1on}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KES1r1Z1on",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KO7fATqh2W",
        "title": "Separation-Based Distance Measures for Causal Graphs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Assessing the accuracy of the output of causal discovery algorithms is crucial in developing and comparing novel methods. Common evaluation metrics such as the structural Hamming distance are useful for assessing individual links of causal graphs. However, many state-of-the-art causal discovery methods do not output single causal graphs, but rather their Markov equivalence classes (MECs) which encode all of the graph's separation and connection statements. In this work, we propose additional measures of distance that capture the difference in separations of two causal graphs which link-based distances are not fit to assess. The proposed distances have low polynomial time complexity and are applicable to directed acyclic graphs (DAGs) as well as to maximal ancestral graph (MAGs) that may contain bidirected edges. We complement our theoretical analysis with toy examples and empirical experiments that highlight the differences to existing comparison metrics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jonas Wahl;Jakob Runge",
        "authorids": "~Jonas_Wahl1;~Jakob_Runge2",
        "gender": "M;M",
        "homepage": "https://jonaswahl.com;https://www.causalinferencelab.com",
        "dblp": "299/8122;120/7695",
        "google_scholar": "https://scholar.google.de/citations?user=U2j_cnQAAAAJ;https://scholar.google.de/citations?user=wtXVvuUAAAAJ",
        "orcid": ";0000-0002-0629-1772",
        "linkedin": "jonas-wahl-30b84a156/;",
        "or_profile": "~Jonas_Wahl1;~Jakob_Runge2",
        "aff": "German Research Center for AI;Universit\u00e4t Potsdam+Technische Universit\u00e4t Berlin+German Aerospace Center+German Aerospace Center, Institute of Data Science+Technische Universit\u00e4t Dresden",
        "aff_domain": "dfki.de;uni-potsdam.de+tu-berlin.de+dlr.de+dlr.de+tu-dresden.de",
        "position": "Researcher;Full Professor+Associate Professor+Group leader+Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nwahl2025separationbased,\ntitle={Separation-Based Distance Measures for Causal Graphs},\nauthor={Jonas Wahl and Jakob Runge},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=KO7fATqh2W}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KO7fATqh2W",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KRQdmPQsvM",
        "title": "ScoreFusion: Fusing Score-based Generative Models via Kullback\u2013Leibler Barycenters",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We introduce ScoreFusion, a theoretically grounded method for fusing multiple pre-trained diffusion models that are assumed to generate from auxiliary populations. ScoreFusion is particularly useful for enhancing the generative modeling of a target population with limited observed data. Our starting point considers the family of KL barycenters of the auxiliary populations, which is proven to be an optimal parametric class in the KL sense, but difficult to learn. Nevertheless, by recasting the learning problem as score matching in denoising diffusion, we obtain a tractable way of computing the optimal KL barycenter weights. We prove a dimension-free sample complexity bound in total variation distance, provided that the auxiliary models are well-fitted for their own task and the auxiliary tasks combined capture the target well. The sample efficiency of ScoreFusion is demonstrated by learning handwritten digits. We also provide a simple adaptation of a Stable Diffusion denoising pipeline that enables sampling from the KL barycenter of two auxiliary checkpoints; on a portrait generation task, our method produces faces that enhance population heterogeneity relative to the auxiliary distributions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hao Liu;Tony Junze Ye;Jose Blanchet;Nian Si",
        "authorids": "~Hao_Liu37;~Tony_Junze_Ye1;~Jose_Blanchet1;~Nian_Si1",
        "gender": ";;M;M",
        "homepage": ";;https://web.stanford.edu/~jblanche/;http://niansi.me",
        "dblp": ";;75/5093.html;254/2589",
        "google_scholar": ";;https://scholar.google.co.in/citations?user=O24CcQQAAAAJ;",
        "orcid": ";;;",
        "linkedin": "hao-liu-925785170/;;jose-blanchet;",
        "or_profile": "~Hao_Liu37;~Tony_Junze_Ye1;~Jose_Blanchet1;~Nian_Si1",
        "aff": "Stanford University;;Stanford University;Hong Kong University of Science and Technology",
        "aff_domain": "stanford.edu;;stanford.edu;hkust.edu.hk",
        "position": "PhD student;;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025scorefusion,\ntitle={ScoreFusion: Fusing Score-based Generative Models via Kullback{\\textendash}Leibler Barycenters},\nauthor={Hao Liu and Tony Junze Ye and Jose Blanchet and Nian Si},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=KRQdmPQsvM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KRQdmPQsvM",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KrYkbkvYKG",
        "title": "HAVER: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning and Monte Carlo Tree Search",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of estimating the \\emph{value} of the largest mean among $K$ distributions via samples from them (rather than estimating \\emph{which} distribution has the largest mean), which arises from various machine learning tasks including Q-learning and Monte Carlo Tree Search (MCTS). While there have been a few proposed algorithms, their performance analyses have been limited to their biases rather than a precise error metric. In this paper, we propose a novel algorithm called HAVER (Head AVERaging) and analyze its mean squared error. Our analysis reveals that HAVER has a compelling performance in two respects. First, HAVER estimates the maximum mean as well as the oracle who knows the identity of the best distribution and reports its sample mean. Second, perhaps surprisingly, HAVER exhibits even better rates than this oracle when there are many distributions near the best one. Both of these improvements are the first of their kind in the literature, and we also prove that the naive algorithm that reports the largest empirical mean does not achieve these bounds.  Finally, we confirm our theoretical findings via numerical experiments where we implement HAVER in bandit, Q-learning, and MCTS algorithms. In these experiments, HAVER consistently outperforms the baseline methods, demonstrating its effectiveness across different applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tuan Nguyen;Jay Barrett;Kwang-Sung Jun",
        "authorids": "~Tuan_Nguyen4;~Jay_Barrett1;~Kwang-Sung_Jun1",
        "gender": "M;M;M",
        "homepage": ";;http://kwangsungjun.github.io",
        "dblp": "252/5833.html;;88/8411",
        "google_scholar": "QaXb3b4AAAAJ;;VgvC7o8AAAAJ",
        "orcid": ";;",
        "linkedin": "tnguyen9210/;jay-barrett-146b7a305/;",
        "or_profile": "~Tuan_Nguyen4;~Jay_Barrett1;~Kwang-Sung_Jun1",
        "aff": "University of Arizona;Department of Computer Science, Cornell University;University of Arizona",
        "aff_domain": "arizona.edu;cs.cornell.edu;cs.arizona.edu",
        "position": "PhD student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nnguyen2025haver,\ntitle={Haver: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning},\nauthor={Tuan Nguyen and Jay Barrett and Kwang-Sung Jun},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=KrYkbkvYKG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KrYkbkvYKG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KskYMxXVKX",
        "title": "Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we study optimization problems of the form $\\min_x \\max_y f(x, y)$, where $f(x, y)$ is defined on a product Riemannian manifold $\\mathcal{M} \\times \\mathcal{N}$ and is $\\mu_x$-strongly geodesically convex (g-convex) in $x$ and $\\mu_y$-strongly g-concave in $y$, for $\\mu_x, \\mu_y \\geq 0$. We design accelerated methods when $f$ is $(L_x, L_y, L_{xy})$-smooth and $\\mathcal{M}$, $\\mathcal{N}$ are Hadamard. To that aim we introduce new g-convex optimization results, of independent interest: we show global linear convergence for metric-projected Riemannian gradient descent and improve existing accelerated methods by reducing geometric constants. Additionally, we complete the analysis of two previous works applying to the Riemannian min-max case by removing an assumption about iterates staying in a pre-specified compact set.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "David Mart\u00ednez-Rubio;Christophe Roux;Christopher Criscitiello;Sebastian Pokutta",
        "authorids": "~David_Mart\u00ednez-Rubio2;~Christophe_Roux1;~Christopher_Criscitiello1;~Sebastian_Pokutta1",
        "gender": ";;M;M",
        "homepage": ";;https://ccriscitiello.github.io/personalwebsite/;http://www.pokutta.com",
        "dblp": ";;242/8862;75/7718",
        "google_scholar": ";;51Axqz8AAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;chris-criscitiello-149a50104/;",
        "or_profile": "~David_Mart\u00ednez-Rubio2;~Christophe_Roux1;~Christopher_Criscitiello1;~Sebastian_Pokutta1",
        "aff": ";;EPFL - EPF Lausanne;ZIB+TU Berlin",
        "aff_domain": ";;epfl.ch;zib.de+tu-berlin.de",
        "position": ";;PhD student;Vice President+Full Professor",
        "bibtex": "@inproceedings{\nmartinez-rubio2025accelerated,\ntitle={Accelerated Methods for Riemannian Min-Max Problems},\nauthor={David Mart{\\'\\i}nez-Rubio and Christophe Roux and Christopher Criscitiello and Sebastian Pokutta},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=KskYMxXVKX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KskYMxXVKX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "L1yW1pX9YW",
        "title": "Tight Analysis of Difference-of-Convex Algorithm (DCA) Improves Convergence Rates for Proximal Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate a difference-of-convex (DC) formulation where the second term is allowed to be weakly convex. We examine the precise behavior of a single iteration of the difference-of-convex algorithm (DCA), providing a tight characterization of the objective function decrease, distinguishing between six distinct parameter regimes. Our proofs, inspired by the performance estimation framework, are notably simplified compared to related prior research. We subsequently derive sublinear convergence rates for the DCA towards critical points, assuming at least one of the functions is smooth. Additionally, we explore the underexamined equivalence between proximal gradient descent (PGD) and DCA iterations, demonstrating how DCA, a parameter-free algorithm, without the need for a stepsize, serves as a tool for studying the exact convergence rates of PGD. Finally, we propose a method to optimize the DC decomposition to achieve optimal convergence rates, potentially transforming the subtracted function to become weakly convex.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Teodor Rotaru;Panagiotis Patrinos;Fran\u00e7ois Glineur",
        "authorids": "~Teodor_Rotaru1;~Panagiotis_Patrinos1;~Fran\u00e7ois_Glineur1",
        "gender": "M;M;",
        "homepage": ";https://homes.esat.kuleuven.be/~ppatrino/index.html;",
        "dblp": ";55/896;",
        "google_scholar": "Y4dNd9kAAAAJ;Qiwt2t8AAAAJ;",
        "orcid": "0000-0003-2039-6228;0000-0003-4824-7697;",
        "linkedin": ";;",
        "or_profile": "~Teodor_Rotaru1;~Panagiotis_Patrinos1;~Fran\u00e7ois_Glineur1",
        "aff": "KU Leuven+UCLouvain;;",
        "aff_domain": "kuleuven.be+uclouvain.be;;",
        "position": "PhD student+PhD student;;",
        "bibtex": "@inproceedings{\nrotaru2025tight,\ntitle={Tight analysis of Difference-of-Convex Algorithm ({DCA}) improves convergence rates for Proximal Gradient Descent},\nauthor={Teodor Rotaru and Panagiotis Patrinos and Fran{\\c{c}}ois Glineur},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=L1yW1pX9YW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=L1yW1pX9YW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "L9sU4lx63Y",
        "title": "Level Set Teleportation: An Optimization Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study level set teleportation, an optimization routine which tries to accelerate gradient descent (GD) by maximizing the gradient norm over a level set of the objective. While teleportation intuitively speeds-up GD via bigger steps, current work lacks convergence theory for convex functions, guarantees for solving the teleportation operator, and even clear empirical evidence showing this acceleration. We resolve these open questions. For convex functions satisfying Hessian stability, we prove that GD with teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where teleportation neither improves nor worsens convergence. To evaluate teleportation in practice, we develop a projected-gradient method requiring only Hessian-vector products. We use this to show that gradient methods with access to a teleportation oracle out-perform their standard versions on a variety of problems. We also find that GD with teleportation is faster than truncated Newton methods, particularly for non-convex optimization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aaron Mishkin;Alberto Bietti;Robert M. Gower",
        "authorids": "~Aaron_Mishkin1;~Alberto_Bietti1;~Robert_M._Gower1",
        "gender": "M;M;M",
        "homepage": "https://www.cs.stanford.edu/~amishkin/;http://alberto.bietti.me;https://gowerrobert.github.io/",
        "dblp": "230/3809;166/6461;143/0056",
        "google_scholar": "j7qgASIAAAAJ;iT7Tp70AAAAJ;okKw87MAAAAJ",
        "orcid": "0000-0002-5072-2314;;",
        "linkedin": ";;",
        "or_profile": "~Aaron_Mishkin1;~Alberto_Bietti1;~Robert_M._Gower1",
        "aff": "Computer Science Department, Stanford University;Flatiron Institute;Flatiron Institute",
        "aff_domain": "cs.stanford.edu;flatironinstitute.org;simonsfoundation.org",
        "position": "PhD student;Researcher;Researcher",
        "bibtex": "@inproceedings{\nmishkin2025level,\ntitle={Level Set Teleportation: An Optimization Perspective},\nauthor={Aaron Mishkin and Alberto Bietti and Robert M. Gower},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=L9sU4lx63Y}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=L9sU4lx63Y",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LBVD4krAq2",
        "title": "Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "During inference for transformer-based large language models (LLM), prefilling is the computation of the key-value (KV) cache for input tokens in the prompt prior to autoregressive generation. For longer input prompt lengths, prefilling will incur a significant overhead on decoding time. In this work, we highlight the following pitfall of prefilling: for batches containing high-varying prompt lengths, significant computation is wasted by the standard practice of padding sequences to the maximum length. As LLMs increasingly support longer context lengths, potentially up to 10 million tokens, variations in prompt lengths within a batch become more pronounced. To address this, we propose prepacking, a simple yet effective method to optimize prefilling computation. To avoid redundant computation on pad tokens, prepacking combines prompts of varying lengths into a sequence and packs multiple sequences into a compact batch using a bin-packing algorithm. It then modifies the attention mask and positional encoding to compute multiple prefilled KV-caches for multiple prompts within a single sequence. On standard curated dataset containing prompts with varying lengths, we obtain a significant speed and memory efficiency improvements as compared to the default padding-based prefilling computation within Huggingface across a range of base model configurations and inference serving scenarios.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siyan Zhao;Daniel Mingyi Israel;Guy Van den Broeck;Aditya Grover",
        "authorids": "~Siyan_Zhao1;~Daniel_Mingyi_Israel1;~Guy_Van_den_Broeck1;~Aditya_Grover1",
        "gender": "F;M;;M",
        "homepage": "https://siyan-zhao.github.io/;https://danielmisrael.github.io/;;https://aditya-grover.github.io",
        "dblp": "161/3857;;;162/5052",
        "google_scholar": ";;;oOhnPUgAAAAJ",
        "orcid": ";;;",
        "linkedin": ";daniel-israel-248757160;;",
        "or_profile": "~Siyan_Zhao1;~Daniel_Mingyi_Israel1;~Guy_Van_den_Broeck1;~Aditya_Grover1",
        "aff": "University of California, Los Angeles;;;University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;;;ucla.edu",
        "position": "PhD student;;;Assistant Professor",
        "bibtex": "@inproceedings{\nzhao2025prepacking,\ntitle={Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models},\nauthor={Siyan Zhao and Daniel Mingyi Israel and Guy Van den Broeck and Aditya Grover},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=LBVD4krAq2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LBVD4krAq2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LGkLTYLIBq",
        "title": "FreqMoE: Enhancing Time Series Forecasting through Frequency Decomposition Mixture of Experts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Long-term time series forecasting is essential in areas like finance and weather prediction. Besides traditional methods that operate in the time domain, many recent models transform time series data into the frequency domain to better capture complex patterns. However, these methods often use filtering techniques to remove certain frequency signals as noise, which may unintentionally discard important information and reduce prediction accuracy. To address this, we propose the Frequency Decomposition Mixture-of-Experts (FreqMoE) model, which dynamically decomposes time series data into frequency bands, each processed by a specialized expert. A gating mechanism adjusts the importance of each output of expert based on frequency characteristics, and the aggregated results are fed into a prediction module that iteratively refines the forecast using residual connections. Our experiments demonstrate that FreqMoE outperforms state-of-the-art models, achieving the best performance on 51 out of 70 metrics across all tested datasets, while significantly reducing the number of required parameters to under 50k, providing notable efficiency advantages. Code is available at: https://github.com/sunbus100/FreqMoE-main",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziqi Liu",
        "authorids": "~Ziqi_Liu11",
        "gender": "M",
        "homepage": "https://github.com/sunbus100",
        "dblp": "",
        "google_scholar": "",
        "orcid": "0009-0001-9393-9484",
        "linkedin": "ziqi-liu-5a70a2331",
        "or_profile": "~Ziqi_Liu11",
        "aff": "Xi'an Jiaotong-Liverpool University",
        "aff_domain": "xjtlu.edu.cn",
        "position": "Undergrad student",
        "bibtex": "@inproceedings{\nliu2025freqmoe,\ntitle={FreqMoE: Enhancing Time Series Forecasting through Frequency Decomposition Mixture of Experts},\nauthor={Ziqi Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=LGkLTYLIBq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LGkLTYLIBq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LZiQJP6TES",
        "title": "Strategic Conformal Prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "When a machine learning model is deployed, its predictions can alter its environment, as better informed agents strategize to suit their own interests. With such alterations in mind, existing approaches to uncertainty quantification break. In this work we propose a new framework, Strategic Conformal Prediction, which is capable of robust uncertainty quantification in such a setting. Strategic Conformal Prediction is backed by a series of theoretical guarantees spanning marginal coverage, training-conditional coverage, tightness and robustness to misspecification that hold in a distribution-free manner. Experimental analysis further validates our method, showing its remarkable effectiveness in face of arbitrary strategic alterations, whereas other methods break.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Csillag;Claudio Jose Struchiner;Guilherme Tegoni Goedert",
        "authorids": "~Daniel_Csillag1;~Claudio_Jose_Struchiner1;~Guilherme_Tegoni_Goedert1",
        "gender": "M;M;M",
        "homepage": "https://dccsillag.xyz;;https://gtgoedert.com",
        "dblp": "320/6802;;",
        "google_scholar": "8QEfviAAAAAJ;kVh9mCwAAAAJ;",
        "orcid": "0009-0009-9449-0496;0000-0003-2114-847X;0000-0002-4759-1296",
        "linkedin": ";;",
        "or_profile": "~Daniel_Csillag1;~Claudio_Jose_Struchiner1;~Guilherme_Tegoni_Goedert1",
        "aff": "Funda\u00e7\u00e3o Get\u00falio Vargas (FGV);Funda\u00e7\u00e3o Get\u00falio Vargas (FGV);Funda\u00e7\u00e3o Get\u00falio Vargas (FGV)",
        "aff_domain": "fgv.br;fgv.br;fgv.br",
        "position": "Postdoc;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncsillag2025strategic,\ntitle={Strategic Conformal Prediction},\nauthor={Daniel Csillag and Claudio Jose Struchiner and Guilherme Tegoni Goedert},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=LZiQJP6TES}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LZiQJP6TES",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Leyh3mDyof",
        "title": "Change Point Detection in Hadamard Spaces by Alternating Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Time series analysis of non-Euclidean data is highly challenging and crucial for many real-world applications. We address the problem of detecting multiple changes in time series within these complex data spaces. Hadamard spaces, which encompass important data spaces like positive semidefinite matrices, certain Wasserstein spaces, and hyperbolic spaces, provide the right general framework to address this complexity. We propose a computationally efficient two-step iterative optimization algorithm called HOP (Hadamard Optimal Partitioning) that detects changes in the sequence of so-called Fr\u00e9chet means. Under mild conditions, the proposed method consistently estimates the change point locations. HOP is highly versatile, accommodating structural assumptions such as cyclic patterns and epidemic settings, making it unique in the literature. We validate its performance in synthetic and real-world scenarios, including applications in human gait analysis using EMG data with low SNR and behavioral analysis of animal motion.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anica Kostic;Vincent Runge;Charles Truong",
        "authorids": "~Anica_Kostic1;~Vincent_Runge1;~Charles_Truong1",
        "gender": "F;M;Unspecified",
        "homepage": "https://www.lse.ac.uk/Statistics/People/Anica-Kostic;https://vincentrunge.github.io;",
        "dblp": ";;207/9811",
        "google_scholar": ";;https://scholar.google.fr/citations?user=3byuqG4AAAAJ",
        "orcid": ";;0000-0002-8527-8161",
        "linkedin": ";;",
        "or_profile": "~Anica_Kostic1;~Vincent_Runge1;~Charles_Truong1",
        "aff": "London School of Economics and Political Science, University of London;Universit\u00e9 d'Evry Val d'Essonne;Ecole Normale Superieure",
        "aff_domain": "lse.ac.uk;univ-evry.fr;ens-paris-saclay.fr",
        "position": "Postdoc;Assistant Professor;Postdoc",
        "bibtex": "@inproceedings{\nkostic2025change,\ntitle={Change Point Detection in Hadamard Spaces by Alternating Minimization},\nauthor={Anica Kostic and Vincent Runge and Charles Truong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Leyh3mDyof}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Leyh3mDyof",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LwmhRJEt2r",
        "title": "The Hardness of Validating Observational Studies with Experimental Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Observational data is often readily available in large quantities, but can lead to biased causal effect estimates due to the presence of unobserved confounding. Recent works attempt to remove this bias by supplementing observational data with experimental data, which, when available, is typically on a smaller scale due to the time and cost involved in running a randomised controlled trial. In this work, we prove a theorem that places fundamental limits on this ``best of both worlds'' approach. Using the framework of impossible inference, we show that although it is possible to use experimental data to \\emph{falsify} causal effect estimates from observational data, in general it is not possible to \\emph{validate} such estimates. Our theorem proves that while experimental data can be used to detect bias in observational studies, without additional assumptions on the smoothness of the correction function, it can not be used to remove it. We provide a practical example of such an assumption, developing a novel Gaussian Process based approach to construct intervals which contain the true treatment effect with high probability, both inside and outside of the support of the experimental data.  We demonstrate our methodology on both simulated and semi-synthetic datasets and make the \\href{https://github.com/Jakefawkes/Obs_and_exp_data}{code available}.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jake Fawkes;Michael O'Riordan;Athanasios Vlontzos;Oriol Corcoll;Ciar\u00e1n Mark Gilligan-Lee",
        "authorids": "~Jake_Fawkes1;~Michael_O'Riordan1;~Athanasios_Vlontzos1;~Oriol_Corcoll1;~Ciar\u00e1n_Mark_Gilligan-Lee1",
        "gender": "M;M;;;M",
        "homepage": "http://csml.stats.ox.ac.uk/people/;;https://thanosvlo.github.io;;https://www.ciarangilliganlee.com/",
        "dblp": ";;186/8028;;",
        "google_scholar": ";;https://scholar.google.com/citations?view_op=list_works;;https://scholar.google.ca/citations?hl=en",
        "orcid": ";0000-0002-0540-208X;;;",
        "linkedin": ";michaeloriordan4;athanasios-vlontzos/;;ciaran-gilligan-lee/",
        "or_profile": "~Jake_Fawkes1;~Michael_O'Riordan1;~Athanasios_Vlontzos1;~Oriol_Corcoll1;~Ciar\u00e1n_Mark_Gilligan-Lee1",
        "aff": "University of Oxford;Spotify;Spotify;;Spotify+University College London",
        "aff_domain": "oxford.ac.uk;spotify.com;spotify.com;;spotify.com+ucl.ac.uk",
        "position": "PhD student;Researcher;Researcher;;Senior Research Manager+Honorary Associate Professor",
        "bibtex": "@inproceedings{\nfawkes2025the,\ntitle={The Hardness of Validating Observational Studies with Experimental Data},\nauthor={Jake Fawkes and Michael O'Riordan and Athanasios Vlontzos and Oriol Corcoll and Ciar{\\'a}n Mark Gilligan-Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=LwmhRJEt2r}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LwmhRJEt2r",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LysApRPxJ8",
        "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The practical deployment of learning-based autonomous systems would greatly benefit from tools that flexibly obtain safety guarantees in the form of certificate functions from data. While the geometrical properties of such certificate functions are well understood, synthesizing them using machine learning techniques still remains a challenge. To mitigate this issue, we propose a diffeomorphic function learning framework where prior structural knowledge of the desired output is encoded in the geometry of a simple surrogate function, which is subsequently augmented through an expressive, topology-preserving state-space transformation. Thereby, we achieve an indirect function approximation framework that is guaranteed to remain in the desired hypothesis space. To this end, we introduce a novel approach to construct diffeomorphic maps based on RBF networks, which facilitate precise, local transformations around data. Finally, we demonstrate our approach by learning diffeomorphic Lyapunov functions from real-world data and apply our method to different attractor systems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Samuel Tesfazgi;Leonhard Sprandl;Sandra Hirche",
        "authorids": "~Samuel_Tesfazgi1;~Leonhard_Sprandl1;~Sandra_Hirche1",
        "gender": ";M;F",
        "homepage": "https://www.ce.cit.tum.de/itr/tesfazgi/;;http://www.itr.ei.tum.de",
        "dblp": ";;89/6985",
        "google_scholar": ";;",
        "orcid": ";0009-0007-8147-1363;",
        "linkedin": ";;",
        "or_profile": "~Samuel_Tesfazgi1;~Leonhard_Sprandl1;~Sandra_Hirche1",
        "aff": ";;Technical University Munich",
        "aff_domain": ";;tum.de",
        "position": ";;Full Professor",
        "bibtex": "@inproceedings{\ntesfazgi2025learning,\ntitle={Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic {RBF} Networks},\nauthor={Samuel Tesfazgi and Leonhard Sprandl and Sandra Hirche},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=LysApRPxJ8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LysApRPxJ8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Lz1IkjFrYe",
        "title": "Choice is what matters after Attention",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The decoding strategies widely used in large language models (LLMs) today are Top-$p$ Sampling and Top-$k$ Sampling, both of which are methods situated between greedy decoding and random sampling. Inspired by the concept of loss aversion from prospect theory in behavioral economics, and the endowment effect as highlighted by Richard H. Thaler, the 2017 Nobel Memorial Prize in Economic Sciences \u2014 particularly the principle that \"the negative utility of an equivalent loss is approximately twice the positive utility of a comparable gain\" \u2014 we have developed a new decoding strategy called Loss Sampling. We have demonstrated the effectiveness and validity of our method on several LLMs, including Llama-2, Llama-3 and Mistral. Our approach improves text quality by 4-30\\% across four pure text tasks while maintaining diversity in text generation. Furthermore, we also extend our method to multimodal large models (LMs) and Beam Search, demonstrating the effectiveness and versatility of Loss Sampling with improvements ranging from 1-10\\%.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chenhan Fu;Guoming Wang;Juncheng Li;Rongxing Lu;Siliang Tang",
        "authorids": "~Chenhan_Fu1;~Guoming_Wang1;~Juncheng_Li3;~Rongxing_Lu1;~Siliang_Tang1",
        "gender": "M;M;M;M;M",
        "homepage": ";;;http://www.cs.unb.ca/~rlu1/;https://person.zju.edu.cn/en/siliang",
        "dblp": ";22/6650.html;182/7674-6;88/3562.html;44/5693",
        "google_scholar": ";;lm9s-QgAAAAJ;DeBXK0UAAAAJ;8e7H3PcAAAAJ",
        "orcid": "0009-0000-5772-5043;;0000-0003-2258-1291;;0000-0002-7356-9711",
        "linkedin": ";;;;siliang-tang-4734272a/",
        "or_profile": "~Chenhan_Fu1;~Guoming_Wang1;~Juncheng_Li3;~Rongxing_Lu1;~Siliang_Tang1",
        "aff": "Zhejiang University;Zhejiang University;Zhejiang University;Queen's University+University of New Brunswick;Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;queensu.ca+cs.unb.ca;zju.edu.cn",
        "position": "MS student;Assistant Professor;Assistant Professor;Full Professor+Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nfu2025choice,\ntitle={Choice is what matters after Attention},\nauthor={Chenhan Fu and Guoming Wang and Juncheng Li and Rongxing Lu and Siliang Tang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Lz1IkjFrYe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Lz1IkjFrYe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MEM4qrGowq",
        "title": "Online-to-PAC generalization bounds under graph-mixing dependencies",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Traditional generalization results in statistical learning require a training data set made of independently drawn examples. Most of the recent efforts to relax this independence assumption have considered either purely temporal (mixing) dependencies, or graph-dependencies, where non-adjacent vertices correspond to independent random variables. Both approaches have their own limitations, the former requiring a temporal ordered structure, and the latter lacking a way to quantify the strength of inter-dependencies. In this work, we bridge these two lines of work by proposing a framework where dependencies decay with graph distance. We derive generalization bounds leveraging the online-to-PAC framework, by deriving a novel concentration result and introducing an online learning framework incorporating the graph structure. The resulting high-probability generalization guarantees depend on both the mixing rate and the graph's chromatic number.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Baptiste Ab\u00e9l\u00e8s;Gergely Neu;Eugenio Clerico",
        "authorids": "~Baptiste_Ab\u00e9l\u00e8s1;~Gergely_Neu1;~Eugenio_Clerico1",
        "gender": "M;M;M",
        "homepage": ";http://cs.bme.hu/~gergo;https://github.com/eclerico",
        "dblp": ";83/7606;",
        "google_scholar": "PRca6iwAAAAJ;https://scholar.google.ch/citations?user=uz27G84AAAAJ;",
        "orcid": ";;",
        "linkedin": "https://www.linkedin.com/feed/;;",
        "or_profile": "~Baptiste_Ab\u00e9l\u00e8s1;~Gergely_Neu1;~Eugenio_Clerico1",
        "aff": "Universitat Pompeu Fabra;Universitat Pompeu Fabra;Universitat Pompeu Fabra",
        "aff_domain": "upf.edu;upf.edu;upf.edu",
        "position": "PhD student;Assistant Professor;Postdoc",
        "bibtex": "@inproceedings{\nabeles2025onlinetopac,\ntitle={Online-to-{PAC} generalization bounds under graph-mixing dependencies},\nauthor={Baptiste Ab{\\'e}l{\\`e}s and Gergely Neu and Eugenio Clerico},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MEM4qrGowq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MEM4qrGowq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MSYG8XHh0U",
        "title": "Functional Stochastic Gradient MCMC for Bayesian Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Classical parameter-space Bayesian inference for Bayesian neural networks (BNNs) suffers from several unresolved prior issues, such as knowledge encoding intractability and pathological behaviours in deep networks, which can lead to improper posterior inference. To address these issues, functional Bayesian inference has recently been proposed leveraging functional priors, such as the emerging functional variational inference. In addition to variational methods, stochastic gradient Markov Chain Monte Carlo (MCMC) is another scalable and effective inference method for BNNs to asymptotically generate samples from the true posterior by simulating continuous dynamics. However, existing MCMC methods perform solely in parameter space and inherit the unresolved prior issues, while extending these dynamics to function space is a non-trivial undertaking. In this paper, we introduce novel functional MCMC schemes, including stochastic gradient versions, based on newly designed diffusion dynamics that can incorporate more informative functional priors. Moreover, we prove that the stationary measure of these functional dynamics is the target posterior over functions. Our functional MCMC schemes demonstrate improved performance in both predictive accuracy and uncertainty quantification on several tasks compared to naive parameter-space MCMC and functional variational inference.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mengjing Wu;Junyu Xuan;Jie Lu",
        "authorids": "~Mengjing_Wu1;~Junyu_Xuan1;~Jie_Lu4",
        "gender": "F;M;",
        "homepage": ";https://www.uts.edu.au/staff/junyu.xuan;",
        "dblp": "285/1673;08/10768;",
        "google_scholar": "https://scholar.google.com.au/citations?view_op=list_works;https://scholar.google.com.au/citations?user=POQ_yJUAAAAJ;",
        "orcid": ";;",
        "linkedin": "mengjing-wu-a42838333/?originalSubdomain=au;;",
        "or_profile": "~Mengjing_Wu1;~Junyu_Xuan1;~Jie_Lu4",
        "aff": "University of Technology Sydney;University of Technology Sydney;",
        "aff_domain": "uts.edu.au;uts.edu.au;",
        "position": "PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nwu2025functional,\ntitle={Functional Stochastic Gradient {MCMC} for Bayesian Neural Networks},\nauthor={Mengjing Wu and Junyu Xuan and Jie Lu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MSYG8XHh0U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MSYG8XHh0U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MWCHedOm2L",
        "title": "Linearized Wasserstein Barycenters: Synthesis, Analysis, Representational Capacity, and Applications",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose the linear barycentric coding model (LBCM) which utilizes the linear optimal transport (LOT) metric for analysis and synthesis of probability measures.  We provide a closed-form solution to the variational problem characterizing the probability measures in the LBCM and establish equivalence of the LBCM to the set of 2-Wasserstein barycenters in the special case of compatible measures. Computational methods for synthesizing and analyzing measures in the LBCM are developed with finite sample guarantees. One of our main theoretical contributions is to identify an LBCM, expressed in terms of a simple family, which is sufficient to express all probability measures on the closed unit interval.  We show that a natural analogous construction of an LBCM in 2 dimensions fails, and we leave it as an open problem to identify the proper extension in more than 1 dimension.  We conclude by demonstrating the utility of LBCM for covariance estimation and data imputation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Matthew Werenski;Brendan Mallery;Shuchin Aeron;James M. Murphy",
        "authorids": "~Matthew_Werenski1;~Brendan_Mallery1;~Shuchin_Aeron2;~James_M._Murphy1",
        "gender": "M;M;M;M",
        "homepage": ";https://brendan-mallery.com/;https://sites.google.com/view/shuchin-aeron/home?authuser=0;https://jmurphy.math.tufts.edu/",
        "dblp": "312/6714;;14/6374;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;T7TUmRMAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Matthew_Werenski1;~Brendan_Mallery1;~Shuchin_Aeron2;~James_M._Murphy1",
        "aff": ";Tufts University;Tufts University+Tufts University;",
        "aff_domain": ";tufts.edu;tufts.edu+tufts.edu;",
        "position": ";PhD student;Assistant Professor+Associate Professor;",
        "bibtex": "@inproceedings{\nwerenski2025linearized,\ntitle={Linearized Wasserstein Barycenters: Synthesis, Analysis, Representational Capacity, and Applications},\nauthor={Matthew Werenski and Brendan Mallery and Shuchin Aeron and James M. Murphy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MWCHedOm2L}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MWCHedOm2L",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MdE3EIOyz0",
        "title": "Harnessing Causality in Reinforcement Learning with Bagged Decision Times",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider reinforcement learning (RL) for a class of problems with bagged decision times. A bag contains a finite sequence of consecutive decision times. The transition dynamics are non-Markovian and non-stationary within a bag. All actions within a bag jointly impact a single reward, observed at the end of the bag. For example, in mobile health, multiple activity suggestions in a day collectively affect a user's daily commitment to being active. Our goal is to develop an online RL algorithm to maximize the discounted sum of the bag-specific rewards. To handle non-Markovian transitions within a bag, we utilize an expert-provided causal directed acyclic graph (DAG). Based on the DAG, we construct states as a dynamical Bayesian sufficient statistic of the observed history, which results in Markov state transitions within and across bags. We then formulate this problem as a periodic Markov decision process (MDP) that allows non-stationarity within a period. An online RL algorithm based on Bellman equations for stationary MDPs is generalized to handle periodic MDPs. We show that our constructed state achieves the maximal optimal value function among all state constructions for a periodic MDP. Finally, we evaluate the proposed method on testbed variants built from real data in a mobile health clinical trial.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daiqi Gao;Hsin-Yu Lai;Predrag Klasnja;Susan Murphy",
        "authorids": "~Daiqi_Gao1;~Hsin-Yu_Lai1;~Predrag_Klasnja1;~Susan_Murphy1",
        "gender": "F;;M;F",
        "homepage": "https://sites.google.com/view/daiqigao;https://hsinyul.mit.edu/;;https://www.seas.harvard.edu/directory/samurphy",
        "dblp": "275/6269;129/2278;;05/3845",
        "google_scholar": ";N-2NS0oAAAAJ;XKqzS2oAAAAJ;https://scholar.google.com.tw/citations?user=q-DPFdUAAAAJ",
        "orcid": ";0000-0002-5783-3156;;",
        "linkedin": ";hsin-yu-jane-lai-b2547685/;;",
        "or_profile": "~Daiqi_Gao1;~Hsin-Yu_Lai1;~Predrag_Klasnja1;~Susan_Murphy1",
        "aff": "Harvard University;Allen Institute;University of Michigan - Ann Arbor;Harvard University",
        "aff_domain": "fas.harvard.edu;alleninstitute.org;umich.edu;harvard.edu",
        "position": "Postdoc;Researcher;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\ngao2025harnessing,\ntitle={Harnessing Causality in Reinforcement Learning with Bagged Decision Times},\nauthor={Daiqi Gao and Hsin-Yu Lai and Predrag Klasnja and Susan Murphy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MdE3EIOyz0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MdE3EIOyz0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MdKkeGJaw9",
        "title": "Cost-Aware Optimal Pairwise Pure Exploration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pure exploration is one of the fundamental problems in multi-armed bandits (MAB). However, existing works mostly focus on specific pure exploration tasks, without a holistic view of the general pure exploration problem. This work fills this gap by introducing a versatile framework to study pure exploration, with a focus on identifying the pairwise relationships between targeted arm pairs. Moreover, unlike existing works that only optimize the stopping time (i.e., sample complexity), this work considers that arms are associated with potentially different costs and targets at optimizing the cumulative cost that occurred during learning. Under the general framework of pairwise pure exploration with arm-specific costs, a performance lower bound is derived. Then, a novel algorithm, termed CAET (Cost-Aware Pairwise Exploration Task), is proposed. CAET builds on the track-and-stop principle with a novel design to handle the arm-specific costs, which can potentially be zero and thus represent a very challenging case. Theoretical analyses prove that the performance of CAET approaches the lower bound asymptotically. Special cases are further discussed, including an extension to regret minimization, which is another major focus of MAB. The effectiveness and efficiency of CAET are also verified through experimental results under various settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Di Wu;Chengshuai Shi;Ruida Zhou;Cong Shen",
        "authorids": "~Di_Wu45;~Chengshuai_Shi1;~Ruida_Zhou1;~Cong_Shen1",
        "gender": "M;M;M;M",
        "homepage": ";https://chengshuai-shi.github.io/;https://sites.google.com/view/ruida-zhou;https://cshen317.github.io/",
        "dblp": ";259/3938;215/2026;79/6027-1.html",
        "google_scholar": ";twvDiW8AAAAJ;kXbo1twAAAAJ;70LBhKcAAAAJ",
        "orcid": ";0000-0002-2727-8251;;0000-0002-3148-4453",
        "linkedin": "DiWu2001/;;;cong-shen-3372404/",
        "or_profile": "~Di_Wu45;~Chengshuai_Shi1;~Ruida_Zhou1;~Cong_Shen1",
        "aff": ";University of Virginia;Amazon;University of Virginia, Charlottesville",
        "aff_domain": ";virginia.edu;amazon.com;virginia.edu",
        "position": ";PhD student;Applied Scientist;Associate Professor",
        "bibtex": "@inproceedings{\nwu2025costaware,\ntitle={Cost-Aware Optimal Pairwise Pure Exploration},\nauthor={Di Wu and Chengshuai Shi and Ruida Zhou and Cong Shen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MdKkeGJaw9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MdKkeGJaw9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MpeO2sEww3",
        "title": "Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mixture-of-expert (MOE) models are popular methods in machine learning, since they can model heterogeneous behaviour across the space of the data using an ensemble collection of learners. These models are especially useful for modelling dynamic data as time-dependent data often exhibit non-stationarity and heavy-tailed errors, which may be inappropriate to model with a typical single expert model. We propose a mixture of Student-$t$ processes with an adaptive structure for the covariance and noise behaviour for each mixture. Moreover, we use a sequential Monte Carlo (SMC) sampler to perform online inference as data arrive in real time. We demonstrate the superiority of our proposed approach over other models on synthetic and real-world datasets to prove the necessity of the novel method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Taole Sha;Michael Minyi Zhang",
        "authorids": "~Taole_Sha1;~Michael_Minyi_Zhang1",
        "gender": "M;",
        "homepage": ";https://michaelzhang01.github.io/",
        "dblp": ";223/7432",
        "google_scholar": "_ZqXKMUAAAAJ;JFLkLhoAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Taole_Sha1;~Michael_Minyi_Zhang1",
        "aff": "University of Hong Kong;The University of Hong Kong",
        "aff_domain": "hku.hk;hku.hk",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nsha2025online,\ntitle={Online Student-\\$t\\$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data},\nauthor={Taole Sha and Michael Minyi Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MpeO2sEww3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MpeO2sEww3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MtH6kn18BG",
        "title": "Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Optimizing expensive, non-convex, black-box Lipschitz continuous functions presents significant challenges, particularly when the Lipschitz constant of the underlying function is unknown. Such problems often demand numerous function evaluations to approximate the global optimum, which can be prohibitive in terms of time, energy, or resources. In this work, we introduce Every Call is Precious (ECP), a novel global optimization algorithm that minimizes unpromising evaluations by strategically focusing on potentially optimal regions. Unlike previous approaches, ECP eliminates the need to estimate the Lipschitz constant, thereby avoiding additional function evaluations. ECP guarantees no-regret performance for infinite evaluation budgets and achieves minimax-optimal regret bounds within finite budgets. Extensive ablation studies validate the algorithm's robustness, while empirical evaluations show that ECP outperforms 10 benchmark algorithms\u2014including Lipschitz, Bayesian, bandits, and evolutionary methods\u2014across 30 multi-dimensional non-convex synthetic and real-world optimization problems, which positions ECP as a competitive approach for global optimization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fares Fourati;Salma Kharrat;Vaneet Aggarwal;Mohamed-Slim Alouini",
        "authorids": "~Fares_Fourati1;~Salma_Kharrat1;~Vaneet_Aggarwal1;~Mohamed-Slim_Alouini1",
        "gender": "M;F;M;M",
        "homepage": "https://fouratifares.github.io/website/;;;https://cemse.kaust.edu.sa/ctl/people/person/mohamed-slim-alouini",
        "dblp": "275/3371;340/3924.html;91/6560;64/6304",
        "google_scholar": "FAmOUOIAAAAJ;;;",
        "orcid": "0000-0002-6913-7035;;;",
        "linkedin": "fares-fourati-96641914a/?originalSubdomain=tn;salma-kharrat-758231151/;;",
        "or_profile": "~Fares_Fourati1;~Salma_Kharrat1;~Vaneet_Aggarwal1;~Mohamed-Slim_Alouini1",
        "aff": "King Abdullah University of Science and Technology;King Abdullah University of Science and Technology;Purdue University;King Abdullah University of Science and Technology",
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;purdue.edu;kaust.edu.sa",
        "position": "PhD student;PhD student;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nfourati2025every,\ntitle={Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants},\nauthor={Fares Fourati and Salma Kharrat and Vaneet Aggarwal and Mohamed-Slim Alouini},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MtH6kn18BG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MtH6kn18BG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MurWORTaF8",
        "title": "Generalization Bounds for Dependent Data using Online-to-Batch Conversion.",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we give generalization bounds of statistical learning algorithms trained on samples drawn from a dependent data source both in expectation and with high probability, using the Online-to-Batch conversion paradigm. We show that the generalization error of statistical learners in the dependent data setting is equivalent to the generalization error of statistical learners in the i.i.d. setting up to a term that depends on the decay rate of the underlying mixing stochastic process, and is independent of the complexity of the statistical learner. Our proof techniques involve defining a new notion of stability of online learning algorithms based on Wasserstein distances, and employing \u201dnear-martingale\u201d concentration bounds for dependent random variables to arrive at appropriate upper bounds for the generalization error of statistical learners trained on dependent data. Finally, we prove that the Exponential Weighted Averages (EWA) algorithm satisfies our new notion of stability, and instantiate our bounds using the EWA algorithm.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sagnik Chatterjee;MANUJ MUKHERJEE;Alhad Sethi",
        "authorids": "~Sagnik_Chatterjee1;~MANUJ_MUKHERJEE1;~Alhad_Sethi1",
        "gender": ";M;M",
        "homepage": ";https://sites.google.com/view/manuj-mukherjee/home;",
        "dblp": ";140/7562;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;",
        "orcid": ";0000-0003-0220-5862;",
        "linkedin": ";;alhad-sethi/",
        "or_profile": "~Sagnik_Chatterjee1;~MANUJ_MUKHERJEE1;~Alhad_Sethi1",
        "aff": ";Indraprastha Institute of Information Technology, Delhi;Indraprastha Institute of Information Technology, Delhi",
        "aff_domain": ";iiitd.ac.in;iiitd.ac.in",
        "position": ";Assistant Professor;Undergrad student",
        "bibtex": "@inproceedings{\nchatterjee2025generalization,\ntitle={Generalization Bounds for Dependent Data using Online-to-Batch Conversion.},\nauthor={Sagnik Chatterjee and MANUJ MUKHERJEE and Alhad Sethi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MurWORTaF8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MurWORTaF8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MvKNEaQMJT",
        "title": "Meta-learning from Heterogeneous Tensors for Few-shot Tensor Completion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose neural network-based models for tensor completion in few observation settings. The proposed model can meta-learn inductive bias from multiple heterogeneous tensors without shared modes. Although many tensor completion methods have been proposed, the existing methods cannot leverage knowledge across heterogeneous tensors, and their performance is low when only a small number of elements are observed. The proposed model encodes each element of a given tensor by considering information about other elements while reflecting the tensor structure via a self-attention mechanism. The missing values are predicted by tensor-specific linear projection from the encoded vectors. The proposed model is shared across different tensors, and it is meta-learned such that the expected tensor completion performance is improved using multiple tensors. By experiments using synthetic and real-world tensors, we demonstrate that the proposed method achieves better performance than the existing meta-learning and tensor completion methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tomoharu Iwata;Atsutoshi Kumagai",
        "authorids": "~Tomoharu_Iwata1;~Atsutoshi_Kumagai2",
        "gender": "M;M",
        "homepage": "http://www.kecl.ntt.co.jp/as/members/iwata/;https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ&hl=ja",
        "dblp": "29/5953;178/8630",
        "google_scholar": "S1F-gScAAAAJ;https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ",
        "orcid": ";0000-0002-2915-4615",
        "linkedin": "tomoharu-iwata-025a493;",
        "or_profile": "~Tomoharu_Iwata1;~Atsutoshi_Kumagai2",
        "aff": "NTT;NTT",
        "aff_domain": "hco.ntt.co.jp;ntt.co.jp",
        "position": "Researcher;Researcher",
        "bibtex": "@inproceedings{\niwata2025metalearning,\ntitle={Meta-learning from Heterogeneous Tensors for Few-shot Tensor Completion},\nauthor={Tomoharu Iwata and Atsutoshi Kumagai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=MvKNEaQMJT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MvKNEaQMJT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Mwzui5H0VN",
        "title": "Fundamental computational limits of weak learnability in high-dimensional multi-index models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-index models - functions which only depend on the covariates through a non-linear transformation of their projection on a subspace - are a useful benchmark for investigating feature learning with neural networks. This paper examines the theoretical boundaries of efficient learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \\textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\\alpha>0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\\alpha>\\alpha_c$. The critical threshold $\\alpha_{c}$ marks the presence of a computational phase transition, in the sense that it is conjectured that no efficient iterative algorithm can succeed for $\\alpha<\\alpha_c$. In a limited but interesting set of really hard directions -akin to the parity problem- $\\alpha_c$ is found to diverge. Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones. Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Emanuele Troiani;Yatin Dandi;Leonardo Defilippis;Lenka Zdeborova;Bruno Loureiro;Florent Krzakala",
        "authorids": "~Emanuele_Troiani2;~Yatin_Dandi1;~Leonardo_Defilippis1;~Lenka_Zdeborova1;~Bruno_Loureiro1;~Florent_Krzakala1",
        "gender": "M;M;M;F;M;",
        "homepage": ";https://yatindandi.github.io/;;https://www.epfl.ch/labs/spoc/prof-lenka-zdeborova/;https://brloureiro.github.io/;http://Krzakala.org",
        "dblp": "267/5270;255/6032;358/3529;27/6064.html;207/1834;25/1282",
        "google_scholar": "https://scholar.google.fr/citations?user=Gh0snLcAAAAJ;UiEzYkMAAAAJ;https://scholar.google.fr/citations?user=-df-QMIAAAAJ;https://scholar.google.fr/citations?user=gkCjy_UAAAAJ;DXl3ir8AAAAJ;https://scholar.google.fr/citations?user=3jDeUlMAAAAJ",
        "orcid": "0000-0003-0968-7585;;;;0000-0002-6327-4688;0000-0003-2313-2578",
        "linkedin": ";;;;bruno-loureiro-43183b14a/;",
        "or_profile": "~Emanuele_Troiani2;~Yatin_Dandi1;~Leonardo_Defilippis1;~Lenka_Zdeborova1;~Bruno_Loureiro1;~Florent_Krzakala1",
        "aff": "School of Computer and Communication Sciences, EPFL - EPF Lausanne;EPFL - EPF Lausanne;Ecole Normale Sup\u00e9rieure, Ecole Normale Sup\u00e9rieure de Paris;Swiss Federal Institute of Technology Lausanne;Ecole Normale Sup\u00e9rieure, Ecole Normale Sup\u00e9rieure de Paris;Swiss Federal Institute of Technology Lausanne",
        "aff_domain": "ic.epfl.ch;epfl.ch;di.ens.fr;epfl.ch;di.ens.fr;epfl.ch",
        "position": "PhD student;PhD student;PhD student;Associate Professor;Researcher;Full Professor",
        "bibtex": "@inproceedings{\ntroiani2025fundamental,\ntitle={Fundamental computational limits of weak learnability in high-dimensional multi-index models},\nauthor={Emanuele Troiani and Yatin Dandi and Leonardo Defilippis and Lenka Zdeborova and Bruno Loureiro and Florent Krzakala},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Mwzui5H0VN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Mwzui5H0VN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "N5dLFZhu7U",
        "title": "Stochastic Weight Sharing for Bayesian Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, We reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D-adaptive Gaussian distributions, Wasserstein distance estimations, and alpha-blending to encode the stochastic behavior of a BNN in a lower-dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks\u2014including CIFAR-10, CIFAR-100, and ImageNet1k\u2014our approach compresses model parameters by approximately 50\u00d7 and reduces model size by 75% while achieving accuracy and uncertainty estimations comparable to state-of-the-art.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Moule Lin;Shuhao Guan;Weipeng Jing;Goetz Botterweck;Andrea Patane",
        "authorids": "~Moule_Lin2;~Shuhao_Guan1;~Weipeng_Jing1;~Goetz_Botterweck1;~Andrea_Patane1",
        "gender": "M;M;M;M;",
        "homepage": "https://moulelin.github.io/;;;https://www.botterweck.de;",
        "dblp": "306/3260.html;379/6113;163/8496-1;60/2897.html;",
        "google_scholar": "https://scholar.google.com.hk/citations?user=4om_v3AAAAAJ;6wA_yLEAAAAJ;cTw5cnwAAAAJ;xsIAZAkAAAAJ;xRzKYP0AAAAJ",
        "orcid": ";0009-0004-7892-5019;;0000-0002-5556-1660;",
        "linkedin": ";;;;",
        "or_profile": "~Moule_Lin2;~Shuhao_Guan1;~Weipeng_Jing1;~Goetz_Botterweck1;~Andrea_Patane1",
        "aff": "University of Dublin, Trinity College;University College Dublin;;University of Dublin, Trinity College+University of Limerick+Lero;University of Dublin, Trinity College",
        "aff_domain": "tcd.ie;ucdconnect.ie;;tcd.ie+ul.ie+lero.ie;tcd.ie",
        "position": "PhD student;PhD student;;Associate Professor+Researcher+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nlin2025stochastic,\ntitle={Stochastic Weight Sharing for Bayesian Neural Networks},\nauthor={Moule Lin and Shuhao Guan and Weipeng Jing and Goetz Botterweck and Andrea Patane},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=N5dLFZhu7U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=N5dLFZhu7U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "N6U1Ue7YgT",
        "title": "Batch, match, and patch: low-rank approximations for score-based variational inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Black-box variational inference (BBVI) scales poorly to high-dimensional problems when it is used to estimate a multivariate Gaussian\napproximation with a full covariance matrix. In this paper, we extend the _batch-and-match_ (BaM) framework for score-based BBVI to\nproblems where it is prohibitively expensive to store such covariance matrices, let alone to estimate them. Unlike classical algorithms for\nBBVI, which use stochastic gradient descent to minimize the reverse Kullback-Leibler divergence, BaM uses more specialized updates\nto match the scores of the target density and its Gaussian approximation. We extend the updates for BaM by integrating them with a more compact parameterization of full covariance matrices. In particular, borrowing ideas from factor analysis, we add an extra step to\neach iteration of BaM---a _patch_---that projects each newly updated covariance matrix into a more efficiently parameterized family of diagonal plus low rank matrices. We evaluate this approach on a variety of synthetic target distributions and real-world problems in\nhigh-dimensional inference.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chirag Modi;Diana Cai;Lawrence K. Saul",
        "authorids": "~Chirag_Modi1;~Diana_Cai1;~Lawrence_K._Saul3",
        "gender": "M;F;",
        "homepage": ";https://www.dianacai.com;",
        "dblp": "57/6166;191/6693;",
        "google_scholar": "yEh-Tj8AAAAJ;WrLjBYgAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Chirag_Modi1;~Diana_Cai1;~Lawrence_K._Saul3",
        "aff": ";Flatiron Institute;",
        "aff_domain": ";flatiron.org;",
        "position": ";Postdoc;",
        "bibtex": "@inproceedings{\nsaul2025batch,\ntitle={Batch, match, and patch: low-rank approximations for score-based variational inference},\nauthor={Lawrence K. Saul and Chirag Modi and Diana Cai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=N6U1Ue7YgT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=N6U1Ue7YgT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NF1WK6BTRZ",
        "title": "Optimal Time Complexity Algorithms for Computing General Random Walk Graph Kernels on Sparse Graphs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present the first linear time complexity randomized algorithms for unbiased approximation of the celebrated family of general random walk kernels (RWKs) for sparse graphs. This includes both labelled and unlabelled instances. The previous fastest methods for general RWKs were of cubic time complexity and not applicable to labelled graphs. Our method samples dependent random walks to compute novel graph embeddings in $R^{d}$ whose dot product is equal to the true RWK in expectation. It does so without instantiating the direct product graph in memory, meaning we can scale to massive datasets that cannot be stored on a single machine. We derive exponential concentration bounds to prove that our estimator is sharp, and show that the ability to approximate general RWKs (rather than just special cases) unlocks efficient implicit graph kernel learning. Our method is up to **27\u00d7** faster than its counterparts for efficient computation on large graphs and scales to graphs **128\u00d7** bigger than largest examples amenable to brute-force computation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Krzysztof Marcin Choromanski;Isaac Reid;Arijit Sehanobish;Kumar Avinava Dubey",
        "authorids": "~Krzysztof_Marcin_Choromanski1;~Isaac_Reid3;~Arijit_Sehanobish1;~Kumar_Avinava_Dubey1",
        "gender": ";M;M;",
        "homepage": ";https://isaac-reid.github.io;https://github.com/arijitthegame/;",
        "dblp": "78/11411;287/4898;249/5322;",
        "google_scholar": ";3JPyAi0AAAAJ;MEby6-QAAAAJ;",
        "orcid": ";0000-0002-1664-1975;0000-0003-2769-2003;",
        "linkedin": ";;arijit-sehanobish-b76627112/;",
        "or_profile": "~Krzysztof_Marcin_Choromanski1;~Isaac_Reid3;~Arijit_Sehanobish1;~Kumar_Avinava_Dubey1",
        "aff": "Google Brain Robotics & Columbia University;University of Cambridge;Kensho Technologies;",
        "aff_domain": "columbia.edu;cam.ac.uk;kensho.com;",
        "position": "research scientist & adjunct assistant professor;PhD student;Applied Scientist;",
        "bibtex": "@inproceedings{\nchoromanski2025optimal,\ntitle={Optimal Time Complexity Algorithms for Computing General Random Walk Graph Kernels on Sparse Graphs},\nauthor={Krzysztof Marcin Choromanski and Isaac Reid and Kumar Avinava Dubey and Arijit Sehanobish},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=NF1WK6BTRZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NF1WK6BTRZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NgltBQndiq",
        "title": "Learning-Augmented Algorithms for Online Concave Packing and Convex Covering Problems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "*Learning-augmented algorithms* have been extensively studied in the computer science community recently, particularly in the context of online problems, in which machine-learning predictors can help provide additional information about the future, in order to overcome classical impossibility results. Such algorithms use *advice* prudently to improve the performance of classical algorithms, while ensuring robustness against inaccurate advice. In this paper, we present learning-augmented algorithmic frameworks for two fundamental optimizations settings, extending and generalizing prior works. For *online packing with concave objectives*, we present a simple but overarching strategy that *switches* between the advice and the state-of-the-art online algorithm. For *online covering with convex objectives*, we greatly extend primal-dual methods for online convex covering programs and previous learning-augmented framework for online covering linear programs from the literature, to many new applications. We show that our algorithms break impossibility results when the advice is accurate, while maintaining comparable performance with state-of-the-art classical online algorithms even when the advice is erroneous.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Elena Grigorescu;Young-San Lin;Maoyuan Song",
        "authorids": "~Elena_Grigorescu1;~Young-San_Lin1;~Maoyuan_Song1",
        "gender": "F;M;M",
        "homepage": ";https://www.cs.purdue.edu/homes/lin532/;https://maoyuans.github.io",
        "dblp": "07/1562;156/3435;329/6071",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;1W8rVegAAAAJ",
        "orcid": ";0000-0002-5719-6708;0009-0007-9389-5075",
        "linkedin": ";young-san-lin-2106bb89/;",
        "or_profile": "~Elena_Grigorescu1;~Young-San_Lin1;~Maoyuan_Song1",
        "aff": ";;Computer Science Department, Purdue University",
        "aff_domain": ";;cs.purdue.edu",
        "position": ";;PhD student",
        "bibtex": "@inproceedings{\ngrigorescu2025learningaugmented,\ntitle={Learning-Augmented Algorithms for Online Concave Packing and Convex Covering Problems},\nauthor={Elena Grigorescu and Young-San Lin and Maoyuan Song},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=NgltBQndiq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NgltBQndiq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NiyGv5qTzI",
        "title": "Leveraging Frozen Batch Normalization for Co-Training in Source-Free Domain Adaptation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Source-free domain adaptation (SFDA) aims to adapt a source model, initially trained on a fully-labeled source domain, to an unlabeled target domain. Previous works assume that the statistics of Batch Normalization layers in the source model capture domain-specific knowledge and directly replace them with target domain-related statistics during training. However, our observations indicate that *source-like* samples in target data exhibit less deviation in the feature space of the source model when preserving the source domain-relevant statistics. In this paper, we propose co-training the source model with frozen Batch Normalization layers as part of the domain adaptation process. Specifically, we combine the source model and the target model to produce more robust pseudo-labels for *global* class clustering and to identify more precise neighbor samples for *local* neighbor clustering. Extensive experiments validate the effectiveness of our approach, showcasing its superiority over current state-of-the-art methods on three standard benchmarks. Our codes are available on https://github.com/SJTU-dxw/BN-SFDA.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xianwen Deng;Yijun Wang;Zhi Xue",
        "authorids": "~Xianwen_Deng1;~Yijun_Wang3;~Zhi_Xue1",
        "gender": "M;;",
        "homepage": "https://github.com/SJTU-dxw;;",
        "dblp": "315/6039;;44/3322",
        "google_scholar": "NtIOraUAAAAJ;;",
        "orcid": "0000-0002-6611-5295;;",
        "linkedin": ";;",
        "or_profile": "~Xianwen_Deng1;~Yijun_Wang3;~Zhi_Xue1",
        "aff": "Shanghai Jiaotong University;;Shanghai  Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;;sjtu.edu.cn",
        "position": "PhD student;;Full Professor",
        "bibtex": "@inproceedings{\ndeng2025rethinking,\ntitle={Rethinking the Role of Batch Normalization in Source-Free Domain Adaptation},\nauthor={Xianwen Deng and Yijun Wang and Zhi Xue},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=NiyGv5qTzI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NiyGv5qTzI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NzFxCmpTSt",
        "title": "Neural Point Processes for Pixel-wise Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study pixel-wise regression problems with sparsely annotated images. Traditional regression methods based on mean squared error emphasize pixels with labels, leading to distorted predictions in unlabeled areas. To address this limitation, we introduce Neural Point Processes, a novel approach that combines 2D Gaussian Processes with neural networks to leverage spatial correlations between sparse labels on images. This approach offers two key advantages: it imposes smoothness constraints on the model output and enables conditional predictions when sparse labels are available at inference time. Empirical results on synthetic and real-world datasets demonstrate a substantial improvement in mean-squared error and $R^2$ scores, outperforming standard regression techniques. On the real-world dataset COWC, we achieve an $R^2$ of $0.769$ with $81$ out of $40,000$ ($0.2$\\%) points labeled, while standard regression loss (MSE) results in an $R^2$ of $0.060$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chengzhi Shi;G\u00f6zde \u00d6zcan;Miquel Sirera Perell\u00f3;Yuanyuan Li;Nina Iftikhar Shamsi;Stratis Ioannidis",
        "authorids": "~Chengzhi_Shi2;~G\u00f6zde_\u00d6zcan1;~Miquel_Sirera_Perell\u00f31;~Yuanyuan_Li4;~Nina_Iftikhar_Shamsi1;~Stratis_Ioannidis1",
        "gender": ";F;M;F;;M",
        "homepage": ";https://gozde-ozcan.github.io;https://mqsirera.github.io/;;;https://ece.northeastern.edu/fac-ece/ioannidis/",
        "dblp": ";283/6022;366/4777;;;42/6940",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;dSHgvhoAAAAJ;;GPIB5kUAAAAJ",
        "orcid": ";0000-0002-2957-6893;0009-0005-8840-6746;;;0000-0001-8355-4751",
        "linkedin": ";gozde-ozcan/;miquel-sirera-perell%C3%B3-975796215/;;;stratis-ioannidis-87b826110",
        "or_profile": "~Chengzhi_Shi2;~G\u00f6zde_\u00d6zcan1;~Miquel_Sirera_Perell\u00f31;~Yuanyuan_Li4;~Nina_Iftikhar_Shamsi1;~Stratis_Ioannidis1",
        "aff": ";Johnson and Johnson;Northeastern University;;;Northeastern University+Meta, Inc.",
        "aff_domain": ";jnj.com;northeastern.edu;;;northeastern.edu+meta.com",
        "position": ";Postdoc;PhD student;;;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nshi2025neural,\ntitle={Neural Point Processes for Pixel-wise Regression},\nauthor={Chengzhi Shi and G{\\\"o}zde {\\\"O}zcan and Miquel Sirera Perell{\\'o} and Yuanyuan Li and Nina Iftikhar Shamsi and Stratis Ioannidis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=NzFxCmpTSt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NzFxCmpTSt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "O65WHcy4FR",
        "title": "Recursive Learning of Asymptotic Variational Objectives",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "General state-space models (SSMs) are widely used in statistical machine learning and are among the most classical generative models for sequential time-series data. SSMs, comprising latent Markovian states, can be subjected to variational inference (VI), but standard VI methods like the importance-weighted autoencoder (IWAE) lack functionality for streaming data. To enable online VI in SSMs when the observations are received in real time, we propose maximising an IWAE-type variational lower bound on the asymptotic contrast function, rather than the standard IWAE ELBO, using stochastic approximation. Unlike the recursive maximum likelihood method, which directly maximises the asymptotic contrast, our approach, called online sequential IWAE (OSIWAE), allows for online learning of both model parameters and a Markovian recognition model for inferring latent states. By approximating filter state posteriors and their derivatives using sequential Monte Carlo (SMC) methods, we create a particle-based framework for online VI in SSMs. This approach is more theoretically well-founded than recently proposed online variational SMC methods. We provide rigorous theoretical results on the learning objective and a numerical study demonstrating the method's efficiency in learning model parameters and particle proposal kernels.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alessandro Mastrototaro;Mathias M\u00fcller;Jimmy Olsson",
        "authorids": "~Alessandro_Mastrototaro1;~Mathias_M\u00fcller2;~Jimmy_Olsson1",
        "gender": "M;;M",
        "homepage": "https://www.kth.se/profile/alemas;https://www.kth.se/Profile/matmul;https://www.kth.se/profile/jimmyol",
        "dblp": ";;",
        "google_scholar": "https://scholar.google.ca/citations?user=SFmDZS8AAAAJ;;xBHS7MAAAAAJ",
        "orcid": "0000-0001-9380-1197;0009-0008-3893-8666;",
        "linkedin": "alessandro-mastrototaro-41b834147;;",
        "or_profile": "~Alessandro_Mastrototaro1;~Mathias_M\u00fcller2;~Jimmy_Olsson1",
        "aff": "KTH Royal Institute of Technology;KTH Royal Institute of Technology;KTH Royal Institute of Technology",
        "aff_domain": "kth.se;kth.se;kth.se",
        "position": "Instructor;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nmastrototaro2025recursive,\ntitle={Recursive Learning of Asymptotic Variational Objectives},\nauthor={Alessandro Mastrototaro and Mathias M{\\\"u}ller and Jimmy Olsson},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=O65WHcy4FR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=O65WHcy4FR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "O7F8yixRBB",
        "title": "Knowledge Graph Completion with Mixed Geometry Tensor Factorization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we propose a new geometric approach for knowledge graph completion  via low rank tensor approximation. We augment a pretrained and well-established Euclidean model based on a Tucker tensor decomposition with a novel hyperbolic interaction term. This correction enables more nuanced capturing of distributional properties in data better aligned with real-world knowledge graphs. By combining two geometries together, our approach improves expressivity of the resulting model achieving new state-of-the-art link prediction accuracy with a significantly lower number of parameters compared to the previous Euclidean and hyperbolic models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Viacheslav Yusupov;Maxim Rakhuba;Evgeny Frolov",
        "authorids": "~Viacheslav_Yusupov1;~Maxim_Rakhuba1;~Evgeny_Frolov1",
        "gender": "M;;M",
        "homepage": "https://www.hse.ru/org/persons/510448524;;",
        "dblp": ";;",
        "google_scholar": "1DEAoUwAAAAJ;;https://scholar.google.ru/citations?user=l6cMdUEAAAAJ",
        "orcid": ";;0000-0003-3679-5311",
        "linkedin": ";;evgenyfrolov/",
        "or_profile": "~Viacheslav_Yusupov1;~Maxim_Rakhuba1;~Evgeny_Frolov1",
        "aff": "Higher School of Economics;;Artificial Intelligence Research Institute+Higher School of Economics+Skolkovo Institute of Science and Technology",
        "aff_domain": "hse.ru;;airi.net+hse.ru+skoltech.ru",
        "position": "Undergrad student;;Researcher+Associate Professor+Researcher",
        "bibtex": "@inproceedings{\nyusupov2025knowledge,\ntitle={Knowledge  Graph Completion with Mixed Geometry Tensor Factorization},\nauthor={Viacheslav Yusupov and Maxim Rakhuba and Evgeny Frolov},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=O7F8yixRBB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=O7F8yixRBB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ORcZelaB17",
        "title": "S-CFE: Simple Counterfactual Explanations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of finding optimal sparse, manifold-aligned counterfactual explanations for classifiers. Canonically, this can be formulated as an optimization problem with multiple non-convex components, including classifier loss functions and manifold alignment (or _plausibility_) metrics. The added complexity of enforcing _sparsity_, or shorter explanations, complicates the problem further. Existing methods often focus on specific models and plausibility measures, relying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we tackle the canonical formulation using the accelerated proximal gradient (APG) method, a simple yet efficient first-order procedure capable of handling smooth non-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$) regularizers. This enables our approach to seamlessly incorporate various classifiers and plausibility measures while producing sparser solutions. Our algorithm only requires differentiable data-manifold regularizers and supports box constraints for bounded feature ranges, ensuring the generated counterfactuals remain \\emph{actionable}. Finally, experiments on real-world datasets demonstrate that our approach effectively produces sparse, manifold-aligned counterfactual explanations while maintaining proximity to the factual data and computational efficiency.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shpresim Sadiku;Moritz Wagner;Sai Ganesh Nagarajan;Sebastian Pokutta",
        "authorids": "~Shpresim_Sadiku1;~Moritz_Wagner1;~Sai_Ganesh_Nagarajan1;~Sebastian_Pokutta1",
        "gender": "M;M;;M",
        "homepage": "https://www.shpresimsadiku.com/;;https://sites.google.com/view/sgnagarajan/home;http://www.pokutta.com",
        "dblp": "362/3054;180/0026-2;171/6916;75/7718",
        "google_scholar": ";;https://scholar.google.com.sg/citations?user=VoaosL4AAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Shpresim_Sadiku1;~Moritz_Wagner1;~Sai_Ganesh_Nagarajan1;~Sebastian_Pokutta1",
        "aff": "Technische Universit\u00e4t Berlin;Technische Universit\u00e4t Berlin;Zuse Institute Berlin;ZIB+TU Berlin",
        "aff_domain": "tu-berlin.de;tu-berlin.de;zib.de;zib.de+tu-berlin.de",
        "position": "PhD student;PhD student;Postdoc;Vice President+Full Professor",
        "bibtex": "@inproceedings{\nsadiku2025scfe,\ntitle={S-{CFE}: Simple Counterfactual Explanations},\nauthor={Shpresim Sadiku and Moritz Wagner and Sai Ganesh Nagarajan and Sebastian Pokutta},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ORcZelaB17}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ORcZelaB17",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OYr0LiyAxb",
        "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Score distillation has emerged as one of the most prevalent approaches for text-to-3D asset synthesis. Essentially, score distillation updates 3D parameters by lifting and back-propagating scores averaged over different views. In this paper, we reveal that the gradient estimation in score distillation is inherent to high variance. Through the lens of variance reduction, the effectiveness of SDS and VSD can be interpreted as applications of various control variates to the Monte Carlo estimator of the distilled score. Motivated by this rethinking and based on Stein's identity, we propose a more general solution to reduce variance for score distillation, termed *Stein Score Distillation (SSD)*. SSD incorporates control variates constructed by Stein identity, allowing for arbitrary baseline functions. This enables us to include flexible guidance priors and network architectures to explicitly optimize for variance reduction. In our experiments, the overall pipeline, dubbed *SteinDreamer*, is implemented by instantiating the control variate with a monocular depth estimator. The results show that SSD can effectively reduce the distillation variance and consistently improve visual quality for both object- and scene-level generation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peihao Wang;Zhiwen Fan;Dejia Xu;Dilin Wang;Sreyas Mohan;Forrest Iandola;Rakesh Ranjan;YILEI LI;qiang liu;Zhangyang Wang;Vikas Chandra",
        "authorids": "~Peihao_Wang1;~Zhiwen_Fan2;~Dejia_Xu1;~Dilin_Wang1;~Sreyas_Mohan1;~Forrest_Iandola1;~Rakesh_Ranjan2;~YILEI_LI1;~qiang_liu4;~Zhangyang_Wang1;~Vikas_Chandra2",
        "gender": "M;;M;;M;;;;;M;M",
        "homepage": "https://peihaowang.github.io/;;https://ir1d.github.io;;https://sreyas-mohan.github.io;http://forrestiandola.com;;https://liyilui.github.io/personal_page/;;https://vita-group.github.io;https://v-chandra.github.io/",
        "dblp": "239/4075;;264/5685;;200/8516;89/10238;;;;119/4026;57/5163",
        "google_scholar": "fqf2tBsAAAAJ;;ET0e93cAAAAJ;;https://scholar.google.co.in/citations?user=jaobZDsAAAAJ;;;iTp5xFcAAAAJ;;pxFyKAIAAAAJ;p-h_BvcAAAAJ",
        "orcid": ";;0000-0001-8474-3095;;;;;;;;",
        "linkedin": "peihao-wang-25a411162/;;;;;;;;;;vchandra/",
        "or_profile": "~Peihao_Wang1;~Zhiwen_Fan2;~Dejia_Xu1;~Dilin_Wang1;~Sreyas_Mohan1;~Forrest_Iandola1;~Rakesh_Ranjan2;~YILEI_LI1;~qiang_liu4;~Zhangyang_Wang1;~Vikas_Chandra2",
        "aff": "University of Texas, Austin;;Luma AI+University of Texas at Austin;;Meta;Meta;;Meta Facebook;;University of Texas at Austin;Meta",
        "aff_domain": "utexas.edu;;lumalabs.ai+utexas.edu;;meta.com;meta.com;;fb.com;;utexas.edu;meta.com",
        "position": "PhD student;;Researcher+PhD student;;Researcher;Researcher;;Researcher;;Associate Professor;Director, AI",
        "bibtex": "@inproceedings{\nwang2025steindreamer,\ntitle={SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity},\nauthor={Peihao Wang and Zhiwen Fan and Dejia Xu and Dilin Wang and Sreyas Mohan and Forrest Iandola and Rakesh Ranjan and YILEI LI and qiang liu and Zhangyang Wang and Vikas Chandra},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=OYr0LiyAxb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OYr0LiyAxb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OaXS8nUlfj",
        "title": "Rate of Model Collapse in Recursive Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Given the ease of creating synthetic data from machine learning models, new models can be potentially trained on synthetic data generated by previous models. This recursive training process raises concerns about the long-term impact on model quality. As models are recursively trained on generated data from previous rounds, their ability to capture the nuances of the original human-generated data may degrade. This is often referred to as model collapse. In this work, we ask how fast model collapse occurs for some well-studied distribution families under maximum likelihood (ML or near ML) estimation during recursive training. Surprisingly, even for fundamental distributions such as discrete and Gaussian distributions, the exact rate of model collapse is unknown. In this work, we theoretically characterize the rate of collapse in these fundamental settings and complement it with experimental evaluations. Our results show that for discrete distributions, the time to forget a symbol is approximately linearly dependent on the number of times it occurred in the original corpus, and for Gaussian models, the standard deviation reduces to zero roughly at $n$ iterations, where $n$ is the number of samples at each iteration. Both of these findings imply that model forgetting, at least in these simple distributions under near ML estimation with many samples, takes a long time.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ananda Theertha Suresh;Andrew Thangaraj;Aditya Nanda Kishore Khandavally",
        "authorids": "~Ananda_Theertha_Suresh1;~Andrew_Thangaraj1;~Aditya_Nanda_Kishore_Khandavally1",
        "gender": "M;M;M",
        "homepage": "https://theertha.info;https://www.ee.iitm.ac.in/andrew/;",
        "dblp": "119/3884;85/5200;",
        "google_scholar": "K6ef57QAAAAJ;https://scholar.google.co.in/citations?user=GijbplwAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;aditya-nanda-kishore-130185252?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app",
        "or_profile": "~Ananda_Theertha_Suresh1;~Andrew_Thangaraj1;~Aditya_Nanda_Kishore_Khandavally1",
        "aff": "Google;Indian Institute of Technology Madras;",
        "aff_domain": "google.com;iitm.ac.in;",
        "position": "Research Scientist;Full Professor;",
        "bibtex": "@inproceedings{\nsuresh2025rate,\ntitle={Rate of Model Collapse in Recursive Training},\nauthor={Ananda Theertha Suresh and Andrew Thangaraj and Aditya Nanda Kishore Khandavally},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=OaXS8nUlfj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OaXS8nUlfj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ObNPRUia5P",
        "title": "Robust Classification by Coupling Data Mollification with Label Smoothing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Introducing training-time augmentations is a key technique to enhance generalization and prepare deep neural networks against test-time corruptions. Inspired by the success of generative diffusion models, we propose a novel approach of coupling data mollification, in the form of image noising and blurring, with label smoothing to align predicted label confidences with image degradation. The method is simple to implement, introduces negligible overheads, and can be combined with existing augmentations. We demonstrate improved robustness and uncertainty quantification on the corrupted image benchmarks of CIFAR, TinyImageNet and ImageNet datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Markus Heinonen;Ba-Hien Tran;Michael Kampffmeyer;Maurizio Filippone",
        "authorids": "~Markus_Heinonen1;~Ba-Hien_Tran2;~Michael_Kampffmeyer1;~Maurizio_Filippone1",
        "gender": "M;;M;M",
        "homepage": "https://users.aalto.fi/~heinom10/;;https://sites.google.com/view/michaelkampffmeyer;",
        "dblp": "22/7709;;191/9382;35/5597",
        "google_scholar": "hFtfHZoAAAAJ;;https://scholar.google.no/citations?user=9lDh2UgAAAAJ;https://scholar.google.com.tw/citations?user=ILUeAloAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Markus_Heinonen1;~Ba-Hien_Tran2;~Michael_Kampffmeyer1;~Maurizio_Filippone1",
        "aff": "Aalto University;;UiT The Arctic University of Norway+Norwegian Computing Center;King Abdullah University of Science and Technology",
        "aff_domain": "aalto.fi;;uit.no+nr.no;kaust.edu.sa",
        "position": "Researcher;;Full Professor+Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nheinonen2025robust,\ntitle={Robust Classification by Coupling Data Mollification with Label Smoothing},\nauthor={Markus Heinonen and Ba-Hien Tran and Michael Kampffmeyer and Maurizio Filippone},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ObNPRUia5P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ObNPRUia5P",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OcZboffErr",
        "title": "Learning to Forget: Bayesian Time Series Forecasting using Recurrent Sparse Spectrum Signature Gaussian Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The signature kernel is a kernel between time series of arbitrary length and comes with strong theoretical guarantees from stochastic analysis. It has found applications in machine learning such as covariance functions for Gaussian processes.\n  A strength of the underlying signature features is that they provide a structured global description of a time series. However, this property can quickly become a curse when local information is essential and forgetting is required; so far this has only been addressed with ad-hoc methods such as slicing the time series into smaller segments.\n  To overcome this, we propose a principled and data-driven approach by introducing a novel forgetting mechanism for signature features.\n  This allows the model to dynamically adapt its observed context length to focus on more recent information. \n  To achieve this, we revisit the recently introduced Random Fourier Signature Features, and develop Random Fourier Decayed Signature Features (RFDSF) with Gaussian processes (GPs).\n  This results in a Bayesian time series forecasting algorithm with variational inference, that offers a scalable probabilistic algorithm that processes and transforms a time series into a joint predictive distribution over the time steps in one pass using recurrence. For example, processing a sequence of length $10^4$ steps in less than $10^{-2}$ seconds and in $\\approx$ 1GB of GPU memory. We demonstrate that the algorithm outperforms other GP-based alternatives and competes with state-of-the-art probabilistic time series forecasting algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Csaba T\u00f3th;Masaki Adachi;Michael A Osborne;Harald Oberhauser",
        "authorids": "~Csaba_T\u00f3th1;~Masaki_Adachi1;~Michael_A_Osborne1;~Harald_Oberhauser1",
        "gender": ";M;;",
        "homepage": ";https://www.masaki-adachi.com;;https://www.maths.ox.ac.uk/people/harald.oberhauser",
        "dblp": ";317/2023;;175/1262",
        "google_scholar": ";;;pQ7hxSIAAAAJ",
        "orcid": ";;;",
        "linkedin": ";masaki-adachi-b349311a2/;;",
        "or_profile": "~Csaba_T\u00f3th1;~Masaki_Adachi1;~Michael_A_Osborne1;~Harald_Oberhauser1",
        "aff": ";Toyota Motor Corporation;;University of Oxford",
        "aff_domain": ";mail.toyota.co.jp;;oxford.ac.uk",
        "position": ";Principal Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\ntoth2025learning,\ntitle={Learning to Forget: Bayesian Time Series Forecasting using Recurrent Sparse Spectrum Signature Gaussian Processes},\nauthor={Csaba T{\\'o}th and Masaki Adachi and Michael A Osborne and Harald Oberhauser},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=OcZboffErr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OcZboffErr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Oq3ldAhvEC",
        "title": "AlleNoise - large-scale text classification benchmark dataset with real-world label noise",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Label noise remains a challenge for training robust classification models. Most methods for mitigating label noise have been benchmarked using primarily datasets with synthetic noise. While the need for datasets with realistic noise distribution has partially been addressed by web-scraped benchmarks such as WebVision and Clothing1M, those benchmarks are restricted to the computer vision domain. With the growing importance of Transformer-based models, it is crucial to establish text classification benchmarks for learning with noisy labels. \n\nIn this paper, we present AlleNoise, a new curated text classification dataset with real-world instance-dependent label noise, containing over 500,000 examples across approximately 5600 classes, complemented with a meaningful, hierarchical taxonomy of categories. The noise distribution comes from actual users of a major e-commerce marketplace, so it realistically reflects the semantics of human mistakes. In addition to the noisy labels, we provide human-verified clean labels, which help to get a deeper insight into the noise distribution, unlike web-scraped datasets typically used in the field. We demonstrate that a representative selection of established methods for learning with noisy labels is inadequate to handle such real-world noise. In addition, we show evidence that these algorithms do not alleviate excessive memorization. As such, with AlleNoise, we set a high bar for the development of label noise methods that can handle real-world label noise in text classification tasks. The code and dataset are available for download at https://github.com/allegro/AlleNoise.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alicja R\u0105czkowska;Aleksandra Osowska-Kurczab;Jacek Szczerbi\u0144ski;Kalina Jasinska-Kobus;Klaudia Nazarko",
        "authorids": "~Alicja_R\u0105czkowska1;~Aleksandra_Osowska-Kurczab1;~Jacek_Szczerbi\u0144ski1;~Kalina_Jasinska-Kobus1;~Klaudia_Nazarko1",
        "gender": "F;F;M;;F",
        "homepage": "https://pl.linkedin.com/in/alicjar%C4%85czkowska/pl;;;;",
        "dblp": "383/8196;;;;",
        "google_scholar": "uoxkPlwAAAAJ;zkC4zlAAAAAJ;OJjMoPoAAAAJ;;",
        "orcid": "0000-0001-5901-4595;0000-0001-5764-522X;;;",
        "linkedin": "https://pl.linkedin.com/in/alicjar%C4%85czkowska/pl;aleksandra-osowska-kurczab?trk=contact-info;szcz/;;https://linkedin.com/in/klaudianazarko/",
        "or_profile": "~Alicja_R\u0105czkowska1;~Aleksandra_Osowska-Kurczab1;~Jacek_Szczerbi\u0144ski1;~Kalina_Jasinska-Kobus1;~Klaudia_Nazarko1",
        "aff": "Allegro;IDEAS NCBR Sp.;Allegro ;;Allegro Sp. z o.o.",
        "aff_domain": "allegro.com;ideas-ncbr.pl;ml.allegro.tech;;allegro.pl",
        "position": "Researcher;Postdoc;Researcher;;Researcher",
        "bibtex": "@inproceedings{\nraczkowska2025allenoise,\ntitle={AlleNoise - large-scale text classification benchmark dataset with real-world label noise},\nauthor={Alicja R{\\k{a}}czkowska and Aleksandra Osowska-Kurczab and Jacek Szczerbi{\\'n}ski and Kalina Jasinska-Kobus and Klaudia Nazarko},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Oq3ldAhvEC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Oq3ldAhvEC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OqOf8mtvVW",
        "title": "Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without prior knowledge about the system\u2019s non-stationarity. A state-of-the-art, black-box algorithm, known as MASTER, is considered, with a focus on identifying the conditions under which it can achieve its stated goals. Specifically, we prove that MASTER's non-stationarity detection mechanism is not triggered for practical choices of horizon, leading to performance akin to a random restarting algorithm. Moreover, we show that the regret bound for MASTER, while being order optimal, stays above the worst-case linear regret until unreasonably large values of the horizon. To validate these observations, MASTER is tested for the special case of piecewise stationary multi-armed bandits, along with methods that employ random restarting, and others that use quickest change detection to restart. A simple, order optimal random restarting algorithm, that has prior knowledge of the non-stationarity is proposed as a baseline. The behavior of the MASTER algorithm is validated in simulations, and it is shown that methods employing quickest change detection are more robust and consistently outperform MASTER and other random restarting approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Argyrios Gerogiannis;Yu-Han Huang;Venugopal Veeravalli",
        "authorids": "~Argyrios_Gerogiannis1;~Yu-Han_Huang2;~Venugopal_Veeravalli1",
        "gender": ";M;M",
        "homepage": ";;http://vvv.ece.illinois.edu",
        "dblp": ";;",
        "google_scholar": ";mQ9nKjAAAAAJ;gruYQKgAAAAJ",
        "orcid": ";;0000-0001-5490-0037",
        "linkedin": ";yu-han-huang-280b66233/;",
        "or_profile": "~Argyrios_Gerogiannis1;~Yu-Han_Huang2;~Venugopal_Veeravalli1",
        "aff": ";University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",
        "aff_domain": ";illinois.edu;illinois.edu",
        "position": ";PhD student;Full Professor",
        "bibtex": "@inproceedings{\ngerogiannis2025is,\ntitle={Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?},\nauthor={Argyrios Gerogiannis and Yu-Han Huang and Venugopal Veeravalli},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=OqOf8mtvVW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OqOf8mtvVW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Ot92gX5gpJ",
        "title": "Dynamic DBSCAN with Euler Tour Sequences",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a fast and dynamic algorithm for Density-Based Spatial Clustering of Applications with Noise (DBSCAN) that efficiently supports online updates.\nTraditional DBSCAN algorithms, designed for batch processing, become computationally expensive when applied to dynamic datasets, particularly in large-scale applications where data continuously evolves.\nTo address this challenge, our algorithm leverages the Euler Tour Trees data structure, enabling dynamic clustering updates without the need to reprocess the entire dataset.\nThis approach preserves a near-optimal accuracy in density estimation, as achieved by the state-of-the-art static DBSCAN method (Esfandiari et al., 2021). \nOur method achieves an improved time complexity of $O(d \\log^3(n) + \\log^4(n))$ for every\ndata point insertion and deletion, where $n$ and $d$ denote the total number of updates and the data dimension, respectively.\nEmpirical studies also demonstrate significant speedups over conventional DBSCANs in real-time clustering of dynamic datasets, while maintaining comparable or superior clustering quality.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Seiyun Shin;Ilan Shomorony;Peter Macgregor",
        "authorids": "~Seiyun_Shin1;~Ilan_Shomorony1;~Peter_Macgregor1",
        "gender": "M;M;",
        "homepage": "https://seiyun-shin.github.io/;http://www.ilanshomorony.com;https://pmacg.io",
        "dblp": "180/8229;31/9223;294/8868",
        "google_scholar": ";fMAg4zEAAAAJ;https://scholar.google.co.uk/citations?user=t72xITMAAAAJ",
        "orcid": ";;0000-0002-1066-8798",
        "linkedin": ";;peter-macgregor-4626a993/",
        "or_profile": "~Seiyun_Shin1;~Ilan_Shomorony1;~Peter_Macgregor1",
        "aff": "Korea Advanced Institute of Science & Technology+Korea Advanced Institute of Science & Technology+University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of St. Andrews",
        "aff_domain": "kaist.edu+kaist.edu+illinois.edu;illinois.edu;st-andrews.ac.uk",
        "position": "MS student+Undergrad student+PhD student;Assistant Professor;Lecturer",
        "bibtex": "@inproceedings{\nshin2025dynamic,\ntitle={Dynamic {DBSCAN} with Euler Tour Sequences},\nauthor={Seiyun Shin and Ilan Shomorony and Peter Macgregor},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Ot92gX5gpJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ot92gX5gpJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Ou6Ud3nZfH",
        "title": "A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration.  Our experiments validate the effectiveness of the proposed approach.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yingqian Cui;Pengfei He;Xianfeng Tang;Qi He;Chen Luo;Jiliang Tang;Yue Xing",
        "authorids": "~Yingqian_Cui1;~Pengfei_He2;~Xianfeng_Tang1;~Qi_He5;~Chen_Luo3;~Jiliang_Tang1;~Yue_Xing1",
        "gender": "F;M;M;;M;M;",
        "homepage": "https://yingqiancui.github.io/;https://pengfeihepower.github.io/;https://xta.ng/;;https://chen-luo.com/;https://www.cse.msu.edu/~tangjili/;https://sites.google.com/site/xingyuecuhk/",
        "dblp": ";37/10219-2;33/7694;;46/4719-3.html;64/10812;185/5744-2.html",
        "google_scholar": "3p67r08AAAAJ;nsSrd6kAAAAJ;u1PEv-QAAAAJ;;4EoNAFcAAAAJ;WtzKMWAAAAAJ;",
        "orcid": ";;;;0000-0001-5339-5817;0000-0001-7125-3898;",
        "linkedin": ";;xianfengtang/;;chen-luo-a7a45b84/;;",
        "or_profile": "~Yingqian_Cui1;~Pengfei_He2;~Xianfeng_Tang1;~Qi_He5;~Chen_Luo3;~Jiliang_Tang1;~Yue_Xing1",
        "aff": "Amazon+Michigan State University;Michigan State University;Amazon;;Amazon;Michigan State University;Michigan State University",
        "aff_domain": "amazon.com+msu.edu;msu.edu;amazon.com;;amazon.com;msu.edu;msu.edu",
        "position": "Applied Scientist Intern+PhD student;PhD student;Researcher;;Researcher;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncui2025a,\ntitle={A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration},\nauthor={Yingqian Cui and Pengfei He and Xianfeng Tang and Qi He and Chen Luo and Jiliang Tang and Yue Xing},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Ou6Ud3nZfH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ou6Ud3nZfH",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "P2PdbGpzxg",
        "title": "The Polynomial Iteration Complexity for Variance Exploding Diffusion Models: Elucidating SDE and ODE Samplers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recently, variance exploding (VE) diffusion models have achieved state-of-the-art (SOTA) performance in two implementations: (1) the SDE-based implementation and (2) the probability flow ODE (PFODE) implementation. However, only a few works analyze the iteration complexity of VE-based models, and most focus on SDE-based implementation with strong assumptions. In this work, we prove the first polynomial iteration complexity under the realistic bounded support assumption for these two implementations. For the SDE-based implementation, we explain why the current SOTA VE-based model performs better than previous VE models. After that, we provide an improved result under the linear subspace data assumption and explain the great performance of VE models under the manifold data. For the PFODE-based implementation, the current results depend exponentially on problem parameters. Inspired by the previous predictor-corrector analysis framework, we propose the PFODE-Corrector algorithm and prove the polynomial complexity for the basic algorithm with uniform stepsize. After that, we show that VE-based models are more suitable for large stepsize and propose an exponential-decay stepsize version algorithm to improve the results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruofeng Yang;Bo Jiang;Shuai Li",
        "authorids": "~Ruofeng_Yang1;~Bo_Jiang2;~Shuai_Li3",
        "gender": "M;M;F",
        "homepage": "https://github.com/wanshuiyin;https://jhc.sjtu.edu.cn/~bjiang/;http://shuaili8.github.io",
        "dblp": "350/4546;34/2005-3.html;57/2281-10",
        "google_scholar": "https://scholar.google.com.hk/citations?user=Cw9HDacAAAAJ;WxAIZtMAAAAJ;https://scholar.google.com.hk/citations?user=kMZgQxcAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Ruofeng_Yang1;~Bo_Jiang2;~Shuai_Li3",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;John Hopcroft Center, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "position": "PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025the,\ntitle={The Polynomial Iteration Complexity for Variance Exploding Diffusion Models: Elucidating {SDE} and {ODE} Samplers},\nauthor={Ruofeng Yang and Bo Jiang and Shuai Li},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=P2PdbGpzxg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=P2PdbGpzxg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "P63lweNABK",
        "title": "Improving Stochastic Cubic Newton with Momentum",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study stochastic second-order methods for solving general non-convex optimization problems. We propose using a special version of momentum to stabilize the stochastic gradient and Hessian estimates in Newton's method. We show that momentum provably improves the variance of stochastic estimates and allows the method to converge for any noise level. Using the cubic regularization technique, we prove a global convergence rate for our method on general non-convex problems to a second-order stationary point, even when using only a single stochastic data sample per iteration. This starkly contrasts with all existing stochastic second-order methods for non-convex problems, which typically require large batches. Therefore, we are the first to demonstrate global convergence for batches of arbitrary size in the non-convex case for the Stochastic Cubic Newton. Additionally, we show improved speed on convex stochastic problems for our regularized Newton methods with momentum.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "El Mahdi Chayti;Nikita Doikov;Martin Jaggi",
        "authorids": "~El_Mahdi_Chayti2;~Nikita_Doikov1;~Martin_Jaggi1",
        "gender": ";;M",
        "homepage": ";https://doikov.com;https://mlo.epfl.ch",
        "dblp": ";222/9897;17/4402",
        "google_scholar": ";YNBhhjUAAAAJ;https://scholar.google.ch/citations?user=r1TJBr8AAAAJ",
        "orcid": ";;0000-0003-1579-5558",
        "linkedin": ";;",
        "or_profile": "~El_Mahdi_Chayti2;~Nikita_Doikov1;~Martin_Jaggi1",
        "aff": ";EPFL - EPF Lausanne;EPFL",
        "aff_domain": ";epfl.ch;epfl.ch",
        "position": ";Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nchayti2025improving,\ntitle={Improving Stochastic Cubic Newton with Momentum},\nauthor={El Mahdi Chayti and Nikita Doikov and Martin Jaggi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=P63lweNABK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=P63lweNABK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "P6vBRODemn",
        "title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, i.e., to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as participating clients typically have small local datasets, making it difficult to unambiguously determine optimal model parameters. Bayesian PFL (BPFL) methods can potentially enhance calibration, but they often come with considerable computational and memory requirements due to the need to track the variances of all the individual model parameters. Furthermore, different clients may exhibit heterogeneous uncertainty levels owing to varying local dataset sizes and distributions. To address these challenges, we propose LR-BPFL, a novel BPFL method that learns a global deterministic model along with personalized low-rank Bayesian corrections. To tailor the local model to each client's inherent uncertainty level, LR-BPFL incorporates an adaptive rank selection mechanism. We evaluate LR-BPFL across a variety of datasets, demonstrating its advantages in terms of calibration, accuracy, as well as computational and memory requirements. The code is available at https://github.com/Bernie0115/LR-BPFL.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Boning Zhang;Dongzhu Liu;Osvaldo Simeone;Guanchu Wang;Dimitrios Pezaros;Guangxu Zhu",
        "authorids": "~Boning_Zhang4;dongzhu.liu@glasgow.ac.uk;~Osvaldo_Simeone2;~Guanchu_Wang1;dimitrios.pezaros@glasgow.ac.uk;~Guangxu_Zhu1",
        "gender": "M;;M;M;;M",
        "homepage": ";;https://nms.kcl.ac.uk/osvaldo.simeone/index.htm;https://guanchuwang.github.io/home;;https://sites.google.com/view/guangxuzhu",
        "dblp": ";;;213/0985;;",
        "google_scholar": "OoJRk4wAAAAJ;;https://scholar.google.co.uk/citations?user=m1xeKH4AAAAJ;_QL5218AAAAJ;;MZrr57QAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Boning_Zhang4;dongzhu.liu@glasgow.ac.uk;~Osvaldo_Simeone2;~Guanchu_Wang1;dimitrios.pezaros@glasgow.ac.uk;~Guangxu_Zhu1",
        "aff": "University of Glasgow;;King's College London;University of North Carolina at Charlotte+Rice University;;",
        "aff_domain": "glasgow.ac.uk;;kcl.ac.uk;charlotte.edu+rice.edu;;",
        "position": "PhD student;;Full Professor;Assistant Professor+PhD student;;",
        "bibtex": "@inproceedings{\nzhang2025personalizing,\ntitle={Personalizing Low-rank Bayesian Neural Networks Via Federated Learning},\nauthor={Boning Zhang and Dongzhu Liu and Osvaldo Simeone and Guanchu Wang and Dimitrios Pezaros and Guangxu Zhu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=P6vBRODemn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=P6vBRODemn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PQobVi9iPN",
        "title": "Flexible Copula-Based Mixed Models in Deep Learning: A Scalable Approach to Arbitrary Marginals",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce copula-based neural networks (COPNN), a novel framework that extends beyond the limitations of Gaussian marginals for random effects in mixed models. COPNN integrates the flexibility of Gaussian copulas in capturing rich dependence structures with arbitrary marginal distributions, with the expressive power of deep neural networks (DNN), allowing it to model large non-Gaussian data in both regression and classification settings, while using batch learning and stochastic gradient descent. Unlike traditional linear and non-linear mixed models, which assume Gaussianity for random effects, COPNN leverages copulas to decouple the marginal distribution from the dependence structure, caused by spatial, temporal and high-cardinality categorical features. This is achieved by minimizing a batch negative log-likelihood (NLL) loss in the continuous case, and a batch negative pairwise log-likelihood in the binary case. We demonstrate COPNN\u2019s effectiveness through extensive experiments on both simulated and real datasets. COPNN reduces NLL and MSE in the regression setting, and improves predictive accuracy in the classification setting, compared to previous state of the art methods which integrate random effects into DNN. Our real-world experiments, conducted on datasets from automotive pricing and retail traffic forecasting, further validate COPNN's ability to improve performance over traditional methods for dealing with high-cardinality categorical features.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Giora Simchoni;Saharon Rosset",
        "authorids": "~Giora_Simchoni1;~Saharon_Rosset1",
        "gender": "M;",
        "homepage": "https://github.com/gsimchoni/;",
        "dblp": "279/5529;09/6597",
        "google_scholar": "z5Akz_YAAAAJ;",
        "orcid": ";",
        "linkedin": "gsimchoni/;",
        "or_profile": "~Giora_Simchoni1;~Saharon_Rosset1",
        "aff": "Tel Aviv University;Tel Aviv University",
        "aff_domain": "tauex.tau.ac.il;tau.ac.il",
        "position": "PhD student;Professor",
        "bibtex": "@inproceedings{\nsimchoni2025flexible,\ntitle={Flexible Copula-Based Mixed Models in Deep Learning: A Scalable Approach to Arbitrary Marginals},\nauthor={Giora Simchoni and Saharon Rosset},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=PQobVi9iPN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PQobVi9iPN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PRvopxdh5K",
        "title": "From Learning to Optimize to Learning Optimization Algorithms",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Towards designing learned optimization algorithms that are usable beyond their training setting, we identify key principles that classical algorithms obey, but have up to now, not been used for Learning to Optimize (L2O). Following these principles, we provide a general design pipeline, taking into account data, architecture and learning strategy, and thereby enabling a synergy between classical optimization and L2O, resulting in a philosophy of Learning Optimization Algorithms. As a consequence our learned algorithms perform well far beyond problems from the training distribution. We demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings at test time.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Camille Castera;Peter Ochs",
        "authorids": "~Camille_Castera1;~Peter_Ochs3",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": ";",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\ncastera2025from,\ntitle={From Learning to Optimize to Learning Optimization Algorithms},\nauthor={Camille Castera and Peter Ochs},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=PRvopxdh5K}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PRvopxdh5K",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PfJvU4bAQX",
        "title": "Information-Theoretic Measures on Lattices for Higher-Order Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Traditional measures based solely on pairwise associations often fail to capture the complex statistical structure of multivariate data.\nExisting approaches for identifying information shared among $d>3$ variables are frequently computationally intractable, asymmetric with respect to a target variable, or unable to account for all the ways in which the joint probability distribution can be factorised. Here we present a systematic framework based on lattice theory to derive higher-order information-theoretic measures for multivariate data. Our construction uses lattice and operator function pairs, whereby an operator function is applied over a lattice that represents the algebraic relationships among variables. We show that many commonly used measures can be derived within this framework, yet they fail to capture all interactions for $d>3$, either because they are defined on restricted sublattices, or because the use of the KL divergence as an operator function, a typical choice, leads to undesired disregard of groups of interactions. To fully characterise all interactions among $d$ variables, we introduce the Streitberg Information, which is defined over the full partition lattice and uses generalised divergences (beyond KL) as operator functions. We validate the Streitberg Information on synthetic data, and illustrate its application in detecting complex interactions among stocks, decoding neural signals, and performing feature selection in machine learning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhaolu Liu;Mauricio Barahona;Robert Peach",
        "authorids": "~Zhaolu_Liu1;~Mauricio_Barahona1;~Robert_Peach1",
        "gender": "Not Specified;Not Specified;M",
        "homepage": "https://timliuzhaolu.github.io;https://www.imperial.ac.uk/people/m.barahona;",
        "dblp": "308/3103;80/8051.html;236/4823.html",
        "google_scholar": "qLUZHCQAAAAJ;https://scholar.google.co.uk/citations?user=weulBoAAAAAJ;",
        "orcid": "0000-0002-8721-7506;0000-0002-1089-5675;0000-0002-8738-5825",
        "linkedin": ";;",
        "or_profile": "~Zhaolu_Liu1;~Mauricio_Barahona1;~Robert_Peach1",
        "aff": "Imperial College London;Imperial College London;Bayerische Julius-Maximilians-Universit\u00e4t W\u00fcrzburg",
        "aff_domain": "ic.ac.uk;imperial.ac.uk;uni-wuerzburg.de",
        "position": "PhD student;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nliu2025informationtheoretic,\ntitle={Information-Theoretic Measures on Lattices for Higher-Order Interactions},\nauthor={Zhaolu Liu and Mauricio Barahona and Robert Peach},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=PfJvU4bAQX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PfJvU4bAQX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Pgcc6fI0dw",
        "title": "Function-Space MCMC for Bayesian Wide Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian Neural Networks represent a fascinating confluence of deep learning and probabilistic reasoning, offering a compelling framework for understanding uncertainty in complex predictive models. In this paper, we investigate the use of the preconditioned Crank-Nicolson algorithm and its Langevin version to sample from a reparametrised posterior distribution of the neural network's weights, as the widths grow larger. In addition to being robust in the infinite-dimensional setting, we prove that the acceptance probabilities of the proposed algorithms approach 1 as the width of the network increases, independently of any stepsize tuning. Moreover, we examine and compare how the mixing speeds of the underdamped Langevin Monte Carlo, the preconditioned Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are influenced by changes in the network width in some real-world cases. Our findings suggest that, in wide Bayesian Neural Networks configurations, the preconditioned Crank-Nicolson algorithm allows for a scalable and more efficient sampling of the reparametrised posterior distribution, as also  evidenced by a higher effective sample size and improved diagnostic results compared with the other analysed algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lucia Pezzetti;Stefano Favaro;Stefano Peluchetti",
        "authorids": "~Lucia_Pezzetti1;~Stefano_Favaro1;~Stefano_Peluchetti1",
        "gender": "F;M;M",
        "homepage": ";https://www.carloalberto.org/person/stefano-favaro/;https://stefanopeluchetti.com",
        "dblp": ";148/7052;128/1385",
        "google_scholar": ";UjIKIf8AAAAJ;w3Gi3TEAAAAJ",
        "orcid": ";0000-0003-0936-9421;",
        "linkedin": "lucia-pezzetti-6aaa55157/;;stefanopeluchetti/",
        "or_profile": "~Lucia_Pezzetti1;~Stefano_Favaro1;~Stefano_Peluchetti1",
        "aff": "ETHZ - ETH Zurich;University of Torino;Sakana AI",
        "aff_domain": "ethz.ch;unito.it;sakana.ai",
        "position": "PhD student;Full Professor;Research Scientist",
        "bibtex": "@inproceedings{\npezzetti2025functionspace,\ntitle={Function-Space {MCMC} for Bayesian Wide Neural Networks},\nauthor={Lucia Pezzetti and Stefano Favaro and Stefano Peluchetti},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Pgcc6fI0dw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Pgcc6fI0dw",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PuSmku46eG",
        "title": "Explaining ViTs Using Information Flow",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Computer vision models can be explained by attributing the output decision to the input pixels. While effective methods for explaining convolutional neural networks have been proposed, these methods often produce low-quality attributions when applied to vision transformers (ViTs). State-of-the-art methods for explaining ViTs capture the flow of patch information using transition matrices. However, we observe that transition matrices alone are not sufficiently expressive to accurately explain ViT models. In this paper, we define a theoretical approach to creating explanations for ViTs called InFlow. The framework models the  patch-to-patch information flow using a combination of transition matrices and patch embeddings. Moreover, we define an algebra for updating the transition matrices of series connected components, diverging paths, and converging paths in the ViT model. This algebra allows the InFlow framework to produce high quality attributions which explain ViT decision making. In experimental evaluation on ImageNet, with three models, InFlow outperforms six ViT attribution methods in the standard insertion, deletion, SIC and AIC metrics by up to 18%. Qualitative results demonstrate InFlow produces more relevant and sharper explanations. Code is publicly available at https://github.com/chasewalker26/InFlow-ViT-Explanation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chase Walker;Md Rubel Ahmed;Sumit Kumar Jha;Rickard Ewetz",
        "authorids": "~Chase_Walker1;~Md_Rubel_Ahmed1;~Sumit_Kumar_Jha2;~Rickard_Ewetz1",
        "gender": "M;M;;M",
        "homepage": ";https://rubelahmed57.github.io/;;https://ewetz.ece.ufl.edu/",
        "dblp": "348/7054;;;127/9041",
        "google_scholar": "j5vNNv4AAAAJ;bYb5WXEAAAAJ;;h_RaG-8AAAAJ",
        "orcid": "0000-0001-8664-5843;;;",
        "linkedin": ";;;",
        "or_profile": "~Chase_Walker1;~Md_Rubel_Ahmed1;~Sumit_Kumar_Jha2;~Rickard_Ewetz1",
        "aff": "University of Florida;Louisiana Tech University;;University of Florida",
        "aff_domain": "ufl.edu;latech.edu;;ufl.edu",
        "position": "PhD student;Assistant Professor;;Associate Professor",
        "bibtex": "@inproceedings{\nwalker2025explaining,\ntitle={Explaining ViTs Using Information Flow},\nauthor={Chase Walker and Md Rubel Ahmed and Sumit Kumar Jha and Rickard Ewetz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=PuSmku46eG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PuSmku46eG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Q1deZl7rFQ",
        "title": "Dissecting the Impact of Model Misspecification in Data-Driven Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Data-driven optimization aims to translate a machine learning model into decision-making by optimizing decisions on estimated costs. Such a pipeline can be conducted by fitting a distributional model which is then plugged into the target optimization problem. While this fitting can utilize traditional methods such as maximum likelihood, a more recent approach uses estimation-optimization integration that minimizes decision error instead of estimation error. Although intuitive, the statistical benefit of the latter approach is not well understood yet is important to guide the prescriptive usage of machine learning. In this paper, we dissect the performance comparisons between these approaches in terms of the amount of model misspecification. In particular, we show how the integrated approach offers a ``universal double benefit'' on the top two dominating terms of regret when the underlying model is misspecified, while the traditional approach can be advantageous when the model is nearly well-specified. Our comparison is powered by finite-sample tail regret bounds that are derived via new higher-order expansions of regrets and the leveraging of a recent Berry-Esseen theorem.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Adam N. Elmachtoub;Henry Lam;Haixiang Lan;Haofeng Zhang",
        "authorids": "~Adam_N._Elmachtoub1;~Henry_Lam1;~Haixiang_Lan1;~Haofeng_Zhang1",
        "gender": "M;;M;",
        "homepage": "http://www.columbia.edu/~ae2516/;http://www.columbia.edu/~khl2114/;;",
        "dblp": "15/9298.html;35/9508;;",
        "google_scholar": "Z-CFWPwAAAAJ;Bnj50x0AAAAJ;;",
        "orcid": ";;;",
        "linkedin": ";;haixiang-lan/;",
        "or_profile": "~Adam_N._Elmachtoub1;~Henry_Lam1;~Haixiang_Lan1;~Haofeng_Zhang1",
        "aff": "Columbia University;Columbia University;Columbia University;",
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu;",
        "position": "Associate Professor;Associate Professor;PhD student;",
        "bibtex": "@inproceedings{\nelmachtoub2025dissecting,\ntitle={Dissecting the Impact of Model Misspecification in Data-driven Optimization},\nauthor={Adam N. Elmachtoub and Henry Lam and Haixiang Lan and Haofeng Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Q1deZl7rFQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Q1deZl7rFQ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Q3wokjRKUM",
        "title": "Stochastic Approximation with Unbounded Markovian Noise: A General-Purpose Theorem",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Motivated by engineering applications such as resource allocation in networks and inventory systems, we consider average-reward Reinforcement Learning with unbounded state space and reward function. Recent work Murthy et al. 2024 studied this problem in the actor-critic framework and established finite sample bounds assuming access to a critic with certain error guarantees. We complement their work by studying Temporal Difference (TD) learning with linear function approximation and establishing finite-time bounds with the optimal sample complexity. These results are obtained using the following general-purpose theorem for non-linear Stochastic Approximation (SA). \n\n\nSuppose that one constructs a Lyapunov function for a non-linear SA with certain drift condition. Then, our theorem establishes finite-time bounds when this SA is driven by unbounded Markovian noise under suitable conditions. It serves as a black box tool to generalize sample guarantees on SA from i.i.d. or martingale difference case to potentially unbounded Markovian noise. The generality and the mild assumptions of the setup enables broad applicability of our theorem. We illustrate its power by studying two more systems: (i) We improve upon the finite-time bounds of Q-learning in Chen et al. 2024 by tightening the error bounds and also allowing for a larger class of behavior policies. (ii) We establish the first ever finite-time bounds for distributed stochastic optimization of high-dimensional smooth strongly convex function using cyclic block coordinate descent.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shaan Ul Haque;Siva Theja Maguluri",
        "authorids": "~Shaan_Ul_Haque1;~Siva_Theja_Maguluri1",
        "gender": "M;",
        "homepage": ";https://sites.google.com/site/sivatheja/",
        "dblp": ";",
        "google_scholar": "nB4-BGAAAAAJ;",
        "orcid": "0000-0001-9557-4071;",
        "linkedin": ";",
        "or_profile": "~Shaan_Ul_Haque1;~Siva_Theja_Maguluri1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nhaque2025stochastic,\ntitle={Stochastic Approximation with Unbounded Markovian Noise: A General-Purpose Theorem},\nauthor={Shaan Ul Haque and Siva Theja Maguluri},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Q3wokjRKUM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Q3wokjRKUM",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QCZsAAmiDk",
        "title": "Towards Cost Sensitive Decision Making",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Many real-world situations allow for the acquisition of additional relevant information when making decisions with limited or uncertain data. However, traditional RL approaches either require all features to be acquired beforehand (e.g. in a MDP) or regard part of them as missing data that cannot be acquired (e.g. in a POMDP). In this work, we consider RL models that may actively acquire features from the environment to improve the decision quality and certainty, while automatically balancing the cost of feature acquisition process and the reward of task decision process. We propose the Active-Acquisition POMDP and identify two types of the acquisition process for different application domains. In order to assist the agent in the actively-acquired partially-observed environment and alleviate the exploration-exploitation dilemma, we develop a model-based approach, where a deep generative model is utilized to capture the dependencies of the features and impute the unobserved features. The imputations essentially represent the beliefs of the agent. Equipped with the dynamics model, we develop hierarchical RL algorithms to resolve both types of the AA-POMDPs. Empirical results demonstrate that our approach achieves considerably better performance than existing POMDP-RL solutions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yang Li;Junier Oliva",
        "authorids": "~Yang_Li19;~Junier_Oliva1",
        "gender": ";M",
        "homepage": ";http://lupalab.com",
        "dblp": ";137/8390",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Yang_Li19;~Junier_Oliva1",
        "aff": ";University of North Carolina, Chapel Hill",
        "aff_domain": ";unc.edu",
        "position": ";Associate Professor",
        "bibtex": "@inproceedings{\nli2025towards,\ntitle={Towards Cost Sensitive Decision Making},\nauthor={Yang Li and Junier Oliva},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=QCZsAAmiDk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QCZsAAmiDk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QDLKRmx07M",
        "title": "Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding how neural networks transform input data across layers is fundamental to unraveling their learning and generalization capabilities. Although prior work has used insights from kernel methods to study neural networks, a global analysis of how the similarity between hidden representations evolves across layers remains underexplored. In this paper, we introduce a theoretical framework for the evolution of the kernel sequence, which measures the similarity between the hidden representation for two different inputs. Operating under the mean-field regime, we show that the kernel sequence evolves deterministically via a kernel map, which only depends on the activation function. By expanding activation using Hermite polynomials and using their algebraic properties, we derive an explicit form for kernel map and fully characterize its fixed points. Our analysis reveals that for nonlinear activations, the kernel sequence converges globally to a unique fixed point, which can correspond to orthogonal or similar representations depending on the activation and network architecture. We further extend our results to networks with residual connections and normalization layers, demonstrating similar convergence behaviors. This work provides new insights into the implicit biases of deep neural networks and how architectural choices influence the evolution of representations across layers.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amir Joudaki;Thomas Hofmann",
        "authorids": "~Amir_Joudaki1;~Thomas_Hofmann1",
        "gender": ";M",
        "homepage": ";http://www.da.inf.ethz.ch/",
        "dblp": ";h/ThHofmann",
        "google_scholar": ";T3hAyLkAAAAJ",
        "orcid": ";",
        "linkedin": ";thomas-hofmann-1ab2402/",
        "or_profile": "~Amir_Joudaki1;~Thomas_Hofmann1",
        "aff": ";Swiss Federal Institute of Technology",
        "aff_domain": ";ethz.ch",
        "position": ";Full Professor",
        "bibtex": "@inproceedings{\njoudaki2025emergence,\ntitle={Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations},\nauthor={Amir Joudaki and Thomas Hofmann},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=QDLKRmx07M}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QDLKRmx07M",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QTMczpOLMG",
        "title": "Rethinking Neural-based Matrix Inversion: Why can't, and Where can",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deep neural networks have achieved substantial success across various scientific computing tasks. A pivotal challenge within this domain is the rapid and parallel approximation of matrix inverses, critical for numerous applications. Despite significant progress, there currently exists no universal neural-based method for approximating matrix inversion. This paper presents a theoretical analysis demonstrating the fundamental limitations of neural networks in developing a generalized matrix inversion model. We expand the class of Lipschitz functions to encompass a wider array of neural network models, thereby refining our theoretical approach. Moreover, we delineate specific conditions under which neural networks can effectively approximate matrix inverses. Our theoretical results are supported by experimental results from diverse matrix datasets, exploring the efficacy of neural networks in addressing the matrix inversion challenge.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuliang Ji;Jian Wu;Yuanzhe Xi",
        "authorids": "~Yuliang_Ji1;~Jian_Wu8;~Yuanzhe_Xi1",
        "gender": ";;M",
        "homepage": ";https://github.com/WuJian1995;http://www.math.emory.edu/~yxi26/",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yuliang_Ji1;~Jian_Wu8;~Yuanzhe_Xi1",
        "aff": ";;Emory University",
        "aff_domain": ";;emory.edu",
        "position": ";;Associate Professor",
        "bibtex": "@inproceedings{\nji2025rethinking,\ntitle={Rethinking Neural-based Matrix Inversion: Why can't, and Where can},\nauthor={Yuliang Ji and Jian Wu and Yuanzhe Xi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=QTMczpOLMG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QTMczpOLMG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QU6V9ccMRw",
        "title": "Double Debiased Machine Learning for Mediation Analysis with Continuous Treatments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Uncovering causal mediation effects is of significant value to practitioners who aim to isolate treatment effects from potential mediator effects. We propose a double machine learning (DML) algorithm for mediation analysis that supports continuous treatments. To estimate the target mediated response curve, our method employs a kernel-based doubly robust moment function for which we prove asymptotic Neyman orthogonality. This allows us to obtain an asymptotic normality with nonparametric convergence rate while allowing for nonparametric or parametric estimation of the nuisance parameters. Subsequently, we derive an optimal bandwidth strategy along with a procedure to estimate asymptotic confidence intervals. Finally, to illustrate the benefits of our method, we provide a numerical evaluation of our approach on a simulation along with an application on medical real-world data to analyze the effect of glycemic control on cognitive functions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Houssam Zenati;Judith Ab\u00e9cassis;Julie Josse;Bertrand Thirion",
        "authorids": "~Houssam_Zenati1;~Judith_Ab\u00e9cassis1;~Julie_Josse1;~Bertrand_Thirion1",
        "gender": "M;F;F;M",
        "homepage": "https://houssamzenati.github.io/;https://judithabk6.github.io/;http://juliejosse.com/;http://pages.saclay.inria.fr/bertrand.thirion",
        "dblp": ";;22/6376;62/2019",
        "google_scholar": "LBqNPp4AAAAJ;https://scholar.google.fr/citations?user=M055Oo8AAAAJ;AlIkUSAAAAAJ;MeKi5_AAAAAJ",
        "orcid": ";0000-0002-5818-8304;;http://  0000-0001-5018-7895",
        "linkedin": "houssam-zenati/;judith-abecassis-0760a26b/;;",
        "or_profile": "~Houssam_Zenati1;~Judith_Ab\u00e9cassis1;~Julie_Josse1;~Bertrand_Thirion1",
        "aff": "University College London, University of London;INRIA;INRIA;INRIA",
        "aff_domain": "ucl.ac.uk;inria.fr;inria.fr;inria.fr",
        "position": "Postdoc;Assistant Professor;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nzenati2025double,\ntitle={Double Debiased Machine Learning for Mediation Analysis with Continuous Treatments},\nauthor={Houssam Zenati and Judith Ab{\\'e}cassis and Julie Josse and Bertrand Thirion},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=QU6V9ccMRw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QU6V9ccMRw",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QW3iLIr3GD",
        "title": "Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We revisit the likelihood ratio between a pretrained large language model (LLM) and its finetuned variant as a criterion for out-of-distribution (OOD) detection. The intuition behind such a criterion is that, the pretrained LLM has the prior knowledge about OOD data due to its large amount of training data, and once finetuned with the in-distribution data, the LLM has sufficient knowledge to distinguish their difference. Leveraging the power of LLMs, we show that, the likelihood ratio can serve as an effective OOD detection criterion. Moreover, we apply the proposed LLM-based likelihood ratio to detect OOD questions in question-answering (QA) systems, which can be used to improve the performance of specialized LLMs for general questions. Given that likelihood can be easily obtained by the loss functions within contemporary neural network frameworks, it is straightforward to implement this approach in practice with **three lines** of code. Since both the pretrained LLMs and its various finetuned models are widely available from online platforms such as Hugging Face, our proposed criterion can be effortlessly incorporated for OOD detection without the need for further training. We conduct comprehensive evaluation across on multiple settings, including far OOD, near OOD, spam detection, and QA scenarios, to demonstrate the effectiveness of the method. Code can be found at https://github.com/andiac/LLMOODratio",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andi Zhang;Tim Z. Xiao;Weiyang Liu;Robert Bamler;Damon Wischik",
        "authorids": "~Andi_Zhang2;~Tim_Z._Xiao1;~Weiyang_Liu1;~Robert_Bamler1;~Damon_Wischik1",
        "gender": "M;;M;M;",
        "homepage": "http://andi.ac;;http://wyliu.com/;https://robamler.github.io/;https://www.cl.cam.ac.uk/~djw1005/",
        "dblp": "200/8255-1;;137/1532;195/6208.html;18/4263.html",
        "google_scholar": "qGAOAoYAAAAJ;;DMjROf0AAAAJ;LwvdNAgAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "zhangandi/;;;;",
        "or_profile": "~Andi_Zhang2;~Tim_Z._Xiao1;~Weiyang_Liu1;~Robert_Bamler1;~Damon_Wischik1",
        "aff": "University of Manchester;;Department of Computer Science and Engineering, The Chinese University of Hong Kong+Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Tuebingen;University of Cambridge",
        "aff_domain": "manchester.ac.uk;;cse.cuhk.edu.hk+tuebingen.mpg.de;uni-tuebingen.de;cam.ac.uk",
        "position": "Postdoc;;Assistant Professor+Postdoc;Assistant Professor;Lecturer",
        "bibtex": "@inproceedings{\nzhang2025your,\ntitle={Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector},\nauthor={Andi Zhang and Tim Z. Xiao and Weiyang Liu and Robert Bamler and Damon Wischik},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=QW3iLIr3GD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QW3iLIr3GD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Qi9HokKRMu",
        "title": "Performative Prediction on Games and Mechanism Design",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Agents often have individual goals which depend on a group's actions. If agents trust a forecast of collective action and adapt strategically, such prediction can influence outcomes non-trivially, resulting in a form of performative prediction. This effect is ubiquitous in scenarios ranging from pandemic predictions to election polls, but existing work has ignored interdependencies among predicted agents. As a first step in this direction, we study a collective risk dilemma where agents dynamically decide whether to trust predictions based on past accuracy. As predictions shape collective outcomes, social welfare arises naturally as a metric of concern. We explore the resulting interplay between accuracy and welfare, and demonstrate that searching for stable accurate predictions can minimize social welfare with high probability in our setting. By assuming knowledge of a Bayesian agent behavior model, we then show how to achieve better trade-offs and use them for mechanism design.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ant\u00f3nio G\u00f3is;Mehrnaz Mofakhami;Fernando P. Santos;Gauthier Gidel;Simon Lacoste-Julien",
        "authorids": "~Ant\u00f3nio_G\u00f3is1;~Mehrnaz_Mofakhami1;~Fernando_P._Santos1;~Gauthier_Gidel1;~Simon_Lacoste-Julien1",
        "gender": ";;M;M;M",
        "homepage": ";https://mhrnz.github.io/;https://fp-santos.github.io;https://gauthiergidel.github.io/;http://www.iro.umontreal.ca/~slacoste/",
        "dblp": "245/9006;;159/4776;188/6326;94/446.html",
        "google_scholar": "loas8EEAAAAJ;https://scholar.google.fr/citations?hl=fr;https://scholar.google.pt/citations?user=Ps7iXn0AAAAJ;https://scholar.google.fr/citations?user=bDrXQPUAAAAJ;oejm5IUAAAAJ",
        "orcid": ";;;;0000-0001-6485-6180",
        "linkedin": ";mehrnazmofakhami98/;;;simon-lacoste-julien-355b9a3",
        "or_profile": "~Ant\u00f3nio_G\u00f3is1;~Mehrnaz_Mofakhami1;~Fernando_P._Santos1;~Gauthier_Gidel1;~Simon_Lacoste-Julien1",
        "aff": "Montreal Institute for Learning Algorithms, University of Montreal, Universit\u00e9 de Montr\u00e9al;Mila - Quebec Artificial Intelligence Institute;University of Amsterdam;Universit\u00e9 de Montr\u00e9al+University of Montreal+Mila - Quebec Artificial Intelligence Institute;University of Montreal+Samsung - SAIT AI Lab, Montreal",
        "aff_domain": "mila.quebec;mila.quebec;uva.nl;umontreal.ca+umontreal.ca+mila.quebec;umontreal.ca+samsung.com",
        "position": "PhD student;Researcher;Assistant Professor;Associate Professor+Assistant Professor+Assistant Professor;Associate Professor+VP Lab Director",
        "bibtex": "@inproceedings{\ngois2025performative,\ntitle={Performative Prediction on Games and Mechanism Design},\nauthor={Ant{\\'o}nio G{\\'o}is and Mehrnaz Mofakhami and Fernando P. Santos and Simon Lacoste-Julien and Gauthier Gidel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Qi9HokKRMu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Qi9HokKRMu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Qkz4VfG5Df",
        "title": "Density Ratio-based Proxy Causal Learning Without Density Ratios",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We address the setting of Proxy Causal Learning (PCL), which has the goal of estimating causal effects from observed data in the presence of hidden confounding. Proxy methods accomplish this task using two proxy variables related to the latent confounder: a treatment proxy (related to the treatment) and an outcome proxy (related to the outcome). Two approaches have been proposed to perform causal effect estimation given proxy variables; however only one of these has found mainstream acceptance, since the other was understood to require density ratio estimation - a challenging task in high dimensions. In the present work, we propose a practical and effective implementation of the second approach, which bypasses explicit density ratio estimation and is suitable for continuous and high-dimensional treatments. We employ kernel ridge regression to derive estimators, resulting in simple closed-form solutions for dose-response and conditional dose-response curves, along with consistency guarantees. Our methods empirically demonstrate superior or comparable performance to existing frameworks on synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bariscan Bozkurt;Ben Deaner;Dimitri Meunier;Liyuan Xu;Arthur Gretton",
        "authorids": "~Bariscan_Bozkurt1;~Ben_Deaner2;~Dimitri_Meunier1;~Liyuan_Xu1;~Arthur_Gretton1",
        "gender": "M;;Not Specified;M;M",
        "homepage": ";https://bendeaner.wordpress.com/;https://dimitri-meunier.github.io;;http://www.gatsby.ucl.ac.uk/~gretton/",
        "dblp": "321/6640;;284/9524;33/9817;56/2574",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;_04YU0EAAAAJ;-DLyhSoAAAAJ;OUv7J6QAAAAJ",
        "orcid": ";;;;",
        "linkedin": "bar%C4%B1%C5%9Fcan-bozkurt-436a5610b/;;;;",
        "or_profile": "~Bariscan_Bozkurt1;~Ben_Deaner2;~Dimitri_Meunier1;~Liyuan_Xu1;~Arthur_Gretton1",
        "aff": "University College London, University of London;University College London, University of London;University College London, University of London;;Google+University College London",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;;deepmind.com+ucl.ac.uk",
        "position": "PhD student;Assistant Professor;PhD student;;Researcher+Professor",
        "bibtex": "@inproceedings{\nbozkurt2025density,\ntitle={Density Ratio-based Proxy Causal Learning Without Density Ratios},\nauthor={Bariscan Bozkurt and Ben Deaner and Dimitri Meunier and Liyuan Xu and Arthur Gretton},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Qkz4VfG5Df}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Qkz4VfG5Df",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "R2Xaxv4ADz",
        "title": "HACSurv: A Hierarchical Copula-Based Approach for Survival Analysis with Dependent Competing Risks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In survival analysis, subjects often face competing risks; for example, individuals with cancer may also suffer from heart disease or other illnesses, which can jointly influence the prognosis of risks and censoring. Traditional survival analysis methods often treat competing risks as independent and fail to accommodate the dependencies between different conditions. In this paper, we introduce HACSurv, a survival analysis method that learns Hierarchical Archimedean Copulas structures and cause-specific survival functions from data with competing risks. HACSurv employs a flexible dependency structure using hierarchical Archimedean copulas to represent the relationships between competing risks and censoring. By capturing the dependencies between risks and censoring, HACSurv improves the accuracy of survival predictions and offers insights into risk interactions. Experiments on synthetic dataset demonstrate that our method can accurately identify the complex dependency structure and precisely predict survival distributions, whereas the compared methods exhibit significant deviations between their predictions and the true distributions. Experiments on multiple real-world datasets also demonstrate that our method achieves better survival prediction compared to previous state-of-the-art methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xin Liu;Weijia Zhang;Min-Ling Zhang",
        "authorids": "~Xin_Liu54;~Weijia_Zhang2;~Min-Ling_Zhang2",
        "gender": "M;M;M",
        "homepage": "http://palm.seu.edu.cn/homepage/liuxin/index.html;https://www.weijiazhangxh.com/;http://palm.seu.edu.cn/zhangml/",
        "dblp": ";158/5387-1;84/271.html",
        "google_scholar": "TP-L-SEAAAAJ;https://scholar.google.com.au/citations?user=7jmAPvAAAAAJ;uFHCIM0AAAAJ",
        "orcid": ";0000-0001-8103-5325;0000-0003-1880-5918",
        "linkedin": ";weijia-zhang-86152337/;",
        "or_profile": "~Xin_Liu54;~Weijia_Zhang2;~Min-Ling_Zhang2",
        "aff": "Southeast University+Monash University;University of Newcastle;Southeast University",
        "aff_domain": "seu.edu.cn+monash.edu;newcastle.edu.au;seu.edu.cn",
        "position": "MS student+MS student;Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nliu2025hacsurv,\ntitle={{HACS}urv: A Hierarchical Copula-based Approach for Survival Analysis with Dependent Competing Risks},\nauthor={Xin Liu and Weijia Zhang and Min-Ling Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=R2Xaxv4ADz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=R2Xaxv4ADz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "R3O1mD9lyZ",
        "title": "Learning Visual-Semantic Subspace Representations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Learning image representations that capture rich semantic relationships remains a significant challenge. Existing approaches are either contrastive, lacking robust theoretical guarantees, or struggle to effectively represent the partial orders inherent to structured visual-semantic data. In this paper, we introduce a nuclear norm-based loss function, grounded in the same information theoretic principles that have proved effective in self-supervised learning. We present a theoretical characterization of this loss, demonstrating that, in addition to promoting class orthogonality, it encodes the spectral geometry of the data within a subspace lattice. This geometric representation allows us to associate logical propositions with subspaces, ensuring that our learned representations adhere to a predefined symbolic structure.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gabriel Moreira;Manuel Marques;Joao Costeira;Alexander G Hauptmann",
        "authorids": "~Gabriel_Moreira1;~Manuel_Marques1;~Joao_Costeira1;~Alexander_G_Hauptmann1",
        "gender": "M;M;;M",
        "homepage": ";http://users.isr.ist.utl.pt/~manuel/;;",
        "dblp": "295/2441;02/6514;;h/AlexanderGHauptmann",
        "google_scholar": "p_NbrpcAAAAJ;https://scholar.google.com/citations?hl=en;Xi33QRIAAAAJ;https://scholar.google.co.uk/citations?user=Py54GcEAAAAJ",
        "orcid": "0000-0001-8564-2835;0000-0003-0532-1869;;",
        "linkedin": ";;;",
        "or_profile": "~Gabriel_Moreira1;~Manuel_Marques1;~Joao_Costeira1;~Alexander_G_Hauptmann1",
        "aff": "Carnegie Mellon University;Instituto Superior T\u00e9cnico;Instituto Superior T\u00e9cnico;School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cmu.edu;tecnico.ulisboa.pt;tecnico.ulisboa.pt;cs.cmu.edu",
        "position": "PhD student;Researcher;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nmoreira2025learning,\ntitle={Learning Visual-Semantic Subspace Representations},\nauthor={Gabriel Moreira and Manuel Marques and Joao Costeira and Alexander G Hauptmann},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=R3O1mD9lyZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=R3O1mD9lyZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "R89NsFVgV5",
        "title": "A Shapley-value Guided Rationale Editor for Rationale Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Rationale learning aims to automatically uncover the underlying explanations for NLP predictions. Previous studies in rationale learning mainly focus on the relevance of independent tokens with the predictions without considering their marginal contribution and the collective readability of extracted rationales. Through an empirical analysis, we argue that the sufficiency, informativeness, and readability of rationales are essential for explaining diverse end-task predictions. Accordingly, we propose Shapley-value Guided Rationale Editor (SHARE), an unsupervised approach that refines editable rationales while predicting task outcomes. SHARE extracts a sequence of tokens as a rationale, providing a collective explanation that is sufficient, informative, and readable. SHARE is highly adaptable for tasks like sentiment analysis, claim verification, and question answering, and can integrate seamlessly with various language models to provide explainability. Extensive experiments demonstrate its effectiveness in balancing sufficiency, informativeness, and readability across diverse applications. Our code and datasets are available at https://github.com/zixinK/SHARE.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zixin Kuang;Meng-Fen Chiang;Wang-Chien Lee",
        "authorids": "~Zixin_Kuang1;~Meng-Fen_Chiang2;~Wang-Chien_Lee1",
        "gender": "F;;M",
        "homepage": ";;http://www.cse.psu.edu/~wul2/",
        "dblp": ";;14/716",
        "google_scholar": ";;https://scholar.google.com.tw/citations?user=9OdHL5wAAAAJ",
        "orcid": ";;0000-0002-8949-489X",
        "linkedin": "zixin-kuang-31491727b/;;",
        "or_profile": "~Zixin_Kuang1;~Meng-Fen_Chiang2;~Wang-Chien_Lee1",
        "aff": "University of Auckland;;Pennsylvania State University",
        "aff_domain": "aucklanduni.ac.nz;;psu.edu",
        "position": "PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nkuang2025a,\ntitle={A Shapley-value Guided Rationale Editor for Rationale Learning},\nauthor={Zixin Kuang and Meng-Fen Chiang and Wang-Chien Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=R89NsFVgV5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=R89NsFVgV5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RFpIosT9WI",
        "title": "Fourier Circuits in Neural Networks and Transformers: A Case Study of Modular Arithmetic with Multiple Inputs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and Transformers. Building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. We direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. Our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer Transformers in addressing this task. A cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the network width. We demonstrate that a neuron count of $ m \\geq 2^{2k-2} \\cdot (p-1) $, these networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $. Furthermore, we establish that each hidden-layer neuron aligns with a specific Fourier spectrum, integral to solving modular addition problems. By correlating our findings with the empirical observations of similar studies, we contribute to a deeper comprehension of the intrinsic computational mechanisms of neural networks. Furthermore, we observe similar computational mechanisms in attention matrices of one-layer Transformers. Our work stands as a significant stride in unraveling their operation complexities, particularly in the realm of complex algebraic tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chenyang Li;Yingyu Liang;Zhenmei Shi;Zhao Song;Tianyi Zhou",
        "authorids": "~Chenyang_Li6;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3;~Tianyi_Zhou4",
        "gender": "M;;M;M;",
        "homepage": "https://github.com/ChenyangLi1;;http://zhmeishi.github.io/;https://www.youtube.com/@simapofang;",
        "dblp": ";;246/5216;76/4051-2;",
        "google_scholar": ";;0oeNnzMAAAAJ;yDZct7UAAAAJ;",
        "orcid": ";;0009-0007-6741-7598;0000-0003-4589-5234;",
        "linkedin": ";;zhenmei-shi-56408a113/;;",
        "or_profile": "~Chenyang_Li6;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3;~Tianyi_Zhou4",
        "aff": "Fuzhou University;;MongoDB+Voyage AI;University of California, Berkeley;",
        "aff_domain": "fzu.edu.cn;;mongodb.com+voyageai.com;berkeley.edu;",
        "position": "Undergrad student;;Researcher+Researcher;Associate Professor;",
        "bibtex": "@inproceedings{\nli2025fourier,\ntitle={Fourier Circuits in Neural Networks and Transformers: A Case Study of Modular Arithmetic with Multiple Inputs},\nauthor={Chenyang Li and Yingyu Liang and Zhenmei Shi and Zhao Song and Tianyi Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RFpIosT9WI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RFpIosT9WI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RJATQnhXyh",
        "title": "HAR-former: Hybrid Transformer with an Adaptive Time-Frequency Representation Matrix for Long-Term Series Forecasting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Time series forecasting is crucial across various fields such as economics, energy, transportation planning, and weather prediction.\nNevertheless, accurately modeling real-world systems is challenging due to their inherent complexity and non-stationarity.\nTraditional methods, which often depend on high-dimensional embeddings, can obscure multivariate relationships and struggle with performance limitations, especially when handling complex temporal patterns. \nTo address these issues, we propose HAR-former, a Hybrid Transformer with an Adaptive Time-Frequency Representation Matrix, which combines the strengths of Multi-Layer Perceptrons (MLPs) and Transformers to process trend and seasonal components, respectively.\nThe HAR-former leverages a novel adaptive time-frequency representation matrix to bridge the gap between the time and frequency domains, allowing the model to capture both long-range dependencies and localized patterns.\nExtensive experimental evaluation on eight real-world benchmark datasets demonstrates that HAR-former outperforms existing state-of-the-art (SOTA) methods, establishing it as a robust solution for complex time series forecasting tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "zhengkenghao;ZI LONG;Shuxin Wang",
        "authorids": "~zhengkenghao1;~ZI_LONG1;~Shuxin_Wang5",
        "gender": "M;M;M",
        "homepage": "https://github.com/kktoucheme;;",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";0009-0009-3088-5261;",
        "linkedin": ";;shuxin-wang-170a2557/",
        "or_profile": "~zhengkenghao1;~ZI_LONG1;~Shuxin_Wang5",
        "aff": ";Shenzhen Technology University;",
        "aff_domain": ";sztu.edu.cn;",
        "position": ";Assistant Professor;",
        "bibtex": "@inproceedings{\nzhengkenghao2025harformer,\ntitle={{HAR}-former: Hybrid Transformer with an Adaptive Time-Frequency Representation Matrix for Long-Term Series Forecasting},\nauthor={zhengkenghao and ZI LONG and Shuxin Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RJATQnhXyh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RJATQnhXyh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RKiOGRrABL",
        "title": "On the Identifiability of Causal Abstractions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Causal representation learning (CRL) enhances machine learning models' robustness and generalizability by learning structural causal models associated with data-generating processes. We focus on a family of CRL methods that uses contrastive data pairs in the observable space, generated before and after a random, unknown intervention, to identify the latent causal model. (Brehmer et al., 2022) showed that this is indeed possible, given that all latent variables can be intervened on individually. However, this is a highly restrictive assumption in many systems. In this work, we instead assume interventions on arbitrary subsets of latent variables, which is more realistic. We introduce a theoretical framework that calculates the degree to which we can identify a causal model, given a set of possible interventions, up to an abstraction that describes the system at a higher level of granularity.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiusi Li;S\u00e9kou-Oumar Kaba;Siamak Ravanbakhsh",
        "authorids": "~Xiusi_Li1;~S\u00e9kou-Oumar_Kaba1;~Siamak_Ravanbakhsh1",
        "gender": ";M;",
        "homepage": ";https://oumarkaba.github.io;",
        "dblp": ";279/3144;",
        "google_scholar": "NfSudKcAAAAJ;https://scholar.google.ca/citations?user=jKqh8jAAAAAJ;",
        "orcid": ";0000-0002-7258-4696;",
        "linkedin": ";oumar-kaba/;",
        "or_profile": "~Xiusi_Li1;~S\u00e9kou-Oumar_Kaba1;~Siamak_Ravanbakhsh1",
        "aff": "McGill University;McGill University;",
        "aff_domain": "mail.mcgill.ca;mcgill.ca;",
        "position": "PhD student;PhD student;",
        "bibtex": "@inproceedings{\nli2025on,\ntitle={On the Identifiability of Causal Abstractions},\nauthor={Xiusi Li and S{\\'e}kou-Oumar Kaba and Siamak Ravanbakhsh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RKiOGRrABL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RKiOGRrABL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RNQzrXWhIs",
        "title": "InfoNCE: Identifying the Gap Between Theory and Practice",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Prior theory work on Contrastive Learning via the InfoNCE loss showed that, under certain assumptions, the learned representations recover the ground-truth latent factors. We argue that these theories overlook crucial aspects of how CL is deployed in practice. Specifically, they either assume equal variance across all latents or that certain latents are kept invariant. However, in practice, positive pairs are often generated using augmentations such as strong cropping to just a few pixels. Hence, a more realistic assumption is that all latent factors change with a continuum of variability across all factors. We introduce AnInfoNCE, a generalization of InfoNCE that can provably uncover the latent factors in this anisotropic setting, broadly generalizing previous identifiability results in CL. We validate our identifiability results in controlled experiments and show that AnInfoNCE increases the recovery of previously collapsed information in CIFAR10 and ImageNet, albeit at the cost of downstream accuracy. \nFinally, we discuss the remaining mismatches between theoretical assumptions and practical implementations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Evgenia Rusak;Patrik Reizinger;Attila Juhos;Oliver Bringmann;Roland S. Zimmermann;Wieland Brendel",
        "authorids": "~Evgenia_Rusak1;~Patrik_Reizinger1;~Attila_Juhos1;~Oliver_Bringmann1;~Roland_S._Zimmermann1;~Wieland_Brendel1",
        "gender": "F;M;;M;;M",
        "homepage": "https://evgeniarusak.github.io/;https://rpatrik96.github.io/;;https://www.embedded.uni-tuebingen.de;;",
        "dblp": "245/2556;249/5412;228/6943;06/6843;;37/11107",
        "google_scholar": "https://scholar.google.de/citations?user=XKc19kkAAAAJ;zIT0fdIAAAAJ;35hg1Z8AAAAJ;pk53ZkAAAAAJ;;v-JL-hsAAAAJ",
        "orcid": ";0000-0001-9861-0293;;0000-0002-1615-507X;;",
        "linkedin": ";patrik-reizinger/;;;;",
        "or_profile": "~Evgenia_Rusak1;~Patrik_Reizinger1;~Attila_Juhos1;~Oliver_Bringmann1;~Roland_S._Zimmermann1;~Wieland_Brendel1",
        "aff": ";Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Max-Planck Institute for Intelligent Systems;FZI Research Center for Information Technology+University of T\u00fcbingen;;ELLIS Institute T\u00fcbingen",
        "aff_domain": ";uni-tuebingen.de;mpg.tuebingen.de;fzi.de+uni-tuebingen.de;;tue.ellis.eu",
        "position": ";PhD student;PhD student;Full Professor+Full Professor;;Principal Researcher",
        "bibtex": "@inproceedings{\nrusak2025infonce,\ntitle={Info{NCE}: Identifying the Gap Between Theory and Practice},\nauthor={Evgenia Rusak and Patrik Reizinger and Attila Juhos and Oliver Bringmann and Roland S. Zimmermann and Wieland Brendel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RNQzrXWhIs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RNQzrXWhIs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RRrftHtEfK",
        "title": "Unbiased and Sign Compression in Distributed Learning: Comparing Noise Resilience via SDEs",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Distributed methods are essential for handling machine learning pipelines comprising large-scale models and datasets. However, their benefits often come at the cost of increased communication overhead between the central server and agents, which can become the main bottleneck, making training costly or even unfeasible in such systems. Compression methods such as quantization and sparsification can alleviate this issue. Still, their robustness to large and heavy-tailed gradient noise, a phenomenon sometimes observed in language modeling, remains poorly understood. This work addresses this gap by analyzing Distributed Compressed SGD (DCSGD) and Distributed SignSGD (DSignSGD) using stochastic differential equations (SDEs). Our results show that DCSGD with unbiased compression is more vulnerable to noise in stochastic gradients, while DSignSGD remains robust, even under large and heavy-tailed noise. Additionally, we propose new scaling rules for hyperparameter tuning to mitigate performance degradation due to compression. These findings are empirically validated across multiple deep learning architectures and datasets, providing practical recommendations for distributed optimization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Enea Monzio Compagnoni;Rustem Islamov;Frank Norbert Proske;Aurelien Lucchi",
        "authorids": "~Enea_Monzio_Compagnoni1;~Rustem_Islamov1;~Frank_Norbert_Proske1;~Aurelien_Lucchi1",
        "gender": "M;M;M;M",
        "homepage": "https://eneamc.github.io/;https://rustem-islamov.github.io/;https://www.mn.uio.no/math/english/people/aca/proske/;http://people.inf.ethz.ch/alucchi/",
        "dblp": "310/1851;285/5128;;14/5780",
        "google_scholar": "6qKgak8AAAAJ;-dlYjUsAAAAJ;;https://scholar.google.ch/citations?user=V1ONSgIAAAAJ",
        "orcid": "0009-0004-7094-2586;;;",
        "linkedin": "eneamc/;rustem-islamov-053345228/;;",
        "or_profile": "~Enea_Monzio_Compagnoni1;~Rustem_Islamov1;~Frank_Norbert_Proske1;~Aurelien_Lucchi1",
        "aff": "University of Basel;University of Basel;University of Oslo;University of Basel",
        "aff_domain": "unibas.ch;unibas.ch;nissen.uio.no;unibas.ch",
        "position": "PhD student;PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncompagnoni2025unbiased,\ntitle={Unbiased and Sign Compression in Distributed Learning: Comparing Noise Resilience via {SDE}s},\nauthor={Enea Monzio Compagnoni and Rustem Islamov and Frank Norbert Proske and Aurelien Lucchi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RRrftHtEfK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RRrftHtEfK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RX61IOwwxD",
        "title": "Differentially private algorithms for linear queries via stochastic convex optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This article establishes a method to answer a finite set of linear queries on a given dataset while ensuring differential privacy. To achieve this, we formulate the corresponding task as a saddle-point problem, i.e. an optimization problem whose solution corresponds to a distribution minimizing the difference between answers to the linear queries based on the true distribution and answers from a differentially private distribution. Against this background, we establish two new algorithms for corresponding differentially private data release: the first is based on the differentially private Frank-Wolfe method, the second combines randomized smoothing with stochastic convex optimization techniques for a solution to the saddle-point problem. While previous works assess the accuracy of differentially private algorithms with reference to the empirical data distribution, a key contribution of our work is a more natural evaluation of the proposed algorithms' accuracy with reference to the true data-generating distribution.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Giorgio Micali;Clement LEZANE;Annika Betken",
        "authorids": "~Giorgio_Micali1;~Clement_LEZANE1;~Annika_Betken1",
        "gender": "M;M;",
        "homepage": ";;https://people.utwente.nl/a.betken",
        "dblp": ";;305/5986",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": "giorgio-micali-61a3a01ba/;cl%C3%A9ment-lezane-7222b01b6/;",
        "or_profile": "~Giorgio_Micali1;~Clement_LEZANE1;~Annika_Betken1",
        "aff": "University of Twente;University of Twente;University of Twente",
        "aff_domain": "utwente.nl;utwente.nl;utwente.nl",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nmicali2025differentially,\ntitle={Differentially private algorithms for linear queries via stochastic convex optimization},\nauthor={Giorgio Micali and Clement LEZANE and Annika Betken},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RX61IOwwxD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RX61IOwwxD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RZryinonfr",
        "title": "Infinite-dimensional Diffusion Bridge Simulation via Operator Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The diffusion bridge, which is a diffusion process conditioned on hitting a specific state within a finite period, has found broad applications in various scientific and engineering fields. However, simulating diffusion bridges for modeling natural data can be challenging due to both the intractability of the drift term and continuous representations of the data. Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain under explored. This paper presents a method that merges score-matching techniques with operator learning, enabling a direct approach to learn the infinite-dimensional bridge and achieving a discretization equivariant bridge simulation. We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data. Our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gefan Yang;Elizabeth Louise Baker;Michael Lind Severinsen;Christy Anna Hipsley;Stefan Sommer",
        "authorids": "~Gefan_Yang1;~Elizabeth_Louise_Baker1;~Michael_Lind_Severinsen1;~Christy_Anna_Hipsley1;~Stefan_Sommer1",
        "gender": "M;;M;F;",
        "homepage": ";;https://globe.ku.dk/staff-list?pure=en/persons/583597;https://www.evomorpholab.com/;",
        "dblp": ";;;;",
        "google_scholar": "tIdJ-YgAAAAJ;;;https://scholar.google.com.au/citations?user=Eyi8JRoAAAAJ;",
        "orcid": ";;0009-0009-3998-7117;;",
        "linkedin": "gefanyang/;;michael-severinsen/;;",
        "or_profile": "~Gefan_Yang1;~Elizabeth_Louise_Baker1;~Michael_Lind_Severinsen1;~Christy_Anna_Hipsley1;~Stefan_Sommer1",
        "aff": "University of Copenhagen;;University of Copenhagen;University of Copenhagen;",
        "aff_domain": "diku.dk;;ucph.dk;ku.edu;",
        "position": "PhD student;;PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\nyang2025infinitedimensional,\ntitle={Infinite-dimensional Diffusion Bridge Simulation via Operator Learning},\nauthor={Gefan Yang and Elizabeth Louise Baker and Michael Lind Severinsen and Christy Anna Hipsley and Stefan Sommer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RZryinonfr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RZryinonfr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Rq9EOLGpIC",
        "title": "Robust Estimation in metric spaces: Achieving Exponential Concentration with a Fr\\'echet Median",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "There is growing interest in developing statistical estimators that achieve exponential concentration around a population target even when the data distribution has heavier than exponential tails. More recent activity has focused on extending such ideas beyond Euclidean spaces to Hilbert spaces and Riemannian manifolds. In this work, we show that such exponential concentration in presence of heavy tails can be achieved over a broader class of parameter spaces called CAT($\\kappa$) spaces, a very general metric space equipped with the minimal essential geometric structure for our purpose, while being sufficiently broad to encompass most typical examples encountered in statistics and machine learning. The key technique is to develop and exploit a general concentration bound for the Fr\\'echet median in CAT($\\kappa$) spaces. We illustrate our theory through a number of examples, and provide empirical support through simulation studies.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jakwang Kim;Jiyoung Park;Anirban Bhattacharya",
        "authorids": "~Jakwang_Kim1;~Jiyoung_Park3;~Anirban_Bhattacharya1",
        "gender": "M;M;M",
        "homepage": "https://sites.google.com/view/jakwangkim/home?authuser=0;https://wldyddl5510.github.io/;https://sites.google.com/view/anirban-bhattacharya-tamu/",
        "dblp": ";63/1243-4;13/10009",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;okljcvgAAAAJ",
        "orcid": ";0009-0007-7283-2315;0000-0001-6197-2055",
        "linkedin": ";;",
        "or_profile": "~Jakwang_Kim1;~Jiyoung_Park3;~Anirban_Bhattacharya1",
        "aff": "University of British Columbia;Texas A&M University - College Station;Texas A&M University - College Station",
        "aff_domain": "ubc.ca;tamu.edu;tamu.edu",
        "position": "Postdoc;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nkim2025robust,\ntitle={Robust Estimation in metric spaces: Achieving Exponential Concentration with a Fr{\\textbackslash}'echet Median},\nauthor={Jakwang Kim and Jiyoung Park and Anirban Bhattacharya},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Rq9EOLGpIC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Rq9EOLGpIC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RqXNXaOF6J",
        "title": "Fine-Tuning with Uncertainty-Aware Priors Makes Vision and Language Foundation Models More Reliable",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fine-tuning off-the-shelf pre-trained neural networks has become the default starting point for a wide range of challenging prediction tasks\u2014especially in computer vision and natural language processing, where pre-trained models trained on millions or even billions of data points are publicly available and can be fine-tuned with a moderate compute budget. However, while fine-tuned models have been shown to significantly improve predictive performance compared to models trained from scratch, they can exhibit poor calibration and fail to reliably identify challenging distribution shifts. In this paper, we improve uncertainty quantification in fine-tuned models by constructing a data-driven uncertainty-aware fine-tuning prior that assigns high probability density to parameters that induce predictive functions with high uncertainty on input points that are meaningfully different from the data. We derive a tractable variational objective to perform approximate inference in models with data-driven uncertainty-aware priors and evaluate models fine-tuned with such priors on different transfer learning tasks. We show that fine-tuning with uncertainty-aware priors significantly improves calibration, selective prediction, and semantic shift detection on computer vision and natural language classification tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tim G. J. Rudner;Xiang Pan;Yucen Lily Li;Ravid Shwartz-Ziv;Andrew Gordon Wilson",
        "authorids": "~Tim_G._J._Rudner2;~Xiang_Pan3;~Yucen_Lily_Li1;~Ravid_Shwartz-Ziv2;~Andrew_Gordon_Wilson1",
        "gender": ";M;;;Not Specified",
        "homepage": ";https://xiangpan.info;https://yucenli.com;;https://cims.nyu.edu/~andrewgw",
        "dblp": ";59/749-1.html;252/6123;;65/10453",
        "google_scholar": ";SxU03foAAAAJ;;;https://scholar.google.com.tw/citations?user=twWX2LIAAAAJ",
        "orcid": ";0000-0002-9828-5416;;;",
        "linkedin": ";;;;",
        "or_profile": "~Tim_G._J._Rudner2;~Xiang_Pan3;~Yucen_Lily_Li1;~Ravid_Shwartz-Ziv2;~Andrew_Gordon_Wilson1",
        "aff": ";New York University;New York University;;New York University",
        "aff_domain": ";nyu.edu;nyu.edu;;nyu.edu",
        "position": ";PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nrudner2025finetuning,\ntitle={Fine-Tuning with Uncertainty-Aware Priors Makes Vision and Language Foundation Models More Reliable},\nauthor={Tim G. J. Rudner and Xiang Pan and Yucen Lily Li and Ravid Shwartz-Ziv and Andrew Gordon Wilson},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RqXNXaOF6J}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RqXNXaOF6J",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Rr7i7aAJfx",
        "title": "Gaussian Smoothing in Saliency Maps: The Stability-Fidelity Trade-Off in Neural Network Interpretability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Saliency maps have been widely used to interpret the decisions of neural network classifiers and discover phenomena from their learned functions. However, standard gradient-based maps are frequently observed to be highly sensitive to the randomness of training data and the stochasticity in the training process. In this work, we study the role of Gaussian smoothing in the well-known Smooth-Grad algorithm in the stability of the gradient-based maps to the randomness of training samples. We extend the algorithmic stability framework to gradient-based interpretation maps and prove bounds on the stability error of standard Simple-Grad, Integrated-Gradients, and Smooth-Grad saliency maps. Our theoretical results suggest the role of Gaussian smoothing in boosting the stability of gradient-based maps to the randomness of training settings. On the other hand, we analyze the faithfulness of the Smooth-Grad maps to the original Simple-Grad and show the lower fidelity under a more intense Gaussian smoothing. We support our theoretical results by performing several numerical experiments on standard image datasets. Our empirical results confirm our hypothesis on the fidelity-stability trade-off in the application of Gaussian smoothing to gradient-based interpretation maps.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhuorui Ye;Farzan Farnia",
        "authorids": "~Zhuorui_Ye1;~Farzan_Farnia1",
        "gender": "M;M",
        "homepage": ";https://www.cse.cuhk.edu.hk/~farnia/",
        "dblp": ";132/7757",
        "google_scholar": "0XUri1UAAAAJ;GYPCqcYAAAAJ",
        "orcid": ";0000-0002-6049-9232",
        "linkedin": "zhuorui-ye-a38773200/;farzan-farnia-00798335",
        "or_profile": "~Zhuorui_Ye1;~Farzan_Farnia1",
        "aff": "Tsinghua University;The Chinese University of Hong Kong",
        "aff_domain": "tsinghua.edu.cn;cuhk.edu.hk",
        "position": "Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nye2025on,\ntitle={On the Trade-Off between Stability and Fidelity of Gaussian-Smoothed Saliency Maps},\nauthor={Zhuorui Ye and Farzan Farnia},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Rr7i7aAJfx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Rr7i7aAJfx",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RwJTeddgRD",
        "title": "Balls-and-Bins Sampling for DP-SGD",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We introduce the _Balls-and-Bins_ sampling for differentially private (DP) optimization methods such as DP-SGD. While it has been common practice to use some form of shuffling in DP-SGD implementations, privacy accounting algorithms have typically assumed that Poisson subsampling is used instead. Recent work by Chua et al. (2024), however, pointed out that shuffling based DP-SGD can have a much larger privacy cost in practical regimes of parameters. In this work we show that the Balls-and-Bins sampling achieves the \"best-of-both\" samplers, namely, the implementation of Balls-and-Bins sampling is similar to that of Shuffling and models trained using DP-SGD with Balls-and-Bins sampling achieve utility comparable to those trained using DP-SGD with Shuffling at the same noise multiplier, and yet, Balls-and-Bins sampling enjoys similar-or-better privacy amplification as compared to Poisson subsampling in practical regimes.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lynn Chua;Badih Ghazi;Charlie Harrison;Pritish Kamath;Ravi Kumar;Ethan Jacob Leeman;Pasin Manurangsi;Amer Sinha;Chiyuan Zhang",
        "authorids": "~Lynn_Chua1;~Badih_Ghazi1;~Charlie_Harrison1;~Pritish_Kamath2;~Ravi_Kumar1;~Ethan_Jacob_Leeman1;~Pasin_Manurangsi2;~Amer_Sinha1;~Chiyuan_Zhang1",
        "gender": "F;;;M;M;M;M;M;M",
        "homepage": ";https://sites.google.com/view/badihghazi/home;;https://pritishkamath.github.io/;https://sites.google.com/site/ravik53/;;https://pasin30055.github.io/;;http://pluskid.org",
        "dblp": "143/4392;125/2134;;https://dblp.org/pers/k/Kamath:Pritish.html;k/RaviKumar.html;140/7652.html;133/2059;;21/8315",
        "google_scholar": "D2SXVSYAAAAJ;GBJLTN8AAAAJ;Q9zSPIQAAAAJ;1JFARhUAAAAJ;J_XhIsgAAAAJ;;35hM-PkAAAAJ;;l_G2vr0AAAAJ",
        "orcid": ";;;;0000-0002-2203-2586;;;;",
        "linkedin": "chua-lynn/;badih-ghazi-608379132/;;;ravi-kumar-a3a9631;https://www.linkedin.com/mwlite/in/ethan-leeman;;amersinha/;",
        "or_profile": "~Lynn_Chua1;~Badih_Ghazi1;~Charlie_Harrison1;~Pritish_Kamath2;~Ravi_Kumar1;~Ethan_Jacob_Leeman1;~Pasin_Manurangsi2;~Amer_Sinha1;~Chiyuan_Zhang1",
        "aff": "Google;Google;Google;Google Research;Google;Google;Google;Research, Google;Google",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;research.google.com;google.com",
        "position": "Researcher;Researcher;Software Engineer;Research Scientist;Research Scientist;Researcher;Research Scientist;Researcher;Research Scientist",
        "bibtex": "@inproceedings{\nchua2025ballsandbins,\ntitle={Balls-and-Bins Sampling for {DP}-{SGD}},\nauthor={Lynn Chua and Badih Ghazi and Charlie Harrison and Pritish Kamath and Ravi Kumar and Ethan Jacob Leeman and Pasin Manurangsi and Amer Sinha and Chiyuan Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=RwJTeddgRD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RwJTeddgRD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "S2QoDt4bw4",
        "title": "A Likelihood Based Approach for Watermark Detection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Watermarking techniques embed statistical signals within content generated by large language models to help trace its source. Although existing methods perform well on long texts, their effectiveness significantly decreases for shorter texts. We introduce a statistical detection approach that improves the power of watermark detection, particularly in shorter texts. Our method leverages both the watermark key sequence and the next token probabilities (NTPs) to determine whether a text is generated by a large language model. We demonstrate the optimality of our approach and analyze its power properties. We also investigate an approach to estimating NTPs and extend our method to scenarios where texts face potential attacks such as substitutions, insertions, or deletions. We validate the effectiveness of our technique using texts generated by Meta-Llama-3-8B from Meta and Mistral-7B-v0.1 from Mistral AI, utilizing prompts extracted from Google's C4 dataset. In scenarios without attacks and with short text lengths, our method demonstrates approximately 65% power improvement compared to the baseline method on average. We release all code publicly at https://github.com/doccstat/llm-watermark-adaptive.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xingchi Li;Guanxun Li;Xianyang Zhang",
        "authorids": "~Xingchi_Li1;~Guanxun_Li1;~Xianyang_Zhang1",
        "gender": ";M;M",
        "homepage": ";http://www.guanxun.li;https://zhangxiany-tamu.github.io/",
        "dblp": ";295/6757;",
        "google_scholar": ";jxFkCp8AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-2449-4469;",
        "linkedin": ";;",
        "or_profile": "~Xingchi_Li1;~Guanxun_Li1;~Xianyang_Zhang1",
        "aff": ";Beijing Normal University;Texas A&M University - College Station",
        "aff_domain": ";bnu.edu.cn;tamu.edu",
        "position": ";Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nli2025a,\ntitle={A Likelihood Based Approach for Watermark Detection},\nauthor={Xingchi Li and Guanxun Li and Xianyang Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=S2QoDt4bw4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=S2QoDt4bw4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "S3zU0LbfZ1",
        "title": "Noisy Low-Rank Matrix Completion via Transformed $L_1$ Regularization and its Theoretical Properties",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper focuses on recovering an underlying matrix from its noisy partial entries, a problem commonly known as matrix completion. We delve into the investigation of a non-convex regularization, referred to as transformed $L_1$ (TL1), which interpolates between the rank and the nuclear norm of matrices through a hyper-parameter $a \\in (0, \\infty)$. While some literature adopts such regularization for matrix completion, it primarily addresses scenarios with uniformly missing entries and focuses on algorithmic advances. To fill in the gap in the current literature, we provide a comprehensive statistical analysis for the estimator from a TL1-regularized recovery model under general sampling distribution. In particular, we show that when $a$ is sufficiently large, the matrix recovered by the TL1-based model enjoys a convergence rate measured by the Frobenius norm, comparable to that of the model based on the nuclear norm, despite the challenges posed by the non-convexity of the TL1 regularization. When $a$ is small enough, we show that the rank of the estimated matrix remains a constant order when the true matrix is exactly low-rank. A trade-off between controlling the error and the rank is established through different choices of tuning parameters. The appealing practical performance of TL1 regularization is demonstrated through a simulation study that encompasses various sampling mechanisms, as well as two real-world applications. Additionally, the role of the hyper-parameter $a$ on the TL1-based model is explored via experiments to offer guidance in practical scenarios.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kun Zhao;Jiayi Wang;Yifei Lou",
        "authorids": "~Kun_Zhao7;~Jiayi_Wang7;~Yifei_Lou2",
        "gender": "F;F;F",
        "homepage": ";https://jiayiwang1017.github.io/;https://sites.google.com/site/louyifei/",
        "dblp": ";;",
        "google_scholar": ";;iCiUflEAAAAJ",
        "orcid": ";;0000-0003-1973-5704",
        "linkedin": "kunzhao-tx314159/;;",
        "or_profile": "~Kun_Zhao7;~Jiayi_Wang7;~Yifei_Lou2",
        "aff": "University of Texas at Dallas;University of Texas at Dallas;University of North Carolina at Chapel Hill",
        "aff_domain": "utdallas.edu;utdallas.edu;unc.edu",
        "position": "PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nzhao2025noisy,\ntitle={Noisy Low-Rank Matrix Completion via Transformed \\$L\\_1\\$ Regularization and its Theoretical Properties},\nauthor={Kun Zhao and Jiayi Wang and Yifei Lou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=S3zU0LbfZ1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=S3zU0LbfZ1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "S8lfepB2fz",
        "title": "Causal Representation Learning from General Environments under Nonparametric Mixing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Causal representation learning aims to recover the latent causal variables and their causal relations, typically represented by directed acyclic graphs (DAGs), from low-level observations such as image pixels. A prevailing line of research exploits multiple environments, which assume how data distributions change, including single-node interventions, coupled interventions, or hard interventions, or parametric constraints on the mixing function or the latent causal model, such as linearity. Despite the novelty and elegance of the results, they are often violated in real problems. Accordingly, we formalize a set of desiderata for causal representation learning that applies to a broader class of environments, referred to as general environments. Interestingly, we show that one can fully recover the latent DAG and identify the latent variables up to minor indeterminacies under a nonparametric mixing function and nonlinear latent causal models, such as additive (Gaussian) noise models or heteroscedastic noise models, by properly leveraging sufficient change conditions on the causal mechanisms up to third-order derivatives. These represent, to our knowledge, the first results to fully recover the latent DAG from general environments under nonparametric mixing. Notably, our results are stronger than many existing works, but require less restrictive assumptions about changing environments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ignavier Ng;Shaoan Xie;Xinshuai Dong;Peter Spirtes;Kun Zhang",
        "authorids": "~Ignavier_Ng1;~Shaoan_Xie4;~Xinshuai_Dong1;~Peter_Spirtes1;~Kun_Zhang1",
        "gender": "M;;M;M;M",
        "homepage": "https://ignavierng.github.io/;https://shaoan.net;https://dongxinshuai.github.io/;https://www.cmu.edu/dietrich/philosophy/people/faculty/spirtes.html;http://www.andrew.cmu.edu/user/kunz1/",
        "dblp": "251/3037;205/9276.html;279/6151.html;87/3550;96/3115-1",
        "google_scholar": ";mChB-hQAAAAJ;A7JyL1sAAAAJ;mar1eCwAAAAJ;RGoypN4AAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Ignavier_Ng1;~Shaoan_Xie4;~Xinshuai_Dong1;~Peter_Spirtes1;~Kun_Zhang1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Mohamed bin Zayed University of Artificial Intelligence+Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;cmu.edu;cmu.edu;mbzuai.ac.ae+cmu.edu",
        "position": "PhD student;PhD student;PhD student;Full Professor;Professor+Associate Professor",
        "bibtex": "@inproceedings{\nng2025causal,\ntitle={Causal Representation Learning from General Environments under Nonparametric Mixing},\nauthor={Ignavier Ng and Shaoan Xie and Xinshuai Dong and Peter Spirtes and Kun Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=S8lfepB2fz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=S8lfepB2fz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "SJS7QTu94D",
        "title": "A primer on linear classification with missing data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Supervised learning with missing data aims at building the best prediction of a target output based on partially-observed inputs. Major approaches to address this problem  can be decomposed into $(i)$ impute-then-predict strategies, which first fill in the empty input components and then apply a unique predictor and $(ii)$ Pattern-by-Pattern (P-b-P) approaches, where a predictor is built on each missing pattern. In this paper, we theoretically analyze how three classical linear classifiers, namely perceptron, logistic regression and linear discriminant analysis (LDA), behave with Missing Completely At Random (MCAR) data, depending on the strategy (imputation or P-b-P) to handle missing values. We prove that both imputation and P-b-P approaches are ill-specified in a logistic regression framework, thus questioning the relevance of such approaches to handle missing data. The most favorable auspices to perform classification with missing data concern P-b-P LDA methods. We provide finite-sample bounds for the excess risk in this framework, even for high-dimensional settings or MNAR data. Experiments illustrate our theoretical findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Angel David REYERO LOBO;Alexis Ayme;Claire Boyer;Erwan Scornet",
        "authorids": "~Angel_David_REYERO_LOBO1;~Alexis_Ayme1;~Claire_Boyer1;~Erwan_Scornet1",
        "gender": "M;M;;M",
        "homepage": "https://angelreyero.github.io/;https://alexisayme.github.io;https://www.imo.universite-paris-saclay.fr/~claire.boyer/;https://erwanscornet.github.io/",
        "dblp": ";;;176/1062",
        "google_scholar": ";;;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Angel_David_REYERO_LOBO1;~Alexis_Ayme1;~Claire_Boyer1;~Erwan_Scornet1",
        "aff": "Universit\u00e9 de Toulouse;Ecole Normale Sup\u00e9rieure de Paris;Universit\u00e9 Paris-Saclay;Sorbonne Universit\u00e9 - Facult\u00e9 des Sciences (Paris VI)",
        "aff_domain": "univ-toulouse.fr;ens.fr;universite-paris-saclay.fr;sorbonne-universite.fr",
        "position": "PhD student;Postdoc;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nlobo2025a,\ntitle={A primer on linear classification with missing data},\nauthor={Angel David REYERO LOBO and Alexis Ayme and Claire Boyer and Erwan Scornet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=SJS7QTu94D}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SJS7QTu94D",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "SOrZWcuz3j",
        "title": "Approximate Equivariance in Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Equivariant neural networks have shown great success in reinforcement learning, improving sample efficiency and generalization when there is symmetry in the task. However, in many problems, only approximate symmetry is present, which makes imposing exact symmetry inappropriate. Recently, approximately equivariant networks have been proposed for supervised classification and modeling physical systems. In this work, we develop approximately equivariant algorithms in reinforcement learning (RL). We define approximately equivariant MDPs and theoretically characterize the effect of approximate equivariance on the optimal Q function. We propose novel RL architectures using relaxed group and steerable convolutions and experiment on several continuous control domains and stock trading with real financial data. Our results demonstrate that the approximately equivariant network performs on par with exactly equivariant networks when exact symmetries are present, and outperforms them when the domains exhibit approximate symmetry. As an added byproduct of these techniques, we observe increased robustness to noise at test time. Our code is available at https://github.com/jypark0/approx_equiv_rl.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jung Yeon Park;Sujay Bhatt;Sihan Zeng;Lawson L.S. Wong;Alec Koppel;Sumitra Ganesh;Robin Walters",
        "authorids": "~Jung_Yeon_Park1;~Sujay_Bhatt1;~Sihan_Zeng1;~Lawson_L.S._Wong2;~Alec_Koppel1;~Sumitra_Ganesh1;~Robin_Walters1",
        "gender": "M;M;M;;M;F;M",
        "homepage": ";;;;http://koppel.netlify.app/;;http://www.robinwalters.com",
        "dblp": "240/2704;184/4793.html;;;149/0076;98/463.html;258/3416",
        "google_scholar": "LZSRm9sAAAAJ;https://scholar.google.co.za/citations?user=ZpcdbtQAAAAJ;3gnMYngAAAAJ;;8ClxyjIAAAAJ;https://scholar.google.com/citations?hl=en;fnprJmUAAAAJ",
        "orcid": ";0000-0002-3586-1789;;;0000-0003-2447-2873;;",
        "linkedin": ";sujay-bhatt;sihan-zeng-a528a490/;;alec-koppel-9860b697/;sumitra-ganesh-0379853;",
        "or_profile": "~Jung_Yeon_Park1;~Sujay_Bhatt1;~Sihan_Zeng1;~Lawson_L.S._Wong2;~Alec_Koppel1;~Sumitra_Ganesh1;~Robin_Walters1",
        "aff": "Northeastern University;JP Morgan AI Research;J.P. Morgan Chase;;J.P. Morgan Chase;J.P. Morgan Chase;Northeastern University ",
        "aff_domain": "northeastern.edu;jpmchase.com;jpmchase.com;;jpmorgan.com;jpmorgan.com;northeastern.edu",
        "position": "PhD student;Researcher;Researcher;;Research Team Lead;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\npark2025approximate,\ntitle={Approximate Equivariance in Reinforcement Learning},\nauthor={Jung Yeon Park and Sujay Bhatt and Sihan Zeng and Lawson L.S. Wong and Alec Koppel and Sumitra Ganesh and Robin Walters},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=SOrZWcuz3j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SOrZWcuz3j",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "SQkur6MNOR",
        "title": "When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The weighted low-rank approximation problem is a fundamental numerical linear algebra problem and has many applications in machine learning. Given a $n \\times n$ weight matrix $W$ and a $n \\times n$ matrix $A$, the goal is to find two low-rank matrices $U, V \\in \\mathbb{R}^{n \\times k}$ such that the cost of $\\\\| W \\circ (U V^\\top - A) \\\\|_F^2$ is minimized. Previous work has to pay $\\Omega(n^2)$ time when matrices $A$ and $W$ are dense, e.g., having $\\Omega(n^2)$ non-zero entries. In this work, we show that there is a certain regime, even if $A$ and $W$ are dense,  we can still hope to solve the weighted low-rank approximation problem in almost linear $n^{1+o(1)}$ time.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chenyang Li;Yingyu Liang;Zhenmei Shi;Zhao Song",
        "authorids": "~Chenyang_Li6;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3",
        "gender": "M;;M;M",
        "homepage": "https://github.com/ChenyangLi1;;http://zhmeishi.github.io/;https://www.youtube.com/@simapofang",
        "dblp": ";;246/5216;76/4051-2",
        "google_scholar": ";;0oeNnzMAAAAJ;yDZct7UAAAAJ",
        "orcid": ";;0009-0007-6741-7598;0000-0003-4589-5234",
        "linkedin": ";;zhenmei-shi-56408a113/;",
        "or_profile": "~Chenyang_Li6;~Yingyu_Liang1;~Zhenmei_Shi1;~Zhao_Song3",
        "aff": "Fuzhou University;;MongoDB+Voyage AI;University of California, Berkeley",
        "aff_domain": "fzu.edu.cn;;mongodb.com+voyageai.com;berkeley.edu",
        "position": "Undergrad student;;Researcher+Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nli2025when,\ntitle={When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time?},\nauthor={Chenyang Li and Yingyu Liang and Zhenmei Shi and Zhao Song},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=SQkur6MNOR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SQkur6MNOR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "SVkYxDYnJK",
        "title": "Towards a mathematical theory for consistency training in diffusion models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Consistency models, which were proposed to mitigate the high computational overhead during the\nsampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical\nperformance. When integrated into the training phase, consistency models attempt to train a sequence\nof consistency functions capable of mapping any point at any time step of the diffusion process to its\nstarting point. Despite the empirical success, a comprehensive theoretical understanding of consistency\ntraining remains elusive. This paper takes a first step towards establishing theoretical underpinnings for\nconsistency models. We demonstrate that, in order to generate samples within $\\varepsilon$ proximity to the target\nin distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency\nlearning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into\nthe validity and efficacy of consistency models, illuminating their utility in downstream inference tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gen Li;Zhihan Huang;Yuting Wei",
        "authorids": "~Gen_Li2;~Zhihan_Huang1;~Yuting_Wei1",
        "gender": "M;M;F",
        "homepage": ";https://statistics.wharton.upenn.edu/profile/zhihanh;https://yutingwei.github.io/",
        "dblp": "28/538-5.html;;184/3856",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;;fsbXdAYAAAAJ",
        "orcid": "0000-0002-3078-9191;;",
        "linkedin": ";;",
        "or_profile": "~Gen_Li2;~Zhihan_Huang1;~Yuting_Wei1",
        "aff": "The Chinese University of Hong Kong;;The Wharton School, University of Pennsylvania",
        "aff_domain": "cuhk.edu.hk;;wharton.upenn.edu",
        "position": "Assistant Professor;;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025towards,\ntitle={Towards a mathematical theory for consistency training in diffusion models},\nauthor={Gen Li and Zhihan Huang and Yuting Wei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=SVkYxDYnJK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SVkYxDYnJK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "T2FJaLgM4r",
        "title": "A Novel Convex Gaussian Min Max Theorem for Repeated Features",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "The Convex Gaussian Min-Max Theorem (CGMT) allows for the study of min-max optimization problems over bilinear Gaussian forms by instead considering an alternative optimization problem whose statistical properties are tied to that of the primary optimization. We prove a generalization of the CGMT to a family of problems in machine learning (ML) with correlated entries in the data matrix. This family includes various familiar examples of problems with shared weights or repeated features. In particular, we make use of our theorem to obtain asymptotically exact learning curves for regression with vector valued labels, regression with complex variables, and regression with convolution.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "David Bosch;Ashkan Panahi",
        "authorids": "~David_Bosch1;~Ashkan_Panahi1",
        "gender": "M;M",
        "homepage": "https://www.chalmers.se/en/staff/Pages/davidbo.aspx;",
        "dblp": ";94/9875",
        "google_scholar": "GzMolykAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~David_Bosch1;~Ashkan_Panahi1",
        "aff": "Chalmers University of Technology;Chalmers University of Technology",
        "aff_domain": "chalmers.se;chalmers.se",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nbosch2025a,\ntitle={A Novel Convex Gaussian Min Max Theorem for Repeated Features},\nauthor={David Bosch and Ashkan Panahi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=T2FJaLgM4r}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=T2FJaLgM4r",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "T9fHiIiPF1",
        "title": "Nonparametric Distributional Regression via Quantile Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper proposes a new approach to estimating the distribution of a response variable conditioned on factors. We model the conditional quantile function as a mixture (weighted sum) of basis quantile functions, with weights depending on these factors. The estimation problem is formulated as a convex optimization problem. The objective function is equivalent to the continuous ranked probability score (CRPS). This approach can be viewed as conducting quantile regressions for all confidence levels simultaneously while inherently avoiding quantile crossing. We use spline functions of factors as a primary example for the weight function. We prove an approximation property of the model. To address computational challenges, we propose a dimensionality reduction method using tensor decomposition and an alternating algorithm. Our approach offers flexibility, interpretability, tractability, and extendability. Numerical experiments demonstrate its effectiveness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Cheng Peng;Stan Uryasev",
        "authorids": "~Cheng_Peng6;~Stan_Uryasev1",
        "gender": "Not Specified;M",
        "homepage": ";http://uryasev.ams.stonybrook.edu/",
        "dblp": ";",
        "google_scholar": "https://scholar.google.com/citations?hl=en;",
        "orcid": ";",
        "linkedin": "cheng-peng-a1a289184;",
        "or_profile": "~Cheng_Peng6;~Stan_Uryasev1",
        "aff": "State University of New York at Stony Brook;State University of New York at Stony Brook",
        "aff_domain": "stonybrook.edu;stonybrook.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\npeng2025nonparametric,\ntitle={Nonparametric Distributional Regression via Quantile Regression},\nauthor={Cheng Peng and Stan Uryasev},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=T9fHiIiPF1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=T9fHiIiPF1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "TPNFDgltgq",
        "title": "Do Regularization Methods for Shortcut Mitigation Work As Intended?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mitigating shortcuts, where models exploit spurious correlations in training data, remains a significant challenge for improving generalization. Regularization methods have been proposed to address this issue by enhancing model generalizability. However, we demonstrate that these methods can sometimes overregularize, inadvertently suppressing causal features along with spurious ones. In this work, we analyze the theoretical mechanisms by which regularization mitigates shortcuts and explore the limits of its effectiveness. Additionally, we identify the conditions under which regularization can successfully eliminate shortcuts without compromising causal features. Through experiments on synthetic and real-world datasets, our comprehensive analysis provides valuable insights into the strengths and limitations of regularization techniques for addressing shortcuts, offering guidance for developing more robust models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haoyang Hong;Ioanna Papanikolaou;Sonali Parbhoo",
        "authorids": "~Haoyang_Hong1;ioanna.papanikolaou23@imperial.ac.uk;~Sonali_Parbhoo2",
        "gender": "M;;",
        "homepage": "https://github.com/Hhy096;;",
        "dblp": ";;",
        "google_scholar": ";;FwEz5s4AAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Haoyang_Hong1;ioanna.papanikolaou23@imperial.ac.uk;~Sonali_Parbhoo2",
        "aff": ";;Imperial College London+Harvard University",
        "aff_domain": ";;imperial.ac.uk+harvard.edu",
        "position": ";;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nhong2025on,\ntitle={On the Pitfalls of Regularization for Shortcut Mitigation},\nauthor={Haoyang Hong and Ioanna Papanikolaou and Sonali Parbhoo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=TPNFDgltgq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TPNFDgltgq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "TXnBBfehBT",
        "title": "Multi-Player Approaches for Dueling Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fine-tuning large deep networks with preference-based human feedback has seen\npromising results. As user numbers grow and tasks shift to complex datasets like images\nor videos, distributed approaches become essential for efficiently gathering feedback. \nTo address this, we introduce a multiplayer dueling bandit problem, highlighting that exploring non-informative candidate pairs becomes especially challenging in a collaborative environment.\nWe demonstrate that the use of a Follow Your Leader black-box approach matches the asymptotic regret lower-bound when utilizing known dueling bandit algorithms as a foundation.\nAdditionally, we propose and analyze a message-passing fully distributed approach with a novel Condorcet-Winner recommendation protocol, resulting in expedited exploration in the non-asymptotic regime which reduces regret. Our experimental comparisons reveal that our multiplayer algorithms surpass single-player benchmark algorithms, underscoring their efficacy in addressing the nuanced challenges of this setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Or Raveh;Junya Honda;Masashi Sugiyama",
        "authorids": "~Or_Raveh1;~Junya_Honda1;~Masashi_Sugiyama1",
        "gender": "M;M;M",
        "homepage": ";http://stat.sys.i.kyoto-u.ac.jp/honda/index.html;http://www.ms.k.u-tokyo.ac.jp/sugi/",
        "dblp": "241/6959;56/9070;35/1228",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=Aw8OrxQAAAAJ;https://scholar.google.co.jp/citations?user=GkYIrlIAAAAJ",
        "orcid": ";;0000-0001-6658-6743",
        "linkedin": "or-raveh-872873105;;",
        "or_profile": "~Or_Raveh1;~Junya_Honda1;~Masashi_Sugiyama1",
        "aff": "The University of Tokyo+RIKEN;Kyoto University;RIKEN+The University of Tokyo",
        "aff_domain": "u-tokyo.ac.jp+riken.jp;kyoto-u.ac.jp;riken.jp+u-tokyo.ac.jp",
        "position": "PhD student+Researcher;Associate Professor;Director+Full Professor",
        "bibtex": "@inproceedings{\nraveh2025multiplayer,\ntitle={Multi-Player Approaches for Dueling Bandits},\nauthor={Or Raveh and Junya Honda and Masashi Sugiyama},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=TXnBBfehBT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TXnBBfehBT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Tb8fIVYrYb",
        "title": "Learning in Herding Mean Field Games: Single-Loop Algorithm with Finite-Time Convergence Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider discrete-time stationary mean field games (MFG) with unknown dynamics and design algorithms for finding the equilibrium with finite-time complexity guarantees. Prior solutions to the problem assume either the contraction of a mean field optimality-consistency operator or strict weak monotonicity, which may be overly restrictive. In this work, we introduce a new class of solvable MFGs, named the \"fully herding class\", which expands the known solvable class of MFGs and for the first time includes problems with multiple equilibria. We propose a direct policy optimization method, Accelerated Single-loop Actor Critic Algorithm for Mean Field Games (ASAC-MFG), that provably finds a global equilibrium for MFGs within this class, under suitable access to a single trajectory of Markovian samples. Different from the prior methods, ASAC-MFG is single-loop and single-sample-path. We establish the finite-time and finite-sample convergence of ASAC-MFG to a mean field equilibrium via new techniques that we develop for multi-time-scale stochastic approximation. We support the theoretical results with illustrative numerical simulations.\n\nWhen the mean field does not affect the transition and reward, a MFG reduces to a Markov decision process (MDP) and ASAC-MFG becomes an actor-critic algorithm for finding the optimal policy in average-reward MDPs, with a sample complexity matching the state-of-the-art. Previous works derive the complexity assuming a contraction on the Bellman operator, which is invalid for average-reward MDPs. We match the rate while removing the untenable assumption through an improved Lyapunov function.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sihan Zeng;Sujay Bhatt;Alec Koppel;Sumitra Ganesh",
        "authorids": "~Sihan_Zeng1;~Sujay_Bhatt1;~Alec_Koppel1;~Sumitra_Ganesh1",
        "gender": "M;M;M;F",
        "homepage": ";;http://koppel.netlify.app/;",
        "dblp": ";184/4793.html;149/0076;98/463.html",
        "google_scholar": "3gnMYngAAAAJ;https://scholar.google.co.za/citations?user=ZpcdbtQAAAAJ;8ClxyjIAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-3586-1789;0000-0003-2447-2873;",
        "linkedin": "sihan-zeng-a528a490/;sujay-bhatt;alec-koppel-9860b697/;sumitra-ganesh-0379853",
        "or_profile": "~Sihan_Zeng1;~Sujay_Bhatt1;~Alec_Koppel1;~Sumitra_Ganesh1",
        "aff": "J.P. Morgan Chase;JP Morgan AI Research;J.P. Morgan Chase;J.P. Morgan Chase",
        "aff_domain": "jpmchase.com;jpmchase.com;jpmorgan.com;jpmorgan.com",
        "position": "Researcher;Researcher;Research Team Lead;Researcher",
        "bibtex": "@inproceedings{\nzeng2025learning,\ntitle={Learning in Herding Mean Field Games: Single-Loop Algorithm with Finite-Time Convergence Analysis},\nauthor={Sihan Zeng and Sujay Bhatt and Alec Koppel and Sumitra Ganesh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Tb8fIVYrYb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Tb8fIVYrYb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ThADV3tAIn",
        "title": "Distribution-Aware Mean Estimation under User-level Local Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of mean estimation under user-level local differential privacy, where $n$ users are contributing through their local pool of data samples.\nPrevious work assume that the number of data samples is the same across users.\nIn contrast, we consider a more general and realistic scenario where each user $u \\in [n]$ owns $m_u$ data samples drawn from some generative distribution $\\mu$; $m_u$ being unknown to the statistician but drawn from a known distribution $M$ over $\\mathbb{N}$.\nBased on a distribution-aware mean estimation algorithm, we establish an $M$-dependent upper bounds on the worst-case risk over $\\mu$ for the task of mean estimation. We then derive a lower bound. The two bounds are asymptotically matching up to logarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Corentin Pla;Maxime Vono;Hugo Richard",
        "authorids": "corentinpla@gmail.com;~Maxime_Vono1;~Hugo_Richard1",
        "gender": ";M;M",
        "homepage": ";https://mvono.github.io/;https://hugorichard.github.io/",
        "dblp": ";;227/3044",
        "google_scholar": ";https://scholar.google.fr/citations?user=R5dfDTAAAAAJ;5KaKAOgAAAAJ",
        "orcid": ";0000-0003-4859-965X;",
        "linkedin": ";maximevono;",
        "or_profile": "corentinpla@gmail.com;~Maxime_Vono1;~Hugo_Richard1",
        "aff": ";Criteo;Criteo",
        "aff_domain": ";criteo.com;criteo.com",
        "position": ";Researcher;Researcher",
        "bibtex": "@inproceedings{\npla2025distributionaware,\ntitle={Distribution-Aware Mean Estimation under User-level Local Differential Privacy},\nauthor={Corentin Pla and Maxime Vono and Hugo Richard},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ThADV3tAIn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ThADV3tAIn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "TxYNVSoAz4",
        "title": "Diffusion Models under Group Transformations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In recent years, diffusion models have become the leading approach for distribution learning. This paper focuses on structure-preserving diffusion models (SPDM), a specific subset of diffusion processes tailored for distributions with inherent structures, such as group symmetries. We complement existing sufficient conditions for constructing SPDMs by proving complementary necessary ones. Additionally, we propose a new framework that considers the geometric structures affecting the diffusion process. Leveraging this framework, we design a structure-preserving bridge model that maintains alignment between the model\u2019s endpoint couplings. Empirical evaluations on equivariant diffusion models demonstrate their effectiveness in learning symmetric distributions and modeling transitions between them. Experiments on real-world medical images confirm that our models preserve equivariance while maintaining high sample quality. We also showcase the practical utility of our framework by implementing an equivariant denoising diffusion bridge model, which achieves reliable equivariant image noise reduction and style transfer, irrespective of prior knowledge of image orientation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haoye Lu;Spencer Szabados;Yaoliang Yu",
        "authorids": "~Haoye_Lu1;~Spencer_Szabados1;~Yaoliang_Yu1",
        "gender": "M;;M",
        "homepage": "https://haoyelu.github.io;;https://cs.uwaterloo.ca/~y328yu/",
        "dblp": ";;90/4989",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;https://scholar.google.ca/citations?user=zbXIQMsAAAAJ",
        "orcid": "0000-0003-0933-2370;;0000-0002-3823-0720",
        "linkedin": "haoyelu/;;",
        "or_profile": "~Haoye_Lu1;~Spencer_Szabados1;~Yaoliang_Yu1",
        "aff": "Skyfall AI+University of Waterloo;;University of Waterloo",
        "aff_domain": "skyfall.ai+uwaterloo.ca;;uwaterloo.ca",
        "position": "Intern+PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nlu2025diffusion,\ntitle={Diffusion Models under Group Transformations},\nauthor={Haoye Lu and Spencer Szabados and Yaoliang Yu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=TxYNVSoAz4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TxYNVSoAz4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "U2RgUAySB6",
        "title": "A Tight Regret Analysis of Non-Parametric Repeated Contextual Brokerage",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study a contextual version of the repeated brokerage problem. \nIn each interaction, two traders with private valuations for an item seek to buy or sell based on the learner's\u2014a broker\u2014proposed price, which is informed by some contextual information.\nThe broker's goal is to maximize the traders' net utility\u2014also known as the gain from trade\u2014by minimizing regret compared to an oracle with perfect knowledge of traders' valuation distributions.\nWe assume that traders' valuations are zero-mean perturbations of the unknown item's current market value\u2014which can change arbitrarily from one interaction to the next\u2014and that similar contexts will correspond to similar market prices.\nWe analyze two feedback settings: full-feedback, where after each interaction the traders' valuations are revealed to the broker, and limited-feedback, where only transaction attempts are revealed.\nFor both feedback types, we propose algorithms achieving tight regret bounds.\nWe further strengthen our performance guarantees by providing a tight $1/2$-approximation result showing that the oracle that knows the traders' valuation distributions achieves at least $1/2$ of the gain from trade of the omniscient oracle that knows in advance the actual realized traders' valuations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fran\u00e7ois Bachoc;Tommaso Cesari;Roberto Colomboni",
        "authorids": "~Fran\u00e7ois_Bachoc1;~Tommaso_Cesari1;~Roberto_Colomboni1",
        "gender": ";;M",
        "homepage": ";;https://sites.google.com/view/robertocolomboni/",
        "dblp": ";;270/0380",
        "google_scholar": ";;XGtfiRcAAAAJ",
        "orcid": ";;0000-0001-9890-9543",
        "linkedin": ";;roberto-colomboni-a922441a6/",
        "or_profile": "~Fran\u00e7ois_Bachoc1;~Tommaso_Cesari1;~Roberto_Colomboni1",
        "aff": ";;Polytechnic University of Milan (POLIMI)",
        "aff_domain": ";;polimi.it",
        "position": ";;Postdoc",
        "bibtex": "@inproceedings{\nbachoc2025a,\ntitle={A Tight Regret Analysis of Non-Parametric Repeated Contextual Brokerage},\nauthor={Fran{\\c{c}}ois Bachoc and Tommaso Cesari and Roberto Colomboni},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=U2RgUAySB6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=U2RgUAySB6",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "U52rGUjYMK",
        "title": "StableMDS: A Novel Gradient Descent-Based Method for Stabilizing and Accelerating Weighted Multidimensional Scaling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multidimensional Scaling (MDS) is an essential technique in multivariate analysis, with Weighted MDS (WMDS) commonly employed for tasks such as dimensionality reduction and graph drawing. However, the optimization of WMDS poses significant challenges due to the highly non-convex nature of its objective function. Stress Majorization, a method classified under the Majorization-Minimization algorithm, is among the most widely used solvers for this problem because it guarantees non-increasing loss values during optimization, even with a non-convex objective function. Despite this advantage, Stress Majorization suffers from high computational complexity, specifically $\\mathcal{O}(\\max(n^3, n^2 p))$ per iteration, where $n$ denotes the number of data points, and $p$ represents the projection dimension, rendering it impractical for large-scale data analysis. To mitigate the computational challenge, we introduce StableMDS, a novel gradient descent-based method that reduces the computational complexity to $\\mathcal{O}(n^2 p)$ per iteration. StableMDS achieves this computational efficiency by applying gradient descent independently to each point, thereby eliminating the need for costly matrix operations inherent in Stress Majorization. Furthermore, we theoretically ensure non-increasing loss values and optimization stability akin to Stress Majorization. These advancements not only enhance computational efficiency but also maintain stability, thereby broadening the applicability of WMDS to larger datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhongxi Fang;Xun Su;Tomohisa Tabuchi;Jianming Huang;Hiroyuki Kasai",
        "authorids": "~Zhongxi_Fang1;~Xun_Su1;~Tomohisa_Tabuchi1;~Jianming_Huang2;~Hiroyuki_Kasai3",
        "gender": "M;M;M;M;",
        "homepage": ";;;;http://kasai.comm.waseda.ac.jp/kasai/",
        "dblp": ";;;20/10004-2.html;",
        "google_scholar": "https://scholar.google.com/citations?hl=ja;;;FU198mUAAAAJ;https://scholar.google.co.jp/citations?user=Tub_DpEAAAAJ",
        "orcid": ";0000-0002-8500-5008;;0000-0003-4678-4480;0000-0003-1161-6823",
        "linkedin": ";xun-su-b6a77a250/;tomohisa-tabuchi-7857a5218;;",
        "or_profile": "~Zhongxi_Fang1;~Xun_Su1;~Tomohisa_Tabuchi1;~Jianming_Huang2;~Hiroyuki_Kasai3",
        "aff": "Waseda University;;Waseda University;Waseda University;Waseda University",
        "aff_domain": "waseda.jp;;waseda.ac.jp;waseda.jp;waseda.jp",
        "position": "PhD student;;MS student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nfang2025stablemds,\ntitle={Stable{MDS}: A Novel Gradient Descent-Based Method for Stabilizing and Accelerating Weighted Multidimensional Scaling},\nauthor={Zhongxi Fang and Xun Su and Tomohisa Tabuchi and Jianming Huang and Hiroyuki Kasai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=U52rGUjYMK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=U52rGUjYMK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "U57CpgcIeL",
        "title": "Distributional Off-policy Evaluation with Bellman Residual Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study distributional off-policy evaluation (OPE), of which the goal is to learn the distribution of the return for a target policy using offline data generated by a different policy. The theoretical foundation of many existing work relies on the supremum-extended statistical distances such as supremum-Wasserstein distance, which are hard to estimate. In contrast, we study the more manageable expectation-extended statistical distances and provide a novel theoretical justification on their validity for learning the return distribution. Based on this attractive property, we propose a new method called Energy Bellman Residual Minimizer (EBRM) for distributional OPE. We provide corresponding in-depth theoretical analyses. We establish a finite-sample error bound for the EBRM estimator under the realizability assumption. Furthermore, we introduce a variant of our method based on a multi-step extension which improves the error bound for non-realizable settings. Notably, unlike prior distributional OPE methods, the theoretical guarantees of our method do not require the completeness assumption.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sungee Hong;Zhengling Qi;Raymond K. W. Wong",
        "authorids": "~Sungee_Hong1;~Zhengling_Qi1;~Raymond_K._W._Wong1",
        "gender": "M;;",
        "homepage": ";https://sites.google.com/view/statsqizl/home?authuser=0;",
        "dblp": ";173/0201;",
        "google_scholar": "esII_m0AAAAJ;;",
        "orcid": ";;",
        "linkedin": "sungee-hong-24b850224/;;",
        "or_profile": "~Sungee_Hong1;~Zhengling_Qi1;~Raymond_K._W._Wong1",
        "aff": "Texas A&M University - College Station;George Washington University;",
        "aff_domain": "tamu.edu;gwu.edu;",
        "position": "PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nhong2025distributional,\ntitle={Distributional Off-policy Evaluation with Bellman Residual Minimization},\nauthor={Sungee Hong and Zhengling Qi and Raymond K. W. Wong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=U57CpgcIeL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=U57CpgcIeL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UEU6hsCWCC",
        "title": "Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present the first theoretical guarantees for zero constraint violation in Online Convex Optimization (OCO) across all rounds, addressing dynamic constraint changes. Unlike existing approaches in constrained OCO, which allow for occasional safety breaches, we provide the first approach for maintaining strict safety under the assumption of gradually evolving constraints, namely the constraints change at most by a small amount between consecutive rounds. This is achieved through a primal-dual approach and Online Gradient Ascent in the dual space. We show that employing a dichotomous learning rate enables ensuring both safety, via zero constraint violation, and sublinear regret. Our framework marks a  departure from previous work by providing the first provable guarantees for maintaining absolute safety in the face of changing constraints in OCO.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bassel Hamoud;Ilnura Usmanova;Kfir Yehuda Levy",
        "authorids": "~Bassel_Hamoud1;~Ilnura_Usmanova1;~Kfir_Yehuda_Levy1",
        "gender": "M;F;M",
        "homepage": ";https://sites.google.com/view/ilnurausmanova/main;http://kfiryehud.wixsite.com/kfir-y-levy",
        "dblp": ";;83/11388",
        "google_scholar": ";;",
        "orcid": "0000-0002-7295-049X;;",
        "linkedin": ";;",
        "or_profile": "~Bassel_Hamoud1;~Ilnura_Usmanova1;~Kfir_Yehuda_Levy1",
        "aff": "Technion - Israel Institute of Technology;Paul Scherrer Institute;Technion - Israel Institute of Technology, Technion",
        "aff_domain": "campus.technion.ac.il;psi.ch;technion.ac.il",
        "position": "PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nhamoud2025safety,\ntitle={Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints},\nauthor={Bassel Hamoud and Ilnura Usmanova and Kfir Yehuda Levy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UEU6hsCWCC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UEU6hsCWCC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ULLi0X0i9s",
        "title": "Transfer Neyman-Pearson Algorithm for Outlier Detection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of transfer learning in outlier detection where target abnormal data is rare. While transfer learning has been considered extensively in traditional classification, the problem of transfer in outlier detection and more generally in imbalanced classification settings has received less attention. We propose a general algorithmic approach which is shown theoretically to yield strong guarantees w.r.t. to a range of changes in abnormal distribution, and at the same time amenable to practical implementation. We then investigate different instantiations of this general algorithmic approach, e.g., based on multi-layer neural networks, and show empirically that they significantly outperform natural extensions of transfer methods from traditional classification (which are the only solutions available at the moment)",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mohammadreza Mousavi Kalan;Eitan J. Neugut;Samory Kpotufe",
        "authorids": "~Mohammadreza_Mousavi_Kalan1;~Eitan_J._Neugut1;~Samory_Kpotufe3",
        "gender": "M;M;M",
        "homepage": ";https://www.columbia.edu/~en2256/;http://www.columbia.edu/~skk2175/",
        "dblp": "207/8487;;",
        "google_scholar": "UaxZ3xgAAAAJ;;9r7_pN8AAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Mohammadreza_Mousavi_Kalan1;~Eitan_J._Neugut1;~Samory_Kpotufe3",
        "aff": "Ecole Nationale de la Statistique et de l'Analyse de l'information;Columbia University;Columbia University",
        "aff_domain": "ensai.fr;columbia.edu;columbia.edu",
        "position": "Assistant Professor;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nkalan2025transfer,\ntitle={Transfer Neyman-Pearson Algorithm for Outlier Detection},\nauthor={Mohammadreza Mousavi Kalan and Eitan J. Neugut and Samory Kpotufe},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ULLi0X0i9s}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ULLi0X0i9s",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UNaXkn4tzt",
        "title": "On Local Posterior Structure in Deep Ensembles",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian Neural Networks (BNNs) often improve model calibration and predictive uncertainty quantification compared to point estimators such as maximum-a-posteriori (MAP). Similarly, deep ensembles (DEs) are also known to improve calibration, and therefore, it is natural to hypothesize that deep ensembles of BNNs (DE-BNNs) should provide even further improvements. In this work, we systematically investigate this across a number of datasets, neural network architectures, and BNN approximation methods and surprisingly find that when the ensembles grow large enough, DEs consistently outperform DE-BNNs on in-distribution data. To shine light on this observation, we conduct several sensitivity and ablation studies. Moreover, we show that even though DE-BNNs outperform DEs on out-of-distribution metrics, this comes at the cost of decreased in-distribution performance. As a final contribution, we open-source the large pool of trained models to facilitate further research on this topic.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mikkel Jordahn;Jonas Vestergaard Jensen;Mikkel N. Schmidt;Michael Riis Andersen",
        "authorids": "~Mikkel_Jordahn1;~Jonas_Vestergaard_Jensen1;~Mikkel_N._Schmidt1;~Michael_Riis_Andersen2",
        "gender": "M;;;",
        "homepage": ";;;",
        "dblp": ";;;142/4181",
        "google_scholar": ";;;",
        "orcid": ";;;0000-0002-7411-5842",
        "linkedin": "mikkel-jordahn-0930b8a4/?originalSubdomain=dk;;;",
        "or_profile": "~Mikkel_Jordahn1;~Jonas_Vestergaard_Jensen1;~Mikkel_N._Schmidt1;~Michael_Riis_Andersen2",
        "aff": "Technical University of Denmark;;;Technical University of Denmark",
        "aff_domain": "dtu.dk;;;dtu.dk",
        "position": "PhD student;;;Associate Professor",
        "bibtex": "@inproceedings{\njordahn2025on,\ntitle={On Local Posterior Structure in Deep Ensembles},\nauthor={Mikkel Jordahn and Jonas Vestergaard Jensen and Mikkel N. Schmidt and Michael Riis Andersen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UNaXkn4tzt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UNaXkn4tzt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UOLZv2azUe",
        "title": "$\\beta$-th order Acyclicity Derivatives for DAG Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider a non-convex optimization  formulation for learning the weighted adjacency matrix $W$ of a directed acyclic graph (DAG) that uses acyclicity constraints that are functions of $|W_{ij}|^\\beta$, for $\\beta \\in \\mathbb{N}$. State-of-the-art algorithms for this problem use gradient-based Karush-Kuhn-Tucker (KKT) optimality conditions which only yield useful search directions for $\\beta =1$. Therefore, constraints with $\\beta \\geq 2$ have been ignored in the literature, and their empirical performance remains unknown. We introduce $\\beta$-th Order Taylor Series Expansion Based Local Search ($\\beta$-LS) which yields actionable descent directions for any $\\beta \\in \\mathbb{N}$. Our empirical experiments show that $2$-LS obtains solutions of  higher quality than $1$-LS, $3$-LS and $4$-LS. $2$-LSopt, an optimized version of $2$-LS, obtains high quality solutions significantly faster than the state of the art which uses $\\beta=1$. Moreover, $2$-LSopt does not need any graph-size specific hyperparameter tuning. We prove that $\\beta$-LSopt is guaranteed to converge to a Coordinate-Wise Local Stationary Point (Cst) for any $\\beta \\in \\mathbb{N}$. If the objective function is convex, $\\beta$-LSopt converges to a local minimum.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Madhumitha Shridharan;Garud Iyengar",
        "authorids": "~Madhumitha_Shridharan1;~Garud_Iyengar1",
        "gender": ";M",
        "homepage": ";http://www.columbia.edu/~gi10/",
        "dblp": ";i/GarudIyengar.html",
        "google_scholar": "https://scholar.google.nl/citations?user=oRMWOq0AAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Madhumitha_Shridharan1;~Garud_Iyengar1",
        "aff": "Columbia University;Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nshridharan2025betath,\ntitle={\\${\\textbackslash}beta\\$-th order Acyclicity Derivatives for {DAG} Learning},\nauthor={Madhumitha Shridharan and Garud Iyengar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UOLZv2azUe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UOLZv2azUe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UWNfWtCXCZ",
        "title": "Almost linear time differentially private release of synthetic graphs",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "In this paper, we give an almost linear time and space algorithms to sample from an exponential mechanism with an $\\ell_1$-score function defined over an exponentially large non-convex set. As a direct result, on input an $n$ vertex $m$ edges graph $G$, we present the first $\\widetilde{O}(m)$ time and $O(m)$ space algorithms for differentially privately outputting an $n$ vertex $O(m)$ edges synthetic graph that approximates all the cuts and the spectrum of $G$. These are the first private algorithms for releasing synthetic graphs that nearly match this task's time and space complexity in the non-private setting while achieving the same (or better) utility as the previous works in the more practical sparse regime. Additionally, our algorithms can be extended to private graph analysis under continual observation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zongrui Zou;Jingcheng Liu;Jalaj Upadhyay",
        "authorids": "~Zongrui_Zou1;~Jingcheng_Liu1;~Jalaj_Upadhyay1",
        "gender": "M;M;",
        "homepage": ";https://liuexp.github.io;",
        "dblp": "289/6549;135/6379-1;",
        "google_scholar": "AxXa9wEAAAAJ;x05pAVUAAAAJ;",
        "orcid": "0000-0001-5224-9586;;",
        "linkedin": ";;",
        "or_profile": "~Zongrui_Zou1;~Jingcheng_Liu1;~Jalaj_Upadhyay1",
        "aff": "Nanjing university;Nanjing University;",
        "aff_domain": "nju.edu.cn;nju.edu.cn;",
        "position": "PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\nzou2025almost,\ntitle={Almost linear time differentially private release of synthetic graphs},\nauthor={Zongrui Zou and Jingcheng Liu and Jalaj Upadhyay},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UWNfWtCXCZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UWNfWtCXCZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UlrAxDAYqE",
        "title": "Common Learning Constraints Alter Interpretations of Direct Preference Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models in the past have typically relied on some form of reinforcement learning with human feedback (RLHF) to better align model responses with human preferences.  However, because of oft-observed instabilities when implementing these RLHF pipelines, various reparameterization techniques have recently been introduced to sidestep the need for separately learning an RL reward model.  Instead, directly fine-tuning for human preferences is achieved via the minimization of a single closed-form training objective, a process originally referred to as direct preference optimization (DPO).  Although effective in certain real-world settings, we detail how the foundational role of DPO reparameterizations (and equivalency to applying RLHF with an optimal reward) may be obfuscated once inevitable optimization constraints are introduced during model training.  This then motivates alternative derivations and analysis of DPO that remain intact even in the presence of such constraints.  As initial steps in this direction, we re-derive DPO from a simple Gaussian estimation perspective, with strong ties to compressive sensing and classical constrained optimization problems involving noise-adaptive, concave regularization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lemin Kong;Xiangkun Hu;Tong He;David Wipf",
        "authorids": "~Lemin_Kong1;~Xiangkun_Hu1;~Tong_He5;~David_Wipf1",
        "gender": ";M;M;M",
        "homepage": ";;https://hetong007.github.io/;http://www.davidwipf.com/",
        "dblp": "320/8260;224/5990;02/1554-2;81/6421",
        "google_scholar": ";_-0MpawAAAAJ;hV5D8GYAAAAJ;YJx1WSgAAAAJ",
        "orcid": ";;;",
        "linkedin": "lemin-kong/;;;",
        "or_profile": "~Lemin_Kong1;~Xiangkun_Hu1;~Tong_He5;~David_Wipf1",
        "aff": "Chinese University of Hong Kong, The Chinese University of Hong Kong;Amazon;Amazon;Amazon AI Research Lab",
        "aff_domain": "se.cuhk.edu.hk;amazon.com;amazon.com;amazon.com",
        "position": "PhD student;Applied Scientist;Researcher;Principal Research Scientist",
        "bibtex": "@inproceedings{\nkong2025common,\ntitle={Common Learning Constraints Alter Interpretations of Direct Preference Optimization},\nauthor={Lemin Kong and Xiangkun Hu and Tong He and David Wipf},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UlrAxDAYqE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UlrAxDAYqE",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UoFlLAUjDJ",
        "title": "A High Dimensional Statistical Model for Adversarial Training: Geometry and Trade-Offs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension  $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. \nWe introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. \nOur main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses for a Block Feature Model. \nOur result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. \nThis goes beyond previous models in the literature, which fail to capture a difference in performance between adversarially trained models in the high sample complexity regime.\nIn particular, we unveil the existence of directions which can be defended without penalising accuracy. \nFinally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kasimir Tanner;Matteo Vilucchio;Bruno Loureiro;Florent Krzakala",
        "authorids": "~Kasimir_Tanner1;~Matteo_Vilucchio1;~Bruno_Loureiro1;~Florent_Krzakala1",
        "gender": ";M;M;",
        "homepage": "https://kasimirtanner.ch;https://github.com/mvilucchio;https://brloureiro.github.io/;http://Krzakala.org",
        "dblp": ";321/1615;207/1834;25/1282",
        "google_scholar": ";lu9hh14AAAAJ;DXl3ir8AAAAJ;https://scholar.google.fr/citations?user=3jDeUlMAAAAJ",
        "orcid": ";0009-0002-3856-8008;0000-0002-6327-4688;0000-0003-2313-2578",
        "linkedin": "kasimir-tanner/;matteo-vilucchio/;bruno-loureiro-43183b14a/;",
        "or_profile": "~Kasimir_Tanner1;~Matteo_Vilucchio1;~Bruno_Loureiro1;~Florent_Krzakala1",
        "aff": ";EPFL - EPF Lausanne;Ecole Normale Sup\u00e9rieure, Ecole Normale Sup\u00e9rieure de Paris;Swiss Federal Institute of Technology Lausanne",
        "aff_domain": ";epfl.ch;di.ens.fr;epfl.ch",
        "position": ";PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\ntanner2025a,\ntitle={A High Dimensional Statistical Model for Adversarial Training: Geometry and Trade-Offs},\nauthor={Kasimir Tanner and Matteo Vilucchio and Bruno Loureiro and Florent Krzakala},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UoFlLAUjDJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UoFlLAUjDJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UyO59CWdvY",
        "title": "Fair Resource Allocation in Weakly Coupled Markov Decision Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider fair resource allocation in sequential decision-making environments modeled as weakly coupled Markov decision processes, where resource constraints couple the action spaces of $N$ sub-Markov decision processes (sub-MDPs) that would otherwise operate independently. We adopt a fairness definition using the generalized Gini function instead of the traditional utilitarian (total-sum) objective. After introducing a general but computationally prohibitive solution scheme based on linear programming, we focus on the homogeneous case where all sub-MDPs are identical. For this case,  we show for the first time that the problem reduces to optimizing the utilitarian objective over the class of \"permutation invariant\" policies. This result is particularly useful as we can exploit efficient algorithms that optimizes the utilitarian objective such as Whittle index policies in restless bandits to solve the problem with this fairness objective. For more general settings, we introduce a count-proportion-based deep reinforcement learning approach. Finally, we validate our theoretical findings with comprehensive experiments, confirming the effectiveness of our proposed method in achieving fairness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaohui Tu;Yossiri Adulyasak;Nima Akbarzadeh;Erick Delage",
        "authorids": "~Xiaohui_Tu1;~Yossiri_Adulyasak1;~Nima_Akbarzadeh1;~Erick_Delage2",
        "gender": ";;;M",
        "homepage": "https://mila.quebec/en/directory/xiaohui-tu;http://yossiri.info/;;http://web.hec.ca/pages/erick.delage/",
        "dblp": ";139/1308;180/5631;26/1546",
        "google_scholar": ";https://scholar.google.ca/citations?user=Ru9Zco8AAAAJ;https://scholar.google.com.tr/citations?user=ZVewNNQAAAAJ;https://scholar.google.ca/citations?user=ciH2ROgAAAAJ",
        "orcid": ";;;0000-0002-6740-3600",
        "linkedin": ";;;erick-delage-2105361/?originalSubdomain=ca",
        "or_profile": "~Xiaohui_Tu1;~Yossiri_Adulyasak1;~Nima_Akbarzadeh1;~Erick_Delage2",
        "aff": "\u00c9cole des Hautes \u00c9tudes Commerciales;\u00c9cole des Hautes \u00c9tudes Commerciales+Department of Computer Science and Operations Research, Universit\u00e9 de Montr\u00e9al;;HEC Montreal+Computer Science Department",
        "aff_domain": "hec.ca;hec.ca+diro.umontreal.ca;;hec.ca+cs.stanford.edu",
        "position": "PhD student;Associate Professor+Adjunct Professor;;Full Professor+Researcher",
        "bibtex": "@inproceedings{\ntu2025fair,\ntitle={Fair Resource Allocation in Weakly Coupled Markov Decision Processes},\nauthor={Xiaohui Tu and Yossiri Adulyasak and Nima Akbarzadeh and Erick Delage},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=UyO59CWdvY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UyO59CWdvY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VCJ8NfVrcO",
        "title": "Fast Convergence of Softmax Policy Mirror Ascent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani et al. (2021) introduced a policy gradient method that corresponds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Unlike that for NPG, generalizing SPMA to the linear function approximation (FA) setting does not require compatible function approximation. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems. We prove that SPMA achieves linear convergence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical performance on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA consistently achieves similar or better performance compared to MDPO, PPO and TRPO.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Reza Asad;Reza Babanezhad Harikandeh;Issam H. Laradji;Nicolas Le Roux;Sharan Vaswani",
        "authorids": "~Reza_Asad1;~Reza_Babanezhad_Harikandeh1;~Issam_H._Laradji1;~Nicolas_Le_Roux2;~Sharan_Vaswani1",
        "gender": "M;M;M;;M",
        "homepage": "https://reza-asad.github.io/;http://babanezhad.ca;https://issamlaradji.github.io/;;http://vaswanis.github.io",
        "dblp": "254/8126;37/8904.html;142/0043;;136/5916",
        "google_scholar": "SJRiDSAAAAAJ;KLrwPsgAAAAJ;https://scholar.google.ca/citations?user=8vRS7F0AAAAJ;;https://scholar.google.ca/citations?user=bDb2zWwAAAAJ",
        "orcid": ";;;;",
        "linkedin": "rezaasad/;;issam-laradji-67ba1a99/;;sharan-vaswani-05b8ab35/",
        "or_profile": "~Reza_Asad1;~Reza_Babanezhad_Harikandeh1;~Issam_H._Laradji1;~Nicolas_Le_Roux2;~Sharan_Vaswani1",
        "aff": "Simon Fraser University;Samsung;ServiceNow;;Simon Fraser University",
        "aff_domain": "sfu.ca;samsung.com;servicenow.com;;sfu.ca",
        "position": "PhD student;Research Scientist;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\nasad2025fast,\ntitle={Fast Convergence of Softmax Policy Mirror Ascent},\nauthor={Reza Asad and Reza Babanezhad Harikandeh and Issam H. Laradji and Nicolas Le Roux and Sharan Vaswani},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VCJ8NfVrcO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VCJ8NfVrcO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VMaEtLG4BJ",
        "title": "Deep Generative Quantile Bayes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We develop a multivariate posterior sampling procedure through deep generative quantile learning.\nSimulation proceeds implicitly through a push-forward mapping that can transform i.i.d. random vectors samples from the posterior.\nWe utilize Monge-Kantorovich depth in multivariate quantiles to directly sample from Bayesian credible sets, a unique feature not offered by typical posterior sampling methods.\nTo enhance training of the quantile mapping, we design a neural network that automatically performs summary statistic extraction.\nThis additional neural network structure has performance benefits including  support shrinkage (i.e. contraction of our posterior approximation) as the observation sample size increases. We demonstrate the usefulness of our approach on several examples where the absence of likelihood renders classical MCMC infeasible. Finally, we provide the following frequentist theoretical justifications for our quantile learning framework: consistency of the estimated vector quantile, of the recovered posterior distribution, and of the corresponding Bayesian credible sets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jungeum Kim;Percy S. Zhai;Veronika Rockova",
        "authorids": "~Jungeum_Kim1;~Percy_S._Zhai1;veronika.rockova@chicagobooth.edu",
        "gender": "F;;",
        "homepage": "https://jungeumkim.com;;",
        "dblp": ";;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Jungeum_Kim1;~Percy_S._Zhai1;veronika.rockova@chicagobooth.edu",
        "aff": "North Carolina State University+University of Chicago;;",
        "aff_domain": "ncsu.edu+uchicago.edu;;",
        "position": "Assistant Professor+Postdoc;;",
        "bibtex": "@inproceedings{\nkim2025deep,\ntitle={Deep Generative Quantile Bayes},\nauthor={Jungeum Kim and Percy S. Zhai and Veronika Rockova},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VMaEtLG4BJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VMaEtLG4BJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VMtZ3UlJ6n",
        "title": "Consistent Amortized Clustering via Generative Flow Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov chain runs and the need for explicit data likelihoods. Existing methods which label each data point sequentially, like the Neural Clustering Process, often lead to cluster assignments highly dependent on the data order. Alternatively, methods that sequentially create full clusters, do not provide assignment probabilities. In this paper, we introduce GFNCP, a novel framework for  amortized clustering. GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward. We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Irit Chelly;Roy Uziel;Oren Freifeld;Ari Pakman",
        "authorids": "~Irit_Chelly1;~Roy_Uziel1;~Oren_Freifeld1;~Ari_Pakman1",
        "gender": "F;;M;M",
        "homepage": "https://irita42.wixsite.com/mysite;https://uzielroy.wixsite.com/uzielroy;https://www.cs.bgu.ac.il/~orenfr/;https://aripakman.github.io/",
        "dblp": "272/0672;259/5174;96/5159;139/1387",
        "google_scholar": "https://scholar.google.co.il/citations?hl=en;WScNlk4AAAAJ;fxzlm6IAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0002-7713-7788;;;0000-0001-8047-2240",
        "linkedin": "iritchelly/;;;",
        "or_profile": "~Irit_Chelly1;~Roy_Uziel1;~Oren_Freifeld1;~Ari_Pakman1",
        "aff": "Ben Gurion University of the Negev;Ben Gurion University of the Negev;Ben-Gurion University;Ben-Gurion University of the Negev",
        "aff_domain": "bgu.ac.il;bgu.ac.il;bgu.ac.il;bgu.ac.il",
        "position": "PhD student;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nchelly2025consistent,\ntitle={Consistent Amortized Clustering via Generative Flow Networks},\nauthor={Irit Chelly and Roy Uziel and Oren Freifeld and Ari Pakman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VMtZ3UlJ6n}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VMtZ3UlJ6n",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VTkvKnFh9L",
        "title": "Q-learning for Quantile MDPs: A Decomposition, Performance, and Convergence Analysis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In Markov decision processes (MDPs), quantile risk measures such as Value-at-Risk are a standard metric for modeling RL agents' preferences for certain outcomes. This paper proposes a new Q-learning algorithm for quantile optimization in MDPs with strong convergence and performance guarantees. The algorithm leverages a new, simple dynamic program (DP) decomposition for quantile MDPs. Compared with prior work, our DP decomposition requires neither known transition probabilities nor solving complex saddle point equations and serves as a suitable foundation for other model-free RL algorithms. Our numerical results in tabular domains show that our Q-learning algorithm converges to its DP variant and outperforms earlier algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jia Lin Hau;Erick Delage;Esther Derman;Mohammad Ghavamzadeh;Marek Petrik",
        "authorids": "~Jia_Lin_Hau1;~Erick_Delage2;~Esther_Derman1;~Mohammad_Ghavamzadeh2;~Marek_Petrik2",
        "gender": "M;M;;;",
        "homepage": ";http://web.hec.ca/pages/erick.delage/;;;",
        "dblp": "329/5798;26/1546;;;",
        "google_scholar": "ygX6pZ0AAAAJ;https://scholar.google.ca/citations?user=ciH2ROgAAAAJ;;;",
        "orcid": ";0000-0002-6740-3600;;;",
        "linkedin": "jia-lin-hau-b61730129/;erick-delage-2105361/?originalSubdomain=ca;;;",
        "or_profile": "~Jia_Lin_Hau1;~Erick_Delage2;~Esther_Derman1;~Mohammad_Ghavamzadeh2;~Marek_Petrik2",
        "aff": "University of New Hampshire;HEC Montreal+Computer Science Department;;;",
        "aff_domain": "unh.edu;hec.ca+cs.stanford.edu;;;",
        "position": "PhD student;Full Professor+Researcher;;;",
        "bibtex": "@inproceedings{\nhau2025qlearning,\ntitle={Q-learning for Quantile {MDP}s: A Decomposition, Performance, and Convergence Analysis},\nauthor={Jia Lin Hau and Erick Delage and Esther Derman and Mohammad Ghavamzadeh and Marek Petrik},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VTkvKnFh9L}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VTkvKnFh9L",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VbEUDj6GAp",
        "title": "Powerful batch conformal prediction for classification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In a split conformal framework with $K$ classes, a calibration sample of $n$ labeled examples is observed for inference on the label of a new unlabeled example. We explore the setting where a 'batch' of $m$ independent such unlabeled examples is given, and the goal is to construct a batch prediction set with 1-$\\alpha$ coverage. Unlike individual prediction sets, the batch prediction set is a collection of label vectors of size $m$, while the calibration sample consists of univariate labels. \nA natural approach is to apply the Bonferroni correction, which concatenates individual prediction sets at level $1-\\alpha/m$.  We propose a uniformly more powerful solution, based on specific combinations of conformal $p$-values that exploit the Simes inequality. We provide a general recipe for valid inference with any combinations of conformal $p$-values, and compare the performance of several useful choices.  Intuitively, the pooled evidence of relatively\n `easy' examples within the batch can help provide narrower batch prediction sets. Additionally, we introduce a more computationally intensive method that aggregates batch scores and can be even more powerful.    The theoretical guarantees are established when all examples are independent and identically distributed (iid), as well as more generally when iid is assumed only conditionally within each class. Notably, our results remain valid under label distribution shift, since the distribution of the labels need not be the same in the calibration sample and in the new batch. The effectiveness of the methods is highlighted through illustrative synthetic and real data examples.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ulysse Gazin;Ruth Heller;Etienne Roquain;Aldo Solari",
        "authorids": "~Ulysse_Gazin1;ruheller@post.tau.ac.il;etienne.roquain@upmc.fr;aldo.solari@unive.it",
        "gender": "Not Specified;;;",
        "homepage": "https://perso.eleves.ens-rennes.fr/people/ulysse.gazin/;;;",
        "dblp": ";;;",
        "google_scholar": ";;;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Ulysse_Gazin1;ruheller@post.tau.ac.il;etienne.roquain@upmc.fr;aldo.solari@unive.it",
        "aff": "Universit\u00e9 Paris Cit\u00e9;;;",
        "aff_domain": "u-paris.fr;;;",
        "position": "PhD student;;;",
        "bibtex": "@inproceedings{\ngazin2025powerful,\ntitle={Powerful batch conformal prediction for classification},\nauthor={Ulysse Gazin and Ruth Heller and Etienne Roquain and Aldo Solari},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VbEUDj6GAp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VbEUDj6GAp",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VcwZ3gtYFY",
        "title": "Multi-marginal Schr\u00f6dinger Bridges with Iterative Reference Refinement",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Practitioners often aim to infer an unobserved population trajectory using sample snapshots at multiple time points. E.g. given single-cell sequencing data, scientists would like to learn how gene expression changes over a cell\u2019s life cycle. But sequencing any cell destroys that cell. So we can access data for any particular cell only at a single time point, but we have data across many cells. The deep learning community has recently explored using Schr\u00f6dinger bridges (SBs) and their extensions in similar settings. However, existing methods either (1) interpolate between just two time points or (2) require a single fixed reference dynamic (often set to Brownian motion within SB). But learning piecewise from adjacent time points can fail to capture long-term dependencies. And practitioners are typically able to specify a model class for the reference dynamic but not the exact values of the parameters within it. So we propose a new method that (1) learns the unobserved trajectories from sample snapshots across multiple time points and (2) requires specification only of a class of reference dynamics, not a single fixed one. We demonstrate the advantages of our method on simulated and real data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yunyi Shen;Renato Berlinghieri;Tamara Broderick",
        "authorids": "~Yunyi_Shen1;~Renato_Berlinghieri1;~Tamara_Broderick2",
        "gender": ";M;",
        "homepage": "https://yunyishen.github.io;http://renatoberlinghieri.github.io;http://tamarabroderick.com/",
        "dblp": "368/5101;;40/7412",
        "google_scholar": ";;dPX0wQcAAAAJ",
        "orcid": ";;",
        "linkedin": ";;tamara-broderick-b20243139/",
        "or_profile": "~Yunyi_Shen1;~Renato_Berlinghieri1;~Tamara_Broderick2",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "position": "PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nshen2025multimarginal,\ntitle={Multi-marginal Schr\\\"odinger Bridges with Iterative Reference Refinement},\nauthor={Yunyi Shen and Renato Berlinghieri and Tamara Broderick},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VcwZ3gtYFY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VcwZ3gtYFY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VmqRTQLhoD",
        "title": "Near-Optimal Sample Complexity for Iterated CVaR Reinforcement Learning with a Generative Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we study the sample complexity problem of risk-sensitive Reinforcement Learning (RL) with a generative model, where we aim to maximize the Conditional Value at Risk (CVaR) with risk tolerance level $\\tau$ at each step, named Iterated CVaR. \n  We develop nearly matching upper and lower bounds on the sample complexity for this problem. Specifically, we first prove that a value iteration-based algorithm, ICVaR-VI, achieves an $\\epsilon$-optimal policy with at most $\\tilde{\\mathcal{O}}\\left(\\frac{SA}{(1-\\gamma)^4\\tau^2\\epsilon^2}\\right)$ samples, where $\\gamma$ is the discount factor, and $S, A$ are the sizes of the state and action spaces. Furthermore, \n  if $\\tau \\geq \\gamma$, then the sample complexity can be further improved to $\\tilde{\\mathcal{O}}\\left( \\frac{SA}{(1-\\gamma)^3\\epsilon^2} \\right)$. \n  We further show a minimax lower bound of ${\\tilde{\\mathcal{O}}}\\left(\\frac{(1-\\gamma \\tau)SA}{(1-\\gamma)^4\\tau\\epsilon^2}\\right)$. \n  For a constant risk level $0<\\tau\\leq 1$, our upper and lower bounds match with each other, demonstrating the tightness and optimality of our analyses.\n  We also investigate a limiting case with a small risk level $\\tau$, called Worst-Path RL, where the objective is to maximize the minimum possible cumulative reward. We develop matching upper and lower bounds of $\\tilde{\\mathcal{O}}\\left(\\frac{SA}{p_{\\min}}\\right)$, where\n  $p_{\\min}$ denotes the minimum non-zero reaching probability of the transition kernel.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zilong Deng;Simon Khan;Shaofeng Zou",
        "authorids": "~Zilong_Deng2;~Simon_Khan1;~Shaofeng_Zou1",
        "gender": "M;M;",
        "homepage": ";;",
        "dblp": ";;",
        "google_scholar": ";mnNHFn8AAAAJ;",
        "orcid": ";;",
        "linkedin": "zilong-deng-4a0536220;;",
        "or_profile": "~Zilong_Deng2;~Simon_Khan1;~Shaofeng_Zou1",
        "aff": "Arizona State University;Air Force Research Laboratory ;",
        "aff_domain": "asu.edu;us.af;",
        "position": "PhD student;Principal Researcher;",
        "bibtex": "@inproceedings{\ndeng2025nearoptimal,\ntitle={Near-Optimal Sample Complexity for Iterated {CV}aR Reinforcement Learning with a Generative Model},\nauthor={Zilong Deng and Simon Khan and Shaofeng Zou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=VmqRTQLhoD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VmqRTQLhoD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Vu4qgEI7kC",
        "title": "Sparse Activations as Conformal Predictors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Conformal prediction is a distribution-free framework for uncertainty quantification that replaces point predictions with sets, offering marginal coverage guarantees (i.e., ensuring that the sets contain the true label with a specified probability, in expectation). \nIn this paper, we uncover a novel connection between conformal prediction and sparse \"softmax-like\" transformations, such as sparsemax and $\\gamma$-entmax (with $\\gamma> 1$), which assign nonzero probability only to some labels. \nWe introduce new non-conformity scores for classification that make the calibration process correspond to the widely used temperature scaling method. At test time, applying these sparse transformations with the calibrated temperature leads to a support set (i.e., the set of labels with nonzero probability) that automatically inherits the coverage guarantees of conformal prediction. \nThrough experiments on computer vision and text classification benchmarks, we demonstrate that the proposed method achieves competitive results in terms of coverage, efficiency, and adaptiveness compared to standard non-conformity scores based on softmax.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Margarida M Campos;Jo\u00e3o C\u00e1lem;Sophia Sklaviadis;Mario A. T. Figueiredo;Andre Martins",
        "authorids": "~Margarida_M_Campos1;joaocalem@inesc-id.pt;~Sophia_Sklaviadis1;~Mario_A._T._Figueiredo1;~Andre_Martins1",
        "gender": "F;;F;M;M",
        "homepage": ";;https://peithous.github.io/folder/Sophia_Sklaviadis_cv.pdf;http://www.lx.it.pt/~mtf/;https://andre-martins.github.io/",
        "dblp": ";;;f/MarioATFigueiredo;m/AndreFTMartins",
        "google_scholar": "vORjPgMAAAAJ;;;S-pd0NwAAAAJ;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ",
        "orcid": ";;;0000-0002-0970-7745;",
        "linkedin": "margaridamcampos/;;;mario-figueiredo-138b099/;",
        "or_profile": "~Margarida_M_Campos1;joaocalem@inesc-id.pt;~Sophia_Sklaviadis1;~Mario_A._T._Figueiredo1;~Andre_Martins1",
        "aff": "Instituto Superior T\u00e9cnico+Instituto de Telecomunica\u00e7\u00f5es, Portugal+Instituto Superior T\u00e9cnico;;Instituto Superior T\u00e9cnico;Instituto Superior T\u00e9cnico+Instituto Superior T\u00e9cnico, University of Lisbon, Portugal+Instituto Superior T\u00e9cnico, University of Lisbon, Portugal+Instituto de Telecomunica\u00e7\u00f5es, Portugal;Instituto Superior T\u00e9cnico+Unbabel",
        "aff_domain": "tecnico.ulisboa.pt+it.pt+tecnico.ulisboa.pt;;tecnico.ulisboa.pt;tecnico.ulisboa.pt+tecnico.ulisboa.pt+tecnico.ulisboa.pt+it.pt;tecnico.ulisboa.pt+unbabel.com",
        "position": "PhD student+Researcher+Assistant Professor;;PhD student;IST Distinguished Professor+Feedzai Professor of Machine Learning+Full Professor+Senior Researcher;Associate Professor+Research Scientist",
        "bibtex": "@inproceedings{\ncampos2025sparse,\ntitle={Sparse Activations as Conformal Predictors},\nauthor={Margarida M Campos and Jo{\\~a}o C{\\'a}lem and Sophia Sklaviadis and Andre Martins and Mario A. T. Figueiredo},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Vu4qgEI7kC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Vu4qgEI7kC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WH9VZ3TEu9",
        "title": "Variational Inference on the Boolean Hypercube with the Quantum Entropy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we derive variational inference upper-bounds on the log-partition function of a pairwize Markov random fields on the Boolean hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We then propose an efficient algorithm to compute these bounds based on primal-dual optimization. An improvement of these bounds through the use of \"hierarchies\", similar to sum-of-squares (SoS) hierarchies is proposed, and we present a greedy algorithm to select among these relaxations. We carry extensive numerical experiments and compare with state-of-the-art methods for this inference problem.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Eliot Beyler;Francis Bach",
        "authorids": "~Eliot_Beyler1;~Francis_Bach1",
        "gender": ";M",
        "homepage": ";http://www.di.ens.fr/~fbach",
        "dblp": ";b/FrancisRBach",
        "google_scholar": ";https://scholar.google.fr/citations?user=6PJWcFEAAAAJ",
        "orcid": ";",
        "linkedin": "https://fr.linkedin.com/in/eliot-beyler-576a15209;",
        "or_profile": "~Eliot_Beyler1;~Francis_Bach1",
        "aff": "INRIA+Ecole Normale Sup\u00e9rieure de Paris;INRIA+Ecole Normale Superieure",
        "aff_domain": "inria.fr+ens.fr;inria.fr+ens.fr",
        "position": "PhD student+PhD student;Faculty+Faculty",
        "bibtex": "@inproceedings{\nbeyler2025variational,\ntitle={Variational Inference on the Boolean Hypercube with the Quantum Entropy},\nauthor={Eliot Beyler and Francis Bach},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WH9VZ3TEu9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WH9VZ3TEu9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WLlAspD7LT",
        "title": "Optimising Clinical Federated Learning through Mode Connectivity-based Model Aggregation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Federated Learning (FL) involves a server aggregating local models from clients to compute a global model. However, this process can struggle to position the global model in low-loss regions of the parameter space for all clients, resulting in subpar convergence and inequitable performance across clients. This issue is particularly pronounced in non-IID settings, common in clinical contexts, where variations in data distribution, class imbalance, and training sample sizes result in client heterogeneity. To address this issue, we propose a mode connectivity-based FL framework that ensures the global model resides within the overlapping low-loss regions of all clients in the parameter space. This framework models the low-loss regions as non-linear mode connections between the current global and local models, and optimises to identify an intersection among these mode connections to define the new global model. This approach enhances training stability and convergence, yielding better and more equitable performance compared to standard FL frameworks like federated averaging. Empirical evaluations across multiple healthcare datasets demonstrate the benefits of the proposed framework.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anshul Thakur;Soheila Molaei;Patrick Schwab;Danielle Belgrave;Kim Branson;David A. Clifton",
        "authorids": "~Anshul_Thakur1;~Soheila_Molaei1;~Patrick_Schwab1;danielle.x.belgrave@gsk.com;kim.m.branson@gsk.com;~David_A._Clifton1",
        "gender": "M;F;;;;M",
        "homepage": ";https://www.researchgate.net/profile/Soheila-Molaei-2;http://schwabpatrick.com;;;http://www.eng.ox.ac.uk/chi",
        "dblp": ";236/6149;152/9378;;;89/6424",
        "google_scholar": "https://scholar.google.com/citations?hl=en;iAq1AngAAAAJ;https://scholar.google.at/citations?hl=de;;;",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Anshul_Thakur1;~Soheila_Molaei1;~Patrick_Schwab1;danielle.x.belgrave@gsk.com;kim.m.branson@gsk.com;~David_A._Clifton1",
        "aff": "University of Oxford;University of Oxford;GlaxoSmithKline plc;;;University of Oxford",
        "aff_domain": "eng.ox.ac.uk;ox.ac.uk;gsk.com;;;ox.ac.uk",
        "position": "Lecturer;Postdoc;Director;;;Full Professor",
        "bibtex": "@inproceedings{\nthakur2025optimising,\ntitle={Optimising Clinical Federated Learning through Mode Connectivity-based Model Aggregation},\nauthor={Anshul Thakur and Soheila Molaei and Patrick Schwab and Danielle Belgrave and Kim Branson and David A. Clifton},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WLlAspD7LT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WLlAspD7LT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WVX8tPoPYO",
        "title": "posteriordb: Testing, Benchmarking and Developing Bayesian Inference Algorithms",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "The general applicability and robustness of posterior inference algorithms is critical to widely used probabilistic programming languages such as Stan, PyMC, Pyro, and Turing.jl. When designing a new inference algorithm, whether it involves Monte Carlo sampling or variational approximation, the fundamental problem is evaluating its accuracy and efficiency across a range of representative target posteriors. To solve this problem, we propose posteriordb, a database of models and data sets defining target densities along with reference Monte Carlo draws. We further provide a guide to the best practices in using posteriordb for algorithm evaluation and comparison. To provide a wide range of realistic posteriors, posteriordb currently comprises 120 representative models with data, and has been instrumental in developing several inference algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "M\u00e5ns Magnusson;Jakob Torgander;Paul-Christian B\u00fcrkner;Lu Zhang;Bob Carpenter;Aki Vehtari",
        "authorids": "~M\u00e5ns_Magnusson2;~Jakob_Torgander1;~Paul-Christian_B\u00fcrkner1;~Lu_Zhang27;~Bob_Carpenter3;~Aki_Vehtari1",
        "gender": "M;M;;F;;M",
        "homepage": "http://www.mansmagnusson.com;https://www.katalog.uu.se/empinfo/?id=N23-1250;;https://luzhangstat.github.io;https://bob-carpenter.github.io/;https://users.aalto.fi/~ave/",
        "dblp": "119/9862;;;;;57/5480",
        "google_scholar": "https://scholar.google.se/citations?user=6AA-AAcAAAAJ;;;;kPtKWAwAAAAJ;tYgN0GsAAAAJ",
        "orcid": ";;;;;0000-0003-2164-9469",
        "linkedin": ";;;;;",
        "or_profile": "~M\u00e5ns_Magnusson2;~Jakob_Torgander1;~Paul-Christian_B\u00fcrkner1;~Lu_Zhang27;~Bob_Carpenter3;~Aki_Vehtari1",
        "aff": "Uppsala University;Uppsala University;;University of Southern California;;Aalto University",
        "aff_domain": "statistik.uu.se;statistik.uu.se;;usc.edu;;aalto.fi",
        "position": "Assistant Professor;PhD student;;Assistant Professor;;Full Professor",
        "bibtex": "@inproceedings{\nmagnusson2025textttposteriordb,\ntitle={{\\textbackslash}texttt\\{posteriordb\\}: Testing, Benchmarking and Developing Bayesian Inference Algorithms},\nauthor={M{\\r{a}}ns Magnusson and Jakob Torgander and Paul-Christian B{\\\"u}rkner and Lu Zhang and Bob Carpenter and Aki Vehtari},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WVX8tPoPYO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WVX8tPoPYO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WWoxHUUeBg",
        "title": "Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The traditional two-stage approach to causal inference first identifies a *single* causal model (or equivalence class of models), which is then used to answer causal queries. However, this neglects any epistemic model uncertainty. In contrast, *Bayesian* causal inference does incorporate epistemic uncertainty into query estimates via Bayesian marginalisation (posterior averaging) over *all* causal models. While principled, this marginalisation over entire causal models, i.e., both causal structures (graphs) and mechanisms, poses a tremendous computational challenge. In this work, we address this challenge by decomposing structure marginalisation into the marginalisation over (i) causal orders and (ii) directed acyclic graphs (DAGs) given an order. We can marginalise the latter in closed form by limiting the number of parents per variable and utilising Gaussian Processes to model mechanisms. To marginalise over orders, we use a sampling-based approximation, for which we devise a novel auto-regressive distribution over causal orders (ARCO). Our method outperforms state-of-the-art in structure learning on simulated non-linear additive noise benchmarks, and yields competitive results on real-world data. Furthermore, we can accurately infer interventional distributions and average causal effects.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Christian Toth;Christian Knoll;Franz Pernkopf;Robert Peharz",
        "authorids": "~Christian_Toth1;~Christian_Knoll1;~Franz_Pernkopf1;~Robert_Peharz5",
        "gender": ";M;M;M",
        "homepage": ";;https://www.spsc.tugraz.at/people/franz-pernkopf.html;https://robert-peharz.github.io/",
        "dblp": ";;97/887;30/9232",
        "google_scholar": ";https://scholar.google.at/citations?user=fn6VhukAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-0552-2874;0000-0003-3920-1419;0000-0002-6356-3367;0000-0002-8644-9655",
        "linkedin": ";;https://www.linkedin.com/company/69666281/admin/feed/posts/;",
        "or_profile": "~Christian_Toth1;~Christian_Knoll1;~Franz_Pernkopf1;~Robert_Peharz5",
        "aff": "Technische Universit\u00e4t Graz;Levata;Graz University of Technology;Technische Universit\u00e4t Graz",
        "aff_domain": "tugraz.at;levata.at;spsc.tugraz.at;tugraz.at",
        "position": "PhD student;Researcher;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntoth2025effective,\ntitle={Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders},\nauthor={Christian Toth and Christian Knoll and Franz Pernkopf and Robert Peharz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WWoxHUUeBg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WWoxHUUeBg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WbyCIiFy8k",
        "title": "Analyzing Generative Models by Manifold Entropic Metrics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Good generative models should not only synthesize high quality data, but also utilize interpretable representations that aid human understanding of their behavior.\nHowever, it is difficult to measure objectively if and to what degree desirable properties of disentangled representations have been achieved.\nInspired by the principle of independent mechanisms, we address this difficulty by introducing a novel set of tractable information-theoretic evaluation metrics.\nWe demonstrate the usefulness of our metrics on illustrative toy examples and conduct an in-depth comparison of various normalizing flow architectures and $\\beta$-VAEs on the EMNIST dataset.\nOur method allows to sort latent features by importance and assess the amount of residual correlations of the resulting concepts.\nThe most interesting finding of our experiments is a ranking of model architectures in terms of their inductive bias to converge to aligned and disentangled representations during training.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Galperin;Ullrich Koethe",
        "authorids": "~Daniel_Galperin1;~Ullrich_Koethe1",
        "gender": "M;M",
        "homepage": ";https://hci.iwr.uni-heidelberg.de/vislearn/people/ullrich-koethe/",
        "dblp": ";15/809",
        "google_scholar": ";gt-yaNMAAAAJ",
        "orcid": ";0000-0001-6036-1287",
        "linkedin": "daniel-galperin-489b8a292/;",
        "or_profile": "~Daniel_Galperin1;~Ullrich_Koethe1",
        "aff": "Heidelberg University, Ruprecht-Karls-Universit\u00e4t Heidelberg;Heidelberg University",
        "aff_domain": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "position": "PhD student;Adjunct Professor",
        "bibtex": "@inproceedings{\ngalperin2025analyzing,\ntitle={Analyzing Generative Models by Manifold Entropic Metrics},\nauthor={Daniel Galperin and Ullrich Koethe},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WbyCIiFy8k}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WbyCIiFy8k",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WdcA7v7uzc",
        "title": "Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modelling causal effects from observational data for deciding policy actions can benefit from being interpretable and transparent; both due to the high stakes involved and the inherent lack of ground truth labels to evaluate the accuracy of such models. To date, attempts at transparent causal effect estimation consist of applying post hoc explanation methods to black-box models, which are not interpretable. Here, we present BICauseTree: an interpretable balancing method that identifies clusters where natural experiments occur locally. Our approach builds on decision trees with a customized objective function to improve balancing and reduce treatment allocation bias. Consequently, it can additionally detect subgroups presenting positivity violations, exclude them, and provide a covariate-based definition of the target population we can infer from and generalize to. We evaluate the method's performance using synthetic and realistic datasets, explore its bias-interpretability tradeoff, and show that it is comparable with existing approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lucile Ter-Minassian;Liran Szlak;Ehud Karavani;Christopher C. Holmes;Yishai Shimoni",
        "authorids": "~Lucile_Ter-Minassian1;~Liran_Szlak2;~Ehud_Karavani1;~Christopher_C._Holmes1;~Yishai_Shimoni1",
        "gender": "F;;;M;M",
        "homepage": ";;;;",
        "dblp": ";173/5246;215/4948;08/6129;39/11007",
        "google_scholar": "PXyI6qkAAAAJ;https://scholar.google.com/citations?hl=iw;KAzt_pYAAAAJ;;wp-F1asAAAAJ",
        "orcid": ";;0000-0002-0187-5437;;0000-0002-4364-4207",
        "linkedin": "lucile-ter-minassian-94428a12b/;;;;",
        "or_profile": "~Lucile_Ter-Minassian1;~Liran_Szlak2;~Ehud_Karavani1;~Christopher_C._Holmes1;~Yishai_Shimoni1",
        "aff": ";International Business Machines;International Business Machines;University of Oxford;International Business Machines",
        "aff_domain": ";ibm.com;ibm.com;ox.ac.uk;ibm.com",
        "position": ";Researcher;Researcher;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nter-minassian2025hierarchical,\ntitle={Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation},\nauthor={Lucile Ter-Minassian and Liran Szlak and Ehud Karavani and Christopher C. Holmes and Yishai Shimoni},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WdcA7v7uzc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WdcA7v7uzc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "We7KbgDN6W",
        "title": "Learning a Single Index Model from Anisotropic Data with Vanilla Stochastic Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the problem of learning a Single Index Model (SIM)---a popular model for studying the ability of neural networks to learn features---from anisotropic Gaussian inputs by training a neuron using vanilla Stochastic Gradient Descent (SGD).\nWhile the isotropic case has been extensively studied, the anisotropic case has received less attention and the impact of the covariance matrix on the learning dynamics remains unclear. For instance, Mousavi-Hosseini et al. (2023b) proposed a spherical SGD that requires a separate estimation of the data covariance matrix, thereby oversimplifying the influence of covariance.\nIn this study, we analyze the learning dynamics of vanilla SGD under the SIM with anisotropic input data, demonstrating that vanilla SGD automatically adapts to the data\u2019s covariance structure.\nLeveraging these results, we derive upper and lower bounds on the sample complexity using a notion of effective dimension that is determined by the structure of the covariance matrix instead of the input data dimension.\nFinally, we validate and extend our theoretical findings through numerical simulations, demonstrating the practical effectiveness of our approach in adapting to anisotropic data, which has implications for efficient training of neural networks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Guillaume Braun;Minh Ha Quang;Masaaki Imaizumi",
        "authorids": "~Guillaume_Braun1;~Minh_Ha_Quang1;~Masaaki_Imaizumi1",
        "gender": "M;M;M",
        "homepage": "https://glmbraun.github.io/;http://www.haquangminh.info/;https://sites.google.com/view/mimaizumi/home",
        "dblp": "287/4972;87/2093;",
        "google_scholar": "lzebyjAAAAAJ;JxhhXEsAAAAJ;https://scholar.google.co.jp/citations?user=6c0Ljd4AAAAJ",
        "orcid": ";0000-0003-3926-8875;",
        "linkedin": ";;masaaki-imaizumi-38600b157/",
        "or_profile": "~Guillaume_Braun1;~Minh_Ha_Quang1;~Masaaki_Imaizumi1",
        "aff": "RIKEN;RIKEN;RIKEN Center for Advanced Intelligence Project+The University of Tokyo",
        "aff_domain": "riken.jp;riken.jp;riken.jp+u-tokyo.ac.jp",
        "position": "Postdoc;Team Leader;Team Director+Associate Professor",
        "bibtex": "@inproceedings{\nbraun2025learning,\ntitle={Learning a Single Index Model from Anisotropic Data with Vanilla Stochastic Gradient Descent},\nauthor={Guillaume Braun and Minh Ha Quang and Masaaki Imaizumi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=We7KbgDN6W}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=We7KbgDN6W",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Wi1IlPePEP",
        "title": "Robust Gradient Descent for Phase Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent progress in robust statistical learning has mainly tackled convex problems, like mean estimation or linear regression, with non-convex challenges receiving less attention. Phase retrieval exemplifies such a non-convex problem, requiring the recovery of a signal from only the magnitudes of its linear measurements, without phase (sign) information. While several non-convex methods, especially those involving the Wirtinger Flow algorithm, have been proposed for noiseless or mild noise settings, developing solutions for heavy-tailed noise and adversarial corruption remains an open challenge. In this paper, we investigate an approach that leverages robust gradient descent techniques to improve the Wirtinger Flow algorithm's ability to simultaneously cope with fourth moment bounded noise and adversarial contamination in both the inputs (covariates) and outputs (responses). We address two scenarios: known zero-mean noise and completely unknown noise. For the latter, we propose a preprocessing step that alters the problem into a new format that does not fit traditional phase retrieval approaches but can still be resolved with a tailored version of the algorithm for the zero-mean noise context.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alex Buna;Patrick Rebeschini",
        "authorids": "~Alex_Buna1;~Patrick_Rebeschini1",
        "gender": "M;M",
        "homepage": "https://www.stats.ox.ac.uk/people/alex-buna-marginean;http://www.stats.ox.ac.uk/~rebeschi/",
        "dblp": ";164/7439",
        "google_scholar": ";",
        "orcid": "0009-0005-0750-445X;0000-0001-7772-4160",
        "linkedin": ";patrick-rebeschini/",
        "or_profile": "~Alex_Buna1;~Patrick_Rebeschini1",
        "aff": "University of Oxford;University of Oxford",
        "aff_domain": "oxford.ac.uk;oxford.ac.uk",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nbuna2025robust,\ntitle={Robust Gradient Descent for Phase Retrieval},\nauthor={Alex Buna and Patrick Rebeschini},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Wi1IlPePEP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Wi1IlPePEP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WmjB45XBAQ",
        "title": "Safe exploration in reproducing kernel Hilbert spaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for safety-critical systems in unknown environments. However, most algorithms make a smoothness assumption, which is encoded by a known bounded norm in a reproducing kernel Hilbert space (RKHS). The RKHS is a potentially infinite-dimensional space, and it remains unclear how to reliably obtain the RKHS norm of an unknown function. In this work, we propose a safe BO algorithm capable of estimating the RKHS norm from data. We provide statistical guarantees on the RKHS norm estimation, integrate the estimated RKHS norm into existing confidence intervals and show that we retain theoretical guarantees, and prove safety of the resulting safe BO algorithm. We apply our algorithm to safely optimize reinforcement learning policies on physics simulators and on a real inverted pendulum, demonstrating improved performance, safety, and scalability compared to the state-of-the-art.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abdullah Tokmak;Kiran G. Krishnan;Thomas B. Sch\u00f6n;Dominik Baumann",
        "authorids": "~Abdullah_Tokmak1;~Kiran_G._Krishnan1;~Thomas_B._Sch\u00f6n1;~Dominik_Baumann1",
        "gender": ";;M;M",
        "homepage": "https://research.aalto.fi/en/persons/abdullah-tokmak;;http://user.it.uu.se/~thosc112/index.html;https://baumanndominik.github.io/",
        "dblp": ";;85/4891;215/4492",
        "google_scholar": "ekTgPqIAAAAJ;3OaKoWcAAAAJ;https://scholar.google.se/citations?user=FUqUC2oAAAAJ;bJX8-CEAAAAJ",
        "orcid": "0009-0004-7345-6807;;0000-0001-5183-234X;0000-0001-7340-2180",
        "linkedin": "abdullahtokmak/;kgkrishnan97/;thomas-sch%C3%B6n-2b587b1/;",
        "or_profile": "~Abdullah_Tokmak1;~Kiran_G._Krishnan1;~Thomas_B._Sch\u00f6n1;~Dominik_Baumann1",
        "aff": "Aalto University;Aalto University;Uppsala University;Aalto University",
        "aff_domain": "aalto.fi;aalto.fi;uu.se;aalto.fi",
        "position": "PhD student;MS student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntokmak2025safe,\ntitle={Safe exploration in reproducing kernel Hilbert spaces},\nauthor={Abdullah Tokmak and Kiran G. Krishnan and Thomas B. Sch{\\\"o}n and Dominik Baumann},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WmjB45XBAQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WmjB45XBAQ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WspiEX6v3r",
        "title": "MODL: Multilearner Online Deep Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Online deep learning tackles the challenge of learning from data streams by balancing two competing goals: fast learning and deep learning. However, existing research primarily emphasizes deep learning solutions, which are more adept at handling the ''deep'' aspect than the ''fast'' aspect of online learning. In this work, we introduce an alternative paradigm through a hybrid multilearner approach. We begin by developing a fast online logistic regression learner, which operates without relying on backpropagation. It leverages closed-form recursive updates of model parameters, efficiently addressing the fast learning component of the online learning challenge. This approach is further integrated with a cascaded multilearner design, where shallow and deep learners are co-trained in a cooperative, synergistic manner to solve the online learning problem. We demonstrate that this approach achieves state-of-the-art performance on standard online learning datasets. We make our code available: https://github.com/AntonValk/MODL",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Antonios Valkanas;Boris N. Oreshkin;Mark Coates",
        "authorids": "~Antonios_Valkanas1;~Boris_N._Oreshkin1;~Mark_Coates1",
        "gender": "M;M;M",
        "homepage": "https://antonvalk.github.io/;;http://www.ece.mcgill.ca/~mcoate/",
        "dblp": "130/7787.html;33/1017;c/MarkCoates",
        "google_scholar": "YY-QwsMAAAAJ;https://scholar.google.ca/citations?user=48MBCeIAAAAJ;https://scholar.google.ca/citations?user=qxWORNoAAAAJ",
        "orcid": ";;0000-0001-5030-1379",
        "linkedin": ";boris-oreshkin-1710061a/;",
        "or_profile": "~Antonios_Valkanas1;~Boris_N._Oreshkin1;~Mark_Coates1",
        "aff": "McGill University;Amazon;McGill University",
        "aff_domain": "mcgill.ca;amazon.com;mcgill.ca",
        "position": "PhD student;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nvalkanas2025modl,\ntitle={{MODL}: Multilearner Online Deep Learning},\nauthor={Antonios Valkanas and Boris N. Oreshkin and Mark Coates},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=WspiEX6v3r}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WspiEX6v3r",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XCmIlemQP5",
        "title": "All or None: Identifiable Linear Properties of Next-Token Predictors in Language Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We analyze identifiability as a possible explanation for the ubiquity of linear properties across language models, such as the vector difference between the representations of \u201ceasy\u201d and \u201ceasiest\u201d being parallel to that between \u201clucky\u201d and \u201cluckiest\u201d. For this, we ask whether finding a linear property in one model implies that any model that induces the same distribution has that property, too. To answer that, we first prove an identifiability result to characterize distribution-equivalent next-token predictors, lifting a diversity requirement of previous results. Second, based on a refinement of relational linearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how many notions of linearity are amenable to our analysis. Finally, we show that under suitable conditions, these linear properties either hold in all or none distribution equivalent next-token predictors.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Emanuele Marconato;Sebastien Lachapelle;Sebastian Weichwald;Luigi Gresele",
        "authorids": "~Emanuele_Marconato1;~Sebastien_Lachapelle1;~Sebastian_Weichwald1;~Luigi_Gresele1",
        "gender": ";M;;M",
        "homepage": ";https://slachapelle.github.io/;https://sweichwald.de;https://lgresele.github.io/",
        "dblp": "321/3331;224/0080;158/0010;211/6114",
        "google_scholar": "H0gXWAgAAAAJ;uxHoJp8AAAAJ;;JdZ8DWwAAAAJ",
        "orcid": ";;;",
        "linkedin": "emanuele-marconato-108449195;s%C3%A9bastien-lachapelle-a4321a122/;;",
        "or_profile": "~Emanuele_Marconato1;~Sebastien_Lachapelle1;~Sebastian_Weichwald1;~Luigi_Gresele1",
        "aff": "University of Trento;Samsung;University of Copenhagen;University of Copenhagen",
        "aff_domain": "unitn.it;samsung.com;math.ku.dk;ku.dk",
        "position": "Postdoc;Researcher;Assistant Professor;Postdoc",
        "bibtex": "@inproceedings{\nmarconato2025all,\ntitle={All or None: Identifiable Linear Properties of Next-Token Predictors in Language Modeling},\nauthor={Emanuele Marconato and Sebastien Lachapelle and Sebastian Weichwald and Luigi Gresele},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XCmIlemQP5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XCmIlemQP5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XP7AUhVhhu",
        "title": "Kernel Single Proxy Control for Deterministic Confounding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of causal effect estimation with an unobserved confounder, where we observe a single proxy variable that is associated with the confounder. Although it has been shown that the recovery of an average causal effect is impossible in general from a single proxy variable, we show that causal recovery is possible if the outcome is generated deterministically. This generalizes existing work on causal methods with a single proxy variable to the continuous treatment setting. We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on challenging synthetic benchmarks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Liyuan Xu;Arthur Gretton",
        "authorids": "~Liyuan_Xu1;~Arthur_Gretton1",
        "gender": "M;M",
        "homepage": ";http://www.gatsby.ucl.ac.uk/~gretton/",
        "dblp": "33/9817;56/2574",
        "google_scholar": "-DLyhSoAAAAJ;OUv7J6QAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Liyuan_Xu1;~Arthur_Gretton1",
        "aff": ";Google+University College London",
        "aff_domain": ";deepmind.com+ucl.ac.uk",
        "position": ";Researcher+Professor",
        "bibtex": "@inproceedings{\nxu2025kernel,\ntitle={Kernel Single Proxy Control for Deterministic Confounding},\nauthor={Liyuan Xu and Arthur Gretton},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XP7AUhVhhu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XP7AUhVhhu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XYDFI8xYVF",
        "title": "Pareto Set Identification With Posterior Sampling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The problem of identifying the best answer among a collection of items having real-valued distribution is well-understood. \n  Despite its practical relevance for many applications, fewer works have studied its extension when multiple and potentially conflicting metrics are available to assess an item's quality.\n  Pareto set identification (PSI) aims to identify the set of answers whose means are not uniformly worse than another.\n  This paper studies PSI in the transductive linear setting with potentially correlated objectives.\n  Building on posterior sampling in both the stopping and the sampling rules, we propose the \\hyperlink{PSIPS}{PSIPS} algorithm that deals simultaneously with structure and correlation without paying the computational cost of existing oracle-based algorithms.\n  Both from a frequentist and Bayesian perspective, \\hyperlink{PSIPS}{PSIPS} is asymptotically optimal.\n  We demonstrate its good empirical performance in real-world and synthetic instances.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Cyrille Kone;Marc Jourdan;Emilie Kaufmann",
        "authorids": "~Cyrille_Kone1;~Marc_Jourdan1;~Emilie_Kaufmann1",
        "gender": "M;M;F",
        "homepage": "http://cyrille-kone.github.io;https://marcjourdan.github.io;https://emiliekaufmann.github.io/",
        "dblp": ";228/8157;67/11350",
        "google_scholar": ";BOXGjhgAAAAJ;9GE1vx4AAAAJ",
        "orcid": ";0000-0002-2449-4549;",
        "linkedin": ";marc-jourdan/;",
        "or_profile": "~Cyrille_Kone1;~Marc_Jourdan1;~Emilie_Kaufmann1",
        "aff": "Universit\u00e9 de Lille;EPFL - EPF Lausanne;CNRS",
        "aff_domain": "univ-lille.fr;epfl.ch;cnrs.fr",
        "position": "PhD student;Postdoc;Researcher",
        "bibtex": "@inproceedings{\nkone2025pareto,\ntitle={Pareto Set Identification With Posterior Sampling},\nauthor={Cyrille Kone and Marc Jourdan and Emilie Kaufmann},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XYDFI8xYVF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XYDFI8xYVF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XYS4sRkMfE",
        "title": "Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Randomization as a mean to improve the adversarial robustness of machine learning models has recently attracted significant attention. Unfortunately, much of the theoretical analysis so far has focused on binary classification, providing only limited insights into the more complex multiclass setting. In this paper, we take a step toward closing this gap by drawing inspiration from the field of graph theory. Our analysis focuses on discrete data distributions, allowing us to cast the adversarial risk minimization problems within the well-established framework of set packing problems. By doing so, we are able to identify three structural conditions on the support of the data distribution that are necessary for randomization to improve robustness. Furthermore, we are able to construct several data distributions where (contrarily to binary classification) switching from a deterministic to a randomized solution significantly reduces the optimal adversarial risk. These findings highlight the crucial role randomization can play in enhancing robustness to adversarial attacks in multiclass classification.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lucas Gnecco Heredia;Matteo Sammut;Muni Sreenivas Pydi;Rafael Pinot;benjamin negrevergne;Yann Chevaleyre",
        "authorids": "~Lucas_Gnecco_Heredia2;~Matteo_Sammut1;~Muni_Sreenivas_Pydi1;~Rafael_Pinot1;~benjamin_negrevergne1;~Yann_Chevaleyre1",
        "gender": "M;M;M;;;M",
        "homepage": ";;https://munisreenivas.github.io/;;;https://www.lamsade.dauphine.fr/~ychevaleyre/",
        "dblp": "325/5719;;194/2444;;;55/5658",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;BT8j_-oAAAAJ;;;SF6g8p4AAAAJ",
        "orcid": "0000-0002-1561-2080;;;;;",
        "linkedin": ";matteo-sammut-03b3331a2?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;;;;yannchevaleyre",
        "or_profile": "~Lucas_Gnecco_Heredia2;~Matteo_Sammut1;~Muni_Sreenivas_Pydi1;~Rafael_Pinot1;~benjamin_negrevergne1;~Yann_Chevaleyre1",
        "aff": ", Universit\u00e9 Paris-Dauphine (Paris IX);;Universit\u00e9 Paris Dauphine - PSL;;;Universit\u00e9 Paris-Dauphine (Paris IX)",
        "aff_domain": "lamsade.dauphine.fr;;lamsade.dauphine.fr;;;dauphine.fr",
        "position": "PhD student;;Postdoc;;;Full Professor",
        "bibtex": "@inproceedings{\nheredia2025unveiling,\ntitle={Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory},\nauthor={Lucas Gnecco Heredia and Matteo Sammut and Muni Sreenivas Pydi and Rafael Pinot and benjamin negrevergne and Yann Chevaleyre},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XYS4sRkMfE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XYS4sRkMfE",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XfNrWdHuoi",
        "title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Rapid growth of high-dimensional datasets in fields such as single-cell RNA sequencing and spatial genomics has led to unprecedented opportunities for scientific discovery, but it also presents unique computational and statistical challenges. Traditional methods struggle with geometry-aware data generation, interpolation along meaningful trajectories, and transporting populations via feasible paths. To address these issues, we introduce Geometry-Aware Generative Autoencoder (GAGA), a novel framework that combines extensible manifold learning with generative modeling. GAGA constructs a neural network embedding space that respects the intrinsic geometries discovered by manifold learning and learns a novel warped Riemannian metric on the data space. This warped metric is derived from both the points on the data manifold and negative samples off the manifold, allowing it to characterize a meaningful geometry across the entire latent space. Using this metric, GAGA can uniformly sample points on the manifold, generate points along geodesics, and interpolate between populations across the learned manifold. GAGA shows competitive performance in simulated and real-world datasets, including a 30% improvement over SOTA in single-cell population-level trajectory inference.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xingzhi Sun;Danqi Liao;Kincaid MacDonald;Yanlei Zhang;Guillaume Huguet;Guy Wolf;Ian Adelstein;Tim G. J. Rudner;Smita Krishnaswamy",
        "authorids": "~Xingzhi_Sun2;~Danqi_Liao2;~Kincaid_MacDonald1;~Yanlei_Zhang1;~Guillaume_Huguet1;~Guy_Wolf1;~Ian_Adelstein1;~Tim_G._J._Rudner2;~Smita_Krishnaswamy1",
        "gender": ";;;M;M;M;;;F",
        "homepage": ";;https://kincaid.ink;https://sites.google.com/view/yanleizhang/home;https://mila.quebec/personne/guillaume-huguet/;http://guywolf.org;https://sites.google.com/view/adelstein;;http://www.krishnaswamylab.org",
        "dblp": ";;;335/2128;286/5365;120/1308;;;74/2457",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;https://scholar.google.com/citations?hl=en;L8kYu9IAAAAJ;g0k3SjcAAAAJ;;;l2Pr9m8AAAAJ",
        "orcid": ";;0009-0006-4686-7488;;;0000-0002-6740-059X;;;",
        "linkedin": "xingzhi-sun/;;kincaid-macdonald-5046891b3;;;;;;",
        "or_profile": "~Xingzhi_Sun2;~Danqi_Liao2;~Kincaid_MacDonald1;~Yanlei_Zhang1;~Guillaume_Huguet1;~Guy_Wolf1;~Ian_Adelstein1;~Tim_G._J._Rudner2;~Smita_Krishnaswamy1",
        "aff": "Yale University;;Princeton University;Montreal Institute for Learning Algorithm;University of Montreal;University of Montreal;Yale University;;Yale University",
        "aff_domain": "yale.edu;;princeton.edu;mila.umontreal.ca;umontreal.ca;umontreal.ca;yale.edu;;yale.edu",
        "position": "PhD student;;PhD student;Postdoc;PhD student;Associate Professor;Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\nsun2025geometryaware,\ntitle={Geometry-Aware Generative Autoencoder for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds},\nauthor={Xingzhi Sun and Danqi Liao and Kincaid MacDonald and Yanlei Zhang and Guillaume Huguet and Guy Wolf and Ian Adelstein and Tim G. J. Rudner and Smita Krishnaswamy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XfNrWdHuoi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XfNrWdHuoi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XjNFnBqrEi",
        "title": "Looped ReLU MLPs May Be All You Need as Practical Programmable Computers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Previous work has demonstrated that attention mechanisms are Turing complete. More recently, it has been shown that a looped 9-layer Transformer can function as a universal programmable computer. In contrast, the multi-layer perceptrons with $\\mathsf{ReLU}$ activation ($\\mathsf{ReLU}$-$\\mathsf{MLP}$), one of the most fundamental components of neural networks, is known to be expressive; specifically, a two-layer neural network is a universal approximator given an exponentially large number of hidden neurons. However, it remains unclear whether a $\\mathsf{ReLU}$-$\\mathsf{MLP}$ can be made into a universal programmable computer using a practical number of weights. In this work, we provide an affirmative answer that a looped 23-layer $\\mathsf{ReLU}$-$\\mathsf{MLP}$ is capable of performing the basic necessary operations, more efficiently and effectively functioning as a programmable computer than a looped Transformer. This indicates simple modules have stronger expressive power than previously expected and have not been fully explored. Our work provides insights into the mechanisms of neural networks and demonstrates that complex tasks, such as functioning as a programmable computer, do not necessarily require advanced architectures like Transformers.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yingyu Liang;Zhizhou Sha;Zhenmei Shi;Zhao Song;Yufa Zhou",
        "authorids": "~Yingyu_Liang1;~Zhizhou_Sha1;~Zhenmei_Shi1;~Zhao_Song3;~Yufa_Zhou1",
        "gender": ";;M;M;M",
        "homepage": ";https://jamessand.github.io;http://zhmeishi.github.io/;https://www.youtube.com/@simapofang;https://masterzhou1.github.io/",
        "dblp": ";359/4308;246/5216;76/4051-2;343/5794-1",
        "google_scholar": ";eAObdYgAAAAJ;0oeNnzMAAAAJ;yDZct7UAAAAJ;bCM-3PYAAAAJ",
        "orcid": ";;0009-0007-6741-7598;0000-0003-4589-5234;",
        "linkedin": ";;zhenmei-shi-56408a113/;;yufazhou/",
        "or_profile": "~Yingyu_Liang1;~Zhizhou_Sha1;~Zhenmei_Shi1;~Zhao_Song3;~Yufa_Zhou1",
        "aff": ";Tsinghua University;MongoDB+Voyage AI;University of California, Berkeley;Duke University+University of Pennsylvania",
        "aff_domain": ";mail.tsinghua.edu.cn;mongodb.com+voyageai.com;berkeley.edu;duke.edu+seas.upenn.edu",
        "position": ";Undergrad student;Researcher+Researcher;Associate Professor;PhD student+MS student",
        "bibtex": "@inproceedings{\nliang2025looped,\ntitle={Looped Re{LU} {MLP}s May Be All You Need as Programmable Computers},\nauthor={Yingyu Liang and Zhizhou Sha and Zhenmei Shi and Zhao Song and Yufa Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XjNFnBqrEi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XjNFnBqrEi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XnDGR5pIwy",
        "title": "Your copula is a classifier in disguise: classification-based copula density estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose reinterpreting copula density estimation as a discriminative task. Under this novel estimation scheme, we train a classifier to distinguish samples from the joint density from those of the product of independent marginals, recovering the copula density in the process. We derive equivalences between well-known copula classes and classification problems naturally arising in our interpretation. Furthermore, we show our estimator achieves theoretical guarantees akin to maximum likelihood estimation. By identifying a connection with density ratio estimation, we benefit from the rich literature and models available for such problems. Empirically, we demonstrate the applicability of our approach by estimating copulas of real and high-dimensional datasets, outperforming competing copula estimators in density evaluation as well as sampling.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "David Huk;Mark Steel;Ritabrata Dutta",
        "authorids": "~David_Huk1;~Mark_Steel1;~Ritabrata_Dutta1",
        "gender": ";M;",
        "homepage": ";https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/steel/;https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/dutta/",
        "dblp": ";;",
        "google_scholar": ";METrs9QAAAAJ;",
        "orcid": ";0000-0001-9858-9279;",
        "linkedin": ";;",
        "or_profile": "~David_Huk1;~Mark_Steel1;~Ritabrata_Dutta1",
        "aff": ";University of Warwick;The university of Warwick",
        "aff_domain": ";warwick.ac.uk;warwick.ac.uk",
        "position": ";Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nhuk2025your,\ntitle={Your copula is a classifier in disguise: classification-based copula density estimation},\nauthor={David Huk and Mark Steel and Ritabrata Dutta},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XnDGR5pIwy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XnDGR5pIwy",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Xr5YpYx8mo",
        "title": "Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Estimating causal effects in social network data presents unique challenges due to the presence of spillover effects and network-induced confounding. While much of the existing literature addresses causal inference in social networks, many methods rely on strong assumptions about the form of network-induced confounding. These assumptions often fail to hold in high-dimensional networks, limiting the applicability of such approaches. To address this, we propose a novel methodology that integrates graph machine learning techniques with the double machine learning framework, facilitating accurate and efficient estimation of both direct and peer effects in a single observational social network. Our estimator achieves semiparametric efficiency under mild regularity conditions, enabling consistent uncertainty quantification. Through extensive simulations, we demonstrate the accuracy, robustness, and scalability of our method. Finally, we apply the proposed approach to examine the impact of Self-Help Group participation on financial risk tolerance, highlighting its practical relevance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Seyedeh Baharan Khatami;Harsh Parikh;Haowei Chen;Sudeepa Roy;Babak Salimi",
        "authorids": "~Seyedeh_Baharan_Khatami1;~Harsh_Parikh1;~Haowei_Chen2;~Sudeepa_Roy1;~Babak_Salimi1",
        "gender": "F;;M;F;M",
        "homepage": "https://baharankh.github.io/;;https://ericchenhw.github.io;https://users.cs.duke.edu/~sudeepa/;https://bsalimi.github.io/",
        "dblp": ";;;77/4620;",
        "google_scholar": ";;;https://scholar.google.com.tw/citations?user=vzfO5TwAAAAJ;",
        "orcid": ";;;0009-0002-8300-7891;",
        "linkedin": "bah-kh/;;eric-chenhaowei/;sudeepa-roy-0b22134b/;",
        "or_profile": "~Seyedeh_Baharan_Khatami1;~Harsh_Parikh1;~Haowei_Chen2;~Sudeepa_Roy1;~Babak_Salimi1",
        "aff": "University of California, San Diego;;Columbia University;Duke University;University of California, San Diego",
        "aff_domain": "ucsd.edu;;columbia.edu;cs.duke.edu;ucsd.edu",
        "position": "PhD student;;MS student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkhatami2025graph,\ntitle={Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects},\nauthor={Seyedeh Baharan Khatami and Harsh Parikh and Haowei Chen and Sudeepa Roy and Babak Salimi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Xr5YpYx8mo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Xr5YpYx8mo",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XrXlAYFpvR",
        "title": "On the Asymptotic Mean Square Error Optimality of Diffusion Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Diffusion models (DMs) as generative priors have recently shown great potential for denoising tasks but lack theoretical understanding with respect to their mean square error (MSE) optimality. This paper proposes a novel denoising strategy inspired by the structure of the MSE-optimal conditional mean estimator (CME). The resulting DM-based denoiser can be conveniently employed using a pre-trained DM, being particularly fast by truncating reverse diffusion steps and not requiring stochastic re-sampling. We present a comprehensive (non-)asymptotic optimality analysis of the proposed diffusion-based denoiser, demonstrating polynomial-time convergence to the CME under mild conditions. Our analysis also derives a novel Lipschitz constant that depends solely on the DM\u2019s hyperparameters. Further, we offer a new perspective on DMs, showing that they inherently combine an asymptotically optimal denoiser with a powerful generator, modifiable by switching re-sampling in the reverse process on or off. The theoretical findings are thoroughly validated with experiments based on various benchmark datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Benedikt Fesl;Benedikt B\u00f6ck;Florian Strasser;Michael Baur;Michael Joham;Wolfgang Utschick",
        "authorids": "~Benedikt_Fesl1;~Benedikt_B\u00f6ck1;~Florian_Strasser1;~Michael_Baur1;~Michael_Joham1;~Wolfgang_Utschick1",
        "gender": "M;M;M;M;M;M",
        "homepage": ";;https://www.ce.cit.tum.de/msv/people/florian-strasser/;;https://www.ce.cit.tum.de/msv/people/michael-joham/;https://www.ce.cit.tum.de/msv/",
        "dblp": ";331/8681;;;;34/5115",
        "google_scholar": "https://scholar.google.de/citations?user=vTfsc3YAAAAJ;https://scholar.google.de/citations?user=4P04LhwAAAAJ;pGeSXIoAAAAJ;https://scholar.google.de/citations?user=DKUggC0AAAAJ;;qflRi8QAAAAJ",
        "orcid": "0000-0002-1431-5885;0009-0009-8604-4269;0000-0001-6249-549X;0000-0002-3697-6828;;0000-0002-2871-4246",
        "linkedin": "https://linkedin.com/in/benedikt-fesl;benedikt-b%C3%B6ck-9bb575267/?originalSubdomain=de;florian-strasser-5344171b4/;;;",
        "or_profile": "~Benedikt_Fesl1;~Benedikt_B\u00f6ck1;~Florian_Strasser1;~Michael_Baur1;~Michael_Joham1;~Wolfgang_Utschick1",
        "aff": "Technische Universit\u00e4t M\u00fcnchen;Technische Universit\u00e4t M\u00fcnchen;Technische Universit\u00e4t M\u00fcnchen;Technische Universit\u00e4t M\u00fcnchen;Technische Universit\u00e4t M\u00fcnchen;Technical University Munich",
        "aff_domain": "tum.de;tum.de;tum.de;tum.de;tum.de;tum.de",
        "position": "PhD student;PhD student;PhD student;PhD student;Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nfesl2025on,\ntitle={On the Asymptotic Mean Square Error Optimality of Diffusion Models},\nauthor={Benedikt Fesl and Benedikt B{\\\"o}ck and Florian Strasser and Michael Baur and Michael Joham and Wolfgang Utschick},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=XrXlAYFpvR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XrXlAYFpvR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Y1BEQEELxI",
        "title": "Feasible Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL demands satisfactory performance *on every individual data point*.\nSince any model that meets the prescribed performance threshold is a valid FL solution, the choice of optimization algorithm and its dynamics play a crucial role in shaping the properties of the resulting solutions. \nIn particular, we study a primal-dual approach which dynamically re-weights the importance of each sample during training. To address the challenge of setting a meaningful threshold in practice, we introduce a relaxation of FL that incorporates slack variables of minimal norm. Our empirical analysis, spanning image classification, age regression, and preference optimization in large language models, demonstrates that models trained via FL can learn from data while displaying improved tail behavior compared to ERM, with only a marginal impact on average performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juan Ramirez;Ignacio Hounie;Juan Elenter;Jose Gallego-Posada;Meraj Hashemizadeh;Alejandro Ribeiro;Simon Lacoste-Julien",
        "authorids": "~Juan_Ramirez2;~Ignacio_Hounie1;~Juan_Elenter1;~Jose_Gallego-Posada1;~Meraj_Hashemizadeh1;~Alejandro_Ribeiro1;~Simon_Lacoste-Julien1",
        "gender": "M;;;M;;M;M",
        "homepage": "https://juan43ramirez.github.io;;https://juanelenter.github.io/;http://gallego-posada.github.io/;;https://alelab.seas.upenn.edu;http://www.iro.umontreal.ca/~slacoste/",
        "dblp": ";;313/9585;211/7701;;32/15;94/446.html",
        "google_scholar": "yop0kRkAAAAJ;V0h3OSYAAAAJ;https://scholar.google.com/citations?hl=en;tfKnkRQAAAAJ;lxlP4BMAAAAJ;7mrPM4kAAAAJ;oejm5IUAAAAJ",
        "orcid": ";;;;;0000-0003-4230-9906;0000-0001-6485-6180",
        "linkedin": "juan-camilo-ramirez-de-los-rios-11ab2b141/;;juan-elenter/;;;;simon-lacoste-julien-355b9a3",
        "or_profile": "~Juan_Ramirez2;~Ignacio_Hounie1;~Juan_Elenter1;~Jose_Gallego-Posada1;~Meraj_Hashemizadeh1;~Alejandro_Ribeiro1;~Simon_Lacoste-Julien1",
        "aff": "University of Montreal;University of Pennsylvania;Spotify;;Mila - Quebec Artificial Intelligence Institute;University of Pennsylvania;University of Montreal+Samsung - SAIT AI Lab, Montreal",
        "aff_domain": "umontreal.ca;upenn.edu;spotify.com;;mila.quebec;upenn.edu;umontreal.ca+samsung.com",
        "position": "PhD student;PhD student;Researcher;;Researcher;Full Professor;Associate Professor+VP Lab Director",
        "bibtex": "@inproceedings{\nramirez2025feasible,\ntitle={Feasible Learning},\nauthor={Juan Ramirez and Ignacio Hounie and Juan Elenter and Jose Gallego-Posada and Meraj Hashemizadeh and Alejandro Ribeiro and Simon Lacoste-Julien},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Y1BEQEELxI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Y1BEQEELxI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Y502d7Ci4U",
        "title": "Multimodal Learning with Uncertainty Quantification based on Discounted Belief Fusion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multimodal AI models are increasingly used in fields like healthcare, finance, and autonomous driving, where information is drawn from multiple sources or modalities such as images, texts, audios, videos. However, effectively managing uncertainty\u2014arising from noise, insufficient evidence, or conflicts between modalities\u2014is crucial for reliable decision-making. Current uncertainty-aware machine learning methods leveraging, for example, evidence averaging, or evidence accumulation underestimate uncertainties in high-conflict scenarios. Moreover, the state-of-the-art evidence averaging strategy is not order invariant and fails to scale to multiple modalities. To address these challenges, we propose a novel multimodal learning method with order-invariant evidence fusion and introduce a conflict-based discounting mechanism that reallocates uncertain mass when unreliable modalities are detected. We provide both theoretical analysis and experimental validation, demonstrating that unlike the previous work, the proposed approach effectively distinguishes between conflicting and non-conflicting samples based on the provided uncertainty estimates, and outperforms the previous models in uncertainty-based conflict detection.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Grigor Bezirganyan;Sana Sellami;Laure Berti-Equille;S\u00e9bastien Fournier",
        "authorids": "~Grigor_Bezirganyan1;~Sana_Sellami1;~Laure_Berti-Equille1;~S\u00e9bastien_Fournier2",
        "gender": ";F;F;M",
        "homepage": ";;https://laureberti.github.io/website/;https://pageperso.lis-lab.fr/sebastien.fournier/",
        "dblp": ";43/5676;b/LaureBertiEquille;84/6245",
        "google_scholar": ";;https://scholar.google.fr/citations?user=pHwxwjwAAAAJ;3mY8NnkAAAAJ",
        "orcid": ";0000-0001-8302-3053;0000-0002-8046-0570;",
        "linkedin": ";;laure-berti-equille/;",
        "or_profile": "~Grigor_Bezirganyan1;~Sana_Sellami1;~Laure_Berti-Equille1;~S\u00e9bastien_Fournier2",
        "aff": ";Acad\u00e9mie d'Aix-Marseille;IRD - Institute of Research for Sustainable Development, France;Universit\u00e9 d'Aix-Marseille",
        "aff_domain": ";ac-aix-marseille.fr;ird.fr;univ-amu.fr",
        "position": ";Assistant Professor;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nbezirganyan2025multimodal,\ntitle={Multimodal Learning with Uncertainty Quantification based on Discounted Belief Fusion},\nauthor={Grigor Bezirganyan and Sana Sellami and Laure Berti-Equille and S{\\'e}bastien Fournier},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Y502d7Ci4U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Y502d7Ci4U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "YIlZdkeb05",
        "title": "Contractivity and linear convergence in bilinear saddle-point problems: An operator-theoretic approach",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the convex-concave bilinear saddle-point problem $\\min_x \\max_y f(x) + y^\\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$ are strongly convex, and suitable rank conditions on the matrix $A$ hold. The solution of this problem is at the core of many machine learning tasks. By employing tools from monotone operator theory, we systematically prove the contractivity (in turn, the linear convergence) of several first-order primal-dual algorithms, including the Chambolle\u2013Pock method. Our approach results in concise  proofs, and it yields new convergence guarantees and tighter bounds compared to known results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Colin Dirren;Mattia Bianchi;Panagiotis D. Grontas;John Lygeros;Florian Dorfler",
        "authorids": "~Colin_Dirren1;~Mattia_Bianchi1;~Panagiotis_D._Grontas1;~John_Lygeros1;~Florian_Dorfler1",
        "gender": "M;M;M;M;M",
        "homepage": ";https://ee.ethz.ch/de/departement/personen-a-bis-z/person-detail.MzE5Mzk2.TGlzdC8zMjc5LC0xNjUwNTg5ODIw.html;https://p-grontas.github.io/;https://control.ee.ethz.ch/people/profile.john-lygeros.html;http://people.ee.ethz.ch/~floriand/",
        "dblp": ";06/11527;263/9616;51/2754;",
        "google_scholar": ";nutwJM4AAAAJ;zGAV5-UAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?view_op=list_works",
        "orcid": "0000-0002-8252-6713;0000-0002-5261-6718;0000-0002-8071-8826;0000-0002-6159-1962;0000-0002-9649-5305",
        "linkedin": ";;;john-lygeros-662b73233/;",
        "or_profile": "~Colin_Dirren1;~Mattia_Bianchi1;~Panagiotis_D._Grontas1;~John_Lygeros1;~Florian_Dorfler1",
        "aff": ";ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;",
        "aff_domain": ";ethz.ch;ethz.ch;ethz.ch;",
        "position": ";Postdoc;PhD student;Full Professor;",
        "bibtex": "@inproceedings{\ndirren2025contractivity,\ntitle={Contractivity and linear convergence  in bilinear saddle-point problems: An operator-theoretic approach},\nauthor={Colin Dirren and Mattia Bianchi and Panagiotis D. Grontas and John Lygeros and Florian Dorfler},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=YIlZdkeb05}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=YIlZdkeb05",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "YXtCD3q53C",
        "title": "Covariance Selection over Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Covariance matrix estimation is a fundamental problem in multivariate data analysis, which becomes particularly challenging in high-dimensional settings due to the curse of dimensionality. To enhance estimation accuracy, structural regularization is often imposed on the precision matrix (the inverse covariance matrix) for covariance selection. In this paper, we study covariance selection in a distributed setting, where data is spread across a network of agents. We formulate the problem as a Gaussian maximum likelihood estimation problem with structural penalties and propose a novel algorithmic framework called NetGGM. Unlike existing methods that rely on a central coordinator, NetGGM operates in a fully decentralized manner with low computational complexity. We provide theoretical guarantees showing that NetGGM converges linearly to the global optimum while ensuring consensus among agents. Numerical experiments validate its convergence properties and demonstrate that it outperforms state-of-the-art methods in precision matrix estimation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenfu Xia;Fengpei Li;Ying Sun;Ziping Zhao",
        "authorids": "~Wenfu_Xia1;~Fengpei_Li2;~Ying_Sun5;~Ziping_Zhao1",
        "gender": "M;M;F;M",
        "homepage": "http://www.ncvxopt.com;https://github.com/fengpeili;https://ysunac.github.io;https://www.zipingzhao.com",
        "dblp": "354/4057;;10/5415-3;13/3015-2",
        "google_scholar": ";;M9uQsUQAAAAJ;https://scholar.google.com.hk/citations?user=fYYmZDsAAAAJ",
        "orcid": ";;;0000-0002-8668-6263",
        "linkedin": ";;;zipingzhao",
        "or_profile": "~Wenfu_Xia1;~Fengpei_Li2;~Ying_Sun5;~Ziping_Zhao1",
        "aff": "ShanghaiTech University;ShanghaiTech University;Pennsylvania State University;ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn;psu.edu;shanghaitech.edu.cn",
        "position": "PhD student;MS student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxia2025covariance,\ntitle={Covariance Selection over Networks},\nauthor={Wenfu Xia and Ziping Zhao and Fengpei Li and Ying Sun},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=YXtCD3q53C}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=YXtCD3q53C",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Ymn23Osn4g",
        "title": "Steinmetz Neural Networks for Complex-Valued Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce a new approach to processing complex-valued data using DNNs consisting of parallel real-valued subnetworks with coupled outputs. Our proposed class of architectures, referred to as Steinmetz Neural Networks, incorporates multi-view learning to construct more interpretable representations in the latent space. Moreover, we present the Analytic Neural Network, which incorporates a consistency penalty that encourages analytic signal representations in the latent space of the Steinmetz neural network. This penalty enforces a deterministic and orthogonal relationship between the real and imaginary components. Using an information-theoretic construction, we demonstrate that the generalization gap upper bound posited by the analytic neural network is lower than that of the general class of Steinmetz neural networks. Our numerical experiments depict the improved performance and robustness to additive noise, afforded by our proposed networks on benchmark datasets and synthetic examples.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shyam Venkatasubramanian;Ali Pezeshki;Vahid Tarokh",
        "authorids": "~Shyam_Venkatasubramanian1;~Ali_Pezeshki1;~Vahid_Tarokh1",
        "gender": ";;",
        "homepage": "https://shyamven.github.io/;;",
        "dblp": "284/0789;;",
        "google_scholar": "vshejS4AAAAJ;5laKwx4AAAAJ;",
        "orcid": ";;",
        "linkedin": "shyam-v/;;",
        "or_profile": "~Shyam_Venkatasubramanian1;~Ali_Pezeshki1;~Vahid_Tarokh1",
        "aff": "Duke University;Colorado State University;",
        "aff_domain": "duke.edu;colostate.edu;",
        "position": "PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nvenkatasubramanian2025steinmetz,\ntitle={Steinmetz Neural Networks for Complex-Valued Data},\nauthor={Shyam Venkatasubramanian and Ali Pezeshki and Vahid Tarokh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Ymn23Osn4g}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ymn23Osn4g",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "YxgThbmXgN",
        "title": "Refined Analysis of Constant Step Size Federated Averaging and Federated Richardson-Romberg Extrapolation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we present a novel analysis of $\\texttt{FedAvg}$ with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution.\nWe provide a first-order bias expansion in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity.\nFinally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Paul Mangold;Alain Oliviero Durmus;Aymeric Dieuleveut;Sergey Samsonov;Eric Moulines",
        "authorids": "~Paul_Mangold1;~Alain_Oliviero_Durmus1;~Aymeric_Dieuleveut1;~Sergey_Samsonov1;~Eric_Moulines1",
        "gender": "M;;M;M;M",
        "homepage": "http://www.pmangold.fr;;http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/;https://www.hse.ru/org/persons/219484540;",
        "dblp": "298/1535;;176/5034;23/8962;54/2358",
        "google_scholar": "https://scholar.google.fr/citations?user=3HUiM0sAAAAJ;;ge-OinUAAAAJ;https://scholar.google.ru/citations?user=8BwDmyMAAAAJ;https://scholar.google.fr/citations?user=_XE1LvQAAAAJ",
        "orcid": "0000-0002-0252-5287;;;0000-0002-0203-2028;0000-0002-2058-0693",
        "linkedin": ";;;;",
        "or_profile": "~Paul_Mangold1;~Alain_Oliviero_Durmus1;~Aymeric_Dieuleveut1;~Sergey_Samsonov1;~Eric_Moulines1",
        "aff": "\u00c9cole Polytechnique;;\u00c9cole Polytechnique;Higher School of Economics;Ecole polytechnique",
        "aff_domain": "polytechnique.edu;;polytechnique.edu;hse.ru;polytechnique.edu",
        "position": "Postdoc;;Full Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nmangold2025refined,\ntitle={Refined Analysis of Constant Step Size Federated Averaging},\nauthor={Paul Mangold and Alain Oliviero Durmus and Aymeric Dieuleveut and Sergey Samsonov and Eric Moulines},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=YxgThbmXgN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=YxgThbmXgN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZAblNqZMU8",
        "title": "High-Dimensional Differential Parameter Inference in Exponential Family using Time Score Matching",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper addresses differential inference in time-varying parametric probabilistic models, like graphical models with changing structures. Instead of estimating a high-dimensional model at each time and estimating changes later, we directly learn the differential parameter, i.e., the time derivative of the parameter. The main idea is treating the time score function of an exponential family model as a linear model of the differential parameter for direct estimation. We use time score matching to estimate parameter derivatives. We prove the consistency of a regularized score matching objective and demonstrate the finite-sample normality of a debiased estimator in high-dimensional settings. Our methodology effectively infers differential structures in high-dimensional graphical models, verified on simulated and real-world datasets. The code reproducing our experiments can be found at: \\url{https://github.com/Leyangw/tsm}.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel James Williams;Leyang Wang;Qizhen Ying;Song Liu;Mladen Kolar",
        "authorids": "~Daniel_James_Williams1;~Leyang_Wang1;~Qizhen_Ying1;~Song_Liu1;~Mladen_Kolar3",
        "gender": "M;M;M;M;",
        "homepage": "https://www.dannyjameswilliams.co.uk;;;http://allmodelsarewrong.net;",
        "dblp": ";;;80/1141-2;",
        "google_scholar": "C7sR0UYAAAAJ;-HIb8AIAAAAJ;;;",
        "orcid": ";0009-0002-5867-8159;;;",
        "linkedin": ";leyang-wang/;qizhen-ying-9a9758240/;;",
        "or_profile": "~Daniel_James_Williams1;~Leyang_Wang1;~Qizhen_Ying1;~Song_Liu1;~Mladen_Kolar3",
        "aff": "Weaviate;University College London;University of Oxford;University of Bristol;",
        "aff_domain": "weaviate.io;ucl.ac.uk;ox.ac.uk;bristol.ac.uk;",
        "position": "Researcher;MS student;MS student;Associate Professor;",
        "bibtex": "@inproceedings{\nwilliams2025highdimensional,\ntitle={High-Dimensional Differential Parameter Inference in Exponential Family using Time Score Matching},\nauthor={Daniel James Williams and Leyang Wang and Qizhen Ying and Song Liu and Mladen Kolar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZAblNqZMU8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZAblNqZMU8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZCWIEDEWZP",
        "title": "Variational Inference in Location-Scale Families: Exact Recovery of the Mean and Correlation Matrix",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Given an intractable target density $p$, variational inference (VI) attempts to find the best approximation $q$ from a tractable family $\\mathcal Q$. This is typically done by minimizing the exclusive Kullback-Leibler divergence, $\\text{KL}(q||p)$. In practice, $\\mathcal Q$ is not rich enough to contain $p$, and the approximation is misspecified even when it is a unique global minimizer of $\\text{KL}(q||p)$. In this paper, we analyze the robustness of VI to these misspecifications when $p$ exhibits certain symmetries and $\\mathcal Q$ is a location-scale family that shares these symmetries. We prove strong guarantees for VI not only under mild regularity conditions but also in the face of severe misspecifications. Namely, we show that (i) VI recovers the mean of $p$ when $p$ exhibits an even symmetry, and (ii) it recovers the correlation matrix of $p$ when in addition $p$ exhibits an elliptical symmetry. These guarantees hold for the mean even when $q$ is factorized and $p$ is not, and for the correlation matrix even when $q$ and $p$ behave differently in their tails. We analyze various regimes of Bayesian inference where these symmetries are useful idealizations, and we also investigate experimentally how VI behaves in their absence.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Charles Margossian;Lawrence K. Saul",
        "authorids": "~Charles_Margossian1;~Lawrence_K._Saul3",
        "gender": "M;",
        "homepage": "https://charlesm93.github.io./;",
        "dblp": ";",
        "google_scholar": "nPtLsvIAAAAJ;",
        "orcid": "0000-0002-3274-5619;",
        "linkedin": "charles-margossian-3428935b/;",
        "or_profile": "~Charles_Margossian1;~Lawrence_K._Saul3",
        "aff": "Flatiron Institute;",
        "aff_domain": "flatironinstitute.org;",
        "position": "Postdoc;",
        "bibtex": "@inproceedings{\nmargossian2025variational,\ntitle={Variational Inference in Location-Scale Families: Exact Recovery of the Mean and Correlation Matrix},\nauthor={Charles Margossian and Lawrence K. Saul},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZCWIEDEWZP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZCWIEDEWZP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZDyi1BeTu7",
        "title": "Domain Adaptation and Entanglement: an Optimal Transport Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current machine learning systems are brittle in the face of distribution shifts (DS), where the target distribution that the system is tested on differs from the source distribution used to train the system. This problem of robustness to DS has been studied extensively in the field of domain adaptation. For deep neural networks, popular methods for unsupervised domain adaptation (UDA) are domain matching methods that try to align the marginal distributions in the feature or output space. The current theoretical understanding of these methods, however, are limited and existing theoretical frameworks are not precise enough to characterize their performance in practice.  To this end, we derive new bounds based on optimal transport that analyze the UDA problem. Our new bound includes a term which we dub as entanglement, consisting of an expectation of Wasserstein distance between conditionals with respect to changing data distributions. Analysis of the entanglement term provides a novel perspective on the unoptimizable aspects of UDA. In various experiments with multiple models across several DS scenarios, we show that this term can be used to explain the varying performance of UDA algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Okan Koc;Alexander Soen;Chao-Kai Chiang;Masashi Sugiyama",
        "authorids": "~Okan_Koc1;~Alexander_Soen1;~Chao-Kai_Chiang1;~Masashi_Sugiyama1",
        "gender": "M;M;M;M",
        "homepage": ";https://alexandersoen.github.io/;;http://www.ms.k.u-tokyo.ac.jp/sugi/",
        "dblp": "173/7673;245/9661.html;34/8336;35/1228",
        "google_scholar": "Nhol8b8AAAAJ;apRX4awAAAAJ;pBQgK_YAAAAJ;https://scholar.google.co.jp/citations?user=GkYIrlIAAAAJ",
        "orcid": ";;;0000-0001-6658-6743",
        "linkedin": ";;;",
        "or_profile": "~Okan_Koc1;~Alexander_Soen1;~Chao-Kai_Chiang1;~Masashi_Sugiyama1",
        "aff": "Center for Advanced Intelligence Project, RIKEN;RIKEN+Australian National University;Tokyo University;RIKEN+The University of Tokyo",
        "aff_domain": "riken.jp;riken.jp+anu.edu.au;u-tokyo.ac.jp;riken.jp+u-tokyo.ac.jp",
        "position": "Postdoc;Intern+PhD student;Project Assistant Professor;Director+Full Professor",
        "bibtex": "@inproceedings{\nkoc2025domain,\ntitle={Domain Adaptation and Entanglement: an Optimal Transport Perspective},\nauthor={Okan Koc and Alexander Soen and Chao-Kai Chiang and Masashi Sugiyama},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZDyi1BeTu7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZDyi1BeTu7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZHk2fqJ4tk",
        "title": "BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between an exposure $X$ and outcome $Y$. An IV must affect $Y$ exclusively through $X$ and be unconfounded with $Y$. We present a framework for relaxing these assumptions with tuneable and interpretable \"budget constraints\". Our algorithm returns a feasible set of causal effects that can be identified exactly given perfect knowledge of observable covariance statistics. This feasible set might contain disconnected sets of possible solutions for the causal effect. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature. An accompanying R package, budgetIVr, is available from CRAN.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jordan Penn;Lee M. Gunderson;Gecia Bravo-Hermsdorff;Ricardo Silva;David Watson",
        "authorids": "~Jordan_Penn1;~Lee_M._Gunderson1;~Gecia_Bravo-Hermsdorff1;~Ricardo_Silva1;~David_Watson2",
        "gender": "M;M;F;M;M",
        "homepage": ";https://leemgunderson.github.io/;https://gecia.github.io/;http://www.homepages.ucl.ac.uk/~ucgtrbd/;http://dswatson.github.io",
        "dblp": "376/5675;236/6235;236/6201;42/2642-1;234/8807.html",
        "google_scholar": ";dXZ2pDsAAAAJ;Jq9GtykAAAAJ;I-ANa0QAAAAJ;BAHkyk8AAAAJ",
        "orcid": "0009-0002-3572-1724;;;;0000-0001-9632-2159",
        "linkedin": ";;;;david-watson-9707a7106/",
        "or_profile": "~Jordan_Penn1;~Lee_M._Gunderson1;~Gecia_Bravo-Hermsdorff1;~Ricardo_Silva1;~David_Watson2",
        "aff": "King's College London, University of London;;;University College London;King's College London, University of London",
        "aff_domain": "kcl.ac.uk;;;ucl.ac.uk;kcl.ac.uk",
        "position": "PhD student;;;Full Professor;Lecturer",
        "bibtex": "@inproceedings{\npenn2025budgetiv,\ntitle={Budget{IV}: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments},\nauthor={Jordan Penn and Lee M. Gunderson and Gecia Bravo-Hermsdorff and Ricardo Silva and David Watson},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZHk2fqJ4tk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZHk2fqJ4tk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZJwMfQ6W9P",
        "title": "Order-Optimal Regret with Novel Policy Gradient Approaches in Infinite-Horizon Average Reward MDPs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present two Policy Gradient-based algorithms with general parametrization in the context of infinite-horizon average reward Markov Decision Process (MDP). The first one employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{2/3})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state-of-the-art $\\tilde{\\mathcal{O}}(T^{3/4})$ regret and achieve the theoretical lower bound. We also show that the average-reward function is approximately $L$-smooth, a result that was previously assumed in earlier works.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Swetha Ganesh;Washim Uddin Mondal;Vaneet Aggarwal",
        "authorids": "~Swetha_Ganesh1;~Washim_Uddin_Mondal1;~Vaneet_Aggarwal1",
        "gender": "F;M;M",
        "homepage": ";https://home.iitk.ac.in/~wmondal/;",
        "dblp": "305/0457;201/9517.html;91/6560",
        "google_scholar": "-Crvn3cAAAAJ;https://scholar.google.co.in/citations?user=CQwhdyIAAAAJ;",
        "orcid": ";0000-0002-2385-6034;",
        "linkedin": ";;",
        "or_profile": "~Swetha_Ganesh1;~Washim_Uddin_Mondal1;~Vaneet_Aggarwal1",
        "aff": "Purdue University+Indian Institute of Science;Indian Institute of Technology, Kanpur;Purdue University",
        "aff_domain": "purdue.edu+iisc.ac.in;iitk.ac.in;purdue.edu",
        "position": "Postdoc+PhD student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nganesh2025orderoptimal,\ntitle={Order-Optimal Regret with Novel Policy Gradient Approaches in Infinite Horizon Average Reward {MDP}s},\nauthor={Swetha Ganesh and Washim Uddin Mondal and Vaneet Aggarwal},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZJwMfQ6W9P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZJwMfQ6W9P",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZT4229EUU8",
        "title": "Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we explore the theoretical properties of subspace recovery using Winsorized Principal Component Analysis (WPCA), utilizing a common data transformation technique that caps extreme values to mitigate the impact of outliers. Despite the widespread use of winsorization in various tasks of multivariate analysis, its theoretical properties, particularly for subspace recovery, have received limited attention. We provide a detailed analysis of the accuracy of WPCA, showing that increasing the number of samples while decreasing the proportion of outliers guarantees the consistency of the sample subspaces from WPCA with respect to the true population subspace. Furthermore, we establish perturbation bounds that ensure the WPCA subspace obtained from contaminated data remains close to the subspace recovered from pure data. Additionally, we extend the classical notion of breakdown points to subspace-valued statistics and derive lower bounds for the breakdown points of WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to outliers while maintaining consistency under mild assumptions. A toy example is provided to numerically illustrate the behavior of the upper bounds for perturbation bounds and breakdown points, emphasizing winsorization's utility in subspace recovery.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sangil Han;Kyoowon Kim;Sungkyu Jung",
        "authorids": "~Sangil_Han1;~Kyoowon_Kim1;~Sungkyu_Jung2",
        "gender": "M;;M",
        "homepage": ";https://github.com/qone1234-10;https://jung.snu.ac.kr/",
        "dblp": ";;86/4234",
        "google_scholar": ";;WYEkUW8AAAAJ",
        "orcid": ";;0000-0002-6023-8956",
        "linkedin": "%EC%83%81%EC%9D%BC-%ED%95%9C-64529a273/?original_referer=;;",
        "or_profile": "~Sangil_Han1;~Kyoowon_Kim1;~Sungkyu_Jung2",
        "aff": "Seoul National University;Seoul National University;Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "position": "Postdoc;MS student;Full Professor",
        "bibtex": "@inproceedings{\nhan2025subspace,\ntitle={Subspace Recovery in Winsorized {PCA}: Insights into Accuracy and Robustness},\nauthor={Sangil Han and Kyoowon Kim and Sungkyu Jung},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZT4229EUU8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZT4229EUU8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZVVIodisBN",
        "title": "Theoretical Analysis of Leave-one-out Cross Validation for Non-differentiable Penalties under High-dimensional Settings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite a large and significant body of recent work focusing on the hyperparameter tuning of regularized models in the high dimensional regime, a theoretical understanding of this problem for non-differentiable penalties such as generalized LASSO and nuclear norm is missing. In this paper we resolve this challenge. We study the hyperparameter tuning problem in the proportional high dimensional regime where both the sample size $n$ and number of features $p$ are large, and $n/p$ and the signal-to-noise ratio (per observation) remain finite. To achieve this goal, we first provide finite-sample upper bounds on the expected squared error of leave-one-out cross-validation (LO) in estimating the out-of-sample risk. Building on this result, we establish the consistency of the hyperparameter tuning method that is based on minimizing LO's estimate.\nOur simulation results confirm the accuracy and sharpness of our theoretical results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haolin Zou;Arnab Auddy;Kamiar Rahnama Rad;Arian Maleki",
        "authorids": "~Haolin_Zou1;~Arnab_Auddy1;~Kamiar_Rahnama_Rad1;~Arian_Maleki1",
        "gender": "M;M;M;M",
        "homepage": ";https://arnab-auddy.github.io;http://www.radspiral.net;https://sites.google.com/site/malekiarian/",
        "dblp": ";;;27/2939",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;w6foXDYAAAAJ;;jUt50EcAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Haolin_Zou1;~Arnab_Auddy1;~Kamiar_Rahnama_Rad1;~Arian_Maleki1",
        "aff": "Columbia University;Ohio State University, Columbus;City University of New York;",
        "aff_domain": "columbia.edu;osu.edu;cuny.edu;",
        "position": "PhD student;Assistant Professor;Associate Professor;",
        "bibtex": "@inproceedings{\nzou2025consistency,\ntitle={Consistency of Leave-one-out Cross Validation for Penalized Regression in High Dimensions},\nauthor={Haolin Zou and Arian Maleki and Arnab Auddy and Kamiar Rahnama Rad},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZVVIodisBN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZVVIodisBN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZleVCnq1tS",
        "title": "Optimal downsampling for Imbalanced Classification with Generalized Linear Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Downsampling or under-sampling is a technique that is utilized in the context of large and highly imbalanced classification models. We study optimal downsampling for imbalanced classification using generalized linear models (GLMs). We propose a pseudo maximum likelihood estimator and study its asymptotic normality in the context of increasingly imbalanced populations relative to an increasingly large sample size. \n\nWe provide theoretical guarantees for the introduced estimator. Additionally, we compute the optimal downsampling rate using a criterion that balances statistical accuracy and computational efficiency. Our numerical experiments, conducted on both synthetic and empirical data, further validate our theoretical results, and demonstrate that the introduced estimator outperforms commonly available alternatives.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yan Chen;Jose Blanchet;Krzysztof Dembczynski;Laura Fee Nern;Aaron Eliasib Flores",
        "authorids": "~Yan_Chen12;~Jose_Blanchet1;~Krzysztof_Dembczynski1;~Laura_Fee_Nern1;~Aaron_Eliasib_Flores1",
        "gender": "F;M;;F;M",
        "homepage": "https://yan-chen.me/;https://web.stanford.edu/~jblanche/;https://research.yahoo.com/researchers/kdembczynski;;",
        "dblp": ";75/5093.html;91/3569;326/1531.html;",
        "google_scholar": "YTmRNO4AAAAJ;https://scholar.google.co.in/citations?user=O24CcQQAAAAJ;https://scholar.google.pl/citations?user=SetMoyoAAAAJ;https://scholar.google.de/citations?user=cBIJYxsAAAAJ;",
        "orcid": ";;0000-0001-7477-6758;0000-0002-1429-8157;",
        "linkedin": "yan-chen-4895b1b1/;jose-blanchet;krzysztof-dembczynski-36155344/;lfee-schneider/;aaron-flores-mercado/",
        "or_profile": "~Yan_Chen12;~Jose_Blanchet1;~Krzysztof_Dembczynski1;~Laura_Fee_Nern1;~Aaron_Eliasib_Flores1",
        "aff": "Duke University;Stanford University;Yahoo Research+Politechnika Poznanska;Yahoo;Yahoo",
        "aff_domain": "duke.edu;stanford.edu;yahooinc.com+put.poznan.pl;yahoo-inc.com;yahooinc.com",
        "position": "PhD student;Professor;Senior Mgr Research+Assistant Professor;Researcher;Researcher",
        "bibtex": "@inproceedings{\nchen2025optimal,\ntitle={Optimal downsampling for Imbalanced Classification with Generalized Linear Models},\nauthor={Yan Chen and Jose Blanchet and Krzysztof Dembczynski and Laura Fee Nern and Aaron Eliasib Flores},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZleVCnq1tS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZleVCnq1tS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Zqoq7L7o2E",
        "title": "Fully Dynamic Adversarially Robust Correlation Clustering in Polylogarithmic Update Time",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the dynamic correlation clustering problem with *adaptive* edge label flips. In correlation clustering, we are given a $n$-vertex complete graph whose edges are labeled either $(+)$ or $(-)$, and the goal is to minimize the total number of $(+)$ edges between clusters and the number of $(-)$ edges within clusters. We consider the dynamic setting with adversarial robustness, in which the *adaptive* adversary can flip the label of an edge based on the current output of the algorithm. Our main result is a randomized algorithm that always maintains an $O(1)$-approximation to the optimal correlation clustering with $O(\\log^{2}{n})$ amortized update time. Prior to our work, no algorithm with $O(1)$-approximation and $\\text{polylog}{(n)}$ update time for the adversarially robust setting was known. We further validate our theoretical results with experiments on synthetic and real-world datasets with competitive empirical performances. Our main technical ingredient is an algorithm that maintains *sparse-dense decomposition* with $\\text{polylog}{(n)}$ update time, which could be of independent interest.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vladimir Braverman;Prathamesh Dharangutte;Shreyas Pai;Vihan Shah;Chen Wang",
        "authorids": "~Vladimir_Braverman1;~Prathamesh_Dharangutte1;~Shreyas_Pai1;~Vihan_Shah1;~Chen_Wang14",
        "gender": "Unspecified;M;;M;M",
        "homepage": "http://www.cs.jhu.edu/~vova/;https://prathameshtd.github.io/;https://shreyaspai.com;https://student.cs.uwaterloo.ca/~v46shah/;https://sites.google.com/view/chen-wang/home",
        "dblp": "14/4758;267/9631;200/7900;312/0996;82/4206-27",
        "google_scholar": "https://scholar.google.com.tw/citations?user=DTthB48AAAAJ;WnkSzHUAAAAJ;;https://scholar.google.ca/citations?hl=en;DnrU0k0AAAAJ",
        "orcid": ";;0000-0003-2409-7807;0009-0004-3024-9226;0000-0003-4044-9438",
        "linkedin": ";;;vihan-shah/;",
        "or_profile": "~Vladimir_Braverman1;~Prathamesh_Dharangutte1;~Shreyas_Pai1;~Vihan_Shah1;~Chen_Wang14",
        "aff": "UT Health+Rice University+Google+Department of Computer Science, Whiting School of Engineering;Rutgers University;Indian Institute of Technology, Madras;University of Waterloo;Rice University+Texas A&M University - College Station",
        "aff_domain": "uth.edu+rice.edu+google.com+cs.jhu.edu;rutgers.edu;iitm.ac.in;uwaterloo.ca;rice.edu+tamu.edu",
        "position": "Full Professor+Full Professor+Researcher+Full Professor;PhD student;Assistant Professor;PhD student;Postdoc+Postdoc",
        "bibtex": "@inproceedings{\nbraverman2025fully,\ntitle={Fully Dynamic Adversarially Robust Correlation Clustering in Polylogarithmic Update Time},\nauthor={Vladimir Braverman and Prathamesh Dharangutte and Shreyas Pai and Vihan Shah and Chen Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=Zqoq7L7o2E}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Zqoq7L7o2E",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZwHk19yMsm",
        "title": "Efficient Optimization Algorithms for Linear Adversarial Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Adversarial training can be used to learn models that are robust against perturbations. For linear models, it can be formulated as a convex optimization problem.  Compared to methods proposed in the context of deep learning, leveraging the optimization structure allows significantly faster convergence rates. Still, the use of generic convex solvers can be inefficient for large-scale problems. Here, we propose tailored optimization algorithms for the adversarial training of linear models, which render large-scale regression and classification problems more tractable. For regression problems, we propose a family of solvers based on iterative ridge regression and, for classification, a family of solvers based on projected gradient descent. The methods are based on extended variable reformulations of the original problem. We illustrate their efficiency in numerical examples.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Antonio H. Ribeiro;Thomas B. Sch\u00f6n;Dave Zachariah;Francis Bach",
        "authorids": "~Antonio_H._Ribeiro1;~Thomas_B._Sch\u00f6n1;~Dave_Zachariah1;~Francis_Bach1",
        "gender": "M;M;;M",
        "homepage": "https://antonior92.github.io/;http://user.it.uu.se/~thosc112/index.html;;http://www.di.ens.fr/~fbach",
        "dblp": "202/1699.html;85/4891;84/2663;b/FrancisRBach",
        "google_scholar": "https://scholar.google.com.br/citations?user=5t_sZdMAAAAJ;https://scholar.google.se/citations?user=FUqUC2oAAAAJ;;https://scholar.google.fr/citations?user=6PJWcFEAAAAJ",
        "orcid": "0000-0003-3632-8529;0000-0001-5183-234X;;",
        "linkedin": ";thomas-sch%C3%B6n-2b587b1/;;",
        "or_profile": "~Antonio_H._Ribeiro1;~Thomas_B._Sch\u00f6n1;~Dave_Zachariah1;~Francis_Bach1",
        "aff": "Uppsala University;Uppsala University;Uppsala University;INRIA+Ecole Normale Superieure",
        "aff_domain": "uu.se;uu.se;it.uu.se;inria.fr+ens.fr",
        "position": "Assistant Professor;Full Professor;Associate Professor;Faculty+Faculty",
        "bibtex": "@inproceedings{\nribeiro2025efficient,\ntitle={Efficient Optimization Algorithms for Linear Adversarial Training},\nauthor={Antonio H. Ribeiro and Thomas B. Sch{\\\"o}n and Dave Zachariah and Francis Bach},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZwHk19yMsm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZwHk19yMsm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZzF0IoV3nX",
        "title": "Improving N-Glycosylation and Biopharmaceutical Production Predictions Using AutoML-Built Residual Hybrid Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "N-glycosylation has many essential biological roles, and is important for biotherapeutics as it can affect drug efficacy, duration of effect, and toxicity. The prediction of N-glycosylation and other important biopharmaceutical production values have mostly been limited to mechanistic modeling. We present a residual hybrid modeling approach that integrates mechanistic modeling with machine learning to produce significantly more accurate predictions for N-glycosylation and bioproduction. For the largest dataset, the residual hybrid models have an average 736-fold reduction in testing prediction error. Furthermore, the residual hybrid models have lower prediction errors than the mechanistic models for all of the predicted variables in the datasets. We provide the automatic machine learning software used in this work, allowing reproduction and use of our software for other tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pedro Seber;Richard Braatz",
        "authorids": "~Pedro_Seber1;~Richard_Braatz1",
        "gender": ";",
        "homepage": "https://github.com/pedroseber;https://web.mit.edu/braatzgroup/",
        "dblp": "371/4079;76/603.html",
        "google_scholar": ";gEBFbiQAAAAJ",
        "orcid": "0009-0002-5085-6774;0000-0003-4304-3484",
        "linkedin": ";richardbraatz/",
        "or_profile": "~Pedro_Seber1;~Richard_Braatz1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nseber2025improving,\ntitle={Improving N-Glycosylation and Biopharmaceutical Production Predictions Using Auto{ML}-Built Residual Hybrid Models},\nauthor={Pedro Seber and Richard Braatz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ZzF0IoV3nX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZzF0IoV3nX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "a0EXk1Vc04",
        "title": "An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Learning from non-independent and non-identically distributed data poses a persistent challenge in statistical learning. In this study, we introduce data-dependent Bernstein inequalities tailored for vector-valued processes in Hilbert space. Our inequalities apply to both \nstationary and non-stationary processes and exploit the potential rapid decay of correlations between temporally separated variables to improve estimation. We demonstrate the utility of these bounds by applying them to covariance operator estimation in the Hilbert-Schmidt norm and to operator learning in dynamical systems, achieving novel risk bounds.  Finally, we perform numerical experiments to illustrate the practical implications of these bounds in both contexts.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Erfan Mirzaei;Andreas Maurer;Vladimir R Kostic;Massimiliano Pontil",
        "authorids": "~Erfan_Mirzaei1;~Andreas_Maurer1;~Vladimir_R_Kostic1;~Massimiliano_Pontil4",
        "gender": ";;M;Not Specified",
        "homepage": ";;https://vladi-iit.github.io/;https://www.iit.it/web/computational-statistics-and-machine-learning",
        "dblp": ";69/6428;94/879;",
        "google_scholar": ";;66gV7SAAAAAJ;lcOacs8AAAAJ",
        "orcid": ";;;0000-0001-9415-098X",
        "linkedin": ";;vladimir-kostic-77500652/;",
        "or_profile": "~Erfan_Mirzaei1;~Andreas_Maurer1;~Vladimir_R_Kostic1;~Massimiliano_Pontil4",
        "aff": ";;Istituto Italiano di Tecnologia+University of Novi Sad;Istituto Italiano di Tecnologia+Universit\u00e0 degli Studi di Genova, Istituto Italiano di Tecnologia+University College London+University College London, University of London",
        "aff_domain": ";;iit.it+uns.ac.rs;iit.it+iit.it+ucl.ac.uk+ucl.ac.uk",
        "position": ";;Researcher+Associate Professor;Principal Researcher+Principal Researcher+Professor+Full Professor",
        "bibtex": "@inproceedings{\nmirzaei2025an,\ntitle={An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications},\nauthor={Erfan Mirzaei and Vladimir R Kostic and Andreas Maurer and Massimiliano Pontil},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=a0EXk1Vc04}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a0EXk1Vc04",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "a4LNkhVEBg",
        "title": "Protein Fitness Landscape: Spectral Graph Theory Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we present a novel theoretical framework for analyzing and modeling protein fitness landscapes using spectral graph theory. By representing the protein sequence space as a generalized Hamming graph and studying its spectral properties, we derive a set of powerful tools for quantifying the ruggedness, epistasis, and other key characteristics of the landscape. We prove strong approximation and sampling results, showing that the landscape can be efficiently learned and optimized from limited and noisy data. Building on this foundation, we introduce Propagational Convolutional Neural Networks (PCNNs), a new class of inductive surrogate oracle. We provide rigorous theoretical guarantees on the generalization and convergence properties of PCNNs, using techniques from the Neural Tangent Kernel framework. Extensive experiments on real-world protein engineering tasks demonstrate the superiority of PCNNs over state-of-the-art methods, achieving higher fitness and better generalization from limited data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hao Zhu;Daniel M. Steinberg;Piotr Koniusz",
        "authorids": "~Hao_Zhu2;~Daniel_M._Steinberg1;~Piotr_Koniusz1",
        "gender": ";;",
        "homepage": ";;https://www.koniusz.com",
        "dblp": ";;25/8616",
        "google_scholar": ";;https://scholar.google.co.uk/citations?user=wZ7-1tUAAAAJ",
        "orcid": ";;0000-0002-6340-5289",
        "linkedin": ";;",
        "or_profile": "~Hao_Zhu2;~Daniel_M._Steinberg1;~Piotr_Koniusz1",
        "aff": ";;University of New South Wales+Australian National University+Data61, CSIRO",
        "aff_domain": ";;unsw.edu.au+anu.edu.au+data61.csiro.au",
        "position": ";;Associate Professor+Associate Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nzhu2025protein,\ntitle={Protein Fitness Landscape: Spectral Graph Theory Perspective},\nauthor={Hao Zhu and Daniel M. Steinberg and Piotr Koniusz},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=a4LNkhVEBg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a4LNkhVEBg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "a8z5Q0WSPL",
        "title": "Training LLMs with MXFP4",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications (GEMMs) and reduce training costs. \nHowever, directly using MXFP4 instead of BF16 during training significantly degrades model quality. \nIn this work, we present the first near-lossless training recipe that uses MXFP4 GEMMs, which are $2\\times$ faster than FP8 on supported hardware.\nOur key insight is to compute unbiased gradient estimates with stochastic rounding (SR), resulting in more accurate model updates.\nHowever, directly applying SR to MXFP4 can result in high variance from block-level outliers, harming convergence.\nTo overcome this, we use the random Hadamard tranform to theoretically bound the variance of SR.\nWe train GPT models up to 6.7B parameters and find that our method induces minimal degradation over mixed-precision BF16 training.\nOur recipe computes $>1/2$ the training FLOPs in MXFP4, enabling an estimated speedup of $>1.3\\times$ over FP8 and $>1.7\\times$ over BF16 during backpropagation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Albert Tseng;Tao Yu;Youngsuk Park",
        "authorids": "~Albert_Tseng1;~Tao_Yu1;~Youngsuk_Park1",
        "gender": ";M;M",
        "homepage": "https://tsengalb99.github.io/;https://ydtydr.github.io/;https://youngsuk0723.github.io/",
        "dblp": "249/9439;;88/11095",
        "google_scholar": ";lbi95bUAAAAJ;jWROvQ0AAAAJ",
        "orcid": ";;0000-0002-0970-9214",
        "linkedin": ";tao-yu-220720182/;y-park",
        "or_profile": "~Albert_Tseng1;~Tao_Yu1;~Youngsuk_Park1",
        "aff": "Cornell University;Amazon;Amazon, AWS AI Labs",
        "aff_domain": "cs.cornell.edu;amazon.com;amazon.com",
        "position": "PhD student;Researcher;Research",
        "bibtex": "@inproceedings{\ntseng2025training,\ntitle={Training {LLM}s with {MXFP}4},\nauthor={Albert Tseng and Tao Yu and Youngsuk Park},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=a8z5Q0WSPL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a8z5Q0WSPL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aGrCXoTB4P",
        "title": "Time-series attribution maps with regularized contrastive learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized contrastive learning algorithm trained on time-series data plus a new attribution method called Inverted Neuron Gradient (collectively named xCEBRA). We show theoretically that xCEBRA has favorable properties for identifying the Jacobian matrix of the data generating process. Empirically, we demonstrate robust approximation of zero vs. non-zero entries in the ground-truth attribution map on synthetic datasets, and significant improvements across previous attribution methods based on feature ablation, Shapley values, and other gradient-based methods. Our work constitutes a first example of identifiable inference of time-series attribution maps and opens avenues to a better understanding of time-series data, such as for neural dynamics and decision-processes within neural networks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Steffen Schneider;Rodrigo Gonz\u00e1lez Laiz;Anastasiia Filippova;Markus Frey;Mackenzie W Mathis",
        "authorids": "~Steffen_Schneider1;~Rodrigo_Gonz\u00e1lez_Laiz1;~Anastasiia_Filippova2;~Markus_Frey1;~Mackenzie_W_Mathis1",
        "gender": ";M;;M;F",
        "homepage": "https://stes.io;https://github.com/gonlairo;;;http://www.mackenziemathislab.org",
        "dblp": "16/8643.html;;;;218/5502",
        "google_scholar": "https://scholar.google.de/citations?user=KR5dj44AAAAJ;;;https://scholar.google.de/citations?user=BR3NilQAAAAJ;IhqY9XgAAAAJ",
        "orcid": "0000-0003-2327-6459;;;0000-0003-0291-3391;0000-0001-7368-4456",
        "linkedin": "https://linkedin.com/in/steffen-schneider;;;;",
        "or_profile": "~Steffen_Schneider1;~Rodrigo_Gonz\u00e1lez_Laiz1;~Anastasiia_Filippova2;~Markus_Frey1;~Mackenzie_W_Mathis1",
        "aff": "Helmholtz Zentrum M\u00fcnchen;Helmholtz Zentrum M\u00fcnchen;;;Swiss Federal Institute of Technology Lausanne",
        "aff_domain": "helmholtz-munich.de;helmholtz-munich.de;;;epfl.ch",
        "position": "Principal Researcher;PhD student;;;Assistant Professor",
        "bibtex": "@inproceedings{\nschneider2025timeseries,\ntitle={Time-series attribution maps with regularized contrastive learning},\nauthor={Steffen Schneider and Rodrigo Gonz{\\'a}lez Laiz and Anastasiia Filippova and Markus Frey and Mackenzie W Mathis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aGrCXoTB4P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aGrCXoTB4P",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aKuEwftRiK",
        "title": "Policy Teaching via Data Poisoning in Learning from Human Preferences",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study data poisoning attacks in learning from human preferences. More specifically, we consider the problem of teaching/enforcing a target policy $\\pi^\\dagger$ by synthesizing preference data. We seek to understand the susceptibility of different preference-based learning paradigms to poisoned preference data by analyzing the number of samples required by the attacker to enforce $\\pi^\\dagger$. We first propose a general data poisoning formulation in learning from human preferences and then study it for two popular paradigms, namely: (a) reinforcement learning from human feedback (RLHF) that operates by learning a reward model using preferences; (b) direct preference optimization (DPO) that directly optimizes policy using preferences. We conduct a theoretical analysis of the effectiveness of data poisoning in a setting where the attacker is allowed to augment a pre-existing dataset and also study its special case where the attacker can synthesize the entire preference dataset from scratch. As our main results, we provide lower/upper bounds on the number of samples required to enforce $\\pi^\\dagger$. Finally, we discuss the implications of our results in terms of the susceptibility of these learning paradigms under such data poisoning attacks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andi Nika;Jonathan N\u00f6ther;Debmalya Mandal;Parameswaran Kamalaruban;Adish Singla;Goran Radanovic",
        "authorids": "~Andi_Nika1;~Jonathan_N\u00f6ther1;~Debmalya_Mandal2;~Parameswaran_Kamalaruban2;~Adish_Singla2;~Goran_Radanovic1",
        "gender": "M;M;M;M;;",
        "homepage": "https://andinika.github.io/;;https://debmandal.github.io;https://markovkernel.net/;;",
        "dblp": "268/2761;341/1645;151/3685;164/7413;;133/1771",
        "google_scholar": "oTIFCrEAAAAJ;;OquWQpEAAAAJ;0ioRCikAAAAJ;;KBG_JlAAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";jonathan-noether-83a03323b/;;;;",
        "or_profile": "~Andi_Nika1;~Jonathan_N\u00f6ther1;~Debmalya_Mandal2;~Parameswaran_Kamalaruban2;~Adish_Singla2;~Goran_Radanovic1",
        "aff": "MPI-SWS;MPI-SWS;University of Warwick;VISA+Featurespace;;MPI-SWS",
        "aff_domain": "mpi-sws.org;mpi-sws.org;warwick.ac.uk;visa.com+featurespace.co.uk;;mpi-sws.org",
        "position": "PhD student;PhD student;Assistant Professor;Researcher+Researcher;;Research group leader",
        "bibtex": "@inproceedings{\nnika2025policy,\ntitle={Policy Teaching via Data Poisoning in Learning from Human Preferences},\nauthor={Andi Nika and Jonathan N{\\\"o}ther and Debmalya Mandal and Parameswaran Kamalaruban and Adish Singla and Goran Radanovic},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aKuEwftRiK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aKuEwftRiK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aNkzzq4clD",
        "title": "Statistical Inference for Feature Selection after Optimal Transport-based Domain Adaptation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Feature Selection (FS) under domain adaptation (DA) is a critical task in machine learning, especially when dealing with limited target data. However, existing methods lack the capability to guarantee the reliability of FS under DA. In this paper, we introduce a novel statistical method to statistically test FS reliability under DA, named SFS-DA (statistical FS-DA). The key strength of SFS-DA lies in its ability to control the false positive rate (FPR) below a pre-specified level $\\alpha$ (e.g., 0.05) while maximizing the true positive rate. Compared to the literature on statistical FS, SFS-DA presents a unique challenge in addressing the effect of DA to ensure the validity of the inference on FS results. We overcome this challenge by leveraging the Selective Inference (SI) framework. Specifically, by carefully examining the FS process under DA whose operations can be characterized by linear and quadratic inequalities, we prove that achieving FPR control in SFS-DA is indeed possible. Furthermore, we enhance the true detection rate by introducing a more strategic approach. Experiments conducted on both synthetic and real-world datasets robustly support our theoretical results, showcasing the SFS-DA\u2019s superior performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nguyen Thang Loi;Duong Tan Loc;Vo Nguyen Le Duy",
        "authorids": "23520872@gm.uit.edu.vn;23520854@gm.uit.edu.vn;~Vo_Nguyen_Le_Duy1",
        "gender": ";;M",
        "homepage": ";;http://vonguyenleduy.github.io",
        "dblp": ";;241/7008",
        "google_scholar": ";;qcpIUoQAAAAJ",
        "orcid": ";;",
        "linkedin": ";;vo-nguyen-le-duy/",
        "or_profile": "23520872@gm.uit.edu.vn;23520854@gm.uit.edu.vn;~Vo_Nguyen_Le_Duy1",
        "aff": ";;University of Information Technology, Vietnam National University - HCM",
        "aff_domain": ";;uit.edu.vn",
        "position": ";;Lecturer",
        "bibtex": "@inproceedings{\nloi2025statistical,\ntitle={Statistical Inference for Feature Selection after Optimal Transport-based Domain Adaptation},\nauthor={Nguyen Thang Loi and Duong Tan Loc and Vo Nguyen Le Duy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aNkzzq4clD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aNkzzq4clD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aQxQQw8Q1k",
        "title": "Classification of High-dimensional Time Series in Spectral Domain Using Explainable Features with Applications to Neuroimaging Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Interpretable classification of time series poses significant challenges in high dimensions. Traditional feature selection methods in the frequency domain often assume sparsity in spectral matrices (or their inverses) which can be restrictive for real-world applications. We propose a model-based approach for classifying high-dimensional stationary time series by assuming sparsity in the difference between spectra. The estimators for the model parameters are proven to be consistent under general conditions. We also introduce a method to select the most discriminatory frequencies, and it possesses the sure screening property.  The novelty of our method lies in the interpretability of the parameters hence suitable for neuroscience where understanding differences in brain network connectivity across various states is crucial. The proposed approach is tested using several simulated examples and applied to EEG and calcium imaging datasets to demonstrate its practical relevance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sarbojit Roy;Malik Shahid Sultan;Tania Reyes Vallejo;Leena Ali Ibrahim;Hernando Ombao",
        "authorids": "~Sarbojit_Roy1;~Malik_Shahid_Sultan1;tania.reyesvallejo@kaust.edu.sa;leena.ibrahim@kaust.edu.sa;~Hernando_Ombao2",
        "gender": "M;M;;;M",
        "homepage": "https://sites.google.com/view/sarbojit-roy/;https://malikshahidsultan.weebly.com/;;;https://cemse.kaust.edu.sa/stat/people/person/hernando-ombao",
        "dblp": ";;;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;dHZ2hbYAAAAJ;;;https://scholar.google.com.ph/citations?user=Cob_WwEAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Sarbojit_Roy1;~Malik_Shahid_Sultan1;tania.reyesvallejo@kaust.edu.sa;leena.ibrahim@kaust.edu.sa;~Hernando_Ombao2",
        "aff": "King Abdullah University of Science and Technology;;;;",
        "aff_domain": "kaust.edu.sa;;;;",
        "position": "Postdoc;;;;",
        "bibtex": "@inproceedings{\nroy2025classification,\ntitle={Classification of High-dimensional Time Series in Spectral Domain Using Explainable Features with Applications to Neuroimaging Data},\nauthor={Sarbojit Roy and Malik Shahid Sultan and Tania Reyes Vallejo and Leena Ali Ibrahim and Hernando Ombao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aQxQQw8Q1k}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aQxQQw8Q1k",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aUbv69Lbqt",
        "title": "Superiority of Multi-Head Attention: A Theoretical Study in Shallow Transformers in In-Context Linear Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing theoretical literature predominantly focuses on providing convergence upper bounds to show that trained transformers with single-/multi-head attention can obtain a good in-context learning performance, our research centers on comparing the exact convergence of single- and multi-head attention more rigorously. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention.\nWhen the number of in-context examples $D$ increases, the prediction loss using single-/multi-head attention is in $O(1/D)$, and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, our technical framework in calculating the exact convergence further facilitates studying more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head attention in the transformer architecture.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yingqian Cui;Jie Ren;Pengfei He;Hui Liu;Jiliang Tang;Yue Xing",
        "authorids": "~Yingqian_Cui1;~Jie_Ren6;~Pengfei_He2;~Hui_Liu8;~Jiliang_Tang1;~Yue_Xing1",
        "gender": "F;M;M;F;M;",
        "homepage": "https://yingqiancui.github.io/;https://renjie3.github.io/;https://pengfeihepower.github.io/;https://scholar.google.com/citations?user=EuzF_zsAAAAJ&hl=en;https://www.cse.msu.edu/~tangjili/;https://sites.google.com/site/xingyuecuhk/",
        "dblp": ";181/2887-19.html;37/10219-2;93/4010-31;64/10812;185/5744-2.html",
        "google_scholar": "3p67r08AAAAJ;;nsSrd6kAAAAJ;;WtzKMWAAAAAJ;",
        "orcid": ";;;0000-0002-3555-3495;0000-0001-7125-3898;",
        "linkedin": ";;;;;",
        "or_profile": "~Yingqian_Cui1;~Jie_Ren6;~Pengfei_He2;~Hui_Liu8;~Jiliang_Tang1;~Yue_Xing1",
        "aff": "Amazon+Michigan State University;Amazon+Michigan State University;Michigan State University;Michigan State University;Michigan State University;Michigan State University",
        "aff_domain": "amazon.com+msu.edu;amazon.com+msu.edu;msu.edu;msu.edu;msu.edu;msu.edu",
        "position": "Applied Scientist Intern+PhD student;Intern+PhD student;PhD student;Assistant Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncui2025superiority,\ntitle={Superiority of Multi-Head Attention: A Theoretical Study in Shallow Transformers in In-Context Linear Regression},\nauthor={Yingqian Cui and Jie Ren and Pengfei He and Hui Liu and Jiliang Tang and Yue Xing},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aUbv69Lbqt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aUbv69Lbqt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aYK2s9W6UF",
        "title": "Beyond Size-Based Metrics: Measuring Task-Specific Complexity in Symbolic Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Symbolic regression (SR) is a machine learning approach aimed at discovering mathematical closed-form expressions that best fit a given dataset. Traditional complexity measures in SR, such as the number of terms or expression tree depth, often fail to capture the difficulty of specific analytical tasks a user might need to perform. In this paper, we introduce a new complexity measure designed to quantify the difficulty of conducting single-feature global perturbation analysis (SGPA)\u2014a type of analysis commonly applied in fields like physics and risk scoring to understand the global impact of perturbing individual input features. We present a unified mathematical framework that formalizes and generalizes these established practices, providing a precise method to assess how challenging it is to apply SGPA to different closed-form equations. This approach enables the definition of novel complexity metrics and constraints directly tied to this practical analytical task. Additionally, we establish a reconstruction theorem, offering potential insights for developing future optimization techniques in SR.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Krzysztof Kacprzyk;Mihaela van der Schaar",
        "authorids": "~Krzysztof_Kacprzyk1;~Mihaela_van_der_Schaar2",
        "gender": ";F",
        "homepage": ";https://www.vanderschaar-lab.com",
        "dblp": ";",
        "google_scholar": ";DZ3S--MAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Krzysztof_Kacprzyk1;~Mihaela_van_der_Schaar2",
        "aff": ";University of Cambridge+University of California, Los Angeles",
        "aff_domain": ";cam.ac.uk+ucla.edu",
        "position": ";Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nkacprzyk2025beyond,\ntitle={Beyond Size-Based Metrics: Measuring Task-Specific Complexity in Symbolic Regression},\nauthor={Krzysztof Kacprzyk and Mihaela van der Schaar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=aYK2s9W6UF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aYK2s9W6UF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "abTZjcI8pt",
        "title": "Meta-learning Task-specific Regularization Weights for Few-shot Linear Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a few-shot learning method for linear regression, which learns how to choose regularization weights from multiple tasks with different feature spaces, and uses the knowledge for unseen tasks. Linear regression is ubiquitous in a wide variety of fields. Although regularization weight tuning is crucial to performance, it is difficult when only a small amount of training data are available. In the proposed method, task-specific regularization weights are generated using a neural network-based model by taking a task-specific training dataset as input, where our model is shared across all tasks. For each task, linear coefficients are optimized by minimizing the squared loss with an L2 regularizer using the generated regularization weights and the training dataset. Our model is meta-learned by minimizing the expected test error of linear regression with the task-specific coefficients using various training datasets. In our experiments using synthetic and real-world datasets, we demonstrate the effectiveness of the proposed method on few-shot regression tasks compared with existing methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tomoharu Iwata;Atsutoshi Kumagai;Yasutoshi Ida",
        "authorids": "~Tomoharu_Iwata1;~Atsutoshi_Kumagai2;~Yasutoshi_Ida1",
        "gender": "M;M;M",
        "homepage": "http://www.kecl.ntt.co.jp/as/members/iwata/;https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ&hl=ja;http://yasutoshi.github.io/",
        "dblp": "29/5953;178/8630;120/6855",
        "google_scholar": "S1F-gScAAAAJ;https://scholar.google.co.jp/citations?user=Q_d8GEIAAAAJ;https://scholar.google.co.jp/citations?user=HFLzlEgAAAAJ",
        "orcid": ";0000-0002-2915-4615;0000-0003-4279-9503",
        "linkedin": "tomoharu-iwata-025a493;;",
        "or_profile": "~Tomoharu_Iwata1;~Atsutoshi_Kumagai2;~Yasutoshi_Ida1",
        "aff": "NTT;NTT;NTT",
        "aff_domain": "hco.ntt.co.jp;ntt.co.jp;ntt.co.jp",
        "position": "Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\niwata2025metalearning,\ntitle={Meta-learning Task-specific Regularization Weights for Few-shot Linear Regression},\nauthor={Tomoharu Iwata and Atsutoshi Kumagai and Yasutoshi Ida},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=abTZjcI8pt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=abTZjcI8pt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ag4zg75Wyc",
        "title": "LITE: Efficiently Estimating Gaussian Probability of Maximality",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of computing the *probability of maximality* (PoM) of a Gaussian random vector, i.e., the probability for each dimension to be maximal. This is a key challenge in applications ranging from Bayesian optimization to reinforcement learning, where the PoM not only helps with finding an optimal action, but yields a fine-grained analysis of the action domain, crucial in tasks such as drug discovery. Existing techniques are costly, scaling polynomially in computation and memory with the vector size. We introduce LITE, the first approach for estimating Gaussian PoM with *almost-linear time and memory* complexity. LITE achieves SOTA accuracy on a number of tasks, while being in practice several orders of magnitude faster than the baselines. This also translates to a better performance on downstream tasks such as entropy estimation and optimal control of bandits. Theoretically, we cast LITE as entropy-regularized UCB and connect it to prior PoM estimators.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicolas Menet;Jonas H\u00fcbotter;Parnian Kassraie;Andreas Krause",
        "authorids": "~Nicolas_Menet1;~Jonas_H\u00fcbotter1;~Parnian_Kassraie1;~Andreas_Krause1",
        "gender": "M;M;F;M",
        "homepage": ";https://jonhue.github.io;https://pkassraie.github.io;https://las.inf.ethz.ch/krausea",
        "dblp": "363/3289;300/4583;216/8534.html;87/1831-1.html",
        "google_scholar": ";pxi_RkwAAAAJ;GFDOkb0AAAAJ;https://scholar.google.ch/citations?user=eDHv58AAAAAJ",
        "orcid": ";;;0000-0001-7260-9673",
        "linkedin": "nicolas-andrin-menet;jonhue/;parnian-kassraie/;krausea/",
        "or_profile": "~Nicolas_Menet1;~Jonas_H\u00fcbotter1;~Parnian_Kassraie1;~Andreas_Krause1",
        "aff": "ETHZ - ETH Zurich;ETH Zurich;Swiss Federal Institute of Technology+Carnegie Mellon University;ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch+cmu.edu;ethz.ch",
        "position": "PhD student;PhD student;PhD student+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nmenet2025lite,\ntitle={{LITE}: Efficiently Estimating Gaussian Probability of Maximality},\nauthor={Nicolas Menet and Jonas H{\\\"u}botter and Parnian Kassraie and Andreas Krause},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ag4zg75Wyc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ag4zg75Wyc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "axbN98w6pe",
        "title": "Epistemic Uncertainty and Excess Risk in Variational Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian inference is widely used in practice due to its ability to assess epistemic uncertainty (EU) in predictions. However, its computational complexity necessitates the use of approximation methods, such as variational inference (VI). When estimating EU within the VI framework, metrics such as the variance of the posterior predictive distribution and conditional mutual information are commonly employed. Despite their practical importance, these metrics lack comprehensive theoretical analysis. In this paper, we investigate these EU metrics by providing their novel relationship to excess risk, which allows for a convergence analysis based on PAC-Bayesian theory. Based on these analyses, we then demonstrate that some existing objective functions of VI  regularize EU metrics in different ways leading to different performance in EU evaluation. Finally, we propose a novel objective function for VI that directly optimizes both prediction and EU under the PAC-Bayesian framework. Experimental results indicate that our algorithm significantly improves EU estimation compared to existing VI methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Futoshi Futami",
        "authorids": "~Futoshi_Futami1",
        "gender": "M",
        "homepage": "",
        "dblp": "209/4960",
        "google_scholar": "https://scholar.google.co.jp/citations?user=WTOG0mMAAAAJ",
        "orcid": "",
        "linkedin": "",
        "or_profile": "~Futoshi_Futami1",
        "aff": "Osaka University",
        "aff_domain": "osaka-u.ac.jp",
        "position": "Lecturer",
        "bibtex": "@inproceedings{\nfutami2025epistemic,\ntitle={Epistemic Uncertainty and Excess Risk in Variational Inference},\nauthor={Futoshi Futami},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=axbN98w6pe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=axbN98w6pe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "axgerltoda",
        "title": "Differentially Private Kernelized Contextual Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of contextual kernel bandits with stochastic contexts, where the underlying reward function belongs to a known Reproducing Kernel Hilbert Space (RKHS). We study this problem under the additional constraint of joint differential privacy, where the agents needs to ensure that the sequence of query points is differentially private with respect to both the sequence of contexts and rewards. We propose a novel algorithm that improves upon the state of the art  and achieves an error rate of $\\mathcal{O}\\left(\\sqrt{\\dfrac{\\gamma_T}{T}} + \\dfrac{\\gamma_T}{T \\varepsilon}\\right)$ after $T$ queries for a large class of kernel families, where $\\gamma_T$ represents the effective dimensionality of the kernel and $\\varepsilon > 0$ is the privacy parameter. Our results are based on novel estimator for the reward function that simultaneously enjoys high utility along with a low-sensitivity to observed rewards and contexts, which is crucial to obtain an improved performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nikola Pavlovic;Sudeep Salgia;Qing Zhao",
        "authorids": "~Nikola_Pavlovic1;~Sudeep_Salgia1;~Qing_Zhao1",
        "gender": "M;M;F",
        "homepage": ";https://sudeepsalgia.github.io/;https://zhao.ece.cornell.edu/",
        "dblp": ";207/8460;",
        "google_scholar": ";Y5d5L84AAAAJ;ymsLVFsAAAAJ",
        "orcid": "0009-0009-8532-6504;;",
        "linkedin": ";;",
        "or_profile": "~Nikola_Pavlovic1;~Sudeep_Salgia1;~Qing_Zhao1",
        "aff": "Cornell University;Carnegie Mellon University;Cornell University",
        "aff_domain": "cornell.edu;cmu.edu;cornell.edu",
        "position": "PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\npavlovic2025differentially,\ntitle={Differentially Private Kernelized Contextual Bandits},\nauthor={Nikola Pavlovic and Sudeep Salgia and Qing Zhao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=axgerltoda}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=axgerltoda",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "b2YideB71N",
        "title": "Independent Learning in Performative Markov Potential Games",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. \nIn this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rilind Sahitaj;Paulius Sasnauskas;Yi\u011fit Yal\u0131n;Debmalya Mandal;Goran Radanovic",
        "authorids": "~Rilind_Sahitaj1;~Paulius_Sasnauskas1;~Yi\u011fit_Yal\u0131n1;~Debmalya_Mandal2;~Goran_Radanovic1",
        "gender": "M;M;M;M;",
        "homepage": "https://www.cs.cit.tum.de/en/dss/members/rilind-sahitaj/;;;https://debmandal.github.io;",
        "dblp": ";;359/8703;151/3685;133/1771",
        "google_scholar": ";wNOrVPkAAAAJ;a37NuYUAAAAJ;OquWQpEAAAAJ;KBG_JlAAAAAJ",
        "orcid": ";0000-0002-6379-013X;0009-0009-7824-4121;;",
        "linkedin": ";;yigityalin/;;",
        "or_profile": "~Rilind_Sahitaj1;~Paulius_Sasnauskas1;~Yi\u011fit_Yal\u0131n1;~Debmalya_Mandal2;~Goran_Radanovic1",
        "aff": "Technical University of Munich+RWTH Aachen;MPI-SWS;MPI-SWS;University of Warwick;MPI-SWS",
        "aff_domain": "tum.de+algo.rwth-aachen.de;mpi-sws.org;mpi-sws.org;warwick.ac.uk;mpi-sws.org",
        "position": "PhD student+PhD student;Intern;PhD student;Assistant Professor;Research group leader",
        "bibtex": "@inproceedings{\nsahitaj2025independent,\ntitle={Independent Learning in Performative Markov Potential Games},\nauthor={Rilind Sahitaj and Paulius Sasnauskas and Yi{\\u{g}}it Yal{\\i}n and Debmalya Mandal and Goran Radanovic},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=b2YideB71N}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=b2YideB71N",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bDuNZJ6O8N",
        "title": "On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work investigates the theoretical boundaries of creating publicly-detectable schemes to enable the provenance of watermarked imagery. Metadata-based approaches like C2PA provide unforgeability and public-detectability. ML techniques offer robust retrieval and watermarking. However, no existing scheme combines robustness, unforgeability, and public-detectability. In this work, we formally define such a scheme and establish its existence. Although theoretically possible, we find that at present, it is intractable to build certain components of our scheme without a leap in deep learning capabilities. We analyze these limitations and propose research directions that need to be addressed before we can practically realize robust and publicly-verifiable provenance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jaiden Fairoze;Guillermo Ortiz-Jimenez;Mel Vecerik;Somesh Jha;Sven Gowal",
        "authorids": "~Jaiden_Fairoze1;~Guillermo_Ortiz-Jimenez1;~Mel_Vecerik1;~Somesh_Jha1;~Sven_Gowal2",
        "gender": "M;;;M;",
        "homepage": "https://www.jaiden.info/;http://gortizji.github.io;https://sites.google.com/corp/view/2020-s3k/home;;",
        "dblp": "277/8240.html;222/2737;;j/SomeshJha;",
        "google_scholar": "cX5JJXYAAAAJ;xAsJnG0AAAAJ;;BaI7l8QAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "jaidenkf/;;;;",
        "or_profile": "~Jaiden_Fairoze1;~Guillermo_Ortiz-Jimenez1;~Mel_Vecerik1;~Somesh_Jha1;~Sven_Gowal2",
        "aff": "Electrical Engineering & Computer Science Department, University of California, Berkeley;Google DeepMind;University College London+Google DeepMind;Department of Computer Science, University of Wisconsin, Madison;",
        "aff_domain": "eecs.berkeley.edu;google.com;ucl.ac.uk+deepmind.com;cs.wisc.edu;",
        "position": "PhD student;Research Scientist;PhD student+Researcher;Full Professor;",
        "bibtex": "@inproceedings{\nfairoze2025on,\ntitle={On the (Im)possibility of Constructing a Robust and Publicly-Detectable Watermark},\nauthor={Jaiden Fairoze and Guillermo Ortiz-Jimenez and Mel Vecerik and Somesh Jha and Sven Gowal},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bDuNZJ6O8N}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bDuNZJ6O8N",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bEg5CZDbLA",
        "title": "Task Shift: From Classification to Regression in Overparameterized Linear Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern machine learning methods have recently demonstrated remarkable capability to generalize under task shift, where latent knowledge is transferred to a different, often more difficult, task under a similar data distribution. We investigate this phenomenon in an overparameterized linear regression setting where the task shifts from classification during training to regression during evaluation. In the zero-shot case, wherein no regression data is available, we prove that task shift is impossible in both sparse signal and random signal models for any Gaussian covariate distribution. In the few-shot case, wherein limited regression data is available, we propose a simple postprocessing algorithm which asymptotically recovers the ground-truth predictor. Our analysis leverages a fine-grained characterization of individual parameters arising from minimum-norm interpolation which may be of independent interest. Our results show that while minimum-norm interpolators for classification cannot transfer to regression a priori, they experience surprisingly structured attenuation which enables successful task shift with limited additional data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tyler LaBonte;Kuo-Wei Lai;Vidya Muthukumar",
        "authorids": "~Tyler_LaBonte1;~Kuo-Wei_Lai1;~Vidya_Muthukumar3",
        "gender": "M;M;F",
        "homepage": "https://tyler-labonte.com;https://kuoweilai.com;https://vmuthukumar.ece.gatech.edu",
        "dblp": "251/5689.html;;149/0019",
        "google_scholar": "0_bKeg4AAAAJ;4xx3pdoAAAAJ;K2OEs2YAAAAJ",
        "orcid": "0000-0002-3781-7212;;",
        "linkedin": "https://linkedin.com/in/tmlabonte;;",
        "or_profile": "~Tyler_LaBonte1;~Kuo-Wei_Lai1;~Vidya_Muthukumar3",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nlabonte2025task,\ntitle={Task Shift: From Classification to Regression in Overparameterized Linear Models},\nauthor={Tyler LaBonte and Kuo-Wei Lai and Vidya Muthukumar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bEg5CZDbLA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bEg5CZDbLA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bHHk0288T8",
        "title": "Hyperbolic Prototypical Entailment Cones for Image Classification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Non-Euclidean geometries have garnered significant research interest, particularly in their application to Deep Learning. Utilizing specific manifolds as embedding spaces has been shown to enhance neural network representational capabilities by aligning these spaces with the data's latent structure. In this paper, we focus on hyperbolic manifolds and introduce a novel framework, Hyperbolic Prototypical Entailment Cones (HPEC). The core innovation of HPEC lies in utilizing angular relationships, rather than traditional distance metrics, to more effectively capture the similarity between data representations and their corresponding prototypes. This is achieved by leveraging hyperbolic entailment cones, a mathematical construct particularly suited for embedding hierarchical structures in the Poincare' Ball, along with a novel Backclip mechanism. Our experimental results demonstrate that this approach significantly enhances performance in high-dimensional embedding spaces. To substantiate these findings, we evaluate HPEC on four diverse datasets across various embedding dimensions, consistently surpassing state-of-the-art methods in Prototype Learning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Samuele Fonio;Roberto Esposito;Marco Aldinucci",
        "authorids": "~Samuele_Fonio1;~Roberto_Esposito1;~Marco_Aldinucci1",
        "gender": "M;M;M",
        "homepage": "https://alpha.di.unito.it/samuele-fonio/;http://informatica.unito.it/do/docenti.pl/Alias?roberto.esposito#tab-profilo;https://alpha.di.unito.it/marco-aldinucci",
        "dblp": "368/3506;50/6005;75/5249.html",
        "google_scholar": ";aBkfc1UAAAAJ;https://scholar.google.it/citations?user=Ip44EZwAAAAJ",
        "orcid": "0009-0003-1870-4233;0000-0001-5366-292X;0000-0001-8788-0829",
        "linkedin": ";;",
        "or_profile": "~Samuele_Fonio1;~Roberto_Esposito1;~Marco_Aldinucci1",
        "aff": "University of Turin;University of Turin;",
        "aff_domain": "unito.it;unito.it;",
        "position": "PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\nfonio2025hyperbolic,\ntitle={Hyperbolic Prototypical Entailment Cones for Image Classification},\nauthor={Samuele Fonio and Roberto Esposito and Marco Aldinucci},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bHHk0288T8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bHHk0288T8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bIfkjlthS0",
        "title": "Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution lies in introducing novel proof techniques that yield tighter bounds for multi-armed BAI compared to existing approaches. Our work provides new insights into Bayesian fixed-budget BAI in structured bandits, and extensive experiments demonstrate the consistent and robust performance of our method in practice across various settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicolas Nguyen;Imad Aouali;Andr\u00e1s Gy\u00f6rgy;Claire Vernade",
        "authorids": "~Nicolas_Nguyen1;~Imad_Aouali2;~Andr\u00e1s_Gy\u00f6rgy2;~Claire_Vernade1",
        "gender": "M;;;F",
        "homepage": ";;;https://www.cvernade.com",
        "dblp": ";;;168/8721",
        "google_scholar": ";;;tE2hCaYAAAAJ",
        "orcid": ";;;",
        "linkedin": "https://www.linkedin.com/authwall?trk=bf&trkInfo=AQFbdqfhCHV3PQAAAY1lnv3YfmeRmOfN5EVbzkYUxxHLAorfxe6CeJAsv5d_AFS4G33XVigRWJlATQQP-e49HxqZFJMl3QKjYIHTF0ezLsjYHcEKGZi3pmSVRazVxhIqHUqntoI=&original_referer=&sessionRedirect=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fnicolas-nguyen-61954119a;;;",
        "or_profile": "~Nicolas_Nguyen1;~Imad_Aouali2;~Andr\u00e1s_Gy\u00f6rgy2;~Claire_Vernade1",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen;;;Eberhard-Karls-Universit\u00e4t T\u00fcbingen",
        "aff_domain": "uni-tuebingen.de;;;uni-tuebingen.de",
        "position": "PhD student;;;Assistant Professor",
        "bibtex": "@inproceedings{\nnguyen2025priordependent,\ntitle={Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits},\nauthor={Nicolas Nguyen and Imad Aouali and Andr{\\'a}s Gy{\\\"o}rgy and Claire Vernade},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bIfkjlthS0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bIfkjlthS0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bLiWzjcG1W",
        "title": "Natural Language Counterfactual Explanations for Graphs Using Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical area of research to unravel the opaque inner logic of (deep) machine learning models. \nAmong the various XAI techniques proposed in the literature, counterfactual explanations stand out as one of the most promising approaches. However, these ``what-if'' explanations are frequently complex and technical, making them difficult for non-experts to understand and, more broadly, challenging for humans to interpret.\nTo bridge this gap, in this work, we exploit the power of open-source Large Language Models to generate natural language explanations when prompted with valid counterfactual instances produced by state-of-the-art explainers for graph-based models. \nExperiments across several graph datasets and counterfactual explainers show that our approach effectively produces accurate natural language representations of counterfactual instances, as demonstrated by key performance metrics",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Flavio Giorgi;Cesare Campagnano;Fabrizio Silvestri;Gabriele Tolomei",
        "authorids": "~Flavio_Giorgi1;~Cesare_Campagnano1;~Fabrizio_Silvestri2;~Gabriele_Tolomei1",
        "gender": "M;M;M;M",
        "homepage": "https://www.flaviogiorgi.com/;https://caesar.one;https://sites.google.com/diag.uniroma1.it/fabriziosilvestri;https://www.di.uniroma1.it/~tolomei",
        "dblp": "338/0062;289/6971.html;s/FabrizioSilvestri;72/7456",
        "google_scholar": "5dFQkoMAAAAJ;k113XQIAAAAJ;pi985dQAAAAJ;Y2R2DXEAAAAJ",
        "orcid": "0009-0003-6654-4879;0000-0002-8362-274X;0000-0001-7669-9055;0000-0001-7471-6659",
        "linkedin": "flavio-giorgi/;caesar-one/;fabrizio-silvestri-a6b0391/;",
        "or_profile": "~Flavio_Giorgi1;~Cesare_Campagnano1;~Fabrizio_Silvestri2;~Gabriele_Tolomei1",
        "aff": "University of Roma \"La Sapienza\";Sapienza University of Rome;Sapienza University of Rome;Sapienza University of Rome",
        "aff_domain": "uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it",
        "position": "PhD student;Postdoc;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\ngiorgi2025natural,\ntitle={Natural Language Counterfactual Explanations for Graphs Using Large Language Models},\nauthor={Flavio Giorgi and Cesare Campagnano and Fabrizio Silvestri and Gabriele Tolomei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bLiWzjcG1W}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bLiWzjcG1W",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bee2G6pEh0",
        "title": "SemlaFlow -- Efficient 3D Molecular Generation with Latent Attention and Equivariant Flow Matching",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Methods for jointly generating molecular graphs along with their 3D conformations have gained prominence recently due to their potential impact on structure-based drug design. Current approaches, however, often suffer from very slow sampling times or generate molecules with poor chemical validity. Addressing these limitations, we propose Semla, a scalable E(3)-equivariant message passing architecture. We further introduce an unconditional 3D molecular generation model, SemlaFlow, which is trained using equivariant flow matching to generate a joint distribution over atom types, coordinates, bond types and formal charges. Our model produces state-of-the-art results on benchmark datasets with as few as 20 sampling steps, corresponding to a two order-of-magnitude speedup compared to state-of-the-art. Furthermore, we highlight limitations of current evaluation methods for 3D generation and propose new benchmark metrics for unconditional molecular generators. Finally, using these new metrics, we compare our model's ability to generate high quality samples against current approaches and further demonstrate SemlaFlow's strong performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ross Irwin;Alessandro Tibo;Jon Paul Janet;Simon Olsson",
        "authorids": "~Ross_Irwin1;~Alessandro_Tibo1;~Jon_Paul_Janet1;~Simon_Olsson1",
        "gender": ";;M;Not Specified",
        "homepage": ";;https://jpjanet.io/;http://www.cse.chalmers.se/~simonols/",
        "dblp": ";;;",
        "google_scholar": ";;https://scholar.google.se/citations?hl=en;",
        "orcid": ";;0000-0001-7825-4797;",
        "linkedin": ";;;",
        "or_profile": "~Ross_Irwin1;~Alessandro_Tibo1;~Jon_Paul_Janet1;~Simon_Olsson1",
        "aff": ";;AstraZeneca;Chalmers University of Technology and University of Gothenburg",
        "aff_domain": ";;astrazeneca.com;chalmers.se",
        "position": ";;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nirwin2025semlaflow,\ntitle={SemlaFlow -- Efficient 3D Molecular Generation with Latent Attention and Equivariant Flow Matching},\nauthor={Ross Irwin and Alessandro Tibo and Jon Paul Janet and Simon Olsson},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bee2G6pEh0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bee2G6pEh0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bppVexkY5N",
        "title": "Fundamental Limits of Perfect Concept Erasure",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Concept erasure is the task of erasing information about a concept (e.g., gender or race) from a representation set while retaining the maximum possible utility -- information from original representations. Concept erasure is useful in several applications, such as removing sensitive concepts to achieve fairness and interpreting the impact of specific concepts on a model's performance. Previous concept erasure techniques have prioritized robustly erasing concepts over retaining the utility of the resultant representations. However, there seems to be an inherent tradeoff between erasure and retaining utility, making it unclear how to achieve perfect concept erasure while maintaining high utility. In this paper, we offer a fresh perspective toward solving this problem by quantifying the fundamental limits of concept erasure through an information-theoretic lens. Using these results, we investigate constraints on the data distribution and the erasure functions required to achieve the limits of perfect concept erasure. Empirically, we show that the derived erasure functions achieve the optimal theoretical bounds. Additionally, we show that our approach outperforms existing methods on a range of synthetic and real-world datasets using GPT-4 representations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Somnath Basu Roy Chowdhury;Kumar Avinava Dubey;Ahmad Beirami;Rahul Kidambi;Nicholas Monath;Amr Ahmed;Snigdha Chaturvedi",
        "authorids": "~Somnath_Basu_Roy_Chowdhury3;~Kumar_Avinava_Dubey1;~Ahmad_Beirami1;~Rahul_Kidambi1;~Nicholas_Monath1;~Amr_Ahmed1;~Snigdha_Chaturvedi2",
        "gender": ";;M;;M;M;F",
        "homepage": "https://www.cs.unc.edu/~somnath/;;https://beirami.github.io/;;https://nmonath.github.io/;https://research.google/people/AmrAhmed/;https://sites.google.com/site/snigdhac/",
        "dblp": "190/7535;;41/9367;;131/4309;49/2951;77/8700",
        "google_scholar": "https://scholar.google.co.in/citations?user=xGbyrIUAAAAJ;;VuKWbMMAAAAJ;;PTfhfCQAAAAJ;ivUi2T0AAAAJ;gZD3EesAAAAJ",
        "orcid": ";;;;0000-0002-5135-2423;;",
        "linkedin": ";;ahmad-beirami-97001962;;nicholas-monath-8627581aa/;amr-ahmed-b998965/;",
        "or_profile": "~Somnath_Basu_Roy_Chowdhury3;~Kumar_Avinava_Dubey1;~Ahmad_Beirami1;~Rahul_Kidambi1;~Nicholas_Monath1;~Amr_Ahmed1;~Snigdha_Chaturvedi2",
        "aff": "Google Research+Department of Computer Science, University of North Carolina, Chapel Hill;;Google+Massachusetts Institute of Technology;;Meta+Google;;Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "google.com+cs.unc.edu;;google.com+mit.edu;;meta.com+google.com;;cs.unc.edu",
        "position": "Research Scientist+PhD student;;Research Scientist+Research Affiliate;;Researcher+Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\nchowdhury2025fundamental,\ntitle={Fundamental Limits of Perfect Concept Erasure},\nauthor={Somnath Basu Roy Chowdhury and Kumar Avinava Dubey and Ahmad Beirami and Rahul Kidambi and Nicholas Monath and Amr Ahmed and Snigdha Chaturvedi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bppVexkY5N}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bppVexkY5N",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bqKRG6Pn0t",
        "title": "Understanding the Effect of GCN Convolutions in Regression Tasks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graph Convolutional Networks (GCNs) have become a pivotal method in machine learning for modeling functions over graphs. Despite their widespread success across various applications, their statistical properties (e.g., consistency, convergence rates) remain ill-characterized. To begin addressing this knowledge gap, we consider networks for which the graph structure implies that neighboring nodes exhibit similar signals and provide statistical theory for the impact of convolution operators. Focusing on estimators based solely on neighborhood aggregation, we examine how two common convolutions\u2014the original GCN and GraphSAGE convolutions\u2014affect the learning error as a function of the neighborhood topology and the number of convolutional layers. We explicitly characterize the bias variance type trade-off incurred by GCNs as a function of the neighborhood size and identify specific graph topologies where convolution operators are less effective. Our theoretical findings are corroborated by synthetic experiments, and provide a start to a deeper quantitative understanding of convolutional effects in GCNs for offering rigorous guidelines for practitioners.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juntong Chen;Johannes Schmidt-Hieber;Claire Donnat;Olga Klopp",
        "authorids": "juntong.chen@utwente.nl;~Johannes_Schmidt-Hieber1;~Claire_Donnat1;olga.klopp@math.cnrs.fr",
        "gender": ";M;F;",
        "homepage": ";https://jschmidthieber.personalweb.utwente.nl/;https://donnate.github.io/;",
        "dblp": ";205/3078;196/2978;",
        "google_scholar": ";https://scholar.google.de/citations?user=gnpVAPAAAAAJ;dOouAwUAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "juntong.chen@utwente.nl;~Johannes_Schmidt-Hieber1;~Claire_Donnat1;olga.klopp@math.cnrs.fr",
        "aff": ";University of Twente;University of Chicago;",
        "aff_domain": ";utwente.nl;uchicago.edu;",
        "position": ";Full Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nchen2025understanding,\ntitle={Understanding the effect of convolutions in {GCN} regression},\nauthor={Juntong Chen and Johannes Schmidt-Hieber and Claire Donnat and Olga Klopp},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bqKRG6Pn0t}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bqKRG6Pn0t",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bwrd3y84te",
        "title": "Paths and Ambient Spaces in Neural Loss Landscapes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding the structure of neural network loss surfaces, particularly the emergence of low-loss tunnels, is critical for advancing neural network theory and practice. In this paper, we propose a novel approach to directly embed loss tunnels into the loss landscape of neural networks. Exploring the properties of these loss tunnels offers new insights into their length and structure and sheds light on some common misconceptions. We then apply our approach to Bayesian neural networks, where we improve subspace inference by identifying pitfalls and proposing a more natural prior that better guides the sampling procedure.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Dold;Julius Kobialka;Nicolai Palm;Emanuel Sommer;David R\u00fcgamer;Oliver D\u00fcrr",
        "authorids": "~Daniel_Dold1;~Julius_Kobialka1;~Nicolai_Palm1;~Emanuel_Sommer1;~David_R\u00fcgamer1;~Oliver_D\u00fcrr1",
        "gender": ";M;M;M;M;M",
        "homepage": "https://www.ios.htwg-konstanz.de/machine-learning-people#;;https://nicolaipalm.github.io;https://www.muniq.ai/;https://davidruegamer.github.io/;https://www.htwg-konstanz.de/hochschule/fakultaeten/informatik/orga/professoren/duerr",
        "dblp": ";372/4558;;;220/5560;",
        "google_scholar": "https://scholar.google.de/citations?user=4NxWqvEAAAAJ;2qTJ3XYAAAAJ;https://scholar.google.de/citations?user=yLb7pfYAAAAJ;https://scholar.google.de/citations?user=qa2P1tYAAAAJ;https://scholar.google.de/citations?user=_DYguksAAAAJ;",
        "orcid": ";0009-0000-6729-3723;;;;",
        "linkedin": ";;;emanuelsommer/;;",
        "or_profile": "~Daniel_Dold1;~Julius_Kobialka1;~Nicolai_Palm1;~Emanuel_Sommer1;~David_R\u00fcgamer1;~Oliver_D\u00fcrr1",
        "aff": "Fachhochschule Konstanz, Hochschule f\u00fcr Technik, Wirtschaft und Gestaltung;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;LMU Munich;Htwg Konstanz",
        "aff_domain": "fh-konstanz.de;lmu.de;lmu.de;lmu.de;lmu.de;htwg-konstanz.de",
        "position": "PhD student;PhD student;PhD student;PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\ndold2025paths,\ntitle={Paths and Ambient Spaces in Neural Loss Landscapes},\nauthor={Daniel Dold and Julius Kobialka and Nicolai Palm and Emanuel Sommer and David R{\\\"u}gamer and Oliver D{\\\"u}rr},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bwrd3y84te}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bwrd3y84te",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bzHWeDHpjX",
        "title": "Bandit Pareto Set Identification in a Multi-Output Linear Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the Pareto Set Identification (PSI) problem in a structured multi-output linear bandit model. In this setting, each arm is associated a feature vector belonging to $\\mathbb{R}^h$ and its mean vector in $\\mathbb{R}^d$ linearly depends on this feature vector through a common unknown matrix $\\Theta \\in \\mathbb{R}^{h \\times d}$. The goal is to identify the set of non-dominated arms by adaptively collecting samples from the arms. We introduce and analyze the first optimal design-based algorithms for PSI, providing nearly optimal guarantees in both the fixed-budget and the fixed-confidence settings. Notably, we show that the difficulty of these tasks mainly depends on the sub-optimality gaps of $h$ arms only. \nOur theoretical results are supported by an extensive benchmark on synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Cyrille Kone;Emilie Kaufmann;Laura Richert",
        "authorids": "~Cyrille_Kone1;~Emilie_Kaufmann1;~Laura_Richert1",
        "gender": "M;F;",
        "homepage": "http://cyrille-kone.github.io;https://emiliekaufmann.github.io/;https://www.bordeaux-population-health.center/the-teams/sistm/",
        "dblp": ";67/11350;",
        "google_scholar": ";9GE1vx4AAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Cyrille_Kone1;~Emilie_Kaufmann1;~Laura_Richert1",
        "aff": "Universit\u00e9 de Lille;CNRS;University of Bordeaux+INRIA",
        "aff_domain": "univ-lille.fr;cnrs.fr;u-bordeaux.fr+inria.fr",
        "position": "PhD student;Researcher;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nkone2025bandit,\ntitle={Bandit Pareto Set Identification in a Multi-Output Linear Model},\nauthor={Cyrille Kone and Emilie Kaufmann and Laura Richert},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bzHWeDHpjX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bzHWeDHpjX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bzR0v2VdEc",
        "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We address the challenge of sequential data-driven decision-making under context distributional uncertainty. This problem arises in numerous real-world scenarios where the learner optimizes black-box objective functions in the presence of uncontrollable contextual variables. We consider the setting where the context distribution is uncertain but known to lie within an ambiguity set defined as a ball in the Wasserstein distance. We propose a novel algorithm for Wasserstein Distributionally Robust Bayesian Optimization that can handle continuous context distributions while maintaining computational tractability. Our theoretical analysis combines recent results in self-normalized concentration in Hilbert spaces and finite-sample bounds for distributionally robust optimization to establish sublinear regret bounds that match state-of-the-art results. Through extensive comparisons with existing approaches on both synthetic and real-world problems, we demonstrate the simplicity, effectiveness, and practical applicability of our proposed method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Francesco Micheli;Efe C. Balta;Anastasios Tsiamis;John Lygeros",
        "authorids": "~Francesco_Micheli1;~Efe_C._Balta1;~Anastasios_Tsiamis1;~John_Lygeros1",
        "gender": "M;;;M",
        "homepage": ";https://www.ebalta.me;https://n.ethz.ch/~atsiamis/;https://control.ee.ethz.ch/people/profile.john-lygeros.html",
        "dblp": ";204/0958;;51/2754",
        "google_scholar": "KObhfJEAAAAJ;https://scholar.google.com/citations?hl=en;CtAqV5cAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0001-8596-8739;0000-0002-7935-7541;0000-0002-6159-1962",
        "linkedin": ";;;john-lygeros-662b73233/",
        "or_profile": "~Francesco_Micheli1;~Efe_C._Balta1;~Anastasios_Tsiamis1;~John_Lygeros1",
        "aff": "ETHZ - ETH Zurich;inspire AG+ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;inspire.ch+ethz.ch;ethz.ch;ethz.ch",
        "position": "PhD student;Group Leader+Guest Senior Scientist;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nmicheli2025wasserstein,\ntitle={Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context},\nauthor={Francesco Micheli and Efe C. Balta and Anastasios Tsiamis and John Lygeros},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=bzR0v2VdEc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bzR0v2VdEc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cAhy0bwMgd",
        "title": "Global Group Fairness in Federated Learning via Function Tracking",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate group fairness regularizers in federated learning, aiming to train a globally fair model in a distributed setting. Ensuring global fairness in distributed training presents unique challenges, as fairness regularizers typically involve probability metrics between distributions across all clients and are not naturally separable by client. To address this, we introduce a function-tracking scheme for the global fairness regularizer based on a Maximum Mean Discrepancy (MMD), which incurs a small communication overhead. This scheme seamlessly integrates into most federated learning algorithms while preserving rigorous convergence guarantees, as demonstrated in the context of FedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD regularization enables straightforward analysis through a change of kernel, leveraging an intuitive interpretation of kernel convolution. Numerical experiments confirm our theoretical insights.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yves Rychener;Daniel Kuhn;Yifan Hu",
        "authorids": "~Yves_Rychener1;~Daniel_Kuhn2;~Yifan_Hu2",
        "gender": "M;;M",
        "homepage": "https://github.com/yvesrychener;;https://sites.google.com/view/yifan-hu",
        "dblp": "281/9820;;",
        "google_scholar": ";;rO2s0EEAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yves_Rychener1;~Daniel_Kuhn2;~Yifan_Hu2",
        "aff": "Swiss Federal Institute of Technology Lausanne;;Rutgers University+EPFL - EPF Lausanne+ETHZ - ETH Zurich",
        "aff_domain": "epfl.ch;;rutgers.edu+epfl.ch+inf.ethz.ch",
        "position": "PhD student;;Assistant Professor+Postdoc+Postdoc",
        "bibtex": "@inproceedings{\nrychener2025global,\ntitle={Global Group Fairness in Federated Learning via Function Tracking},\nauthor={Yves Rychener and Daniel Kuhn and Yifan Hu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cAhy0bwMgd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cAhy0bwMgd",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cDYM7MGYFX",
        "title": "Analyzing the Role of Permutation Invariance in Linear Mode Connectivity",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "It was empirically observed in Enterazi et al. (2021) that when accounting for the permutation invariance of neural networks, there is likely no loss barrier along the linear interpolation between two SGD solutions -- a phenomenon known as linear mode connectivity (LMC) modulo permutation. This phenomenon has sparked significant attention due to both its theoretical interest and practical relevance in applications such as model merging. In this paper, we provide a fine-grained analysis of this phenomenon for two-layer ReLU networks under a teacher-student setup. We show that as the student network width $m$ increases,  the LMC loss barrier modulo permutation exhibits a double descent behavior. Particularly, when $m$ is sufficiently large, the barrier decreases to zero at a rate $O(m^{-1/2})$. Notably,  this rate does not suffer from the curse of dimensionality and demonstrates how substantial permutation can reduce the LMC loss barrier. Moreover, we observe a sharp transition in the sparsity of GD/SGD solutions when increasing the learning rate and investigate how this sparsity preference affects the LMC loss barrier modulo permutation. Experiments on both synthetic and MNIST datasets corroborate our theoretical predictions and reveal a similar trend for more complex network architectures.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Keyao Zhan;Puheng Li;Lei Wu",
        "authorids": "~Keyao_Zhan1;~Puheng_Li1;~Lei_Wu1",
        "gender": "M;M;M",
        "homepage": "https://sites.google.com/view/kyzhan;https://leo-li.com;https://leiwu0.github.io/",
        "dblp": ";334/5625;",
        "google_scholar": ";keEBSDUAAAAJ;CMweeYcAAAAJ",
        "orcid": ";;",
        "linkedin": ";puheng-li-b67742190/;",
        "or_profile": "~Keyao_Zhan1;~Puheng_Li1;~Lei_Wu1",
        "aff": "Harvard University;Amazon+Stanford University;Peking University",
        "aff_domain": "g.harvard.edu;amazon.com+stanford.edu;math.pku.edu.cn",
        "position": "PhD student;Intern+PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nzhan2025analyzing,\ntitle={Analyzing the Role of Permutation Invariance in Linear Mode Connectivity},\nauthor={Keyao Zhan and Puheng Li and Lei Wu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cDYM7MGYFX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cDYM7MGYFX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cEsJBwP9xw",
        "title": "A Computation-Efficient Method of Measuring Dataset Quality based on the Coverage of the Dataset",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Evaluating dataset quality is an essential task, as the performance of artificial intelligence (AI) systems heavily depends on it. A traditional method for evaluating dataset quality involves training an AI model on the dataset and testing it on a separate test set. However, this approach requires significant computational time. In this paper, we propose a computationally efficient method for quantifying dataset quality. Specifically, our method measures how well the dataset covers the input probability distribution, ensuring that a high-quality dataset minimizes out-of-distribution inputs.\nWe present a GPU-accelerated algorithm for approximately implementing the proposed method. We highlight three applications of our approach. First, it can evaluate the impact of data management practices, such as data cleaning and core set selection. We experimentally demonstrate that the quality assessment provided by our method strongly correlates with the traditional approach, achieving an $R^2 \\geq 0.985$ in most cases while being 60-1200 times faster. Second, it can monitor the quality of continuously growing datasets with computation time proportional to the added data size. Finally, our method can estimate the performance of traditional methods for large datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Beomjun Kim;Jaehwan Kim;Kangyeon Kim;Sunwoo Kim;Heejin Ahn",
        "authorids": "~Beomjun_Kim3;john@aimmo.co.kr;kky3528@kaist.ac.kr;~Sunwoo_Kim5;~Heejin_Ahn1",
        "gender": "M;;;M;F",
        "homepage": "https://sites.google.com/view/beomjunkim/;;;;https://cis.kaist.ac.kr/",
        "dblp": ";;;;124/4058",
        "google_scholar": "3ueGSnEAAAAJ;;;;0EVeMOEAAAAJ",
        "orcid": "0000-0003-0223-5973;;;;0000-0001-9153-3491",
        "linkedin": ";;;sunwoo-kim-299493201/;",
        "or_profile": "~Beomjun_Kim3;john@aimmo.co.kr;kky3528@kaist.ac.kr;~Sunwoo_Kim5;~Heejin_Ahn1",
        "aff": "Massachusetts Institute of Technology;;;Seoul National University;Korea Advanced Institute of Science & Technology",
        "aff_domain": "mit.edu;;;snu.ac.kr;kaist.ac.kr",
        "position": "MS student;;;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025a,\ntitle={A Computation-Efficient Method of Measuring Dataset Quality based on the Coverage of the Dataset},\nauthor={Beomjun Kim and Jaehwan Kim and Kangyeon Kim and Sunwoo Kim and Heejin Ahn},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cEsJBwP9xw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cEsJBwP9xw",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cGnnwEm2gu",
        "title": "Distributional Counterfactual Explanations With Optimal Transport",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Counterfactual explanations (CE) are the de facto method for providing insights into black-box decision-making models by identifying\nalternative inputs that lead to different outcomes.\nHowever, existing CE approaches, including group and global methods, focus predominantly on specific input modifications, lacking the ability to capture nuanced distributional characteristics that influence model outcomes across the entire input-output spectrum.\nThis paper proposes distributional counterfactual explanation (DCE), shifting\nfocus to the distributional properties of observed and counterfactual data, thus providing broader insights. DCE is particularly beneficial for stakeholders making strategic decisions based on statistical data analysis, as it makes the statistical distribution of the counterfactual resembles the one of the factual when aligning model outputs with a target distribution---something that the existing CE methods cannot fully achieve.\nWe leverage optimal transport (OT) to formulate a chance-constrained optimization problem, deriving a counterfactual distribution aligned with its factual counterpart, supported by statistical confidence.\nThe efficacy of this approach is demonstrated through experiments, highlighting its potential to provide deeper insights\ninto decision-making models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lei You;Lele Cao;Mattias Nilsson;Bo Zhao;Lei Lei",
        "authorids": "~Lei_You1;~Lele_Cao1;~Mattias_Nilsson1;~Bo_Zhao14;~Lei_Lei1",
        "gender": "M;M;;M;M",
        "homepage": "https://leiyou.me;http://llcresearch.com;;https://zbjob.github.io/;",
        "dblp": "43/2613-2;155/3234;;94/4810-19.html;",
        "google_scholar": "https://scholar.google.dk/citations?user=DTpo_scAAAAJ;xM2shP8AAAAJ;;https://scholar.google.de/citations?user=R6igYYYAAAAJ;",
        "orcid": ";0000-0002-5680-9031;;0000-0002-0768-3444;",
        "linkedin": ";caolele/;mattias-nilsson-b674b610/;bo-zhao-054b7aa8/;",
        "or_profile": "~Lei_You1;~Lele_Cao1;~Mattias_Nilsson1;~Bo_Zhao14;~Lei_Lei1",
        "aff": "Technical University of Denmark;Microsoft (ABK);Neko Health;Aalto University;Interdisciplinary Centre for Security, Reliability and Trust (SnT)",
        "aff_domain": "dtu.dk;microsoft.com;nekohealth.com;aalto.fi;uni.lu",
        "position": "Assistant Professor;Senior Principal AI Researcher;Principal Researcher;Assistant Professor;Research associate",
        "bibtex": "@inproceedings{\nyou2025distributional,\ntitle={Distributional Counterfactual Explanations With Optimal Transport},\nauthor={Lei You and Lele Cao and Mattias Nilsson and Bo Zhao and Lei Lei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cGnnwEm2gu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cGnnwEm2gu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cPbEcEkTYe",
        "title": "Sketch-and-Project Meets Newton Method: Global $O(1/k^2)$ Convergence with Low-Rank Updates",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we propose the first sketch-and-project Newton method with the fast $O(1/k^2$) global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of the Newton method, ii) as a cubically regularized Newton method in the sketched subspaces, and iii) as a damped Newton method in the sketched subspaces.\n\nSGN inherits the best of all three worlds: the cheap iteration costs of the sketch-and-project methods, the state-of-the-art $O(1/k^2)$ global convergence rate of the full-rank Newton-like methods, and the algorithm simplicity of the damped Newton methods. Finally, we demonstrate its comparable empirical performance to the baseline algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Slavomir Hanzely",
        "authorids": "~Slavomir_Hanzely1",
        "gender": "",
        "homepage": "https://sites.google.com/view/slavomirhanzely",
        "dblp": "",
        "google_scholar": "ksqGMoEAAAAJ",
        "orcid": "",
        "linkedin": "",
        "or_profile": "~Slavomir_Hanzely1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae",
        "position": "Postdoc",
        "bibtex": "@inproceedings{\nhanzely2025sketchandproject,\ntitle={Sketch-and-Project Meets Newton Method: Global \\${\\textbackslash}mathcal O {\\textbackslash}left( k{\\textasciicircum}\\{-2\\} {\\textbackslash}right)\\$ Convergence with Low-Rank Updates},\nauthor={Slavomir Hanzely},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cPbEcEkTYe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cPbEcEkTYe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cTAI6fpB9q",
        "title": "DPFL: Decentralized Personalized Federated Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work addresses the challenges of data heterogeneity and communication constraints in decentralized federated learning (FL). We introduce decentralized personalized FL (DPFL), a bi-level optimization framework that enhances personalized FL by leveraging combinatorial relationships among clients, enabling fine-grained and targeted collaborations. By employing a constrained greedy algorithm, DPFL constructs a collaboration graph that guides clients in choosing suitable collaborators, enabling personalized model training tailored to local data while respecting a fixed and predefined communication and resource budget. Our theoretical analysis demonstrates that the proposed objective for constructing the collaboration graph yields superior or equivalent performance compared to any alternative collaboration structures, including pure local training. Extensive experiments across diverse datasets show that DPFL consistently outperforms existing methods, effectively handling non-IID data, reducing communication overhead, and improving resource efficiency in real-world decentralized FL scenarios. The code can be accessed at: https://github.com/salmakh1/DPFL.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Salma Kharrat;Marco Canini;Samuel Horv\u00e1th",
        "authorids": "~Salma_Kharrat1;~Marco_Canini1;~Samuel_Horv\u00e1th1",
        "gender": "F;M;M",
        "homepage": ";https://mcanini.github.io;https://sites.google.com/view/samuelhorvath",
        "dblp": "340/3924.html;24/5715;234/8604",
        "google_scholar": ";c-rwMUkAAAAJ;k252J7kAAAAJ",
        "orcid": ";0000-0002-5051-4283;0000-0003-0619-9260",
        "linkedin": "salma-kharrat-758231151/;;samuel-horvath/",
        "or_profile": "~Salma_Kharrat1;~Marco_Canini1;~Samuel_Horv\u00e1th1",
        "aff": "King Abdullah University of Science and Technology;KAUST;MBZUAI",
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;mbzuai.ac.ae",
        "position": "PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkharrat2025dpfl,\ntitle={{DPFL}: Decentralized Personalized Federated Learning},\nauthor={Salma Kharrat and Marco Canini and Samuel Horv{\\'a}th},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cTAI6fpB9q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cTAI6fpB9q",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cUTzrd9mIA",
        "title": "Theory of Agreement-on-the-Line in Linear Models and Gaussian Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Under distribution shifts, deep networks exhibit a surprising phenomenon: in-distribution (ID) versus out-of-distribution (OOD) accuracy is often strongly linearly correlated across architectures and hyperparameters, accompanied by the same linear trend in ID versus OOD agreement between the predictions of any pair of such independently trained networks. The latter phenomenon called ``agreement-on-the-line'' enables precise unlabeled OOD performance estimation of models. In this work, we discover that agreement-on-the-line emerges even in linear classifiers over Gaussian class conditional distributions. We provide theoretical guarantees for this phenomenon in classifiers optimized via randomly initialized gradient descent, approximated by linear interpolations between random vectors and the Bayes-optimal classifier. Next, we prove a lower bound on the residual of the correlation between ID versus OOD agreement that grows proportionally with the residual of accuracy. Real-world experiments on CIFAR10C shifts, validate our findings and the broader relevance of our theoretical framework.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Christina Baek;Aditi Raghunathan;J Zico Kolter",
        "authorids": "~Christina_Baek2;~Aditi_Raghunathan1;~J_Zico_Kolter1",
        "gender": ";F;",
        "homepage": "https://kebaek.github.io;https://www.cs.cmu.edu/~aditirag/;",
        "dblp": "202/7238;166/1409;",
        "google_scholar": ";Ch9iRwQAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Christina_Baek2;~Aditi_Raghunathan1;~J_Zico_Kolter1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;",
        "aff_domain": "cmu.edu;cmu.edu;",
        "position": "PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nbaek2025theory,\ntitle={Theory of Agreement-on-the-Line in Linear Models and Gaussian Data},\nauthor={Christina Baek and Aditi Raghunathan and J Zico Kolter},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cUTzrd9mIA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cUTzrd9mIA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cVukn0ZUO8",
        "title": "Large Covariance Matrix Estimation With Nonnegative Correlations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Covariance matrix estimation is a fundamental problem in multivariate data analysis. \nIn many situations, it is often observed that variables exhibit a positive linear dependency, indicating a positive linear correlation. This paper tackles the challenge of estimating covariance matrices with positive correlations in high-dimensional settings. We propose a positive definite thresholding covariance estimation problem that includes nonconvex sparsity penalties and nonnegative correlation constraints. To address this problem, we introduce a multistage adaptive estimation algorithm based on majorization-minimization (MM). This algorithm progressively refines the estimates by solving a weighted $\\ell_{1}$-regularized problem at each stage. Additionally, we present a comprehensive theoretical analysis that characterizes the estimation error associated with the estimates generated by the MM algorithm. The analysis reveals that the error comprises two components: the optimization error and the statistical error. The optimization error decreases to zero at a linear rate, allowing the proposed estimator to eventually reach the oracle statistical rate under mild conditions. Furthermore, we explore various extensions based on the proposed estimation technique. Our theoretical findings are supported by extensive numerical experiments conducted on both synthetic and real-world datasets.\nFurthermore, we demonstrate that the proposed estimation technique can be expanded to the correlation matrix estimation scenario.\nOur theoretical findings are corroborated through extensive numerical experiments on both synthetic data and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yixin Yan;QIAO YANG;Ziping Zhao",
        "authorids": "~Yixin_Yan2;~QIAO_YANG1;~Ziping_Zhao1",
        "gender": "M;M;M",
        "homepage": ";https://sem.shanghaitech.edu.cn/2021/1220/c9437a269618/page.htm;https://www.zipingzhao.com",
        "dblp": ";;13/3015-2",
        "google_scholar": ";;https://scholar.google.com.hk/citations?user=fYYmZDsAAAAJ",
        "orcid": ";0000-0003-2780-4984;0000-0002-8668-6263",
        "linkedin": "yixin-yan-8b7885327/;;zipingzhao",
        "or_profile": "~Yixin_Yan2;~QIAO_YANG1;~Ziping_Zhao1",
        "aff": "ShanghaiTech University;ShanghaiTech University;ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "position": "MS student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyan2025large,\ntitle={Large Covariance Matrix Estimation With Nonnegative Correlations},\nauthor={Yixin Yan and Ziping Zhao and QIAO YANG},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cVukn0ZUO8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cVukn0ZUO8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cX6al5QIbN",
        "title": "Efficient Estimation of a Gaussian Mean with Local Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we study the problem of estimating the unknown mean $\\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way. In the high-privacy regime ($\\epsilon\\le 1$), we identify an optimal privacy mechanism that minimizes the variance of the estimator asymptotically. Our main technical contribution is the maximization of the Fisher-Information of the sanitized data with respect to the local privacy mechanism $Q$. We find that the exact solution $Q_{\\theta,\\epsilon}$ of this maximization is the sign mechanism that applies randomized response to the sign of $X_i-\\theta$, where $X_1,\\dots, X_n$ are the confidential iid original samples. However, since this optimal local mechanism depends on the unknown mean $\\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups. The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\\theta$ by $\\tilde{\\theta_{n_1}}$. Then this estimate is updated by applying the sign mechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nikita Kalinin;Lukas Steinberger",
        "authorids": "~Nikita_Kalinin1;~Lukas_Steinberger1",
        "gender": ";M",
        "homepage": "https://npkalinin.github.io/;https://smale.univie.ac.at",
        "dblp": "383/7978;",
        "google_scholar": "nblAbqUAAAAJ;https://scholar.google.de/citations?hl=en",
        "orcid": ";0000-0002-2376-114X",
        "linkedin": ";",
        "or_profile": "~Nikita_Kalinin1;~Lukas_Steinberger1",
        "aff": "Institute of Science and Technology;Universit\u00e4t Vienna",
        "aff_domain": "ist.ac.at;univie.ac.at",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nnikita2025efficient,\ntitle={Efficient Estimation of a Gaussian Mean  with Local Differential Privacy},\nauthor={Nikita Kalinin and Lukas Steinberger},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cX6al5QIbN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cX6al5QIbN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ciUhokfeX2",
        "title": "On Preference-based Stochastic Linear Contextual Bandits with Knapsacks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper studies the problem of preference-based stochastic linear contextual bandits with knapsack constraints (PbLinCBwK).\nWe propose budget-aware optimistic and randomized exploration algorithms that achieve a regret of ${O}((\\kappa+\\frac{T\\nu^*}{B})\\sqrt{T}\\log T),$ for any total budget $B=\\Omega(\\sqrt{T}).$\nThe parameters $\\kappa$ and $\\frac{T\\nu^*}{B}$ capture the effects of preference feedback and knapsack constraints, respectively. Our regret performance is near-optimal and matches the bound of LinCBwK under the mild condition $B=\\Omega(\\sqrt{T}).$ \nTo achieve these results, we view the process of budget consumption and stopping time as Markov processing and analyze it via the Lyapunov drift method, which is translated into the strong regret guarantee. \nThe experiments on synthetic PbLinCBwK and online content moderation setting further justify the theoretical results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xin Liu",
        "authorids": "~Xin_Liu14",
        "gender": "",
        "homepage": "",
        "dblp": "76/1820-49",
        "google_scholar": "y0U4EF4AAAAJ",
        "orcid": "",
        "linkedin": "",
        "or_profile": "~Xin_Liu14",
        "aff": "ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cm",
        "position": "Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025on,\ntitle={On Preference-based Stochastic Linear Contextual Bandits with Knapsacks},\nauthor={Xin Liu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ciUhokfeX2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ciUhokfeX2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cj5L29VWol",
        "title": "How Well Can Transformers Emulate In-Context Newton's Method?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\\epsilon$ error with only a logarithmic to the error more layers. Our results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Angeliki Giannou;Liu Yang;Tianhao Wang;Dimitris Papailiopoulos;Jason D. Lee",
        "authorids": "~Angeliki_Giannou1;~Liu_Yang6;~Tianhao_Wang1;~Dimitris_Papailiopoulos1;~Jason_D._Lee1",
        "gender": "F;;M;M;M",
        "homepage": "https://sites.google.com/view/angeliki-giannou/home;https://leiay.github.io/;https://tianhaowang.ttic.edu;http://papail.io;https://jasondlee88.github.io/",
        "dblp": "283/5898.html;;145/3288-2;;88/3262",
        "google_scholar": ";ul5MsOIAAAAJ;m45LD1kAAAAJ;hYi6i9sAAAAJ;GR_DsT0AAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Angeliki_Giannou1;~Liu_Yang6;~Tianhao_Wang1;~Dimitris_Papailiopoulos1;~Jason_D._Lee1",
        "aff": "University of Wisconsin - Madison;University of Wisconsin - Madison+Google;University of California, San Diego+Toyota Technological Institute at Chicago;Microsoft Research+University of Wisconsin - Madison;Princeton University",
        "aff_domain": "wisc.edu;wisc.edu+google.com;ucsd.edu+ttic.edu;research.microsoft.com+wisc.edu;princeton.edu",
        "position": "PhD student;PhD student+Intern;Assistant Professor+Research Assistant Professor;Principal Researcher+Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ngiannou2025how,\ntitle={How Well Can Transformers Emulate In-Context Newton's Method?},\nauthor={Angeliki Giannou and Liu Yang and Tianhao Wang and Dimitris Papailiopoulos and Jason D. Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cj5L29VWol}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cj5L29VWol",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cpGW5oI1vs",
        "title": "Adaptive Convergence Rates for Log-Concave Maximum Likelihood",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the task of estimating a log-concave density in $\\mathbb{R}^d$ using the Maximum Likelihood Estimator, known as the log-concave MLE. We show that for every $d \\geq 4$, the log-concave MLE attains an \\emph{adaptive rate} when the negative logarithm of the underlying density is the maximum of $k$ affine functions, meaning that the estimation error for such a density is significantly lower than the minimax rate for the class of log-concave densities. Specifically, we prove that for such densities, the risk of the log-concave MLE is of order $c(k) \\cdot n^{-\\frac{4}{d}}$ in terms of the Hellinger squared distance. This result complements the work of (Kim et al. AoS 2018) and Feng et al. (AoS 2021), who addressed the cases $d = 1$ and $d \\in \\{2,3\\}$, respectively. Our proof provides a unified and relatively simple approach for all $d \\geq 1$, and is based on techniques from stochastic convex geometry and empirical process theory, which may be of independent interest.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gil Kur;Aditya Guntuboyina",
        "authorids": "~Gil_Kur2;~Aditya_Guntuboyina1",
        "gender": "M;M",
        "homepage": ";https://www.stat.berkeley.edu/~aditya/",
        "dblp": "236/4833;81/9434",
        "google_scholar": "yDkAhccAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Gil_Kur2;~Aditya_Guntuboyina1",
        "aff": "Department of Computer Science, ETHZ - ETH Zurich;University of California-Berkeley+University of California, Berkeley",
        "aff_domain": "inf.ethz.ch;+berkeley.edu",
        "position": "Postdoc;+Associate Professor",
        "bibtex": "@inproceedings{\nkur2025adaptive,\ntitle={Adaptive Convergence Rates for Log-Concave Maximum Likelihood},\nauthor={Gil Kur and Aditya Guntuboyina},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=cpGW5oI1vs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cpGW5oI1vs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "d7pfWnuKT4",
        "title": "Bayesian Decision Theory on Decision Trees: Uncertainty Evaluation and Interpretability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deterministic decision trees have difficulty in evaluating uncertainty especially for small samples. To solve this problem, we interpret the decision trees as stochastic models and consider prediction problems in the framework of Bayesian decision theory. Our models have three kinds of parameters: a tree shape, leaf parameters, and inner parameters. To make Bayesian optimal decisions, we have to calculate the posterior distribution of these parameters. Previously, two types of methods have been proposed. One marginalizes out the leaf parameters and samples the tree shape and the inner parameters by Metropolis-Hastings (MH) algorithms. The other marginalizes out both the leaf parameters and the tree shape based on a concept called meta-trees and approximates the posterior distribution for the inner parameters by a bagging-like method. In this paper, we propose a novel MH algorithm where the leaf parameters and the tree shape are marginalized out by using the meta-trees and only the inner parameters are sampled. Moreover, we update all the inner parameters simultaneously in each MH step. This algorithm accelerates the convergence and mixing of the Markov chain. We evaluate our algorithm on various benchmark datasets with other state-of-the-art methods. Further, our model provides a novel statistical evaluation of feature importance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuta Nakahara;Shota Saito;Naoki Ichijo;Koki Kazama;Toshiyasu Matsushima",
        "authorids": "~Yuta_Nakahara1;~Shota_Saito3;~Naoki_Ichijo1;~Koki_Kazama1;~Toshiyasu_Matsushima1",
        "gender": "M;M;M;;M",
        "homepage": "https://researchmap.jp/y-nak?lang=en;https://researchmap.jp/wa-shota;;https://researchmap.jp/KokiKAZAMA;http://www.matsu.mgmt.waseda.ac.jp/?la=en",
        "dblp": "194/7906;;;194/7877;00/3771.html",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=UhyeylAAAAAJ;;;",
        "orcid": "0000-0002-0553-7910;0000-0003-2256-4598;;;",
        "linkedin": ";;naoki-ichijo-23ba27211?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BIA0w7%2Br9Q0qdt6l2iiIG8A%3D%3D;;",
        "or_profile": "~Yuta_Nakahara1;~Shota_Saito3;~Naoki_Ichijo1;~Koki_Kazama1;~Toshiyasu_Matsushima1",
        "aff": "Waseda University;Gunma University;Waseda University;Shonan Institute of Technology+Yokohama College of Commerce;Waseda University",
        "aff_domain": "waseda.jp;gunma-u.ac.jp;waseda.jp;shonan-it.ac.jp+shodai.ac.jp;waseda.edu",
        "position": "Assistant Professor;Associate Professor;PhD student;Assistant Professor+Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nnakahara2025bayesian,\ntitle={Bayesian Decision Theory on Decision Trees: Uncertainty Evaluation and Interpretability},\nauthor={Yuta Nakahara and Shota Saito and Naoki Ichijo and Koki Kazama and Toshiyasu Matsushima},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=d7pfWnuKT4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=d7pfWnuKT4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dEwvHEHxPc",
        "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. Depending on the differentiability of the objective function, we propose two different sampling methods. For differentiable objectives, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. For non-differentiable objectives, we propose an iterative importance sampling strategy using the diffusion model as the proposal distribution. Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective molecule optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lingkai Kong;Yuanqi Du;Wenhao Mu;Kirill Neklyudov;Valentin De Bortoli;Dongxia Wu;Haorui Wang;Aaron M Ferber;Yian Ma;Carla P Gomes;Chao Zhang",
        "authorids": "~Lingkai_Kong1;~Yuanqi_Du1;~Wenhao_Mu1;~Kirill_Neklyudov1;~Valentin_De_Bortoli1;~Dongxia_Wu1;~Haorui_Wang1;~Aaron_M_Ferber1;~Yian_Ma1;~Carla_P_Gomes1;~Chao_Zhang15",
        "gender": "M;M;M;M;;M;M;M;M;;",
        "homepage": "https://lingkai-kong.com/;https://yuanqidu.github.io/;https://github.com/SSSScuderia;https://necludov.github.io/;https://vdeborto.github.io/;https://dongxiaw.github.io/online-cv/;;https://aaron-ferber.github.io/;https://sites.google.com/view/yianma;;http://chaozhang.org/",
        "dblp": "20/10253;266/2837;;195/1093;224/9338;;;163/7788;;;94/3019-14",
        "google_scholar": "https://scholar.google.com/citations?hl=en;fAc_zZMAAAAJ;;https://scholar.google.ru/citations?user=eOttYWgAAAAJ;;jZb2e8cAAAAJ;;TuVq07oAAAAJ;A0TFlacAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0001-6480-513X;;;;;;;;;;0000-0003-3009-598X",
        "linkedin": ";;wenhao-mu/;;;dongxia-wu-2021/;haorui-wang-6a1a92185/;aaron-ferber-64a73980/;;;",
        "or_profile": "~Lingkai_Kong1;~Yuanqi_Du1;~Wenhao_Mu1;~Kirill_Neklyudov1;~Valentin_De_Bortoli1;~Dongxia_Wu1;~Haorui_Wang1;~Aaron_M_Ferber1;~Yian_Ma1;~Carla_P_Gomes1;~Chao_Zhang15",
        "aff": "Harvard University;Cornell University;University of Michigan - Ann Arbor;Universit\u00e9 de Montr\u00e9al+Mila - Quebec Artificial Intelligence Institute;University of Oxford;University of California, San Diego;Georgia Institute of Technology;;University of California, San Diego;;Georgia Institute of Technology",
        "aff_domain": "harvard.edu;cornell.edu;umich.edu;umontreal.ca+mila.quebec;ox.ac.uk;ucsd.edu;gatech.edu;;ucsd.edu;;gatech.edu",
        "position": "Postdoc;PhD student;PhD student;Assistant Professor+Assistant Professor;Postdoc;Ph.D student;PhD student;;Assistant Professor;;Assistant Professor",
        "bibtex": "@inproceedings{\nkong2025diffusion,\ntitle={Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints},\nauthor={Lingkai Kong and Yuanqi Du and Wenhao Mu and Kirill Neklyudov and Valentin De Bortoli and Dongxia Wu and Haorui Wang and Aaron M Ferber and Yian Ma and Carla P Gomes and Chao Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dEwvHEHxPc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dEwvHEHxPc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dT0ldWDBto",
        "title": "Learning from biased positive-unlabeled data via threshold calibration",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Learning from positive and unlabeled data (PU learning) aims to train a binary classification model when only positive and unlabeled examples are available.  Typically, learners assume that there is a labeling mechanism that determines which positive labels are observed. A particularly challenging setting arises when the observed positive labels are a biased sample from the positive distribution. Current approaches either require estimating the propensity scores, which are the instance-specific probabilities that a positive example's label will be observed, or make overly restricting assumptions about the labeling mechanism. We make a novel assumption about the labeling mechanism which we show is more general than several commonly used existing ones. Moreover, the combination of our novel assumption and theoretical results from robust statistics can simplify the process of learning from biased PU data. Empirically, our approach offers superior predictive and run time performance compared to the state-of-the-art methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pawe\u0142 Teisseyre;Timo Martens;Jessa Bekker;Jesse Davis",
        "authorids": "~Pawe\u0142_Teisseyre1;~Timo_Martens1;~Jessa_Bekker1;~Jesse_Davis1",
        "gender": "M;M;F;M",
        "homepage": "https://home.ipipan.waw.pl/p.teisseyre/index.html;;https://jessa.github.io/;https://people.cs.kuleuven.be/~jesse.davis/",
        "dblp": "70/10842.html;;178/3249.html;d/JesseDavis",
        "google_scholar": "https://scholar.google.pl/citations?user=mzLMwTcAAAAJ;56Y4ao4AAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=gz74XOYAAAAJ",
        "orcid": "0000-0002-4296-9819;;0000-0003-1928-7374;0000-0002-3748-9263",
        "linkedin": "pawe%C5%82-teisseyre-phd-17883b19/?originalSubdomain=pl;timo-martens-6091b7196/;jessabekker;",
        "or_profile": "~Pawe\u0142_Teisseyre1;~Timo_Martens1;~Jessa_Bekker1;~Jesse_Davis1",
        "aff": "Warsaw University of Technology+Polish Academy of Sciences;KU Leuven;KU Leuven;KU Leuven",
        "aff_domain": "pw.edu.pl+ipipan.waw.pl;kuleuven.be;kuleuven.be;kuleuven.be",
        "position": "Assistant Professor+Assistant Professor;PhD student;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nteisseyre2025learning,\ntitle={Learning from biased positive-unlabeled data via threshold calibration},\nauthor={Pawe{\\l} Teisseyre and Timo Martens and Jessa Bekker and Jesse Davis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dT0ldWDBto}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dT0ldWDBto",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dTbfmEa8Vt",
        "title": "On Tractability of Learning Bayesian Networks with Ancestral Constraints",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Expert knowledge can greatly reduce the complexity of Bayesian network structure learning by constraining the search space. These constraints can come in the form of ancestral constraints that relate to the existence of paths between nodes. When the constraints are compiled into a directed acyclic graph, the complexity of learning with ancestral constraints is connected to the number of ideals of the constraint graph. First, we consider precedence constraints which define a partial order that the structure must obey. Taking the path cover number of the constraint graph as a parameter, we extend earlier results to the problems of sampling and weighted counting of network structures. We also consider the problems with related ancestral constraints which state that a node must or cannot be an ancestor of another. With positive ancestral constraints, we show that the problems are tractable under the additional assumption that the constraint graph has only a small number of incomparable edges. On the other hand, the optimization problem is NP-hard with negative ancestral constraints when the path cover number is at least two. Finally, we show that these problems become fixed-parameter tractable if the constraints are compatible with a subclass of partial orders called bucket orders.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juha Harviainen;Pekka Parviainen",
        "authorids": "~Juha_Harviainen1;~Pekka_Parviainen1",
        "gender": "M;",
        "homepage": "https://juhaharviainen.com/;",
        "dblp": "258/7754;32/8336",
        "google_scholar": "ftsJV7kAAAAJ;",
        "orcid": "0000-0002-4581-840X;",
        "linkedin": ";",
        "or_profile": "~Juha_Harviainen1;~Pekka_Parviainen1",
        "aff": "University of Helsinki;University of Bergen",
        "aff_domain": "helsinki.fi;uib.no",
        "position": "Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nharviainen2025on,\ntitle={On Tractability of Learning Bayesian Networks with Ancestral Constraints},\nauthor={Juha Harviainen and Pekka Parviainen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dTbfmEa8Vt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dTbfmEa8Vt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dZCQCanOAg",
        "title": "On Tradeoffs in Learning-Augmented Algorithms",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The field of learning-augmented algorithms has gained significant attention in recent years. Using potentially inaccurate predictions, these algorithms must exhibit three key properties: consistency, robustness, and smoothness. In scenarios with stochastic predictions, a strong average-case performance is required. Typically, the design of such algorithms involves a natural tradeoff between consistency and robustness, and previous works aimed to achieve Pareto-optimal tradeoffs for specific problems. However, in some settings, this comes at the expense of smoothness. In this paper, we explore the tradeoffs between all the mentioned criteria and show how they can be balanced.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziyad Benomar;Vianney Perchet",
        "authorids": "~Ziyad_Benomar1;~Vianney_Perchet3",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ziyad_Benomar1;~Vianney_Perchet3",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\nbenomar2025on,\ntitle={On Tradeoffs in Learning-Augmented Algorithms},\nauthor={Ziyad Benomar and Vianney Perchet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dZCQCanOAg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dZCQCanOAg",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dwBB0xmtBr",
        "title": "Structure based SAT dataset for analysing GNN generalisation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Satisfiability (SAT) solvers based on techniques such as conflict driven clause learning (CDCL) have produced excellent performance on both synthetic and real world industrial problems. While these CDCL solvers only operate on a per-problem basis, graph neural network (GNN) based solvers bring new benefits to the field by allowing practitioners to exploit knowledge gained from previously \nsolved problems to expedite solving of new SAT problems. However, one specific area that is often studied in the context of CDCL solvers, but largely overlooked in GNN solvers, is the relationship between graph theoretic measure of structure in SAT problems and the generalisation ability of GNN solvers. To bridge the gap between structural graph properties (e.g., modularity, self-similarity) and the generalisability (or lack thereof) of GNN based SAT solvers, we present StructureSAT: a curated dataset, along with code to further generate novel examples, containing a diverse set of SAT problems from well known problem domains. Furthermore, we utilise a novel splitting method that focuses on deconstructing the families into more detailed hierarchies based on their structural properties. With the new dataset, we aim to help explain problematic generalisation in existing GNN SAT solvers by exploiting knowledge of structural graph properties. We conclude with multiple future directions that can help researchers in GNN based SAT solving develop more effective and generalisable SAT solvers.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yi Fu;Anthony Tompkins;Yang Song;Maurice Pagnucco",
        "authorids": "~Yi_Fu2;~Anthony_Tompkins1;~Yang_Song4;~Maurice_Pagnucco1",
        "gender": ";M;;M",
        "homepage": ";https://github.com/MushroomHunting;http://www.cse.unsw.edu.au/~ysong/;http://www.cse.unsw.edu.au/~morri/",
        "dblp": ";218/7382;24/4470-1;p/MauricePagnucco",
        "google_scholar": "hz10F-cAAAAJ;https://scholar.google.com.au/citations?user=1-FnHS0AAAAJ;https://scholar.google.com.au/citations?user=7u3M9hMAAAAJ;lqjmockAAAAJ",
        "orcid": ";;0000-0003-1283-1672;0000-0001-7712-6646",
        "linkedin": ";;;mauricepagnucco/?originalSubdomain=au",
        "or_profile": "~Yi_Fu2;~Anthony_Tompkins1;~Yang_Song4;~Maurice_Pagnucco1",
        "aff": "University of New South Wales;University of New South Wales;University of New South Wales;University of New South Wales",
        "aff_domain": "unsw.edu.au;unsw.edu.au;unsw.edu.au;unsw.edu.au",
        "position": "MS student;Researcher;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nfu2025structure,\ntitle={Structure based {SAT} dataset for analysing {GNN} generalisation},\nauthor={Yi Fu and Anthony Tompkins and Yang Song and Maurice Pagnucco},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dwBB0xmtBr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dwBB0xmtBr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dwyKBQmOjR",
        "title": "Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accuracy-on-the-line is a widely observed phenomenon in machine learning, where a model's accuracy on in-distribution (ID) and out-of-distribution (OOD) data is positively correlated across different hyperparameters and data configurations. But when does this useful relationship break down? In this work, we explore its robustness. The key observation is that noisy data and the presence of nuisance features can be sufficient to shatter the Accuracy-on-the-line phenomenon. In these cases, ID and OOD accuracy can become negatively correlated, leading to \"Accuracy-on-the-wrong-line\". This phenomenon can also occur in the presence of spurious (shortcut) features, which tend to overshadow the more complex signal (core, non-spurious) features, resulting in a large nuisance feature space. Moreover, scaling to larger datasets does not mitigate this undesirable behaviour and may even exacerbate it. We formally prove a lower bound on OOD error in a linear classification model, characterising the conditions on the noise and nuisance features for a large OOD error. We finally demonstrate this phenomenon across both synthetic and real datasets with noisy data and nuisance features.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amartya Sanyal;Yaxi Hu;Yaodong Yu;Yian Ma;Yixin Wang;Bernhard Sch\u00f6lkopf",
        "authorids": "~Amartya_Sanyal1;~Yaxi_Hu1;~Yaodong_Yu4;~Yian_Ma1;~Yixin_Wang1;~Bernhard_Sch\u00f6lkopf1",
        "gender": "M;F;M;M;;",
        "homepage": "https://amartya18x.github.io;;https://yaodongyu.github.io;https://sites.google.com/view/yianma;;",
        "dblp": "203/8807;322/1093.html;;;;",
        "google_scholar": ";;bZ9oyW8AAAAJ;A0TFlacAAAAJ;gFLW9qcAAAAJ;",
        "orcid": "0000-0002-4190-0449;;;;0000-0002-6617-4842;",
        "linkedin": ";yaxi-hu-8910b5233/;;;;",
        "or_profile": "~Amartya_Sanyal1;~Yaxi_Hu1;~Yaodong_Yu4;~Yian_Ma1;~Yixin_Wang1;~Bernhard_Sch\u00f6lkopf1",
        "aff": "Copenhagen University;Max Planck Institute for Intelligent Systems;OpenAI;University of California, San Diego;University of Michigan - Ann Arbor;",
        "aff_domain": "ku.dk;is.tuebingen.mpg.de;openai.com;ucsd.edu;umich.edu;",
        "position": "Assistant Professor;PhD student;Researcher;Assistant Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nsanyal2025accuracy,\ntitle={Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation},\nauthor={Amartya Sanyal and Yaxi Hu and Yaodong Yu and Yian Ma and Yixin Wang and Bernhard Sch{\\\"o}lkopf},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=dwyKBQmOjR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dwyKBQmOjR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "e4VaDn99jz",
        "title": "Approximating the Total Variation Distance between Gaussians",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The total variation distance is a metric of central importance in statistics and probability theory.  However, somewhat surprisingly, questions about computing it *algorithmically* appear not to have been systematically studied until very recently.  In this paper, we contribute to this line of work by studying this question in the important special case of multivariate Gaussians.  More formally, we consider the problem of approximating the total variation distance between two multivariate Gaussians to within an $\\epsilon$-relative error.  Previous works achieved a *fixed* constant relative error approximation via closed-form formulas. In this work, we give algorithms that given any two $n$-dimensional Gaussians $D_1,D_2$, and any error bound $\\epsilon > 0$, approximate the total variation distance $D := d_{TV}(D_1,D_2)$ to $\\epsilon$-relative accuracy in $\\mathrm{poly}(n,\\frac{1}{\\epsilon},\\log \\frac{1}{D})$ operations.  The main technical tool in our work is a reduction that helps us extend the recent progress on computing the TV-distance between *discrete* random variables to our continuous setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arnab Bhattacharyya;Weiming Feng;Piyush Srivastava",
        "authorids": "~Arnab_Bhattacharyya1;~Weiming_Feng1;~Piyush_Srivastava1",
        "gender": "M;M;",
        "homepage": "https://warwick.ac.uk/fac/sci/dcs/people/arnab_bhattacharyya/;https://fwm94.github.io/;",
        "dblp": "64/574.html;132/0888-1;",
        "google_scholar": "eECXWqUAAAAJ;uh1-AvwAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Arnab_Bhattacharyya1;~Weiming_Feng1;~Piyush_Srivastava1",
        "aff": "The University of Warwick;University of Hong Kong+ETHZ - ETH Zurich;",
        "aff_domain": "warwick.ac.uk;hku.hk+ethz.ch;",
        "position": "Associate Professor;Assistant Professor+Postdoc;",
        "bibtex": "@inproceedings{\nbhattacharyya2025approximating,\ntitle={Approximating the Total Variation Distance between Gaussians},\nauthor={Arnab Bhattacharyya and Weiming Feng and Piyush Srivastava},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=e4VaDn99jz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=e4VaDn99jz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eIyOtZ9tgl",
        "title": "Scalable Out-of-Distribution Robustness in the Presence of Unobserved Confounders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the task of out-of-distribution (OOD) generalization, where the distribution shift is due to an unobserved confounder ($Z$) affecting both the covariates ($X$) and the labels ($Y$). This confounding introduces heterogeneity in the predictor, i.e., $P(Y \\mid X) = E_{P(Z \\mid X)}[P(Y \\mid X,Z)]$, making traditional covariate and label shift assumptions unsuitable. OOD generalization differs from traditional domain adaptation in that it does not assume access to the covariate distribution ($X^\\text{te}$) of the test samples during training. These conditions create a challenging scenario for OOD robustness: (a) $Z^\\text{tr}$ is an unobserved confounder during training, (b) $P^\\text{te}(Z) \\neq P^\\text{tr}(Z)$, (c) $X^\\text{te}$ is unavailable during training, and (d) the predictive distribution depends on $P^\\text{te}(Z)$. While prior work has developed complex predictors requiring multiple additional variables for identifiability of the latent distribution, we explore a set of identifiability assumptions that yield a surprisingly simple predictor using only a single additional variable. Our approach demonstrates superior empirical performance on several benchmark tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Parjanya Prajakta Prashant;Seyedeh Baharan Khatami;Bruno Ribeiro;Babak Salimi",
        "authorids": "~Parjanya_Prajakta_Prashant1;~Seyedeh_Baharan_Khatami1;~Bruno_Ribeiro1;~Babak_Salimi1",
        "gender": ";F;M;M",
        "homepage": ";https://baharankh.github.io/;https://www.cs.purdue.edu/homes/ribeirob/;https://bsalimi.github.io/",
        "dblp": ";;15/606;",
        "google_scholar": "ZzPh3dcAAAAJ;;KIEleCsAAAAJ;",
        "orcid": ";;0000-0002-3527-6192;",
        "linkedin": ";bah-kh/;;",
        "or_profile": "~Parjanya_Prajakta_Prashant1;~Seyedeh_Baharan_Khatami1;~Bruno_Ribeiro1;~Babak_Salimi1",
        "aff": "University of California, San Diego;University of California, San Diego;Purdue University;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;purdue.edu;ucsd.edu",
        "position": "MS student;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nprashant2025scalable,\ntitle={Scalable Out-of-Distribution Robustness in the Presence of Unobserved Confounders},\nauthor={Parjanya Prajakta Prashant and Seyedeh Baharan Khatami and Bruno Ribeiro and Babak Salimi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eIyOtZ9tgl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eIyOtZ9tgl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eJkNMwzZzy",
        "title": "On the Sample Complexity of Next-Token Prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Next-token prediction with cross-entropy loss is the objective of choice in sequence and language modeling. Despite its widespread use, there is a lack of theoretical analysis regarding the generalization of models trained using this objective. In this work, we provide an analysis of empirical risk minimization for sequential inputs generated by order-$k$ Markov chains. Assuming bounded and Lipschitz logit functions, our results show that in-sample prediction error decays optimally with the number of tokens, whereas out-of-sample error incurs an additional term related to the mixing properties of the Markov chain. These rates depend on the statistical complexity of the hypothesis class and can lead to generalization errors that do not scale exponentially with the order of the Markov chain---unlike classical $k$-gram estimators. Finally, we discuss the possibility of achieving generalization rates independent of mixing.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "O\u011fuz Kaan Y\u00fcksel;Nicolas Flammarion",
        "authorids": "~O\u011fuz_Kaan_Y\u00fcksel1;~Nicolas_Flammarion1",
        "gender": ";M",
        "homepage": ";",
        "dblp": ";164/7417",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~O\u011fuz_Kaan_Y\u00fcksel1;~Nicolas_Flammarion1",
        "aff": ";Swiss Federal Institute of Technology Lausanne",
        "aff_domain": ";epfl.ch",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nyuksel2025on,\ntitle={On the Sample Complexity of Next-Token Prediction},\nauthor={O{\\u{g}}uz Kaan Y{\\\"u}ksel and Nicolas Flammarion},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eJkNMwzZzy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eJkNMwzZzy",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eWl1TW1biy",
        "title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The objective of transfer learning is to enhance estimation and inference in a target data by leveraging knowledge gained from additional sources. Recent studies have explored transfer learning for independent observations in complex, high-dimensional models assuming sparsity, yet research on time series models remains limited. Our focus is on transfer learning for sequences of observations with temporal dependencies and a more intricate model parameter structure. Specifically, we investigate the vector autoregressive model (VAR), a widely recognized model for time series data, where the transition matrix can be deconstructed into a combination of a sparse matrix and a low-rank one. We propose a new transfer learning algorithm tailored for estimating high-dimensional VAR models characterized by low-rank and sparse structures. Additionally, we present a novel approach for selecting informative observations from auxiliary datasets. Theoretical guarantees are established, encompassing model parameter consistency, informative set selection, and the asymptotic distribution of estimators under mild conditions. The latter facilitates the construction of entry-wise confidence intervals for model parameters. Finally, we demonstrate the empirical efficacy of our methodologies through both simulated and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mingliang Ma;Abolfazl Safikhani",
        "authorids": "~Mingliang_Ma1;~Abolfazl_Safikhani1",
        "gender": "M;M",
        "homepage": ";https://sites.google.com/site/abolfazlsafikhani/",
        "dblp": ";246/5524",
        "google_scholar": ";https://scholar.google.com/citations?hl=en",
        "orcid": ";",
        "linkedin": "mingliang-ma-b491bb239;",
        "or_profile": "~Mingliang_Ma1;~Abolfazl_Safikhani1",
        "aff": ";George Mason University",
        "aff_domain": ";gmu.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nma2025transfer,\ntitle={Transfer Learning for High-dimensional Reduced Rank Time Series Models},\nauthor={Mingliang Ma and Abolfazl Safikhani},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eWl1TW1biy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eWl1TW1biy",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eX94LTe7f1",
        "title": "Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of monitoring machine learning models under gradual distribution shifts, where circumstances change slowly over time, often leading to unnoticed yet significant declines in accuracy. To address this, we propose Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free method that estimates performance changes by modeling gradual shifts using optimal transport. In addition, IUPM quantifies the uncertainty in the performance prediction and introduces an active labeling procedure to restore a reliable estimate under a limited labeling budget. Our experiments show that IUPM outperforms existing performance estimation baselines in various gradual shift scenarios and that its uncertainty awareness guides label acquisition more effectively compared to other strategies.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexander Koebler;Thomas Decker;Ingo Thon;Volker Tresp;Florian Buettner",
        "authorids": "~Alexander_Koebler1;~Thomas_Decker1;~Ingo_Thon1;~Volker_Tresp1;~Florian_Buettner1",
        "gender": ";;;M;",
        "homepage": ";;;https://www.dbs.ifi.lmu.de/~tresp/;",
        "dblp": "343/0559;351/4532;26/5660;t/VolkerTresp;245/4220",
        "google_scholar": "lSZZT9UAAAAJ;Et8pIioAAAAJ;;xIJHTUwAAAAJ;AaPKbPAAAAAJ",
        "orcid": ";;;0000-0001-9428-3686;0000-0001-5587-6761",
        "linkedin": "alexander-koebler/;;;volker-tresp-8110a118/;",
        "or_profile": "~Alexander_Koebler1;~Thomas_Decker1;~Ingo_Thon1;~Volker_Tresp1;~Florian_Buettner1",
        "aff": "Siemens AG+Johann Wolfgang Goethe Universit\u00e4t Frankfurt am Main;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen+Siemens AG;;Ludwig Maximilian University of Munich+Siemens Corporate Research;Deutsches Krebsforschungszentrum",
        "aff_domain": "siemens.com+uni-frankfurt.de;lmu.de+siemens.com;;lmu.de+siemens.com;dkfz.de",
        "position": "PhD student+PhD student;PhD student+PhD student;;Associate Professor+Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nkoebler2025incremental,\ntitle={Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention},\nauthor={Alexander Koebler and Thomas Decker and Ingo Thon and Volker Tresp and Florian Buettner},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eX94LTe7f1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eX94LTe7f1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "enWAn9QtYz",
        "title": "On the Relationship Between Robustness and Expressivity of Graph Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip attacks (BFAs) by introducing an analytical framework to study the influence of architectural features, graph properties, and their interaction. The expressivity of GNNs refers to their ability to distinguish non-isomorphic graphs and depends on the encoding of node neighborhoods. We examine the vulnerability of neural multiset functions commonly used for this purpose and establish formal criteria to characterize a GNN's susceptibility to losing expressivity due to BFAs. This enables an analysis of the impact of homophily, graph structural variety, feature encoding, and activation functions on GNN robustness. We derive theoretical bounds for the number of bit flips required to degrade GNN expressivity on a dataset, identifying ReLU-activated GNNs operating on highly homophilous graphs with low-dimensional or one-hot encoded features as particularly susceptible. Empirical results using ten real-world datasets confirm the statistical significance of our key theoretical insights and offer actionable results to mitigate BFA risks in expressivity-critical applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lorenz Kummer;Wilfried N. Gansterer;Nils Morten Kriege",
        "authorids": "~Lorenz_Kummer1;~Wilfried_N._Gansterer1;~Nils_Morten_Kriege1",
        "gender": "M;;M",
        "homepage": "https://ufind.univie.ac.at/en/index.html;;https://kriegegroup.univie.ac.at/",
        "dblp": "298/2002;;97/8178",
        "google_scholar": "OhFnjr0AAAAJ;;https://scholar.google.de/citations?user=wGT17PcAAAAJ",
        "orcid": "0000-0001-6538-9107;;0000-0003-2645-947X",
        "linkedin": "lorenz-kummer-573a0017a/;;",
        "or_profile": "~Lorenz_Kummer1;~Wilfried_N._Gansterer1;~Nils_Morten_Kriege1",
        "aff": "Universit\u00e4t Wien;;Universit\u00e4t Vienna",
        "aff_domain": "univie.ac.at;;univie.ac.at",
        "position": "PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nkummer2025on,\ntitle={On the Relationship Between Robustness and Expressivity of Graph Neural Networks},\nauthor={Lorenz Kummer and Wilfried N. Gansterer and Nils Morten Kriege},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=enWAn9QtYz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=enWAn9QtYz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eq87myBFi3",
        "title": "Data Reconstruction Attacks and Defenses: A Systematic Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical grounding, and cannot disentangle the usefulness of defending methods from the computational limitation of attacking methods. In this work, we propose to view the problem as an inverse problem, enabling us to theoretically and systematically evaluate the data reconstruction attack. On various defense methods, we derived the algorithmic upper bound and the matching (in feature dimension and architecture dimension) information-theoretical lower bound on the reconstruction error for two-layer neural networks. To complement the theoretical results and investigate the utility-privacy trade-off, we defined a natural evaluation metric of the defense methods with similar utility loss among the strongest attacks. We further propose a strong reconstruction attack that helps update some previous understanding of the strength of defense methods under our proposed evaluation metric.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sheng Liu;Zihan Wang;Yuxiao Chen;Qi Lei",
        "authorids": "~Sheng_Liu2;~Zihan_Wang20;~Yuxiao_Chen6;~Qi_Lei1",
        "gender": ";M;M;F",
        "homepage": "https://shengliu66.github.io/;;https://github.com/cyx78;https://cecilialeiqi.github.io/",
        "dblp": ";;;",
        "google_scholar": "rzhzR-cAAAAJ;ZBF2zKMAAAAJ;;kGOgaowAAAAJ",
        "orcid": ";;;",
        "linkedin": ";zihan-wang-3b0050249/;;",
        "or_profile": "~Sheng_Liu2;~Zihan_Wang20;~Yuxiao_Chen6;~Qi_Lei1",
        "aff": "Stanford University;New York University;Peking University;New York University",
        "aff_domain": "stanford.edu;nyu.edu;stu.pku.edu.cn;nyu.edu",
        "position": "Postdoc;PhD student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025data,\ntitle={Data Reconstruction Attacks and Defenses: A Systematic Evaluation},\nauthor={Sheng Liu and Zihan Wang and Yuxiao Chen and Qi Lei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eq87myBFi3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eq87myBFi3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "euoG9AE8qF",
        "title": "Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Probabilistic classifiers are central for making informed decisions under uncertainty. Based on the maximum expected utility principle, optimal decision rules can be derived using the posterior class probabilities and misclassification costs. Yet, in practice only learned approximations of the oracle posterior probabilities are available. In this work, we quantify the excess risk (a.k.a. regret) incurred using approximate posterior probabilities in batch binary decision-making. We provide analytical expressions for miscalibration-induced regret ($R^{CL}$), as well as tight and informative upper and lower bounds on the regret of calibrated classifiers ($R^{GL}$). These expressions allow us to identify regimes where recalibration alone addresses most of the regret, and regimes where the regret is dominated by the grouping loss, which calls for post-training beyond recalibration. Crucially, both $R^{CL}$ and $R^{Gl}$ can be estimated in practice using a calibration curve and a recent grouping loss estimator. On NLP experiments, we show that these quantities identify when the expected gain of more advanced post-training is worth the operational cost. Finally, we highlight the potential of multicalibration approaches as efficient alternatives to costlier fine-tuning approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexandre Perez-Lebel;Gael Varoquaux;Sanmi Koyejo;Matthieu Doutreligne;Marine Le Morvan",
        "authorids": "~Alexandre_Perez-Lebel1;~Gael_Varoquaux1;~Sanmi_Koyejo1;~Matthieu_Doutreligne1;~Marine_Le_Morvan2",
        "gender": ";M;;M;F",
        "homepage": ";http://gael-varoquaux.info;;https://info.m2dou.fr/;https://marinelm.github.io/",
        "dblp": ";36/7585;;https://dblp.uni-trier.de/pid/228/5806.html;202/2253",
        "google_scholar": "A0M2jTMAAAAJ;https://scholar.google.fr/citations?user=OGGu384AAAAJ;;lXOz9tkAAAAJ;wbTRhwcAAAAJ",
        "orcid": "my-orcid?orcid=0000-0003-0556-0763;;;;",
        "linkedin": "perez-alexandre/;;;;",
        "or_profile": "~Alexandre_Perez-Lebel1;~Gael_Varoquaux1;~Sanmi_Koyejo1;~Matthieu_Doutreligne1;~Marine_Le_Morvan2",
        "aff": ";INRIA;;INRIA;INRIA",
        "aff_domain": ";inria.fr;;inria.fr;inria.fr",
        "position": ";Full Professor;;PhD student;Researcher",
        "bibtex": "@inproceedings{\nperez-lebel2025decision,\ntitle={Decision from Suboptimal Classifiers: Excess Risk Pre- and Post-Calibration},\nauthor={Alexandre Perez-Lebel and Gael Varoquaux and Sanmi Koyejo and Matthieu Doutreligne and Marine Le Morvan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=euoG9AE8qF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=euoG9AE8qF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ezTiGtV1O2",
        "title": "Stein Boltzmann Sampling: A Variational Approach for Global Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we present a deterministic particle-based method for global optimization of continuous Sobolev functions, called *Stein Boltzmann Sampling* (SBS). SBS initializes uniformly a number of particles representing candidate solutions, then uses the *Stein Variational Gradient Descent* (SVGD) algorithm to sequentially and deterministically move those particles in order to approximate a target distribution whose mass is concentrated around promising areas of the domain of the optimized function. The target is chosen to be a properly parametrized Boltzmann distribution. For the purpose of global optimization, we adapt the generic SVGD theoretical framework allowing to address more general target distributions over a compact subset of $\\mathbb{R}^d$, and we prove SBS's asymptotic convergence. In addition to the main SBS algorithm, we present two variants: the SBS-PF that includes a particle filtering strategy, and the SBS-HYBRID one that uses SBS or SBS-PF as a continuation after other particle- or distribution-based optimization methods. A detailed comparison with state-of-the-art methods on benchmark functions demonstrates that SBS and its variants are highly competitive, while the combination of the two variants provides the best trade-off between accuracy and computational cost.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ga\u00ebtan Serr\u00e9;Argyris Kalogeratos;Nicolas Vayatis",
        "authorids": "~Ga\u00ebtan_Serr\u00e91;~Argyris_Kalogeratos1;~Nicolas_Vayatis1",
        "gender": "M;M;M",
        "homepage": "http://www.gaetanserre.fr/;http://kalogeratos.com;",
        "dblp": ";06/1545;00/582",
        "google_scholar": "C2T0J0wAAAAJ;oEcLSFEAAAAJ;",
        "orcid": ";0000-0003-1436-3593;",
        "linkedin": ";argyris-kalogeratos-9ab09026/;",
        "or_profile": "~Ga\u00ebtan_Serr\u00e91;~Argyris_Kalogeratos1;~Nicolas_Vayatis1",
        "aff": "\u00c9cole Normale Sup\u00e9rieure Paris-Saclay;Ecole Normale Superieure;Ecole Normale Superieure Paris-Saclay",
        "aff_domain": "ens-paris-saclay.fr;ens-paris-saclay.fr;ens-paris-saclay.fr",
        "position": "PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nserre2025stein,\ntitle={Stein Boltzmann Sampling: A Variational Approach for Global Optimization},\nauthor={Ga{\\\"e}tan Serr{\\'e} and Argyris Kalogeratos and Nicolas Vayatis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ezTiGtV1O2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ezTiGtV1O2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eznvJaqwz2",
        "title": "Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Variational Inference (VI) optimizes varia- tional parameters to closely align a variational distribution with the true posterior, being ap- proached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a variational parameter space. Subsequently, we propose Wasserstein gradient descent for solving this optimization, where black-box VI and natural-gradient VI can be interpreted as special cases of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the pro- posed methods through experiments on syn- thetic and real-world datasets, supplemented by theoretical analyses.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dai Hai Nguyen;Tetsuya Sakurai;Hiroshi Mamitsuka",
        "authorids": "~Dai_Hai_Nguyen1;~Tetsuya_Sakurai2;~Hiroshi_Mamitsuka1",
        "gender": ";M;",
        "homepage": ";http://www.cs.tsukuba.ac.jp/~sakurai/index.html;",
        "dblp": ";89/3511.html;59/2081",
        "google_scholar": ";;",
        "orcid": ";0000-0002-5789-7547;",
        "linkedin": ";;",
        "or_profile": "~Dai_Hai_Nguyen1;~Tetsuya_Sakurai2;~Hiroshi_Mamitsuka1",
        "aff": ";University of Tsukuba, The University of Tsukuba;",
        "aff_domain": ";cs.tsukuba.ac.jp;",
        "position": ";Full Professor;",
        "bibtex": "@inproceedings{\nnguyen2025wasserstein,\ntitle={Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference},\nauthor={Dai Hai Nguyen and Tetsuya Sakurai and Hiroshi Mamitsuka},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=eznvJaqwz2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eznvJaqwz2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "f4nWNn0RjV",
        "title": "ChronosX: Adapting Pretrained Time Series Models with Exogenous Variables",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Covariates provide valuable information on external factors that influence time series and are critical in many real-world time series forecasting tasks. For example, in retail, covariates may indicate promotions or peak dates such as holiday seasons that heavily influence demand forecasts. Recent advances in pretraining large language model architectures for time series forecasting have led to highly accurate forecasters. However, the majority of these models do not readily use covariates as they are often specific to a certain task or domain. This paper introduces a new method to incorporate covariates into pretrained time series forecasting models. Our proposed approach incorporates covariate information into pretrained forecasting models through modular blocks that inject past and future covariate information, without necessarily modifying the pretrained model in consideration. In order to evaluate our approach, we introduce a benchmark composed of 32 different synthetic datasets with varying dynamics to evaluate the effectivity of forecasting models with covariates. Extensive evaluations on both synthetic and real datasets show that our approach effectively incorporates covariate information into pretrained models, outperforming existing baselines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sebastian Pineda Arango;Pedro Mercado;Shubham Kapoor;Abdul Fatir Ansari;Lorenzo Stella;Huibin Shen;Hugo Henri Joseph Senetaire;Ali Caner Turkmen;Oleksandr Shchur;Danielle C. Maddix;Michael Bohlke-Schneider;Bernie Wang;Syama Sundar Rangapuram",
        "authorids": "~Sebastian_Pineda_Arango1;~Pedro_Mercado2;~Shubham_Kapoor1;~Abdul_Fatir_Ansari2;~Lorenzo_Stella1;~Huibin_Shen1;~Hugo_Henri_Joseph_Senetaire1;~Ali_Caner_Turkmen1;~Oleksandr_Shchur1;~Danielle_C._Maddix1;~Michael_Bohlke-Schneider1;~Bernie_Wang1;~Syama_Sundar_Rangapuram1",
        "gender": "M;;;M;M;M;M;M;M;;M;M;M",
        "homepage": ";;;https://abdulfatir.com;https://lostella.github.io/;;https://orbit.dtu.dk/en/persons/hugo-henri-joseph-s%C3%A9n%C3%A9taire;http://caner.io;http://shchur.github.io/;https://dcmaddix.github.io/;;http://web.mit.edu/~ywang02/www/;",
        "dblp": "271/4257;;193/4889;202/5475.html;158/5141;118/6073;;;210/2544;216/8804;242/8809;43/8355-1;30/11348",
        "google_scholar": "8UI_0B0AAAAJ;;4LLHaikAAAAJ;https://scholar.google.com.sg/citations?user=BZ0EoqIAAAAJ;Y3ag8YsAAAAJ;https://scholar.google.de/citations?view_op=list_works;SY5DUXkAAAAJ;;np39q6IAAAAJ;IPDByA8AAAAJ;https://scholar.google.de/citations?user=19k2WQEAAAAJ;IKUm624AAAAJ;i3UOvhYAAAAJ",
        "orcid": ";;;;0000-0002-8304-7327;;;;;;0000-0002-4969-2218;0000-0002-0291-7184;",
        "linkedin": "sebaspine/;;shubhamkapoor007;abdulfatir/;;;https://fr.linkedin.com/in/hugo-senetaire-5771a1142;;;danielle-maddix-robinson/;michael-bohlke-schneider-16a4ab93/;;",
        "or_profile": "~Sebastian_Pineda_Arango1;~Pedro_Mercado2;~Shubham_Kapoor1;~Abdul_Fatir_Ansari2;~Lorenzo_Stella1;~Huibin_Shen1;~Hugo_Henri_Joseph_Senetaire1;~Ali_Caner_Turkmen1;~Oleksandr_Shchur1;~Danielle_C._Maddix1;~Michael_Bohlke-Schneider1;~Bernie_Wang1;~Syama_Sundar_Rangapuram1",
        "aff": "Universit\u00e4t Freiburg;;Amazon;AWS AI Labs (Amazon);Amazon;Amazon;Technical University of Denmark;Amazon;Amazon;AWS AI Labs;;Amazon;Amazon Development Center Germany",
        "aff_domain": "uni-freiburg.de;;amazon.com;amazon.de;amazon.com;amazon.com;dtu.dk;amazon.com;amazon.com;amazon.com;;amazon.com;amazon.de",
        "position": "PhD student;;Senior Applied Scientist ;Senior Scientist;Researcher;Machine Learning Scientist;PhD student;Applied Scientist;Researcher;Applied Scientist;;Principal Researcher;Researcher",
        "bibtex": "@inproceedings{\narango2025chronosx,\ntitle={ChronosX: Adapting Pretrained Time Series Models with Exogenous Variables},\nauthor={Sebastian Pineda Arango and Pedro Mercado and Shubham Kapoor and Abdul Fatir Ansari and Lorenzo Stella and Huibin Shen and Hugo Henri Joseph Senetaire and Ali Caner Turkmen and Oleksandr Shchur and Danielle C. Maddix and Bernie Wang and Michael Bohlke-Schneider and Syama Sundar Rangapuram},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=f4nWNn0RjV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=f4nWNn0RjV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fThPOZ1GKW",
        "title": "On adaptivity and minimax optimality of two-sided nearest neighbors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Nearest neighbor (NN) algorithms have been extensively used for missing data problems in recommender systems and sequential decision-making systems. Prior theoretical analysis has established favorable guarantees for NN when the underlying data is sufficiently smooth and the missingness probabilities are lower bounded. Here we analyze NN with non-smooth non-linear functions with vast amounts of missingness. In particular, we consider matrix completion settings where the entries of the underlying matrix follow an latent non-linear factor model, with the non-linearity belonging to a H\u00f6lder function class that is less smooth than Lipschitz. Our results establish following favorable properties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN adapts to the smoothness of the non-linearity, (2) under certain regularity conditions, the NN error rate matches the rate obtained by an oracle equipped with the knowledge of both the row and column latent factors, and finally (3) NN's MSE is non-trivial for a wide range of settings even when several matrix entries might be missing deterministically. We support our theoretical findings via extensive numerical simulations and a case study with data from a mobile health study, HeartSteps.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tathagata Sadhukhan;Manit Paul;Raaz Dwivedi",
        "authorids": "~Tathagata_Sadhukhan1;~Manit_Paul1;~Raaz_Dwivedi1",
        "gender": "M;M;M",
        "homepage": ";https://statistics.wharton.upenn.edu/profile/paulman/;https://raazdwivedi.github.io/",
        "dblp": ";;180/9006",
        "google_scholar": "HdLuMawAAAAJ;sR3rcjAAAAAJ;9ehX_58AAAAJ",
        "orcid": ";;",
        "linkedin": "tathagata-sadhukhan-31a81a23a/;;raaz-dwivedi",
        "or_profile": "~Tathagata_Sadhukhan1;~Manit_Paul1;~Raaz_Dwivedi1",
        "aff": "Cornell University;The Wharton School, University of Pennsylvania;Cornell University",
        "aff_domain": "cornell.edu;wharton.upenn.edu;cornell.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nsadhukhan2025on,\ntitle={On adaptivity and minimax optimality of two-sided nearest neighbors},\nauthor={Tathagata Sadhukhan and Manit Paul and Raaz Dwivedi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fThPOZ1GKW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fThPOZ1GKW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fVVoSOUQ8U",
        "title": "Quantile Additive Trend Filtering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces and analyzes quantile additive trend filtering, a novel approach to model the conditional quantiles of the response variable given multivariate covariates. Under the assumption that the true model is additive, and that the components are functions whose $r$th order weak derivatives have bounded total variation, our estimator is a constrained version of quantile trend filtering within additive models. The primary theoretical contributions are the error rate of our estimator in both fixed and growing input dimensions. In the fixed dimension case, we show that our estimator attains a rate that mirrors the non-quantile minimax rate for additive trend filtering, featuring the main term $n^{-2r/(2r+1)}$. For growing input dimension ($d$), our rate has an additional polynomial factor $d^{(2r+2)/(2r+1)}$.\nWe propose a practical algorithm for implementing quantile additive trend filtering using dimension-wise backfitting.  Experiments in both real data and simulations confirm our theoretical findings. We provide a public implementation of the algorithm at https://github.com/zzh237/QATF.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhi Zhang;Kyle Ritscher;OSCAR HERNAN MADRID PADILLA",
        "authorids": "~Zhi_Zhang1;~Kyle_Ritscher1;~OSCAR_HERNAN_MADRID_PADILLA2",
        "gender": ";M;",
        "homepage": ";;https://hernanmp.github.io/",
        "dblp": ";;",
        "google_scholar": "O__axAoAAAAJ;https://scholar.google.com/citations?view_op=list_works;",
        "orcid": ";;",
        "linkedin": ";kyleritscher33/;",
        "or_profile": "~Zhi_Zhang1;~Kyle_Ritscher1;~OSCAR_HERNAN_MADRID_PADILLA2",
        "aff": "University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu;ucla.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025quantile,\ntitle={Quantile Additive Trend Filtering},\nauthor={Zhi Zhang and Kyle Ritscher and OSCAR HERNAN MADRID PADILLA},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fVVoSOUQ8U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fVVoSOUQ8U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "faax2oJkba",
        "title": "Synthesis and Analysis of Data as Probability Measures With Entropy-Regularized Optimal Transport",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider synthesis and analysis of probability measures using the entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn divergence. The synthesis problem consists of computing the barycenter, with respect to these costs, of reference measures given a set of coefficients belonging to the simplex. The analysis problem consists of finding the coefficients for the closest barycenter in the Wasserstein-2 distance to a given measure. Under the weakest assumptions on the measures thus far in the literature, we compute the derivative of the entropy-regularized Wasserstein-2 cost.  We leverage this to establish a characterization of barycenters with respect to the entropy-regularized Wasserstein-2 cost as solutions that correspond to a fixed point of an average of the entropy-regularized displacement maps.  This characterization yields a finite-dimensional, convex, quadratic program for solving the analysis problem when the measure being analyzed is a barycenter with respect to the entropy-regularized Wasserstein-2 cost. We show that these coefficients, as well as the value of the barycenter functional, can be estimated from samples with dimension-independent rates of convergence, and that barycentric coefficients are stable with respect to perturbations in the Wasserstein-2 metric. We employ the barycentric coefficients as features for classification of corrupted point cloud data, and show that compared to neural network baselines, our approach is more efficient in small training data regimes.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Brendan Mallery;James M. Murphy;Shuchin Aeron",
        "authorids": "~Brendan_Mallery1;~James_M._Murphy1;~Shuchin_Aeron2",
        "gender": "M;M;M",
        "homepage": "https://brendan-mallery.com/;https://jmurphy.math.tufts.edu/;https://sites.google.com/view/shuchin-aeron/home?authuser=0",
        "dblp": ";;14/6374",
        "google_scholar": ";;T7TUmRMAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Brendan_Mallery1;~James_M._Murphy1;~Shuchin_Aeron2",
        "aff": "Tufts University;;Tufts University+Tufts University",
        "aff_domain": "tufts.edu;;tufts.edu+tufts.edu",
        "position": "PhD student;;Assistant Professor+Associate Professor",
        "bibtex": "@inproceedings{\nmallery2025synthesis,\ntitle={Synthesis and Analysis of Data as Probability Measures With Entropy-Regularized Optimal Transport},\nauthor={Brendan Mallery and James M. Murphy and Shuchin Aeron},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=faax2oJkba}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=faax2oJkba",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fgEJ8iEqjk",
        "title": "Towards Regulatory-Confirmed Adaptive Clinical Trials: Machine Learning Opportunities and Solutions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Randomized Controlled Trials (RCTs) are the gold standard for evaluating the effect of new medical treatments. Treatments must pass stringent regulatory conditions in order to be approved for widespread use, yet even after the regulatory barriers are crossed, real-world challenges might arise: Who should get the treatment? What is its true clinical utility? Are there discrepancies in the treatment effectiveness across diverse and under-served populations? We introduce two new objectives for future clinical trials that integrate regulatory constraints and treatment policy value for both the entire population and under-served populations, thus answering some of the questions above in advance. Designed to meet these objectives, we formulate Randomize First Augment Next (RFAN), a new framework for designing Phase III clinical trials. Our framework consists of a standard randomized component followed by an adaptive one, jointly meant to efficiently and safely acquire and assign patients into treatment arms during the trial. Then, we propose strategies for implementing RFAN based on causal, deep Bayesian active learning. Finally, we empirically evaluate the performance of our framework using synthetic and real-world semi-synthetic datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Omer Noy Klein;Alihan H\u00fcy\u00fck;Ron Shamir;Uri Shalit;Mihaela van der Schaar",
        "authorids": "~Omer_Noy_Klein1;~Alihan_H\u00fcy\u00fck1;~Ron_Shamir1;~Uri_Shalit1;~Mihaela_van_der_Schaar2",
        "gender": ";;M;M;F",
        "homepage": ";;http://www.cs.tau.ac.il/~rshamir/;;https://www.vanderschaar-lab.com",
        "dblp": ";227/2296;;87/7049;",
        "google_scholar": ";EMq6KwMAAAAJ;https://scholar.google.com.tw/citations?user=ri4rEPMAAAAJ;https://scholar.google.co.il/citations?user=aeGDj-IAAAAJ;DZ3S--MAAAAJ",
        "orcid": ";;;0000-0002-4026-2692;",
        "linkedin": ";;;;",
        "or_profile": "~Omer_Noy_Klein1;~Alihan_H\u00fcy\u00fck1;~Ron_Shamir1;~Uri_Shalit1;~Mihaela_van_der_Schaar2",
        "aff": ";Harvard University;Tel Aviv University;Tel Aviv University+Google;University of Cambridge+University of California, Los Angeles",
        "aff_domain": ";harvard.edu;tau.ac.il;tau.ac.il+google.com;cam.ac.uk+ucla.edu",
        "position": ";Postdoc;Full Professor;Associate Professor+Researcher;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nklein2025towards,\ntitle={Towards Regulatory-Confirmed Adaptive Clinical Trials: Machine Learning Opportunities and Solutions},\nauthor={Omer Noy Klein and Alihan H{\\\"u}y{\\\"u}k and Ron Shamir and Uri Shalit and Mihaela van der Schaar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fgEJ8iEqjk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fgEJ8iEqjk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fiKtvQN1Sj",
        "title": "Advancing Fairness in Precision Medicine: A Universal Framework for Optimal Treatment Estimation in Censored Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In healthcare and precision medicine, estimating optimal treatment regimes for right-censored data while ensuring fairness across ethnic subgroups is crucial but remains underexplored. The problem presents two key challenges: measuring heterogeneous treatment effects (HTE) under fairness constraints and dealing with censoring mechanisms. We propose a general framework for estimating HTE using nonparametric methods and integrating user-controllable fairness constraints to address these problems. Under mild regularization assumptions, our method is theoretically grounded, demonstrating the double robustness property of the HTE estimator. Using this framework, we demonstrate that optimal treatment strategies balance fairness and utility. Using extensive simulations and real-world data analysis, we uncovered the potential of this method to guide the selection of treatment methods that are equitable and effective.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hongni Wang;Junxi Zhang;Na Li;Linglong Kong;Bei Jiang;Xiaodong Yan",
        "authorids": "~Hongni_Wang1;~Junxi_Zhang1;~Na_Li14;~Linglong_Kong2;~Bei_Jiang1;~Xiaodong_Yan1",
        "gender": "F;M;F;M;F;M",
        "homepage": ";https://junxi-zhang.github.io/;https://sam.sdufe.edu.cn/info/1105/2503.htm;https://www.ualberta.ca/~lkong;https://www.ualberta.ca/~bei1;https://yanxiaodong128.github.io/index.html",
        "dblp": ";;;35/8525;190/4697;",
        "google_scholar": ";https://scholar.google.ca/citations?hl=en;;https://scholar.google.ca/citations?hl=en;https://scholar.google.ca/citations?user=MfOZ8G0AAAAJ;",
        "orcid": "0000-0002-5786-2715;0000-0001-5318-2045;;0000-0003-3011-9216;0000-0002-0033-839X;",
        "linkedin": ";;;;;",
        "or_profile": "~Hongni_Wang1;~Junxi_Zhang1;~Na_Li14;~Linglong_Kong2;~Bei_Jiang1;~Xiaodong_Yan1",
        "aff": "Shandong University of Finance and Economics;Concordia University;Shandong University of Finance and Economics;University of Alberta;University of Alberta;Shandong University",
        "aff_domain": "sdufe.edu.cn;concordia.ca;sdufe.edu.cn;ualberta.ca;ualberta.ca;edu.cn",
        "position": "PhD student;Assistant Professor;Full Professor;Full Professor;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nwang2025advancing,\ntitle={Advancing Fairness in Precision Medicine: A Universal Framework for Optimal Treatment Estimation in Censored Data},\nauthor={Hongni Wang and Junxi Zhang and Na Li and Bei Jiang and Linglong Kong and Xiaodong Yan},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fiKtvQN1Sj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fiKtvQN1Sj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fm4PSeWaaQ",
        "title": "Asynchronous Decentralized Optimization with Constraints: Achievable Speeds of Convergence for Directed Graphs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a novel decentralized convex optimization algorithm called ASY-DAGP, where each agent has its own distinct objective function and constraint set. Agents compute at different speeds, and their communication is delayed and directed. Employing local buffers, ASY-DAGP enhances asynchronous communication and is robust to challenging scenarios such as message failure. We validate these features by numerical experiments. By analyzing ASY-DAGP, we provide the first sublinear convergence rate for the above setup under mild assumptions. This rate depends on a novel characterization of delay profiles, which we term the delay factor. We calculate the delay factor for the well-known bounded delay profiles, providing new insights for these scenarios. Our analysis is conducted by introducing a novel approach tied to the celebrated PEP framework. Our approach does not require the design of Lyapunov functions and instead provides a novel insight into the optimization algorithms as linear systems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Firooz Shahriari-Mehr;Ashkan Panahi",
        "authorids": "~Firooz_Shahriari-Mehr1;~Ashkan_Panahi1",
        "gender": "M;M",
        "homepage": ";",
        "dblp": "282/4482;94/9875",
        "google_scholar": ";",
        "orcid": "0000-0003-2374-2341;",
        "linkedin": "firooz-shahriari-a3335b173/;",
        "or_profile": "~Firooz_Shahriari-Mehr1;~Ashkan_Panahi1",
        "aff": "Chalmers University of Technology;Chalmers University of Technology",
        "aff_domain": "chalmers.se;chalmers.se",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nshahriari-mehr2025asynchronous,\ntitle={Asynchronous Decentralized Optimization with Constraints: Achievable Speeds of Convergence for Directed Graphs},\nauthor={Firooz Shahriari-Mehr and Ashkan Panahi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fm4PSeWaaQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fm4PSeWaaQ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ftE0BWlWoY",
        "title": "Axiomatic Explainer Globalness via Optimal Transport",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Explainability methods are often challenging to evaluate and compare. With a multitude of explainers available, practitioners must often compare and select explainers based on quantitative evaluation metrics. One particular differentiator between explainers is the diversity of explanations for a given dataset; i.e. whether all explanations are identical, unique and uniformly distributed, or somewhere between these two extremes. In this work, we define a complexity measure for explainers, globalness, which enables deeper understanding of the distribution of explanations produced by feature attribution and feature selection methods for a given dataset. We establish the axiomatic properties that any such measure should possess and prove that our proposed measure, Wasserstein Globalness, meets these criteria. We validate the utility of Wasserstein Globalness using image, tabular, and synthetic datasets, empirically showing that it both facilitates meaningful comparison between explainers and improves the selection process for explainability methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Davin Hill;Joshua Bone;Aria Masoomi;Max Torop;Jennifer Dy",
        "authorids": "~Davin_Hill1;~Joshua_Bone1;~Aria_Masoomi1;~Max_Torop1;~Jennifer_Dy1",
        "gender": ";M;M;M;",
        "homepage": ";https://josh-bone.github.io/;;https://maxtorop.github.io/;https://mllabneu.github.io/",
        "dblp": ";;242/9324;305/7085;24/6000",
        "google_scholar": ";cwUkEJ8AAAAJ;KXcX8coAAAAJ;NjhrmBEAAAAJ;6h7b0fAAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;aria-masoomi-779a02232;max-torop-048ab4a9/;",
        "or_profile": "~Davin_Hill1;~Joshua_Bone1;~Aria_Masoomi1;~Max_Torop1;~Jennifer_Dy1",
        "aff": ";Northeastern University;Northeastern University;Northeastern University;Northeastern University",
        "aff_domain": ";neu.edu;northeastern.edu;northeastern.edu;northeastern.edu",
        "position": ";PhD student;PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nhill2025axiomatic,\ntitle={Axiomatic Explainer Globalness via Optimal Transport},\nauthor={Davin Hill and Joshua Bone and Aria Masoomi and Max Torop and Jennifer Dy},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ftE0BWlWoY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ftE0BWlWoY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fy1c84vhAD",
        "title": "Application of Structured State Space Models to High energy physics with locality sensitive hashing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern high-energy physics (HEP) experiments are increasingly challenged by the vast size and complexity of their datasets, particularly regarding large-scale point cloud processing and long sequences. In this study, to address these challenges, we explore the application of structured state space models (SSMs), proposing one of the first trials to integrate local-sensitive hashing into either a hybrid or pure Mamba Model. Our results demonstrate that pure SSMs could serve as powerful backbones for HEP problems involving tasks for long sequence data with local inductive bias. By integrating locality-sensitive hashing into Mamba blocks, we achieve significant improvements over traditional backbones in key HEP tasks, surpassing them in inference speed and physics metrics while reducing computational overhead. In key tests, our approach demonstrated promising results, presenting a viable alternative to traditional transformer backbones by significantly reducing FLOPS while maintaining robust performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Cheng Jiang;Sitian Qian",
        "authorids": "~Cheng_Jiang5;~Sitian_Qian1",
        "gender": "M;M",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": "0009-0005-0253-5716;",
        "linkedin": ";sitian-qian-7154241a0",
        "or_profile": "~Cheng_Jiang5;~Sitian_Qian1",
        "aff": "University of Edinburgh;Peking University, Tsinghua University",
        "aff_domain": "edinburgh.org;pku.edu.cn",
        "position": "PhD student;PhD student",
        "bibtex": "@inproceedings{\njiang2025application,\ntitle={Application of Structured State Space Models to High energy physics with locality sensitive hashing},\nauthor={Cheng Jiang and Sitian Qian},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=fy1c84vhAD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fy1c84vhAD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "g5ml9INmja",
        "title": "Optimizing Neural Network Training and Quantization with Rooted Logistic Objectives",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "First-order methods are widely employed for training neural networks that are used in practical applications. For classification of input features, Cross-Entropy based loss functions are often preferred since they are differentiable everywhere. Recent optimization results show that the convergence properties of first-order methods such as gradient descent are intricately tied to the separability of datasets and the induced loss landscape. We introduce  Rooted Logistic Objectives (RLO) to improve practical convergence behavior with benefits for downstream tasks. We show that our proposed loss satisfies strict convexity properties and has better condition number properties that will benefit practical implementations. To evaluate our proposed RLO, we compare its performance on various classification benchmarks. Our results illustrate that training procedure converges faster with RLO in many cases. Furthermore, on two downstream tasks viz., post-training quantization and finetuning on quantized space, we show that it is possible to ensure lower performance degradation while using reduced precision for sequence prediction tasks in large language models over state of the art methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhu Wang;Praveen Raj Veluswami;Harsh Mishra;Sathya N. Ravi",
        "authorids": "~Zhu_Wang2;~Praveen_Raj_Veluswami1;~Harsh_Mishra1;~Sathya_N._Ravi1",
        "gender": "F;M;M;M",
        "homepage": ";;;http://sathyaravi.com",
        "dblp": ";;;159/2123",
        "google_scholar": "mMyQX4oAAAAJ;;;FW-0thoAAAAJ",
        "orcid": ";;;0000-0003-3881-6323",
        "linkedin": ";praveenraj-v/;harsh-mishra-515624144;sathya-narayanan-ravi-74a5a128/",
        "or_profile": "~Zhu_Wang2;~Praveen_Raj_Veluswami1;~Harsh_Mishra1;~Sathya_N._Ravi1",
        "aff": "Northwestern University+University of Illinois at Chicago;University of Illinois at Chicago;Rothamsted Research ;University of Illinois, Chicago",
        "aff_domain": "northwestern.edu+cs.uic.edu;uic.edu;rothamsted.ac.uk;uic.edu",
        "position": "Postdoc+PhD student;Research Assistant Volunteer;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025optimizing,\ntitle={Optimizing Neural Network Training and Quantization with Rooted Logistic Objectives},\nauthor={Zhu Wang and Praveen Raj Veluswami and Harsh Mishra and Sathya N. Ravi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=g5ml9INmja}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=g5ml9INmja",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "g6Pv8jRgqu",
        "title": "Linear Submodular Maximization with Bandit Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Leveraging the intrinsic structure of submodular functions to design more sample-efficient algorithms for submodular maximization (SM) has gained significant attention in recent studies. In a number of real-world applications such as diversified recommender systems and data summarization, the submodular function exhibits additional linear structure. In this paper, we consider the problem of linear submodular maximization under the bandit feedback in the pure-exploration setting, where the submodular objective function is defined as $f:2^U \\rightarrow\\mathbb{R}\\_{\\ge 0}$, where $f=\\sum_{i=1}^dw_iF_{i}$. It is assumed that we have value oracle access to the functions $F_i$, but the coefficients $w_i$ are unknown, and $f$ can only be accessed via noisy queries. To harness the linear structure, \nwe develop algorithms inspired by adaptive allocation algorithms in the best-arm identification for the linear bandit, with approximation guarantees arbitrarily close to the setting where we have value oracle access to $f$. Our approach efficiently leverages information from prior samples, offering a significant improvement in sample efficiency. Experimental results on both synthetic datasets and real-world datasets demonstrate the superior performance of our method compared to baseline algorithms, particularly in terms of sample efficiency.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenjing Chen;Victoria G. Crawford",
        "authorids": "~Wenjing_Chen1;~Victoria_G._Crawford1",
        "gender": "F;F",
        "homepage": "https://momofodr.github.io/;https://people.tamu.edu/~vcrawford/",
        "dblp": "74/8490-1;199/1873",
        "google_scholar": "EM6QsYgAAAAJ;iJeTOGAAAAAJ",
        "orcid": ";",
        "linkedin": "wenjing-chen-ba0a351b2/;",
        "or_profile": "~Wenjing_Chen1;~Victoria_G._Crawford1",
        "aff": "Texas A&M University - College Station;Texas A&M University - College Station",
        "aff_domain": "tamu.edu;tamu.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nchen2025linear,\ntitle={Linear Submodular Maximization with Bandit Feedback},\nauthor={Wenjing Chen and Victoria G. Crawford},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=g6Pv8jRgqu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=g6Pv8jRgqu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "g8DVAMyn1j",
        "title": "Fixed-Budget Change Point Identification in Piecewise Constant Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the piecewise constant bandit problem where the expected reward is a piecewise constant function with one change point (discontinuity) across the action space $[0,1]$ and the learner's aim is to locate the change point. Under the assumption of a fixed exploration budget, we provide the first non-asymptotic analysis of policies designed to locate abrupt changes in the mean reward function under bandit feedback. We study the problem under a large and small budget regime, and for both settings establish lower bounds on the error probability and provide algorithms with near matching upper bounds. Interestingly, our results show a separation in the complexity of the two regimes. We then propose a regime adaptive algorithm which is near optimal for both small and large budgets simultaneously.\nWe complement our theoretical analysis with experimental results in simulated environments to support our findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Joseph Lazzaro;Ciara Pike-Burke",
        "authorids": "~Joseph_Lazzaro1;~Ciara_Pike-Burke2",
        "gender": ";",
        "homepage": ";https://www.ma.imperial.ac.uk/~cpikebur/",
        "dblp": ";202/1263",
        "google_scholar": ";Hl1vu1MAAAAJ",
        "orcid": ";",
        "linkedin": "https://linkedin.com/in/joseph-lazzaro-33a7ab227;",
        "or_profile": "~Joseph_Lazzaro1;~Ciara_Pike-Burke2",
        "aff": "Imperial College London;Imperial College London",
        "aff_domain": "imperial.ac.uk;imperial.ac.uk",
        "position": "PhD student;Lecturer",
        "bibtex": "@inproceedings{\nlazzaro2025fixedbudget,\ntitle={Fixed-Budget Change Point Identification in Piecewise Constant Bandits},\nauthor={Joseph Lazzaro and Ciara Pike-Burke},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=g8DVAMyn1j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=g8DVAMyn1j",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "gJkLbMH9zR",
        "title": "LMEraser: Large Model Unlearning via Adaptive Prompt Tuning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To address the growing demand for privacy protection in machine learning, we propose an efficient and exact machine unlearning method for Large Models, called LMEraser. LMEraser takes a divide-and-conquer strategy with an adaptive prompt tuning mechanism to isolate data influence effectively. The training dataset is partitioned into public and private datasets. Public data are used to train the backbone of the model. Private data are clustered based on their diversity, and each cluster tunes a tailored prompt independently. This approach enables targeted unlearning by updating affected prompts, significantly reduces unlearning costs and maintains high model performance. Evaluations show that LMEraser reduces unlearning costs by 100 times compared to prior work without compromising model utility.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jie Xu;Zihan Wu;Cong Wang;Xiaohua Jia",
        "authorids": "~Jie_Xu21;~Zihan_Wu2;~Cong_Wang10;~Xiaohua_Jia1",
        "gender": "F;;;M",
        "homepage": "https://xujie.ink/;;;http://www.cs.cityu.edu.hk/~jia",
        "dblp": "37/5126-31;;;j/XiaohuaJia.html",
        "google_scholar": "https://scholar.google.com.hk/citations?hl=zh-CN;;;",
        "orcid": "0000-0002-9924-4157;;;0000-0001-8702-8302",
        "linkedin": ";;;",
        "or_profile": "~Jie_Xu21;~Zihan_Wu2;~Cong_Wang10;~Xiaohua_Jia1",
        "aff": "City University of Hong Kong;;;City University of Hong Kong",
        "aff_domain": "cityu.edu.hk;;;cityu.edu.hk",
        "position": "Postdoc;;;Full Professor",
        "bibtex": "@inproceedings{\nxu2025lmeraser,\ntitle={{LME}raser: Large Model Unlearning via Adaptive Prompt Tuning},\nauthor={Jie Xu and Zihan Wu and Cong Wang and Xiaohua Jia},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=gJkLbMH9zR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gJkLbMH9zR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "gQS2ki9VUK",
        "title": "Primal-Dual Spectral Representation for Off-policy Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Off-policy evaluation (OPE) is one of the most fundamental problems in reinforcement learning (RL) to estimate the expected long-term payoff of a given target policy with *only* experiences from another behavior policy that is potentially unknown. The distribution correction estimation (DICE) family of estimators have advanced the state of the art in OPE by breaking the *curse of horizon*. However, the major bottleneck of applying DICE estimators lies in the difficulty of solving the saddle-point optimization involved, especially with neural network implementations. In this paper, we tackle this challenge by establishing a *linear representation* of value function and stationary distribution correction ratio, *i.e.*, primal and dual variables in the DICE framework, using the spectral decomposition of the transition operator. Such primal-dual representation not only bypasses the non-convex non-concave optimization in vanilla DICE, therefore enabling an computational efficient algorithm, but also paves the way for more efficient utilization of historical data. We highlight that our algorithm, **SpectralDICE**, is the first to leverage the linear representation of primal-dual variables that is both computation and sample efficient, the performance of which is supported by a rigorous theoretical sample complexity guarantee and a thorough empirical evaluation on various benchmarks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yang Hu;Tianyi Chen;Na Li;Kai Wang;Bo Dai",
        "authorids": "~Yang_Hu6;~Tianyi_Chen7;~Na_Li3;~Kai_Wang5;~Bo_Dai1",
        "gender": "M;M;F;M;",
        "homepage": "http://huyangsh.github.io;https://github.com/control-cyber;https://nali.seas.harvard.edu/;https://guaguakai.github.io/;https://bo-dai.github.io/",
        "dblp": ";93/4437-1;;78/2022-40;64/2903",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;qdGelXoAAAAJ;gGSsQmsAAAAJ;TIKl_foAAAAJ",
        "orcid": ";0009-0000-1203-6746;;0000-0002-2446-987X;0009-0002-8070-574X",
        "linkedin": ";;;guaguakai/;",
        "or_profile": "~Yang_Hu6;~Tianyi_Chen7;~Na_Li3;~Kai_Wang5;~Bo_Dai1",
        "aff": "Harvard University;Georgia Institute of Technology;Harvard University;Georgia Institute of Technology;Georgia Institute of Technology+Google Brain",
        "aff_domain": "g.harvard.edu;gatech.edu;harvard.edu;gatech.edu;gatech.edu+google.com",
        "position": "PhD student;PhD student;Full Professor;Assistant Professor;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nchen2025primaldual,\ntitle={Primal-Dual Spectral Representation for Off-policy Evaluation},\nauthor={Tianyi Chen and Yang Hu and Na Li and Kai Wang and Bo Dai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=gQS2ki9VUK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gQS2ki9VUK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "gY7Igf5Jwt",
        "title": "On the Power of Multitask Representation Learning with Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Representation learning, particularly multi-task representation learning, has gained widespread popularity in various deep learning applications, ranging from computer vision to natural language processing, due to its remarkable generalization performance. Despite its growing use, our understanding of the underlying mechanisms remains limited. In this paper, we provide a theoretical analysis elucidating why multi-task representation learning outperforms its single-task counterpart in scenarios involving over-parameterized two-layer convolutional neural networks trained by gradient descent. Our analysis is based on a data model that encompasses both task-shared and task-specific features, a setting commonly encountered in real-world applications. We also present experiments on synthetic and real-world data to illustrate and validate our theoretical findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qiaobo Li;Zixiang Chen;Yihe Deng;Yiwen Kou;Yuan Cao;Quanquan Gu",
        "authorids": "~Qiaobo_Li1;~Zixiang_Chen1;~Yihe_Deng1;~Yiwen_Kou1;~Yuan_Cao1;~Quanquan_Gu1",
        "gender": "M;M;F;F;M;M",
        "homepage": ";https://sites.google.com/view/zxchen;;https://evankou.github.io/;https://yuancaohku.github.io/;http://web.cs.ucla.edu/~qgu/",
        "dblp": ";137/3624;230/8011;323/9058;;50/4597",
        "google_scholar": ";6nrCHr0AAAAJ;7Lix1poAAAAJ;https://scholar.google.com/citations?hl=en;-VGnHI4AAAAJ;GU9HgNAAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "qiaobo-li-581815251/;;;yiwen-kou-5a444916b/;;",
        "or_profile": "~Qiaobo_Li1;~Zixiang_Chen1;~Yihe_Deng1;~Yiwen_Kou1;~Yuan_Cao1;~Quanquan_Gu1",
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign;SalesForce.com+ University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;University of Hong Kong;ByteDance Inc.+University of California, Los Angeles",
        "aff_domain": "cs.illinois.edu;salesforce.com+cs.ucla.edu;ucla.edu;ucla.edu;hku.hk;bytedance.com+cs.ucla.edu",
        "position": "PhD student;Researcher+PhD student;PhD student;PhD student;Assistant Professor;Research Scientist+Associate Professor",
        "bibtex": "@inproceedings{\nli2025on,\ntitle={On the Power of Multitask Representation Learning with Gradient Descent},\nauthor={Qiaobo Li and Zixiang Chen and Yihe Deng and Yiwen Kou and Yuan Cao and Quanquan Gu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=gY7Igf5Jwt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gY7Igf5Jwt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "gpsym9QNoN",
        "title": "A Safe Exploration Approach to Constrained Markov Decision Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider discounted infinite-horizon constrained Markov decision processes (CMDPs), where the goal is to find an optimal policy that maximizes the expected cumulative reward while satisfying expected cumulative constraints. Motivated by the application of CMDPs in online learning for safety-critical systems, we focus on developing a model-free and $\\textit{simulator-free}$ algorithm that ensures $\\textit{constraint satisfaction during learning}$. To this end, we employ the LB-SGD algorithm proposed in (Usmanova\net al., 2024), which utilizes an interior-point approach based on the log-barrier function of the CMDP. Under the commonly assumed conditions of relaxed Fisher non-degeneracy and bounded transfer error in policy parameterization, we establish the theoretical properties of the LB-SGD algorithm. In particular, unlike existing CMDP approaches that ensure policy feasibility only upon convergence, the LB-SGD algorithm guarantees feasibility throughout the learning process and converges to the $\\varepsilon$-optimal policy with a sample complexity of $\\tilde{\\mathcal{O}}(\\varepsilon^{-6})$. Compared to the state-of-the-art policy gradient-based algorithm, C-NPG-PDA, the LB-SGD algorithm requires an additional $\\mathcal{O}(\\varepsilon^{-2})$ samples to ensure policy feasibility during learning with the same Fisher non-degenerate parameterization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tingting Ni;Maryam Kamgarpour",
        "authorids": "~Tingting_Ni1;~Maryam_Kamgarpour1",
        "gender": "F;F",
        "homepage": ";https://www.epfl.ch/labs/sycamore/",
        "dblp": "344/5091;https://dblp.org/pers/k/Kamgarpour:Maryam.html",
        "google_scholar": ";https://scholar.google.ch/citations?user=m6YgGqAAAAAJ",
        "orcid": ";",
        "linkedin": "tingting-ni1998;",
        "or_profile": "~Tingting_Ni1;~Maryam_Kamgarpour1",
        "aff": "School of Computer and Communication Sciences, EPFL - EPF Lausanne;EPFL - EPF Lausanne",
        "aff_domain": "ic.epfl.ch;epfl.ch",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nni2025a,\ntitle={A safe exploration approach to constrained Markov decision processes},\nauthor={Tingting Ni and Maryam Kamgarpour},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=gpsym9QNoN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gpsym9QNoN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h0GSgts5UJ",
        "title": "Optimistic Safety for Online Convex Optimization with Unknown Linear Constraints",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of online convex optimization (OCO) under unknown linear constraints that are either static, or stochastically time-varying. For this problem, we introduce an algorithm that we term Optimistically Safe OCO (OSOCO) and show that it enjoys $\\tilde{O}(\\sqrt{T})$ regret and no constraint violation. In the case of static linear constraints, this improves on the previous best known $\\tilde{O}(T^{2/3})$ regret under the same assumptions. In the case of stochastic time-varying constraints, our work supplements existing results that show $O(\\sqrt{T})$ regret and $O(\\sqrt{T})$ cumulative violation under more general convex constraints and a different set of assumptions. In addition to our theoretical guarantees, we also give numerical results that further validate the effectiveness of our approach.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Spencer Hutchinson;Tianyi Chen;Mahnoosh Alizadeh",
        "authorids": "~Spencer_Hutchinson1;~Tianyi_Chen5;~Mahnoosh_Alizadeh1",
        "gender": ";M;",
        "homepage": ";https://chentianyi1991.github.io/;",
        "dblp": "337/8629;;",
        "google_scholar": "49JcQJoAAAAJ;kFwvv38AAAAJ;",
        "orcid": "0000-0003-2190-3631;;",
        "linkedin": "spencer-hutchinson-836bb7177?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BYk%2FmiG%2BfSUmq30gbTzi8TA%3D%3D;;",
        "or_profile": "~Spencer_Hutchinson1;~Tianyi_Chen5;~Mahnoosh_Alizadeh1",
        "aff": "University of California, Santa Barbara+Mitsubishi Electric Research Labs;Cornell University+Rensselaer Polytechnic Institute;",
        "aff_domain": "ucsb.edu+merl.com;cornell.edu+rpi.edu;",
        "position": "PhD student+Intern;Associate Professor+Assistant Professor;",
        "bibtex": "@inproceedings{\nhutchinson2025optimistic,\ntitle={Optimistic Safety for Online Convex Optimization with Unknown Linear Constraints},\nauthor={Spencer Hutchinson and Tianyi Chen and Mahnoosh Alizadeh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h0GSgts5UJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h0GSgts5UJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h2VtCgNd4n",
        "title": "Theoretical Convergence Guarantees for Variational Autoencoders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Variational Autoencoders (VAE) are popular generative models used to sample from complex data distributions. Despite their empirical success in various machine learning tasks, significant gaps remain in understanding their theoretical properties, particularly regarding convergence guarantees. This paper aims to bridge that gap by providing non-asymptotic convergence guarantees for VAE trained using both Stochastic Gradient Descent and Adam algorithms. We derive a convergence rate of $\\mathcal{O}(\\log n / \\sqrt{n})$, where $n$ is the number of iterations of the optimization algorithm, with explicit dependencies on the batch size, the number of variational samples, and other key hyperparameters. Our theoretical analysis applies to both Linear VAE and Deep Gaussian VAE, as well as several VAE variants, including $\\beta$-VAE and IWAE. Additionally, we empirically illustrate the impact of hyperparameters on convergence, offering new insights into the theoretical understanding of VAE training.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sobihan Surendran;Antoine Godichon-Baggioni;Sylvain Le Corff",
        "authorids": "~Sobihan_Surendran1;~Antoine_Godichon-Baggioni1;~Sylvain_Le_Corff1",
        "gender": "M;M;M",
        "homepage": ";http://godichon.perso.math.cnrs.fr;https://sylvainlc.github.io/",
        "dblp": ";;29/10875",
        "google_scholar": ";;gHRCj-EAAAAJ",
        "orcid": ";;0000-0001-5211-2328",
        "linkedin": "sobihan-surendran-859272200/?originalSubdomain=fr;;",
        "or_profile": "~Sobihan_Surendran1;~Antoine_Godichon-Baggioni1;~Sylvain_Le_Corff1",
        "aff": "Sorbonne Universit\u00e9 - Facult\u00e9 des Sciences (Paris VI);Sorbonne Universit\u00e9 - Facult\u00e9 des Sciences (Paris VI);Sorbonne Universit\u00e9, LPSM",
        "aff_domain": "sorbonne-universite.fr;sorbonne-universite.fr;sorbonne-universite.fr",
        "position": "PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nsurendran2025theoretical,\ntitle={Theoretical Convergence Guarantees for Variational Autoencoders},\nauthor={Sobihan Surendran and Antoine Godichon-Baggioni and Sylvain Le Corff},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h2VtCgNd4n}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h2VtCgNd4n",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h2WIiouJEm",
        "title": "Bridging Multiple Worlds: Multi-marginal Optimal Transport for Causal Partial-identification Problem",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Under the prevalent potential outcome model in causal inference, each unit is associated with multiple potential outcomes but at most one of which is observed, leading to many causal quantities being only partially identified. The inherent missing data issue echoes the multi-marginal optimal transport (MOT) problem, where marginal distributions are known, but how the marginals couple to form the joint distribution is unavailable. In this paper, we cast the causal partial identification problem in the framework of MOT with $K$ margins and $d$-dimensional outcomes and obtain the exact partial identified set. In order to estimate the partial identified set via MOT, statistically,\nwe establish a convergence rate of the plug-in MOT estimator for the $\\ell_2$ cost function stemming from the variance minimization problem and prove it is minimax optimal for arbitrary $K$ and $d \\le 4$. We also extend the convergence result to general quadratic objective functions. Numerically, we demonstrate the efficacy of our method over synthetic datasets and several real-world datasets where our proposal consistently outperforms the baseline by a significant margin (over 70\\%). In addition, we provide efficient off-the-shelf implementations of MOT with general objective functions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zijun Gao;Shu Ge;Jian Qian",
        "authorids": "~Zijun_Gao1;~Shu_Ge1;~Jian_Qian2",
        "gender": "F;F;",
        "homepage": "https://zijungao.github.io;;https://sites.google.com/view/jianqian/about",
        "dblp": "239/5605;;",
        "google_scholar": ";;",
        "orcid": "0000-0003-4863-1656;;",
        "linkedin": ";ge-shu/;jian-qian/",
        "or_profile": "~Zijun_Gao1;~Shu_Ge1;~Jian_Qian2",
        "aff": "University of Southern California;;Massachusetts Institute of Technology",
        "aff_domain": "usc.edu;;mit.edu",
        "position": "Assistant Professor;;PhD student",
        "bibtex": "@inproceedings{\ngao2025bridging,\ntitle={Bridging Multiple Worlds: Multi-marginal Optimal Transport for Causal Partial-identification Problem},\nauthor={Zijun Gao and Shu Ge and Jian Qian},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h2WIiouJEm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h2WIiouJEm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h666JodPjL",
        "title": "Approximate information maximization for bandit games",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Entropy maximization and free energy minimization are general physics principles for modeling dynamic systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al. 2000), and navigation in random environments using information maximization (Vergassola et al. 2007). Building on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated, analytical physics-based representation of the entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the multi-armed bandit problem with Gaussian rewards. Since it encompasses the system's properties in a single, global functional, this approach can be efficiently adapted to more complex bandit settings. This calls for further investigation of information maximization approaches for multi-armed bandit problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alex Barbier Chebbah;Christian L Vestergaard;Jean Baptiste Masson;Etienne Boursier",
        "authorids": "~Alex_Barbier_Chebbah1;~Christian_L_Vestergaard1;~Jean_Baptiste_Masson1;~Etienne_Boursier1",
        "gender": "M;M;M;M",
        "homepage": "https://research.pasteur.fr/fr/member/alex-barbier-chebbah/;https://research.pasteur.fr/en/member/fr-christian-vestergaard/;https://research.pasteur.fr/en/team/decision-and-bayesian-computation/;https://eboursier.github.io/",
        "dblp": ";;;203/8633",
        "google_scholar": "orKu1bMAAAAJ;nur1OEEAAAAJ;5f93Do0AAAAJ;https://scholar.google.fr/citations?user=-9todDUAAAAJ",
        "orcid": "0000-0002-9523-8411;0000-0001-5329-475X;0000-0002-5484-9056;",
        "linkedin": "https://fr.linkedin.com/in/alex-barbier-chebbah-144b482b9;;jean-baptiste-masson-a2494791/;",
        "or_profile": "~Alex_Barbier_Chebbah1;~Christian_L_Vestergaard1;~Jean_Baptiste_Masson1;~Etienne_Boursier1",
        "aff": "Institut Pasteur;CNRS;INRIA+CNRS;INRIA",
        "aff_domain": "pasteur.fr;cnrs.fr;inria.fr+cnrs.fr;inria.fr",
        "position": "Postdoc;Principal Researcher;Principal Researcher+Principal Researcher;Researcher",
        "bibtex": "@inproceedings{\nchebbah2025approximate,\ntitle={Approximate information maximization for bandit games},\nauthor={Alex Barbier Chebbah and Christian L Vestergaard and Jean Baptiste Masson and Etienne Boursier},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h666JodPjL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h666JodPjL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h6BirxTrpA",
        "title": "Sampling From Multiscale Densities With Delayed Rejection Generalized Hamiltonian Monte Carlo",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Hamiltonian Monte Carlo (HMC) is the mainstay of applied Bayesian inference for differentiable models. However, HMC still struggles to sample from hierarchical models that induce densities with multiscale geometry: a large step size is needed to efficiently explore low curvature regions while a small step size is needed to accurately explore high curvature regions. We introduce the delayed rejection generalized HMC (DR-G-HMC) sampler that overcomes this challenge by employing dynamic step size selection, inspired by differential equation solvers. In generalized HMC, each iteration does a single leapfrog step. DR-G-HMC sequentially makes proposals with geometrically decreasing step sizes upon rejection of earlier proposals. This simulates Hamiltonian dynamics that can adjust its step size along a (stochastic) Hamiltonian trajectory to deal with regions of high curvature. DR-G-HMC makes generalized HMC competitive by decreasing the number of rejections which otherwise cause inefficient backtracking and prevents directed movement. We present experiments to demonstrate that DR-G-HMC (1) correctly samples from multiscale densities, (2) makes generalized HMC methods competitive with the state of the art No-U-Turn sampler, and (3) is robust to tuning parameters.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gilad Turok;Chirag Modi;Bob Carpenter",
        "authorids": "~Gilad_Turok1;~Chirag_Modi1;~Bob_Carpenter3",
        "gender": ";M;",
        "homepage": "https://gil2rok.github.io/;;https://bob-carpenter.github.io/",
        "dblp": ";57/6166;",
        "google_scholar": ";yEh-Tj8AAAAJ;kPtKWAwAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Gilad_Turok1;~Chirag_Modi1;~Bob_Carpenter3",
        "aff": "Flatiron Institute;;",
        "aff_domain": "flatironinstitute.org;;",
        "position": "Researcher;;",
        "bibtex": "@inproceedings{\nturok2025sampling,\ntitle={Sampling From Multiscale Densities With Delayed Rejection Generalized Hamiltonian Monte Carlo},\nauthor={Gilad Turok and Chirag Modi and Bob Carpenter},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h6BirxTrpA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h6BirxTrpA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "h7AalFFQsK",
        "title": "Randomized Iterative Solver as Iterative Refinement: A Simple Fix Towards Backward Stability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Iterative sketching and sketch-and-precondition are well-established randomized algorithms for solving large-scale over-determined linear least-squares problems. In this paper, we introduce a new perspective that interprets Iterative Sketching and Sketching-and-Precondition as forms of Iterative Refinement. We also examine the numerical stability of two distinct refinement strategies: iterative refinement and recursive refinement, which progressively improve the accuracy of a sketched linear solver. Building on this insight, we propose a novel algorithm, Sketched Iterative and Recursive Refinement (SIRR), which combines both refinement methods. \nSIRR demonstrates a four order of magnitude improvement in backward error compared to iterative sketching, achieved simply by reorganizing the computational order, ensuring that the computed solution exactly solves a modified least-squares system where the coefficient matrix deviates only slightly from the original matrix.\nTo the best of our knowledge, SIRR is the first asymptotically fast, single-stage randomized least-squares solver that achieves both forward and backward stability.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruihan Xu;Yiping Lu",
        "authorids": "ruihanx@uchicago.edu;~Yiping_Lu1",
        "gender": ";M",
        "homepage": ";https://2prime.github.io/",
        "dblp": ";93/683-1",
        "google_scholar": ";NmhvVBgAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "ruihanx@uchicago.edu;~Yiping_Lu1",
        "aff": ";Northwestern University",
        "aff_domain": ";northwestern.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025randomized,\ntitle={Randomized Iterative Solver as Iterative Refinement: A Simple Fix Towards Backward Stability},\nauthor={Ruihan Xu and Yiping Lu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=h7AalFFQsK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h7AalFFQsK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hAQnKnkCT4",
        "title": "Lower Bounds for Time-Varying Kernelized Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The optimization of black-box functions with noisy observations is a fundamental problem with widespread applications, and has been widely studied under the assumption that the function lies in a reproducing kernel Hilbert space (RKHS).  This problem has been studied extensively in the stationary setting, and near-optimal regret bounds are known via developments in both upper and lower bounds.  In this paper, we consider non-stationary scenarios, which are crucial for certain applications but are currently less well-understood.  Specifically, we provide the first algorithm-independent lower bounds, where the time variations are subject satisfying a total variation budget according to some function norm.  Under $\\ell_{\\infty}$-norm variations, our bounds are found to be close to an existing upper bound (Hong et al., 2023).  Under RKHS norm variations, the upper and lower bounds are still reasonably close but with more of a gap, raising the interesting open question of whether non-minor improvements in the upper bound are possible.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xu Cai;Jonathan Scarlett",
        "authorids": "~Xu_Cai1;~Jonathan_Scarlett1",
        "gender": "M;M",
        "homepage": "https://www.linkedin.com/in/xu-cai-0aa329136/;https://www.comp.nus.edu.sg/~scarlett/",
        "dblp": "60/149;78/9667",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=a4D08aQAAAAJ",
        "orcid": "0009-0007-0224-2002;",
        "linkedin": ";",
        "or_profile": "~Xu_Cai1;~Jonathan_Scarlett1",
        "aff": "National University of Singapore+SUN YAT-SEN UNIVERSITY;National University of Singapore",
        "aff_domain": "nus.edu+sysu.edu.cn;nus.edu.sg",
        "position": "PhD student+MS student;Associate Professor",
        "bibtex": "@inproceedings{\ncai2025lower,\ntitle={Lower Bounds for Time-Varying Kernelized Bandits},\nauthor={Xu Cai and Jonathan Scarlett},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hAQnKnkCT4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hAQnKnkCT4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hI7m3y9c4s",
        "title": "On Distributional Discrepancy for Experimental Design with General Assignment Probabilities",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We investigate experimental design for randomized controlled trials (RCTs) with both equal and unequal treatment-control assignment probabilities. Our work makes progress on the connection between the distributional discrepancy minimization (DDM) problem introduced by Harshaw et al. (2024) and the design of RCTs. We make two main contributions: First, we prove that approximating the optimal solution of the DDM problem within a certain constant error is NP-hard. Second, we introduce a new Multiplicative Weights Update (MWU) algorithm for the DDM problem, which improves the Gram-Schmidt walk algorithm used by Harshaw et al. (2024) when assignment probabilities are unequal. \nBuilding on the framework of Harshaw et al. (2024) and our MWU algorithm, we then develop the MWU design, which reduces the worst-case mean-squared error in estimating the average treatment effect. Finally, we present a comprehensive simulation study comparing our design with commonly used designs.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anup Rao;Peng Zhang",
        "authorids": "~Anup_Rao1;~Peng_Zhang48",
        "gender": ";",
        "homepage": ";",
        "dblp": "63/6846;",
        "google_scholar": "pkwXPU0AAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Anup_Rao1;~Peng_Zhang48",
        "aff": "Adobe Systems;",
        "aff_domain": "adobe.com;",
        "position": "Researcher;",
        "bibtex": "@inproceedings{\nrao2025on,\ntitle={On Distributional Discrepancy for Experimental Design with General Assignment Probabilities},\nauthor={Anup Rao and Peng Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hI7m3y9c4s}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hI7m3y9c4s",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hJicYBZ1Yz",
        "title": "Efficient and Asymptotically Unbiased Constrained Decoding for Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In real-world applications of large language models, outputs are often required to be confined: selecting items from predefined product or document sets, generating phrases that comply with safety standards, or conforming to specialized formatting styles. To control the generation, constrained decoding has been widely adopted. However, existing prefix-tree-based constrained decoding is inefficient under GPU-based model inference paradigms, and it introduces unintended biases into the output distribution. This paper introduces Dynamic Importance Sampling for Constrained Decoding (DISC) with GPU-based Parallel Prefix-Verification (PPV), a novel algorithm that leverages dynamic importance sampling to achieve theoretically guaranteed asymptotic unbiasedness and overcomes the inefficiency of prefix-tree. Extensive experiments demonstrate the superiority of our method over existing methods in both efficiency and output quality. These results highlight the potential of our methods to improve constrained generation in applications where adherence to specific constraints is essential.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haotian Ye;Himanshu Jain;Chong You;Ananda Theertha Suresh;Haowei Lin;James Zou;Felix Yu",
        "authorids": "~Haotian_Ye1;~Himanshu_Jain3;~Chong_You1;~Ananda_Theertha_Suresh1;~Haowei_Lin1;~James_Zou1;~Felix_Yu1",
        "gender": "M;M;;M;M;;M",
        "homepage": "https://haotianye.com;;;https://theertha.info;https://linhaowei1.github.io/;;http://felixyu.org",
        "dblp": "284/0539;;;119/3884;235/2798;;23/10574",
        "google_scholar": "VU4chlsAAAAJ;JtrH9jQAAAAJ;;K6ef57QAAAAJ;Ng-DmJgAAAAJ;23ZXZvEAAAAJ;lYvF6cUAAAAJ",
        "orcid": ";;;;0009-0006-9809-4835;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Haotian_Ye1;~Himanshu_Jain3;~Chong_You1;~Ananda_Theertha_Suresh1;~Haowei_Lin1;~James_Zou1;~Felix_Yu1",
        "aff": "Stanford University;Google;;Google;Peking University;Stanford University;Google",
        "aff_domain": "stanford.edu;google.com;;google.com;pku.edu.cn;stanford.edu;google.com",
        "position": "PhD student;Researcher;;Research Scientist;PhD student;Assistant Professor;Research Scientist",
        "bibtex": "@inproceedings{\nye2025efficient,\ntitle={Efficient and Asymptotically Unbiased Constrained Decoding for Large Language Models},\nauthor={Haotian Ye and Himanshu Jain and Chong You and Ananda Theertha Suresh and Haowei Lin and James Zou and Felix Yu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hJicYBZ1Yz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hJicYBZ1Yz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hbknVIJf2W",
        "title": "Decision-Point Guided Safe Policy Improvement",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Within batch reinforcement learning, safe policy improvement seeks to ensure that the learned policy performs at least as well as the behavior policy that generated the dataset.  The core challenge is seeking improvements while balancing risk when many state-action pairs may be infrequently visited.  In this work, we introduce Decision Points RL (DPRL), an algorithm that restricts the set of state-action pairs (or regions for continuous states) considered for improvement. DPRL ensures high-confidence improvement in densely visited states (called `decision points') while still utilizing data from sparsely visited states by using them for trajectory-based value estimates. By selectively limiting the state-actions where the policy deviates from the behavior, we achieve tighter theoretical guarantees that depend only on the counts of frequently observed state-action pairs rather than on state-action space size. Our empirical results confirm DPRL provides both safety and performance improvements across synthetic and real-world applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abhishek Sharma;Leo Benac;Sonali Parbhoo;Finale Doshi-Velez",
        "authorids": "~Abhishek_Sharma5;~Leo_Benac1;~Sonali_Parbhoo2;~Finale_Doshi-Velez1",
        "gender": ";M;;F",
        "homepage": "https://abhishekshar.com/;https://scholar.google.com/citations?user=PEehBN4AAAAJ&hl=en&oi=ao;;https://finale.seas.harvard.edu/",
        "dblp": ";;;64/7056",
        "google_scholar": "1cfEdWIAAAAJ;;FwEz5s4AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Abhishek_Sharma5;~Leo_Benac1;~Sonali_Parbhoo2;~Finale_Doshi-Velez1",
        "aff": "Harvard University;Harvard University;Imperial College London+Harvard University;Harvard University+Harvard University",
        "aff_domain": "harvard.edu;g.harvard.edu;imperial.ac.uk+harvard.edu;+harvard.edu",
        "position": "PhD student;PhD student;Assistant Professor+Postdoc;Assistant Professor+Professor",
        "bibtex": "@inproceedings{\nsharma2025decisionpoint,\ntitle={Decision-Point Guided Safe Policy Improvement},\nauthor={Abhishek Sharma and Leo Benac and Sonali Parbhoo and Finale Doshi-Velez},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hbknVIJf2W}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hbknVIJf2W",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hphdX8WlcT",
        "title": "Understanding the Learning Dynamics of LoRA: A Gradient Flow Perspective on Low-Rank Adaptation in Matrix Factorization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite the empirical success of Low-Rank Adaptation (LoRA) in fine-tuning pre-trained models, there is little theoretical understanding of how first-order methods with carefully crafted initialization adapt models to new tasks. In this work, we take the first step towards bridging this gap by theoretically analyzing the learning dynamics of LoRA for matrix factorization (MF) under gradient flow (GF), emphasizing the crucial role of initialization. For small initialization, we theoretically show that GF converges to a neighborhood of the optimal solution, with smaller initialization leading to lower final error. Our analysis shows that the final error is affected by the misalignment between the singular spaces of the pre-trained model and the target matrix, and reducing the initialization scale improves alignment. To address this misalignment, we propose a spectral initialization for LoRA in MF and theoretically prove that GF with small spectral initialization converges to the fine-tuning task with arbitrary precision. Numerical experiments from MF and image classification validate our findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziqing Xu;Hancheng Min;Lachlan Ewen MacDonald;Jinqi Luo;Salma Tarmoun;Enrique Mallada;Rene Vidal",
        "authorids": "~Ziqing_Xu1;~Hancheng_Min1;~Lachlan_Ewen_MacDonald1;~Jinqi_Luo1;~Salma_Tarmoun1;~Enrique_Mallada1;~Rene_Vidal1",
        "gender": "M;M;;;F;M;",
        "homepage": ";https://hanchmin.github.io/;https://researchers.adelaide.edu.au/profile/lachlan.macdonald;;;http://mallada.ece.jhu.edu;http://www.vision.jhu.edu",
        "dblp": "294/0111;226/6324;306/7691;;;;v/ReneVidal",
        "google_scholar": "https://scholar.google.com.au/citations?user=hcKJ0BIAAAAJ;XgQjPZIAAAAJ;r953DlQAAAAJ;;;ZvRFA04AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;0000-0003-1568-1833;",
        "linkedin": "ziqing-xu-b01811161/;;;;salma-tarmoun-94aa5158/;emallada/;rene-vidal-74844928/",
        "or_profile": "~Ziqing_Xu1;~Hancheng_Min1;~Lachlan_Ewen_MacDonald1;~Jinqi_Luo1;~Salma_Tarmoun1;~Enrique_Mallada1;~Rene_Vidal1",
        "aff": "The Wharton School, University of Pennsylvania;Shanghai Jiaotong University+University of Pennsylvania;University of Pennsylvania;;University of Pennsylvania;Johns Hopkins University;University of Pennsylvania+Amazon",
        "aff_domain": "wharton.upenn.edu;sjtu.edu.cn+seas.upenn.edu;upenn.edu;;upenn.edu;jhu.edu;upenn.edu+amazon.com",
        "position": "PhD student;Associate Professor+Postdoc;Postdoc;;PhD student;Associate Professor;Full Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nxu2025understanding,\ntitle={Understanding the Learning Dynamics of Lo{RA}: A Gradient Flow Perspective on Low-Rank Adaptation in Matrix Factorization},\nauthor={Ziqing Xu and Hancheng Min and Jinqi Luo and Lachlan Ewen MacDonald and Salma Tarmoun and Enrique Mallada and Rene Vidal},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hphdX8WlcT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hphdX8WlcT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hqCarqCR5g",
        "title": "Reward Maximization for Pure Exploration: Minimax Optimal Good Arm Identification for Nonparametric Multi-Armed Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In multi-armed bandits, reward maximization and pure exploration are often at odds with each other. The former focuses on exploiting arms with the highest means, while the latter may require constant exploration across all arms. In this work, we focus on good arm identification (GAI), a pure exploration objective that aims to label arms with means above a threshold as quickly as possible. We show that GAI can be efficiently solved by combining a reward-maximizing sampling algorithm with a novel nonparametric anytime-valid sequential test for labeling arm means.\n\nWe begin by presenting the theoretical guarantees of our proposed sequential test. Under nonparametric assumptions, our test ensures strict error control and asymptotically achieves the minimax optimal e-power, a notion of power for anytime-valid tests. Building on this, we propose an algorithm for GAI by pairing regret-minimizing sampling schemes with our sequential test as a stopping criterion. We show that this approach achieves minimax optimal stopping times for labeling arms with means above a threshold, under an error probability constraint \u03b4. Our empirical results validate our approach beyond the minimax setting, reducing the expected number of samples for all stopping times by at least 50% across both synthetic and real-world settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Brian M Cho;Dominik Meier;Kyra Gan;Nathan Kallus",
        "authorids": "~Brian_M_Cho1;~Dominik_Meier1;~Kyra_Gan1;~Nathan_Kallus1",
        "gender": "M;;;",
        "homepage": "https://bcho.page/;;;http://nathankallus.com/",
        "dblp": "32/3261-1.html;;;142/2900",
        "google_scholar": "https://scholar.google.co.jp/citations?user=9k0bfB0AAAAJ;gUpyXTwAAAAJ;;K2WfIlsAAAAJ",
        "orcid": "0000-0003-3558-0415;0000-0003-3680-8426;;0000-0003-1672-0507",
        "linkedin": "brian-cho-5a7876172/;;;",
        "or_profile": "~Brian_M_Cho1;~Dominik_Meier1;~Kyra_Gan1;~Nathan_Kallus1",
        "aff": "Cornell University;Department of Computer Science, Cornell University;;Netflix+Cornell University",
        "aff_domain": "cornell.edu;cs.cornell.edu;;netflix.com+cornell.edu",
        "position": "PhD student;PhD student;;Research Director+Associate Professor",
        "bibtex": "@inproceedings{\ncho2025reward,\ntitle={Reward Maximization for Pure Exploration: Minimax Optimal Good Arm Identification for Nonparametric Multi-Armed Bandits},\nauthor={Brian M Cho and Dominik Meier and Kyra Gan and Nathan Kallus},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hqCarqCR5g}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hqCarqCR5g",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hsM969HT5W",
        "title": "High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient Descent under Heavy-tailed Noise",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study high-probability convergence in online learning, in the presence of heavy-tailed noise. To combat the heavy tails, a general framework of nonlinear SGD methods is considered, subsuming several popular nonlinearities like sign, quantization, component-wise and joint clipping. In our work the nonlinearity is treated in a black-box manner, allowing us to establish unified guarantees for a broad range of nonlinear methods. For symmetric noise and non-convex costs we establish convergence of gradient norm-squared, at a rate $\\widetilde{\\mathcal{O}}(t^{-1/4})$, while for the last iterate of strongly convex costs we establish convergence to the population optima, at a rate $\\mathcal{O}(t^{-\\zeta})$, where $\\zeta \\in (0,1)$ depends on noise and problem parameters. Further, if the noise is a (biased) mixture of symmetric and non-symmetric components, we show convergence to a neighbourhood of stationarity, whose size depends on the mixture coefficient, nonlinearity and noise. Compared to state-of-the-art, who only consider clipping and require unbiased noise with bounded $p$-th moments, $p \\in (1,2]$, we provide guarantees for a broad class of nonlinearities, without any assumptions on noise moments. While the rate exponents in state-of-the-art depend on noise moments and vanish as $p \\rightarrow 1$, our exponents are constant and strictly better whenever $p < 6/5$ for non-convex and $p < 8/7$ for strongly convex costs. Experiments validate our theory, showing that clipping is not always the optimal nonlinearity, further underlining the value of a general framework.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aleksandar Armacki;Shuhua Yu;Pranay Sharma;Gauri Joshi;Dragana Bajovic;Dusan Jakovetic;Soummya Kar",
        "authorids": "~Aleksandar_Armacki2;~Shuhua_Yu1;~Pranay_Sharma2;~Gauri_Joshi1;~Dragana_Bajovic1;~Dusan_Jakovetic1;~Soummya_Kar1",
        "gender": ";M;;;;;",
        "homepage": "https://aarmacki.github.io/;;;;;https://people.dmi.uns.ac.rs/~dusan.jakovetic/;",
        "dblp": "250/9800.html;45/7938;;;;;31/2011",
        "google_scholar": "21d_U30AAAAJ;RGvf7SwAAAAJ;;;;;",
        "orcid": "0000-0001-7916-585X;0000-0002-6054-673X;;;;;",
        "linkedin": "aleksandar-armacki/;shuhua-yu-55312711a/;;;dragana-bajovic/;;",
        "or_profile": "~Aleksandar_Armacki2;~Shuhua_Yu1;~Pranay_Sharma2;~Gauri_Joshi1;~Dragana_Bajovic1;~Dusan_Jakovetic1;~Soummya_Kar1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;;;University of Novi Sad;University of Novi Sad Faculty of Sciences;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;;;uns.ac.rs;dmi.uns.ac.rs;andrew.cmu.edu",
        "position": "PhD student;PhD student;;;Associate Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\narmacki2025highprobability,\ntitle={High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise},\nauthor={Aleksandar Armacki and Shuhua Yu and Pranay Sharma and Gauri Joshi and Dragana Bajovic and Dusan Jakovetic and Soummya Kar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hsM969HT5W}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hsM969HT5W",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "huPj11VPlU",
        "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple agents to achieve a common goal. A key challenge in MARL is credit assignment, which involves assessing each agent's contribution to the shared reward. Given the diversity of tasks, agents may perform different types of coordination, with rewards attributed to diverse and often overlapping agent subsets. In this work, we formalize the credit assignment level as the number of agents cooperating to obtain a reward, and address scenarios with multiple coexisting levels. We introduce a multi-level advantage formulation that performs explicit counterfactual reasoning to infer credits across distinct levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures agent contributions at multiple levels by integrating advantage functions that reason about individual, joint, and correlated actions. Utilizing an attention-based framework, MACA identifies correlated agent relationships and constructs multi-level advantages to guide policy learning. Comprehensive experiments on challenging Starcraft v1 & v2 tasks demonstrate MACA's superior performance, underscoring its efficacy in complex credit assignment scenarios.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xutong Zhao;Yaqi Xie",
        "authorids": "~Xutong_Zhao1;~Yaqi_Xie1",
        "gender": ";F",
        "homepage": "https://tongtongx.github.io/;https://yaqi-xie.me/",
        "dblp": "320/8138;237/8691",
        "google_scholar": ";lBCCo0EAAAAJ",
        "orcid": ";0009-0005-0458-9419",
        "linkedin": ";yaqi-xie/",
        "or_profile": "~Xutong_Zhao1;~Yaqi_Xie1",
        "aff": "Mila;Carnegie Mellon University",
        "aff_domain": "mila.quebec;cmu.edu",
        "position": "PhD student;Postdoc",
        "bibtex": "@inproceedings{\nzhao2025multilevel,\ntitle={Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning},\nauthor={Xutong Zhao and Yaqi Xie},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=huPj11VPlU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=huPj11VPlU",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hyALFH7E9J",
        "title": "Memorization in Attention-only Transformers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent research has explored the memorization capacity of multi-head attention, but these\nfindings are constrained by unrealistic limitations on the context size. We present a novel proof\nfor language-based Transformers that extends the current hypothesis to any context size. Our\napproach improves upon the state-of-the-art by achieving more effective exact memorization\nwith an attention layer, while also introducing the concept of approximate memorization of\ndistributions. Through experimental validation, we demonstrate that our proposed bounds\nmore accurately reflect the true memorization capacity of language models, and provide a precise\ncomparison with prior work.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "L\u00e9o Dana;Muni Sreenivas Pydi;Yann Chevaleyre",
        "authorids": "~L\u00e9o_Dana1;~Muni_Sreenivas_Pydi1;~Yann_Chevaleyre1",
        "gender": "M;M;M",
        "homepage": ";https://munisreenivas.github.io/;https://www.lamsade.dauphine.fr/~ychevaleyre/",
        "dblp": ";194/2444;55/5658",
        "google_scholar": ";BT8j_-oAAAAJ;SF6g8p4AAAAJ",
        "orcid": ";;",
        "linkedin": "l%C3%A9o-dana-b169b7207/;;yannchevaleyre",
        "or_profile": "~L\u00e9o_Dana1;~Muni_Sreenivas_Pydi1;~Yann_Chevaleyre1",
        "aff": "INRIA;Universit\u00e9 Paris Dauphine - PSL;Universit\u00e9 Paris-Dauphine (Paris IX)",
        "aff_domain": "inria.fr;lamsade.dauphine.fr;dauphine.fr",
        "position": "Intern;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\ndana2025memorization,\ntitle={Memorization in Attention-only Transformers},\nauthor={L{\\'e}o Dana and Muni Sreenivas Pydi and Yann Chevaleyre},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hyALFH7E9J}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hyALFH7E9J",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "hzi5UXGqHb",
        "title": "Representer Theorems for Metric and Preference Learning: Geometric Insights and Algorithms",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We develop a mathematical framework to address a broad class of metric and preference learning problems within a Hilbert space. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem for this task can be derived by regularizing the problem with respect to the norm inherent in the task structure. For the general task of metric learning, our framework leads to a simple and self-contained representer theorem and offers new geometric insights into the derivation of representer theorems for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHSs), we illustrate how our representer theorem can be used to express the solution of the learning problems in terms of finite kernel terms similar to classical representer theorems. Lastly, our representer theorem leads to a novel nonlinear algorithm for metric and preference learning. We compare our algorithm against challenging baseline methods on real-world rank inference benchmarks, where it achieves competitive performance. Notably, our approach significantly outperforms vanilla ideal point methods and surpasses strong baselines across multiple datasets. Code available at: https://github.com/PeymanMorteza/Metric-Preference-Learning-RKHS",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peyman Morteza",
        "authorids": "~Peyman_Morteza2",
        "gender": "",
        "homepage": "",
        "dblp": "",
        "google_scholar": "",
        "orcid": "",
        "linkedin": "",
        "or_profile": "",
        "aff": "",
        "aff_domain": "",
        "position": "",
        "bibtex": "@inproceedings{\nmorteza2025representer,\ntitle={Representer Theorems for Metric and Preference Learning: A Geometric Perspective},\nauthor={Peyman Morteza},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=hzi5UXGqHb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hzi5UXGqHb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iEBRC0uzqt",
        "title": "Computation-Aware Kalman Filtering and Smoothing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Kalman filtering and smoothing are the foundational mechanisms for efficient inference in Gauss-Markov models.\nHowever, their time and memory complexities scale prohibitively with the size of the state space.\nThis is particularly problematic in spatiotemporal regression problems, where the state dimension scales with the number of spatial observations.\nExisting approximate frameworks leverage low-rank approximations of the covariance matrix.\nBut since they do not model the error introduced by the computational approximation, their predictive uncertainty estimates can be overly optimistic.\nIn this work, we propose a probabilistic numerical method for inference in high-dimensional Gauss-Markov models which mitigates these scaling issues.\nOur matrix-free iterative algorithm leverages GPU acceleration and crucially enables a tunable trade-off between computational cost and predictive uncertainty.\nFinally, we demonstrate the scalability of our method on a large-scale climate dataset.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Marvin Pf\u00f6rtner;Jonathan Wenger;Jon Cockayne;Philipp Hennig",
        "authorids": "~Marvin_Pf\u00f6rtner1;~Jonathan_Wenger1;~Jon_Cockayne1;~Philipp_Hennig1",
        "gender": ";M;;M",
        "homepage": ";https://jonathanwenger.netlify.app/;http://www.joncockayne.com;http://mml.inf.uni-tuebingen.de",
        "dblp": ";242/9063;180/5684;08/9077",
        "google_scholar": ";https://scholar.google.com/citations?hl=de;https://scholar.google.co.uk/citations?user=ZYBXsvAAAAAJ;https://scholar.google.de/citations?user=UeG5w08AAAAJ",
        "orcid": ";0000-0003-2261-1331;;0000-0001-7293-6092",
        "linkedin": ";;;",
        "or_profile": "~Marvin_Pf\u00f6rtner1;~Jonathan_Wenger1;~Jon_Cockayne1;~Philipp_Hennig1",
        "aff": ";Columbia University;University of Southampton;University of T\u00fcbingen",
        "aff_domain": ";columbia.edu;soton.ac.uk;uni-tuebingen.de",
        "position": ";Postdoc;Lecturer;Full Professor",
        "bibtex": "@inproceedings{\npfortner2025computationaware,\ntitle={Computation-Aware Kalman Filtering and Smoothing},\nauthor={Marvin Pf{\\\"o}rtner and Jonathan Wenger and Jon Cockayne and Philipp Hennig},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iEBRC0uzqt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iEBRC0uzqt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iF4MLcnF2u",
        "title": "Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The rise of foundation models fine-tuned on human feedback from potentially untrusted users has increased the risk of adversarial data poisoning, necessitating the study of robustness of learning algorithms against such attacks. Existing research on provable certified robustness against data poisoning attacks primarily focuses on certifying robustness for static adversaries who modify a fraction of the dataset used to train the model before the training algorithm is applied. In practice, particularly when learning from human feedback in an online sense, adversaries can observe and react to the learning process and inject poisoned samples that optimize adversarial objectives better than when they are restricted to poisoning a static dataset once, before the learning algorithm is applied. Indeed, it has been shown in prior work that online dynamic adversaries can be significantly more powerful than static ones. We present a novel framework for computing certified bounds on the impact of dynamic poisoning, and use these certificates to design robust learning algorithms. We give an illustration of the framework for the mean estimation problem and binary classification problems and outline directions for extending this in further work.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Avinandan Bose;Laurent Lessard;Maryam Fazel;Krishnamurthy Dj Dvijotham",
        "authorids": "~Avinandan_Bose1;~Laurent_Lessard1;~Maryam_Fazel1;~Krishnamurthy_Dj_Dvijotham1",
        "gender": "M;;F;",
        "homepage": "https://avinandan22.github.io/;;;",
        "dblp": "305/7490;;10/2309;",
        "google_scholar": "https://scholar.google.com/citations?pli=1;;vlN_kRoAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Avinandan_Bose1;~Laurent_Lessard1;~Maryam_Fazel1;~Krishnamurthy_Dj_Dvijotham1",
        "aff": "Department of Computer Science;;University of Washington, Seattle;",
        "aff_domain": "cs.washington.edu;;uw.edu;",
        "position": "PhD student;;Full Professor;",
        "bibtex": "@inproceedings{\nbose2025keeping,\ntitle={Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning},\nauthor={Avinandan Bose and Laurent Lessard and Maryam Fazel and Krishnamurthy Dj Dvijotham},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iF4MLcnF2u}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iF4MLcnF2u",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iKBIop621r",
        "title": "Understanding GNNs and Homophily in Dynamic Node Classification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Homophily, as a measure, has been critical to increasing our understanding of graph neural networks (GNNs). However, to date this measure has only been analyzed in the context of static graphs. In our work, we explore homophily in dynamic settings. Focusing on graph convolutional networks (GCNs), we demonstrate theoretically that in dynamic settings, current GCN discriminative performance is characterized by the probability that a node's future label is the same as its neighbors' current labels. Based on this insight, we propose dynamic homophily, a new measure of homophily that applies in the dynamic setting. This new measure correlates with GNN discriminative performance and sheds light on how to potentially design more powerful GNNs for dynamic graphs. Leveraging a variety of dynamic node classification datasets, we demonstrate that popular GNNs are not robust to low dynamic homophily. Going forward, our work represents an important step towards understanding homophily and GNN performance in dynamic node classification.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Ito;Danai Koutra;Jenna Wiens",
        "authorids": "~Michael_Ito1;~Danai_Koutra1;~Jenna_Wiens1",
        "gender": "M;F;F",
        "homepage": "https://www.michaelito-ai.com/;http://web.eecs.umich.edu/~dkoutra/;http://www-personal.umich.edu/~wiensj/",
        "dblp": ";91/9987;63/10451",
        "google_scholar": ";https://scholar.google.com.tw/citations?user=bDrA1-8AAAAJ;fvEfKxkAAAAJ",
        "orcid": ";0000-0002-3206-8179;0000-0002-1057-7722",
        "linkedin": ";;",
        "or_profile": "~Michael_Ito1;~Danai_Koutra1;~Jenna_Wiens1",
        "aff": "University of Michigan - Ann Arbor;University of Michigan - Ann Arbor+Amazon;University of Michigan Ann Arbor",
        "aff_domain": "umich.edu;umich.edu+amazon.com;umich.edu",
        "position": "PhD student;Associate Professor+Scholar;Associate Professor",
        "bibtex": "@inproceedings{\nito2025understanding,\ntitle={Understanding {GNN}s and Homophily in Dynamic Node Classification},\nauthor={Michael Ito and Danai Koutra and Jenna Wiens},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iKBIop621r}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iKBIop621r",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iLM3CeY2zb",
        "title": "Nystr\u00f6m Kernel Stein Discrepancy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Kernel methods underpin many of the most successful approaches in data science and statistics, and they allow representing probability measures as elements of a reproducing kernel Hilbert space without loss of information. Recently, the kernel Stein discrepancy (KSD), which combines Stein's method with the flexibility of kernel techniques, gained considerable attention. Through the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it is sufficient to know the target distribution up to a multiplicative constant. However, the typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity, which hinders their application in large-scale settings. In this work, we propose a Nystr\u00f6m-based KSD acceleration---with runtime $\\mathcal{O} \\left(mn+m^3\\right)$ for $n$ samples and $m\\ll n$ Nystr\u00f6m points---, show its $\\sqrt{n}$-consistency with a classical sub-Gaussian assumption, and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks. We also show the $\\sqrt n$-consistency of the quadratic-time KSD estimator.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Florian Kalinke;Zolt\u00e1n Szab\u00f3;Bharath Sriperumbudur",
        "authorids": "~Florian_Kalinke1;~Zolt\u00e1n_Szab\u00f31;~Bharath_Sriperumbudur1",
        "gender": ";;",
        "homepage": "https://flopska.com/;;https://bharathsv.github.io/",
        "dblp": "278/7911;;01/6464",
        "google_scholar": "KYWxZHEAAAAJ;;https://scholar.google.de/citations?user=yHDXov0AAAAJ",
        "orcid": "0000-0002-0443-6288;;",
        "linkedin": ";;",
        "or_profile": "~Florian_Kalinke1;~Zolt\u00e1n_Szab\u00f31;~Bharath_Sriperumbudur1",
        "aff": "Karlsruher Institut f\u00fcr Technologie+Karlsruhe Institut f\u00fcr Technologie;;Pennsylvania State University",
        "aff_domain": "kit.edu+kit.edu;;psu.edu",
        "position": "Postdoc+PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nkalinke2025nystrm,\ntitle={Nystr\\\"om Kernel Stein Discrepancy},\nauthor={Florian Kalinke and Zolt{\\'a}n Szab{\\'o} and Bharath Sriperumbudur},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iLM3CeY2zb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iLM3CeY2zb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iUYgEe5gmO",
        "title": "Scalable spectral representations for multiagent reinforcement learning in network MDPs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Network Markov Decision Processes (MDPs), which are the de-facto model for multi-agent control, pose a significant challenge to efficient learning caused by the exponential growth of the global state-action space with the number of agents. In this work, utilizing the exponential decay property of network dynamics, we first derive scalable spectral local representations for multiagent reinforcement learning in network MDPs, which induces a network linear subspace for the local $Q$-function of each agent. Building on these local spectral representations, we design a scalable algorithmic framework for multiagent reinforcement learning in continuous state-action network MDPs, and provide end-to-end guarantees for the convergence of our algorithm. Empirically, we validate the effectiveness of our scalable representation-based approach on two benchmark problems, and demonstrate the advantages of our approach over generic function approximation approaches to representing the local $Q$-functions.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhaolin Ren;Runyu Zhang;Bo Dai;Na Li",
        "authorids": "~Zhaolin_Ren1;~Runyu_Zhang1;~Bo_Dai1;~Na_Li3",
        "gender": "M;;;F",
        "homepage": ";https://dianyu420376.github.io/runyu-cathy-zhang.github.io/;https://bo-dai.github.io/;https://nali.seas.harvard.edu/",
        "dblp": ";;64/2903;",
        "google_scholar": ";h3SuftsAAAAJ;TIKl_foAAAAJ;qdGelXoAAAAJ",
        "orcid": ";;0009-0002-8070-574X;",
        "linkedin": "zhaolin-ren-1b1b94108;;;",
        "or_profile": "~Zhaolin_Ren1;~Runyu_Zhang1;~Bo_Dai1;~Na_Li3",
        "aff": "Harvard University;Harvard University;Georgia Institute of Technology+Google Brain;Harvard University",
        "aff_domain": "harvard.edu;harvard.edu;gatech.edu+google.com;harvard.edu",
        "position": "PhD student;PhD student;Assistant Professor+Research Scientist;Full Professor",
        "bibtex": "@inproceedings{\nren2025scalable,\ntitle={Scalable Spectral Representation for Networked Multiagent Control},\nauthor={Zhaolin Ren and Runyu Zhang and Bo Dai and Na Li},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iUYgEe5gmO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iUYgEe5gmO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iaeFTpT3e7",
        "title": "Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Bayesian deep learning all too often underfits so that the Bayesian prediction is less accurate than a simple point estimate. Uncertainty quantification then comes at the cost of accuracy. For linearized models, the null space of the generalized Gauss-Newton matrix corresponds to parameters that preserve the training predictions of the point estimate. We propose to build Bayesian approximations in this null space, thereby guaranteeing that the Bayesian predictive does not underfit. We suggest a matrix-free algorithm for projecting onto this null space, which scales linearly with the number of parameters and quadratically with the number of output dimensions. We further propose an approximation that only scales linearly with parameters to make the method applicable to generative models. An extensive empirical evaluation shows that the approach scales to large models, including vision transformers with 28 million parameters. Code is available at: https://github.com/h-roy/projected-bayes",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Marco Miani;Hrittik Roy;S\u00f8ren Hauberg",
        "authorids": "~Marco_Miani1;~Hrittik_Roy2;~S\u00f8ren_Hauberg1",
        "gender": "M;M;M",
        "homepage": "https://www.linkedin.com/in/marco-miani/;http://www.compute.dtu.dk;http://www2.compute.dtu.dk/~sohau/",
        "dblp": "296/1592;;39/7226",
        "google_scholar": "https://scholar.google.com/citations?hl=it;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Marco_Miani1;~Hrittik_Roy2;~S\u00f8ren_Hauberg1",
        "aff": "Technical University of Denmark;Technical University of Denmark;Technical University of Denmark",
        "aff_domain": "dtu.dk;dtu.dk;dtu.dk",
        "position": "PhD student;PhD student;Professor",
        "bibtex": "@inproceedings{\nmiani2025bayes,\ntitle={Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections},\nauthor={Marco Miani and Hrittik Roy and S{\\o}ren Hauberg},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=iaeFTpT3e7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iaeFTpT3e7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ilNQ2m4GTy",
        "title": "Visualizing token importance for black-box language models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the problem of auditing *black-box* large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question---can we understand how the outputs of black-box LLMs depend on *each input token*? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a *practical tool* for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Paulius Rauba;Qiyao Wei;Mihaela van der Schaar",
        "authorids": "~Paulius_Rauba1;~Qiyao_Wei1;~Mihaela_van_der_Schaar2",
        "gender": "M;M;F",
        "homepage": ";https://qiyaowei.github.io;https://www.vanderschaar-lab.com",
        "dblp": ";327/3121;",
        "google_scholar": ";;DZ3S--MAAAAJ",
        "orcid": ";;",
        "linkedin": "paulius-rauba/;qiyaowei;",
        "or_profile": "~Paulius_Rauba1;~Qiyao_Wei1;~Mihaela_van_der_Schaar2",
        "aff": "University of Cambridge;University of Cambridge;University of Cambridge+University of California, Los Angeles",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk+ucla.edu",
        "position": "PhD student;PhD student;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nrauba2025auditing,\ntitle={Auditing language models with distribution-based sensitivity analysis},\nauthor={Paulius Rauba and Qiyao Wei and Mihaela van der Schaar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ilNQ2m4GTy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ilNQ2m4GTy",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ipopfqHYxN",
        "title": "Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression---with genetic variants as instruments---to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shimeng Huang;Niklas Pfister;Jack Bowden",
        "authorids": "~Shimeng_Huang1;~Niklas_Pfister1;j.bowden2@exeter.ac.uk",
        "gender": ";;",
        "homepage": ";https://niklaspfister.github.io/;",
        "dblp": ";222/3117;",
        "google_scholar": ";u2G6pzcAAAAJ;",
        "orcid": ";0000-0001-6203-9777;",
        "linkedin": ";;",
        "or_profile": "~Shimeng_Huang1;~Niklas_Pfister1;j.bowden2@exeter.ac.uk",
        "aff": ";Lakera AI;",
        "aff_domain": ";lakera.ai;",
        "position": ";Researcher;",
        "bibtex": "@inproceedings{\nhuang2025sparse,\ntitle={Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding},\nauthor={Shimeng Huang and Niklas Pfister and Jack Bowden},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ipopfqHYxN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ipopfqHYxN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "j3Aa0YAmBE",
        "title": "Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we present the Federated Upper Confidence Bound Value Iteration algorithm ($\\texttt{Fed-UCBVI}$), a novel extension of the $\\texttt{UCBVI}$ algorithm (Azar et al., 2017) tailored for the federated learning framework. We prove that the regret of $\\texttt{Fed-UCBVI}$ scales as $\\tilde O(\\sqrt{H^3 |S| |A| T / M})$, with a small additional term due to heterogeneity, where $|S|$ is the number of states, $|A|$ is the number of actions, $H$ is the episode length,  $M$ is the number of agents, and $T$ is the number of episodes. Notably, in the single-agent setting, this upper bound matches the minimax lower bound up to polylogarithmic factors, while in the multi-agent scenario, $\\texttt{Fed-UCBVI}$ has linear speed-up. To conduct our analysis, we introduce a new measure of heterogeneity, which may hold independent theoretical interest. Furthermore, we show that, unlike existing federated reinforcement learning approaches,  $\\texttt{Fed-UCBVI}$'s communication complexity only marginally increases with the number of agents.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Safwan Labbi;Daniil Tiapkin;Lorenzo Mancini;Paul Mangold;Eric Moulines",
        "authorids": "~Safwan_Labbi1;~Daniil_Tiapkin1;~Lorenzo_Mancini1;~Paul_Mangold1;~Eric_Moulines1",
        "gender": "M;M;;M;M",
        "homepage": ";https://d-tiapkin.github.io/;;http://www.pmangold.fr;",
        "dblp": ";267/5445;;298/1535;54/2358",
        "google_scholar": "lJp5xdQAAAAJ;https://scholar.google.ru/citations?user=AB23PXQAAAAJ;;https://scholar.google.fr/citations?user=3HUiM0sAAAAJ;https://scholar.google.fr/citations?user=_XE1LvQAAAAJ",
        "orcid": ";0000-0002-8832-7926;;0000-0002-0252-5287;0000-0002-2058-0693",
        "linkedin": "https://linkedin.com/in/safwan-labbi-615051167;daniil-tiapkin-049714240/;lorenzomancini-1/;;",
        "or_profile": "~Safwan_Labbi1;~Daniil_Tiapkin1;~Lorenzo_Mancini1;~Paul_Mangold1;~Eric_Moulines1",
        "aff": "\u00c9cole Polytechnique;Ecole Polytechnique+Universit\u00e9 Paris-Saclay;\u00c9cole Polytechnique;\u00c9cole Polytechnique;Ecole polytechnique",
        "aff_domain": "polytechnique.edu;polytechnique.edu+universite-paris-saclay.fr;polytechnique.edu;polytechnique.edu;polytechnique.edu",
        "position": "PhD student;PhD student+PhD student;PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nlabbi2025federated,\ntitle={Federated {UCBVI}: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents},\nauthor={Safwan Labbi and Daniil Tiapkin and Lorenzo Mancini and Paul Mangold and Eric Moulines},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=j3Aa0YAmBE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=j3Aa0YAmBE",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "j4CbQGb0iF",
        "title": "Understanding Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic Analysis and Global Optimality",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The goal of the Inverse reinforcement learning (IRL) task is to identify the underlying reward function and the corresponding optimal policy\nfrom a set of expert demonstrations.\nWhile most IRL algorithms\u2019 theoretical guarantees rely on a linear reward structure, we aim to extend the theoretical understanding of IRL to scenarios where the reward function is parameterized by neural networks. \nMeanwhile, conventional IRL algorithms usually adopt a nested structure, leading to computational inefficiency, especially in high-dimensional settings.\nTo address this problem, we propose the first two-timescale single-loop IRL algorithm under neural network parameterized\nreward and provide a non-asymptotic convergence analysis under overparameterization.\nAlthough prior optimality results for linear rewards do not apply, we show that our algorithm can identify the globally optimal reward\nand policy under certain neural network structures.\nThis is the first IRL algorithm with a non-asymptotic convergence guarantee that\nprovably achieves global optimality in neural network settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruijia Zhang;Siliang Zeng;Chenliang Li;Alfredo Garcia;Mingyi Hong",
        "authorids": "~Ruijia_Zhang1;~Siliang_Zeng1;~Chenliang_Li3;~Alfredo_Garcia1;~Mingyi_Hong1",
        "gender": "M;M;M;M;M",
        "homepage": ";https://siliangzeng.github.io/index.html;;https://agarcia.engr.tamu.edu;http://people.ece.umn.edu/~mhong/mingyi.html",
        "dblp": ";38/9;;;57/8053",
        "google_scholar": ";IfqsDyYAAAAJ;;;qRnP-p0AAAAJ",
        "orcid": ";;;;",
        "linkedin": "https://www.linkedin.cn/incareer/in/ruijia-zhang-858a3620b;;https://www.linkedin.cn/incareer/in/%E7%90%9B%E8%89%AF-%E6%9D%8E-5a333a23b;;",
        "or_profile": "~Ruijia_Zhang1;~Siliang_Zeng1;~Chenliang_Li3;~Alfredo_Garcia1;~Mingyi_Hong1",
        "aff": "Johns Hopkins University;University of Minnesota, Twin Cities;Texas A&M University - College Station;Texas A&M University - College Station;Amazon+University of Minnesota, Minneapolis",
        "aff_domain": "jh.edu;umn.edu;tamu.edu;tamu.edu;amazon.com+umn.edu",
        "position": "PhD student;PhD student;PhD student;Full Professor;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025understanding,\ntitle={Understanding Inverse Reinforcement Learning under Overparameterization: Non-Asymptotic Analysis and Global Optimality},\nauthor={Ruijia Zhang and Siliang Zeng and Chenliang Li and Alfredo Garcia and Mingyi Hong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=j4CbQGb0iF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=j4CbQGb0iF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jMV46QmlAB",
        "title": "Out-of-distribution robustness for multivariate analysis via causal regularisation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a regularisation strategy of classical machine learning algorithms rooted in causality that ensures robustness against distribution shifts. Building upon the anchor regression framework, we demonstrate how incorporating a straightforward regularisation term into the loss function of classical multivariate analysis algorithms, such as (orthonormalized) partial least squares, reduced-rank regression, and multiple linear regression, enables out-of-distribution generalisation. Our framework allows users to efficiently verify the compatibility of a loss function with the regularisation strategy. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with multivariate analysis approaches and its role in enhancing replicability while guarding against distribution shifts. The extended anchor framework advances causal inference methodologies, addressing the need for reliable out-of-distribution generalisation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Homer Durand;Gherardo Varando;Nathan Mankovich;Gustau Camps-Valls",
        "authorids": "~Homer_Durand1;~Gherardo_Varando1;~Nathan_Mankovich1;~Gustau_Camps-Valls1",
        "gender": "M;M;M;M",
        "homepage": "https://homerdurand.github.io/;https://www.uv.es/varando/;https://natemankovich.weebly.com/;http://www.uv.es/gcamps",
        "dblp": ";150/6914;;32/5293",
        "google_scholar": ";https://scholar.google.es/citations?user=ALjiTg0AAAAJ;d9V9fdkAAAAJ;6mgnauMAAAAJ",
        "orcid": ";0000-0002-6708-1103;;0000-0003-1683-2138",
        "linkedin": ";;nathan-mankovich-b1293717/;gcampsvalls/",
        "or_profile": "~Homer_Durand1;~Gherardo_Varando1;~Nathan_Mankovich1;~Gustau_Camps-Valls1",
        "aff": "Image Processing Lab;Universidad de Valencia;Universidad de Valencia;Universitat de Val\u00e8ncia",
        "aff_domain": "isp.uv.es;uv.es;uv.es;uv.es",
        "position": "PhD student;Assistant Professor;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\ndurand2025outofdistribution,\ntitle={Out-of-distribution robustness for multivariate analysis via causal regularisation},\nauthor={Homer Durand and Gustau Camps-Valls and Gherardo Varando and Nathan Mankovich},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=jMV46QmlAB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jMV46QmlAB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jVmqbiBhWc",
        "title": "Changepoint Estimation in Sparse Dynamic Stochastic Block Models under Near-Optimal Signal Strength",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider the offline changepoint estimation problem in the context of multilayer stochastic block models. We develop an algorithm involving suitably chosen CUSUM statistics based on the adjacency matrices of the observed networks for estimating a single changepoint present in the input data. We provide rigorous theoretical guarantees on the performance of the proposed method when one or more of the following phenomena occur at the changepoint: (a) merging of communities, (b) splitting of communities, and (c) changes in the connection probabilities among the communities. We derive a lower bound on the minimax detectability threshold involving the relevant signal strength parameter and show that the proposed algorithm can estimate the changepoint consistently when the signal strength is above a small multiplicative factor times the minimax detectability threshold. We do not make any a priori assumption on the sparsity of the underlying networks and only require that the overall average degree goes to infinity. Via simulation experiments, we empirically show that the proposed algorithm works in regimes of signal strength where global network changepoint estimation algorithms that do not take into account the community structure, fail to estimate an existing changepoint correctly. Finally, we apply our algorithm to a series of networks constructed using roll call data from the US senate and obtain changepoint(s) which align with those reported in the political science literature regarding the phenomenon of increasing political polarization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shirshendu Chatterjee;Soumendu Sundar Mukherjee;TAMOJIT SADHUKHAN",
        "authorids": "~Shirshendu_Chatterjee1;~Soumendu_Sundar_Mukherjee2;~TAMOJIT_SADHUKHAN1",
        "gender": "M;;M",
        "homepage": "https://shirshendu.ccny.cuny.edu/;;https://sites.google.com/view/tamojitsadhukhan",
        "dblp": ";;",
        "google_scholar": "fkVBF7QAAAAJ;;40LzxXQAAAAJ",
        "orcid": "0000-0002-1344-7624;;0000-0003-4864-2398",
        "linkedin": "shirshendu-chatterjee-7b618124;;tamojitsadhukhan9619",
        "or_profile": "~Shirshendu_Chatterjee1;~Soumendu_Sundar_Mukherjee2;~TAMOJIT_SADHUKHAN1",
        "aff": "CUNY City College of NY+City University of New York;;Indian Statistical Institute, Kolkata",
        "aff_domain": "ccny.cuny.edu+gc.cuny.edu;;isical.ac.in",
        "position": "Associate Professor+Associate Professor;;PhD student",
        "bibtex": "@inproceedings{\nchatterjee2025changepoint,\ntitle={Changepoint estimation in sparse dynamic stochastic block models under near-optimal signal strength},\nauthor={Shirshendu Chatterjee and Soumendu Sundar Mukherjee and TAMOJIT SADHUKHAN},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=jVmqbiBhWc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jVmqbiBhWc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jj1o2SD3PB",
        "title": "Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Minimax optimization recently is widely applied in many machine learning tasks such as generative adversarial networks, robust learning and reinforcement learning. In the paper, we study a class of nonconvex-nonconcave minimax optimization with nonsmooth regularization, where the objective function is possibly nonconvex on primal variable $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition on dual variable $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any specifical types. Theoretically, we prove that our methods have the best known sample complexity of $\\tilde{O}(\\epsilon^{-3})$ only requiring one sample at each loop in finding an $\\epsilon$-stationary solution. Some numerical experiments on PL-game and Wasserstein-GAN demonstrate the efficiency of our proposed methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Feihu Huang;Chunyu Xuan;Xinrui Wang;Siqi Zhang;Songcan Chen",
        "authorids": "~Feihu_Huang1;~Chunyu_Xuan1;~Xinrui_Wang4;~Siqi_Zhang7;~Songcan_Chen1",
        "gender": "M;M;;;",
        "homepage": ";https://github.com/HarryXuancy;;;",
        "dblp": "169/6247;;;;",
        "google_scholar": "tRQwlHUAAAAJ;;;https://scholar.google.com/citations?view_op=list_works;",
        "orcid": "0000-0003-0806-6074;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Feihu_Huang1;~Chunyu_Xuan1;~Xinrui_Wang4;~Siqi_Zhang7;~Songcan_Chen1",
        "aff": "Nanjing University of Aeronautics and Astronautics;Aalto University+Xi'an Jiaotong University;;Nanjing University of Aeronautics and Astronautics;",
        "aff_domain": "nuaa.edu.cn;aalto.fi+xjtu.edu.cn;;nuaa.edu.cn;",
        "position": "Full Professor;PhD student+MS student;;PhD student;",
        "bibtex": "@inproceedings{\nhuang2025enhanced,\ntitle={Enhanced Adaptive Gradient Algorithms for Nonsmooth Nonconvex-{PL} Minimax Optimization},\nauthor={Feihu Huang and Xinrui Wang and Siqi Zhang and Chunyu Xuan and Songcan Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=jj1o2SD3PB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jj1o2SD3PB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jrh3nHUMsK",
        "title": "Partial Information Decomposition for Data Interpretability and Feature Selection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection. Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature\u2019s contribution to synergistic information, and the amount of this information that is redundant. In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features. We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Charles Westphal;Stephen Hailes;Mirco Musolesi",
        "authorids": "~Charles_Westphal1;~Stephen_Hailes1;~Mirco_Musolesi2",
        "gender": "M;M;",
        "homepage": ";http://www0.cs.ucl.ac.uk/staff/S.Hailes/;",
        "dblp": ";85/6195;",
        "google_scholar": "z95QotgAAAAJ;https://scholar.google.com.tw/citations?user=LJD3OxkAAAAJ;",
        "orcid": "0009-0000-0136-7910;0000-0001-7375-3642;",
        "linkedin": ";;",
        "or_profile": "~Charles_Westphal1;~Stephen_Hailes1;~Mirco_Musolesi2",
        "aff": "University College London, University of London;University College London;",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;",
        "position": "PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nwestphal2025partial,\ntitle={Partial Information Decomposition for Data Interpretability and Feature Selection},\nauthor={Charles Westphal and Stephen Hailes and Mirco Musolesi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=jrh3nHUMsK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jrh3nHUMsK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jzTz0hzl68",
        "title": "Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features. Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights. In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices. Our method, based on dynamic programming, is efficient and capable of estimating the moments of the operator spectrum. We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra. Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chanwoo Chun;SueYeon Chung;Daniel Lee",
        "authorids": "~Chanwoo_Chun1;~SueYeon_Chung1;~Daniel_Lee1",
        "gender": ";F;M",
        "homepage": ";https://sites.google.com/site/sueyeonchung/;",
        "dblp": ";173/5418;",
        "google_scholar": "nmMGiiUAAAAJ;h7yVv0QAAAAJ;J0l7wWwAAAAJ",
        "orcid": ";;",
        "linkedin": "chan-woo-chun-b00182133/;;",
        "or_profile": "~Chanwoo_Chun1;~SueYeon_Chung1;~Daniel_Lee1",
        "aff": "Flatiron Institute+Cornell University+Weill Cornell Medicine, Cornell University;Harvard University+Flatiron Institute / Simons Foundation+New York University;Cornell University",
        "aff_domain": "flatironinstitute.org+cornell.edu+med.cornell.edu;harvard.edu+simonsfoundation.org+nyu.edu;cornell.edu",
        "position": "Intern+PhD student+PhD student;Assistant Professor+Principal Investigator+Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nchun2025estimating,\ntitle={Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices},\nauthor={Chanwoo Chun and SueYeon Chung and Daniel Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=jzTz0hzl68}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jzTz0hzl68",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kB0pDFWD8d",
        "title": "Active Feature Acquisition for Personalised Treatment Assignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Making treatment effect estimation actionable for personalized decision-making requires overcoming the costs and delays of acquiring necessary features. While many machine learning models estimate Conditional Average Treatment Effects (CATE), they mostly assume that _all_ relevant features are readily available at prediction time \u2013 a scenario that is rarely realistic. In practice, acquiring features, such as medical tests, can be both expensive and time-consuming, highlighting the need for strategies that select the most informative features for each individual, enhancing decision accuracy while controlling costs. Existing active feature acquisition (AFA) methods, developed for supervised learning, fail to address the unique challenges of CATE, such as confounding, overlap, and the structural similarities of potential outcomes under different treatments. To tackle these challenges, we propose specialised feature acquisition metrics and estimation strategies tailored to the CATE setting. We demonstrate the effectiveness of our methods through experiments on synthetic datasets designed to reflect common biases and data issues. In doing so, this work aims to bridge the gap between cutting-edge CATE estimation techniques and their practical, cost-efficient application in personalised treatment assignment.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Julianna Piskorz;Nicol\u00e1s Astorga;Jeroen Berrevoets;Mihaela van der Schaar",
        "authorids": "~Julianna_Piskorz1;~Nicol\u00e1s_Astorga1;~Jeroen_Berrevoets1;~Mihaela_van_der_Schaar2",
        "gender": "F;M;;F",
        "homepage": ";;https://jeroenbe.github.io;https://www.vanderschaar-lab.com",
        "dblp": ";;236/4591;",
        "google_scholar": ";oLiBK8cAAAAJ;https://scholar.google.be/citations?user=Bq1dFNQAAAAJ;DZ3S--MAAAAJ",
        "orcid": ";;;",
        "linkedin": "julianna-piskorz;;;",
        "or_profile": "~Julianna_Piskorz1;~Nicol\u00e1s_Astorga1;~Jeroen_Berrevoets1;~Mihaela_van_der_Schaar2",
        "aff": "University of Cambridge;University of Cambridge;Ataraxis.ai;University of Cambridge+University of California, Los Angeles",
        "aff_domain": "cam.ac.uk;cam.ac.uk;ataraxis.ai;cam.ac.uk+ucla.edu",
        "position": "PhD student;PhD student;Researcher;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\npiskorz2025active,\ntitle={Active Feature Acquisition for Personalised Treatment Assignment},\nauthor={Julianna Piskorz and Nicol{\\'a}s Astorga and Jeroen Berrevoets and Mihaela van der Schaar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kB0pDFWD8d}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kB0pDFWD8d",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kOTUgBknsK",
        "title": "Trustworthy assessment of heterogeneous treatment effect estimator via analysis of relative error",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accurate heterogeneous treatment effect (HTE) estimation is essential for personalized recommendations, making it important to evaluate and compare HTE estimators. Traditional assessment methods are inapplicable due to missing counterfactuals.\nCurrent HTE evaluation methods rely on additional estimation or matching on test data, often ignoring the uncertainty introduced and potentially leading to incorrect conclusions.\nWe propose incorporating uncertainty quantification into HTE estimator comparisons. In addition, we suggest shifting the focus to the estimation and inference of the relative error between methods rather than their absolute errors. Methodology-wise, we develop a relative error estimator based on the efficient influence function and establish its asymptotic distribution for inference. Compared to absolute error-based methods, the relative error estimator  (1) is less sensitive to the error of nuisance function estimators, satisfying a \"global double robustness\" property, and (2) its confidence intervals are often narrower, making it more powerful for determining the more accurate HTE estimator. Through extensive empirical study of the ACIC challenge benchmark datasets, we show that the relative error-based method more effectively identifies the better HTE estimator with statistical confidence, even with a moderately large test dataset or inaccurate\nnuisance estimators.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zijun Gao",
        "authorids": "~Zijun_Gao1",
        "gender": "F",
        "homepage": "https://zijungao.github.io",
        "dblp": "239/5605",
        "google_scholar": "",
        "orcid": "0000-0003-4863-1656",
        "linkedin": "",
        "or_profile": "~Zijun_Gao1",
        "aff": "University of Southern California",
        "aff_domain": "usc.edu",
        "position": "Assistant Professor",
        "bibtex": "@inproceedings{\ngao2025trustworthy,\ntitle={Trustworthy assessment of heterogeneous treatment effect estimator via analysis of relative error},\nauthor={Zijun Gao},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kOTUgBknsK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kOTUgBknsK",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kREJgA7pV2",
        "title": "Training Neural Samplers with Reverse Diffusive KL Divergence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Training generative models to sample from unnormalized density functions is an important and challenging task in machine learning. \nTraditional training methods often rely on the reverse Kullback-Leibler (KL) divergence due to its tractability. However, the mode-seeking behavior of reverse KL hinders effective approximation of multi-modal target distributions. To address this, we propose to minimize the reverse KL along diffusion trajectories of both model and target densities. We refer to this objective as the reverse diffusive KL divergence, which allows the model to capture multiple modes. Leveraging this objective, we train neural samplers that can efficiently generate samples from the target distribution in one step. We demonstrate that our method enhances sampling performance across various Boltzmann distributions, including both synthetic multi-modal densities and n-body particle systems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiajun He;Wenlin Chen;Mingtian Zhang;David Barber;Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
        "authorids": "~Jiajun_He3;~Wenlin_Chen2;~Mingtian_Zhang1;~David_Barber1;~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1",
        "gender": "M;;M;M;",
        "homepage": ";https://wenlin-chen.github.io/;http://mingtian.ai;http://www.cs.ucl.ac.uk/staff/D.Barber/;",
        "dblp": "205/5074-3;;230/8340;;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=Nej1FcgAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "jiajun-he-76a59526b/;;;;",
        "or_profile": "~Jiajun_He3;~Wenlin_Chen2;~Mingtian_Zhang1;~David_Barber1;~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1",
        "aff": "University of Cambridge;University of Cambridge+Max Planck Institute for Intelligent Systems;University College London;University College London+University College London;",
        "aff_domain": "cam.ac.uk;cam.ac.uk+tuebingen.mpg.de;ucl.ac.uk;ucl.ac.uk+;",
        "position": "PhD student;PhD Student+Doctoral Researcher;Researcher;Associate Professor+Full Professor;",
        "bibtex": "@inproceedings{\nhe2025training,\ntitle={Training Neural Samplers with Reverse Diffusive {KL} Divergence},\nauthor={Jiajun He and Wenlin Chen and Mingtian Zhang and David Barber and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kREJgA7pV2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kREJgA7pV2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kVWy8yIHs1",
        "title": "Approximate Global Convergence of Independent Learning in Multi-Agent Systems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Independent learning (IL) is a popular approach for achieving scalability in large-scale multi-agent systems, yet it typically lacks global convergence guarantees. In this paper, we study two representative algorithms\u2014independent $Q$-learning and independent natural actor-critic\u2014within both value-based and policy-based frameworks, and provide the first finite-sample analysis for approximate global convergence. Our results show that IL can achieve global convergence up to a fixed error arising from agent interdependence, which characterizes the fundamental limit of IL in achieving true global convergence. To establish these results, we develop a novel approach by constructing a separable Markov decision process (MDP) for convergence analysis and then bounding the gap caused by the model discrepancy between this separable MDP and the original one. Finally, we present numerical experiments using a synthetic MDP and an electric vehicle charging example to demonstrate our findings and the practical applicability of IL.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruiyang Jin;Zaiwei Chen;Yiheng Lin;Jie Song;Adam Wierman",
        "authorids": "~Ruiyang_Jin2;~Zaiwei_Chen1;~Yiheng_Lin1;~Jie_Song6;~Adam_Wierman1",
        "gender": ";;M;F;M",
        "homepage": ";;;http://www2.coe.pku.edu.cn/faculty/songjie/indexen.html;https://adamwierman.com/",
        "dblp": ";;;;56/4447",
        "google_scholar": "WeXuoUsAAAAJ;;S1wSEggAAAAJ;;4OvOdSgAAAAJ",
        "orcid": ";;;;0000-0002-5923-0199",
        "linkedin": ";;;;adam-wierman-a529474/",
        "or_profile": "~Ruiyang_Jin2;~Zaiwei_Chen1;~Yiheng_Lin1;~Jie_Song6;~Adam_Wierman1",
        "aff": "City University of Hong Kong;;California Institute of Technology;Peking University;California Institute of Technology",
        "aff_domain": "cityu.edu.hk;;caltech.edu;pku.edu.cn;caltech.edu",
        "position": "Postdoc;;PhD student;Full Professor;Professor",
        "bibtex": "@inproceedings{\njin2025approximate,\ntitle={Approximate Global Convergence of Independent Learning in Multi-Agent Systems},\nauthor={Ruiyang Jin and Zaiwei Chen and Yiheng Lin and Jie Song and Adam Wierman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kVWy8yIHs1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kVWy8yIHs1",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kXC0Sdf8KN",
        "title": "A Unified Evaluation Framework for Epistemic Predictions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Predictions of uncertainty-aware models are diverse, ranging from single point estimates (often averaged over prediction samples) to predictive distributions, to set-valued or credal-set representations. We propose a novel unified evaluation framework for uncertainty-aware classifiers, applicable to a wide range of model classes, which allows users to tailor the trade-off between accuracy and precision of predictions via a suitably designed performance metric. This makes possible the selection of the most suitable model for a particular real-world application as a function of the desired trade-off. Our experiments, concerning Bayesian, ensemble, evidential, deterministic, credal and belief function classifiers on the CIFAR-10, MNIST and CIFAR-100 datasets, show that the metric behaves as desired.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shireen Kudukkil Manchingal;Muhammad Mubashar;Kaizheng Wang;Fabio Cuzzolin",
        "authorids": "~Shireen_Kudukkil_Manchingal1;~Muhammad_Mubashar1;~Kaizheng_Wang2;~Fabio_Cuzzolin1",
        "gender": "F;M;M;M",
        "homepage": "https://www.brookes.ac.uk/profiles/student/shireen-kudukkil-manchingal/;;https://www.kuleuven.be/wieiswie/nl/person/00151239;https://www.brookes.ac.uk/profiles/staff/fabio-cuzzolin",
        "dblp": "322/5736;288/7538;;60/2919",
        "google_scholar": "RioUSBEAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;https://scholar.google.co.uk/citations?user=T8LkBTYAAAAJ",
        "orcid": "0009-0001-6597-4290;0009-0002-8149-2168;0000-0001-8268-962X;0000-0002-9271-2130",
        "linkedin": "shireenkudukkil/;muhammad-mubashar-719b05167/;;fabio-cuzzolin-b481a928/",
        "or_profile": "~Shireen_Kudukkil_Manchingal1;~Muhammad_Mubashar1;~Kaizheng_Wang2;~Fabio_Cuzzolin1",
        "aff": ";Oxford Brookes University;KU Leuven;Oxford Brookes University",
        "aff_domain": ";brookes.ac.uk;kuleuven.be;brookes.ac.uk",
        "position": ";PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nmanchingal2025a,\ntitle={A Unified Evaluation Framework for Epistemic Predictions},\nauthor={Shireen Kudukkil Manchingal and Muhammad Mubashar and Kaizheng Wang and Fabio Cuzzolin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kXC0Sdf8KN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kXC0Sdf8KN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kYJSAg7Try",
        "title": "Ant Colony Sampling with GFlowNets for Combinatorial Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present the Generative Flow Ant Colony Sampler (GFACS), a novel meta-heuristic method that hierarchically combines amortized inference and parallel stochastic search. Our method first leverages Generative Flow Networks (GFlowNets) to amortize a multi-modal prior distribution over combinatorial solution space that encompasses both high-reward and diversified solutions. This prior is iteratively updated via parallel stochastic search in the spirit of Ant Colony Optimization (ACO), leading to the posterior distribution that generates near-optimal solutions. Extensive experiments across seven combinatorial optimization problems demonstrate GFACS's promising performances.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Minsu Kim;Sanghyeok Choi;Hyeonah Kim;Jiwoo Son;Jinkyoo Park;Yoshua Bengio",
        "authorids": "~Minsu_Kim2;~Sanghyeok_Choi1;~Hyeonah_Kim1;~Jiwoo_Son2;~Jinkyoo_Park1;~Yoshua_Bengio1",
        "gender": "M;M;F;;M;M",
        "homepage": "https://minsuukim.github.io/;https://hyeok9855.github.io/;;;http://silab.kaist.ac.kr/;http://yoshuabengio.org",
        "dblp": ";338/9899;;348/9675;156/7535;56/953",
        "google_scholar": "https://scholar.google.ca/citations?user=VvyLuhAAAAAJ;wQqjHlIAAAAJ;;zHyj8zAAAAAJ;sH2a0nkAAAAJ;kukA0LcAAAAJ",
        "orcid": ";;0000-0002-0629-1879;0009-0008-1032-6318;0000-0003-2620-1479;",
        "linkedin": ";hyeok9855/;hyeonahkimm/;jiwoo-son-303b31284/;;yoshuabengio/?originalSubdomain=ca",
        "or_profile": "~Minsu_Kim2;~Sanghyeok_Choi1;~Hyeonah_Kim1;~Jiwoo_Son2;~Jinkyoo_Park1;~Yoshua_Bengio1",
        "aff": "Mila - Quebec Artificial Intelligence Institute+Korea Advanced Institute of Science & Technology+Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Montreal Institute for Learning Algorithms, University of Montreal, Universit\u00e9 de Montr\u00e9al+Korea Advanced Institute of Science & Technology;Omelet;Korea Advanced Institute of Science & Technology;University of Montreal",
        "aff_domain": "mila.quebec+kaist.ac.kr+kaist.ac.kr;kaist.ac.kr;mila.umontreal.ca+kaist.edu;omelet.ai;kaist.ac.kr;umontreal.ca",
        "position": "Postdoc+Postdoc+PhD student;MS student;Postdoc+PhD student;Researcher;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nkim2025ant,\ntitle={Ant Colony Sampling with {GF}lowNets for Combinatorial Optimization},\nauthor={Minsu Kim and Sanghyeok Choi and Hyeonah Kim and Jiwoo Son and Jinkyoo Park and Yoshua Bengio},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kYJSAg7Try}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kYJSAg7Try",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kgoIpF8Xi0",
        "title": "Calibrated Computation-Aware Gaussian Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Gaussian processes are notorious for scaling cubically with the size of the training set, preventing application to very large regression problems. Computation-aware Gaussian processes (CAGPs) tackle this scaling issue by exploiting probabilistic linear solvers to reduce complexity, widening the posterior with additional *computational* uncertainty due to reduced computation. However, the most commonly used CAGP framework results in (sometimes dramatically) conservative uncertainty quantification, making the posterior difficult to use in practice. In this work, we prove that if the utilised probabilistic linear solver is *calibrated*, in a rigorous statistical sense, then so too is the induced CAGP. We thus propose a new CAGP framework, CAGP-GS, based on using Gauss-Seidel iterations for the underlying probabilistic linear solver. CAGP-GS performs favourably compared to existing approaches when the test set is low-dimensional and few iterations are performed. We test the calibratedness on a synthetic problem, and compare the performance to existing approaches on a large-scale global temperature regression problem.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Disha Hegde;Mohamed Adil;Jon Cockayne",
        "authorids": "~Disha_Hegde2;~Mohamed_Adil1;~Jon_Cockayne1",
        "gender": ";M;",
        "homepage": "https://hegdedisha.github.io/;;http://www.joncockayne.com",
        "dblp": ";;180/5684",
        "google_scholar": ";;https://scholar.google.co.uk/citations?user=ZYBXsvAAAAAJ",
        "orcid": "0009-0009-0490-0130;;",
        "linkedin": "disha-hegde-bb4141171;mohamed-adil-1270a810b;",
        "or_profile": "~Disha_Hegde2;~Mohamed_Adil1;~Jon_Cockayne1",
        "aff": "University of Southampton;Independent Researcher;University of Southampton",
        "aff_domain": "soton.ac.uk;alum.iisc.ac.in;soton.ac.uk",
        "position": "PhD student;Researcher;Lecturer",
        "bibtex": "@inproceedings{\nhegde2025calibrated,\ntitle={Calibrated Computation-Aware Gaussian Processes},\nauthor={Disha Hegde and Mohamed Adil and Jon Cockayne},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kgoIpF8Xi0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kgoIpF8Xi0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kh8wgqToLl",
        "title": "Parameter estimation in state space models using particle importance sampling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "State-space models have been used in many applications, including econometrics, engi- neering, medical research, etc. The maximum likelihood estimation (MLE) of the static pa- rameter of general state-space models is not straightforward because the likelihood func- tion is intractable. It is popular to use the sequential Monte Carlo(SMC) method to per- form gradient ascent optimisation in either offline or online fashion. One problem with existing online SMC methods for MLE is that the score estimators are inconsistent, i.e. the bias does not vanish with increasing particle size. In this paper, two SMC algorithms are proposed based on an importance sampling weight function to use each set of generated particles more efficiently. The first one is an offline algorithm that locally approximates the likelihood function using importance sam- pling, where the locality is adapted by the effective sample size (ESS). The second one is a semi-online algorithm that has a compu- tational cost linear in the particle size and uses score estimators that are consistent. We study its consistency and asymptotic normal- ity. Their computational superiority is illus- trated in numerical studies for long time se- ries.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuxiong Gao;Wentao Li;Rong Chen",
        "authorids": "~Yuxiong_Gao1;~Wentao_Li5;~Rong_Chen6",
        "gender": "M;M;M",
        "homepage": ";https://research.manchester.ac.uk/en/persons/wentao.li;http://stat.rutgers.edu/home/rongchen/",
        "dblp": ";;",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=sWz1n1sAAAAJ;",
        "orcid": ";;",
        "linkedin": "yuxiong-gao-7a0441309/;;",
        "or_profile": "~Yuxiong_Gao1;~Wentao_Li5;~Rong_Chen6",
        "aff": "University of Manchester;University of Manchester;",
        "aff_domain": "manchester.ac.uk;manchester.ac.uk;",
        "position": "PhD student;Lecturer;",
        "bibtex": "@inproceedings{\ngao2025parameter,\ntitle={Parameter estimation in state space models using particle importance sampling},\nauthor={Yuxiong Gao and Wentao Li and Rong Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kh8wgqToLl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kh8wgqToLl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kiVz2xXjEm",
        "title": "Federated Communication-Efficient Multi-Objective Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study a federated version of multi-objective optimization (MOO), where a single model is trained to optimize multiple objective functions. MOO has been extensively studied in the centralized setting but is less explored in federated or distributed settings. We propose FedCMOO, a novel communication-efficient federated multi-objective optimization (FMOO) algorithm that improves the error convergence performance of the model compared to existing approaches. Unlike prior works, the communication cost of FedCMOO does not scale with the number of objectives, as each client sends a single aggregated gradient, obtained using randomized SVD (singular value decomposition), to the central server. We provide a convergence analysis of the proposed method for smooth non-convex objective functions under milder assumptions than in prior work. In addition, we introduce a variant of FedCMOO that allows users to specify a preference over the objectives in terms of a desired ratio of the final objective values. Through extensive experiments, we demonstrate the superiority of our proposed method over baseline approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Baris Askin;Pranay Sharma;Gauri Joshi;Carlee Joe-Wong",
        "authorids": "~Baris_Askin1;~Pranay_Sharma2;~Gauri_Joshi1;~Carlee_Joe-Wong1",
        "gender": ";;;F",
        "homepage": ";;;https://www.andrew.cmu.edu/user/cjoewong/",
        "dblp": ";;;40/9937.html",
        "google_scholar": ";;;XEztdZgAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Baris_Askin1;~Pranay_Sharma2;~Gauri_Joshi1;~Carlee_Joe-Wong1",
        "aff": ";;;Carnegie Mellon University",
        "aff_domain": ";;;cmu.edu",
        "position": ";;;Assistant Professor",
        "bibtex": "@inproceedings{\naskin2025federated,\ntitle={Federated Communication-Efficient Multi-Objective Optimization},\nauthor={Baris Askin and Pranay Sharma and Gauri Joshi and Carlee Joe-Wong},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kiVz2xXjEm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kiVz2xXjEm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ksUAp7YQ2v",
        "title": "Deep Clustering via Probabilistic Ratio-Cut Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a novel approach for optimizing the graph ratio-cut by modeling the binary assignments as random variables. We provide an upper bound on the expected ratio-cut, as well as an unbiased estimate of its gradient, to learn the parameters of the assignment variables in an online setting. The clustering resulting from our probabilistic approach (PRCut) outperforms the Rayleigh quotient relaxation of the combinatorial problem, its online learning extensions, and several widely used methods. We demonstrate that the PRCut clustering closely aligns with the similarity measure and can perform as well as a supervised classifier when label-based similarities are provided. This novel approach can leverage out-of-the-box self-supervised representations to achieve competitive performance and serve as an evaluation method for the quality of these representations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ayoub Ghriss;Claire Monteleoni",
        "authorids": "~Ayoub_Ghriss1;~Claire_Monteleoni1",
        "gender": ";F",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";FqNPXeoAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ayoub_Ghriss1;~Claire_Monteleoni1",
        "aff": ";University of Colorado, Boulder+INRIA",
        "aff_domain": ";colorado.edu+inria.fr",
        "position": ";Full Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nghriss2025deep,\ntitle={Deep Clustering via Probabilistic Ratio-Cut Optimization},\nauthor={Ayoub Ghriss and Claire Monteleoni},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ksUAp7YQ2v}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ksUAp7YQ2v",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ktJGAHECma",
        "title": "Koopman-Equivariant Gaussian Processes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and\nrepresentational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance \u2013 which we refer to as Koopman equivariance \u2013 we obtain a  GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Petar Bevanda;Max Beier;Alexandre Capone;Stefan Georg Sosnowski;Sandra Hirche;Armin Lederer",
        "authorids": "~Petar_Bevanda1;~Max_Beier1;~Alexandre_Capone1;~Stefan_Georg_Sosnowski1;~Sandra_Hirche1;~Armin_Lederer1",
        "gender": "M;M;M;;F;",
        "homepage": "https://www.ce.cit.tum.de/itr/bevanda/;;https://acapone1.github.io/;;http://www.itr.ei.tum.de;",
        "dblp": "284/8547;308/6359;238/8124.html;;89/6985;202/5716",
        "google_scholar": "Hne4SYQAAAAJ;https://scholar.google.de/citations?user=iZppzEYAAAAJ;VD_C8GcAAAAJ;;;6yB84RUAAAAJ",
        "orcid": "0000-0001-8205-3322;0000-0001-7772-2819;0000-0002-4358-0012;;;0000-0001-6263-5608",
        "linkedin": "https://linkedin.com/in/petar-bevanda-5a4498120;;alexandre-c-92b678134/;;;",
        "or_profile": "~Petar_Bevanda1;~Max_Beier1;~Alexandre_Capone1;~Stefan_Georg_Sosnowski1;~Sandra_Hirche1;~Armin_Lederer1",
        "aff": "Technische Universit\u00e4t M\u00fcnchen, Chair of  Information-oriented Control;Technische Universit\u00e4t M\u00fcnchen;Carnegie Mellon University;;Technical University Munich;National University of Singapore+ETHZ - ETH Zurich",
        "aff_domain": "tum.de;tum.de;cmu.edu;;tum.de;nus.edu.sg+ethz.ch",
        "position": "PhD student;PhD student;Postdoc;;Full Professor;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nbevanda2025koopmanequivariant,\ntitle={Koopman-Equivariant Gaussian Processes},\nauthor={Petar Bevanda and Max Beier and Alexandre Capone and Stefan Georg Sosnowski and Sandra Hirche and Armin Lederer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ktJGAHECma}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ktJGAHECma",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kuxXSg08nb",
        "title": "Recurrent Neural Goodness-of-Fit Test for Time Series",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Time series data are crucial across diverse domains such as finance and healthcare, where accurate forecasting and decision-making rely on advanced modeling techniques. While generative models have shown great promise in capturing the intricate dynamics inherent in time series, evaluating their performance remains a major challenge. Traditional evaluation metrics fall short due to the temporal dependencies and potential high dimensionality of the features. In this paper, we propose the REcurrent NeurAL (RENAL) Goodness-of-Fit test, a novel and statistically rigorous framework for evaluating generative time series models. By leveraging recurrent neural networks, we transform the time series into conditionally independent data pairs, enabling the application of a chi-square-based goodness-of-fit test to the temporal dependencies within the data. This approach offers a robust, theoretically grounded solution for assessing the quality of generative models, particularly in settings with limited time sequences. We demonstrate the efficacy of our method across both synthetic and real-world datasets, outperforming existing methods in terms of reliability and accuracy. Our method fills a critical gap in the evaluation of time series generative models, offering a tool that is both practical and adaptable to high-stakes applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aoran Zhang;Wenbin Zhou;Liyan Xie;Shixiang Zhu",
        "authorids": "~Aoran_Zhang1;~Wenbin_Zhou1;~Liyan_Xie2;~Shixiang_Zhu1",
        "gender": "F;M;F;M",
        "homepage": ";;https://mypage.cuhk.edu.cn/academics/xieliyan/;https://sites.google.com/view/woodyzhu",
        "dblp": ";;195/1316;133/3853",
        "google_scholar": ";;KtLwkBYAAAAJ;v6_Gv6IAAAAJ",
        "orcid": ";;;0000-0002-2241-6096",
        "linkedin": "aoran-zhang-641312232/;wenbin-zhou-33462a270/;;shixiang-zhu-26b956a0/",
        "or_profile": "~Aoran_Zhang1;~Wenbin_Zhou1;~Liyan_Xie2;~Shixiang_Zhu1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;University of Minnesota - Twin Cities;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;umn.edu;cmu.edu",
        "position": "Undergrad student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025recurrent,\ntitle={Recurrent Neural Goodness-of-Fit Test for Time Series},\nauthor={Aoran Zhang and Wenbin Zhou and Liyan Xie and Shixiang Zhu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=kuxXSg08nb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kuxXSg08nb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "l0gLH8H3Xe",
        "title": "When the Universe is Too Big: Bounding Consideration Probabilities for Plackett-Luce Rankings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The widely used Plackett-Luce ranking model assumes that individuals rank items by making repeated choices from a universe of items. But in many cases the universe is too big for people to plausibly consider all options. In the choice literature, this issue has been addressed by supposing that individuals first sample a small consideration set and then choose among the considered items. However, inferring unobserved consideration sets (or item consideration probabilities) in this ``consider then choose'' setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known.\n  We apply the consider-then-choose framework to top-$k$ rankings, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set.\n  While item consideration probabilities remain non-identified in this setting, we prove that we can infer bounds on the relative values of consideration probabilities. \n  Additionally, given a condition on the expected consideration set size and known item utilities, we derive absolute upper and lower bounds on item consideration probabilities.\nWe also provide algorithms to tighten those bounds on consideration probabilities by propagating inferred constraints.\n  Thus, we show that we can learn useful information about consideration probabilities despite not being able to identify them precisely.\n  We demonstrate our methods on a ranking dataset from a psychology experiment with two different ranking tasks (one with fixed consideration sets and one with unknown consideration sets).\n  This combination of data allows us to estimate utilities and then learn about unknown consideration probabilities using our bounds.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ben Aoki-Sherwood;Catherine Bregou;David Liben-Nowell;Kiran Tomlinson;Thomas Zeng",
        "authorids": "~Ben_Aoki-Sherwood1;~Catherine_Bregou1;~David_Liben-Nowell1;~Kiran_Tomlinson1;~Thomas_Zeng1",
        "gender": ";F;M;;",
        "homepage": ";https://catherinebregou.github.io/;https://cs.carleton.edu/faculty/dln;https://www.kirantomlinson.com/;",
        "dblp": ";;34/5224;234/3203;70/8836",
        "google_scholar": ";;;7faaZ1cAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "ben-aoki-sherwood/;;;;tzeng200/",
        "or_profile": "~Ben_Aoki-Sherwood1;~Catherine_Bregou1;~David_Liben-Nowell1;~Kiran_Tomlinson1;~Thomas_Zeng1",
        "aff": ";Carleton College;Carleton College;Microsoft;University of Wisconsin - Madison",
        "aff_domain": ";carleton.edu;carleton.edu;microsoft.com;wisc.edu",
        "position": ";Undergrad student;Full Professor;Researcher;PhD student",
        "bibtex": "@inproceedings{\naoki-sherwood2025when,\ntitle={When the Universe is Too Big: Bounding Consideration Probabilities for Plackett-Luce Rankings},\nauthor={Ben Aoki-Sherwood and Catherine Bregou and David Liben-Nowell and Kiran Tomlinson and Thomas Zeng},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=l0gLH8H3Xe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=l0gLH8H3Xe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "l5ODkiD4x9",
        "title": "Learning signals defined on graphs with optimal transport and Gaussian process regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In computational physics, machine learning has now emerged as a powerful complementary tool to explore efficiently candidate designs in engineering studies. Outputs in such supervised problems are signals defined on meshes, and a natural question is the extension of general scalar output regression models to such complex outputs. Changes between input geometries in terms of both size and adjacency structure in particular make this transition non-trivial. In this work, we propose an innovative strategy for Gaussian process regression where inputs are large and sparse graphs with continuous node attributes and outputs are signals defined on the nodes of the associated inputs. The methodology relies on the combination of regularized optimal transport, dimension reduction techniques, and the use of Gaussian processes indexed by graphs. In addition to enabling signal prediction, the main point of our proposal is to come with confidence intervals on node values, which is crucial for uncertainty quantification and active learning. Numerical experiments highlight the efficiency of the method to solve real problems in fluid dynamics and solid mechanics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Raphael Carpintero Perez;S\u00e9bastien Da Veiga;Josselin Garnier;Brian Staber",
        "authorids": "~Raphael_Carpintero_Perez2;~S\u00e9bastien_Da_Veiga1;~Josselin_Garnier1;~Brian_Staber1",
        "gender": "M;;M;M",
        "homepage": ";;https://www.josselin-garnier.org;https://bstaber.github.io/",
        "dblp": ";;63/3241;181/2793",
        "google_scholar": "nJvHg2MAAAAJ;;nJDy0O8AAAAJ;https://scholar.google.fr/citations?user=61j2VawAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;brian-staber/",
        "or_profile": "~Raphael_Carpintero_Perez2;~S\u00e9bastien_Da_Veiga1;~Josselin_Garnier1;~Brian_Staber1",
        "aff": "\u00c9cole Polytechnique;;Ecole polytechnique;Safran",
        "aff_domain": "polytechnique.edu;;polytechnique.edu;safrangroup.com",
        "position": "PhD student;;Full Professor;Researcher",
        "bibtex": "@inproceedings{\nperez2025learning,\ntitle={Learning signals defined on graphs with optimal transport and Gaussian process regression},\nauthor={Raphael Carpintero Perez and S{\\'e}bastien Da Veiga and Josselin Garnier and Brian Staber},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=l5ODkiD4x9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=l5ODkiD4x9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lUu0cJ2ifh",
        "title": "Statistical Guarantees for Unpaired Image-to-Image Cross-Domain Analysis using GANs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The field of unpaired image-to-image translation has undergone a significant transformation with the introduction of Generative Adversarial Networks (GANs), with CycleGAN and DiscoGAN as prominent variants.   While these models show impressive empirical performance, their statistical properties are \n\t\tunder-studied. In this paper, we propose a framework for analyzing the generalization error in cross-domain deep generative models. Our findings reveal that when provided with independent and identically distributed (i.i.d.) samples from two domains, the translation error, measured under the Wasserstein-1 loss, scales as $\\tilde{\\mathcal{O}} \\left(\\min(n, m)^{-1/\\max(d,\\tilde{d})}\\right)$,\n\t\tprovided that the true model possesses sufficient smoothness and the network sizes are chosen appropriately.\n\t\tHere, $n$ and $m$ represent the sizes of the sample sets, while $d$ and $\\tilde{d}$ denote the dimensions of the respective data domains.  Furthermore, we highlight the importance of a cycle loss term for ensuring distributional cycle consistency.\tAdditionally, we provide insights into the relationship between the network size and the number of data points. Notably, as the true model exhibits greater smoothness, it suffices to work with smaller networks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Saptarshi Chakraborty;Peter Bartlett",
        "authorids": "~Saptarshi_Chakraborty1;~Peter_Bartlett1",
        "gender": ";M",
        "homepage": ";https://www.stat.berkeley.edu/~bartlett/",
        "dblp": ";https://dblp.org/pers/hd/b/Bartlett:Peter_L=",
        "google_scholar": ";yQNhFGUAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Saptarshi_Chakraborty1;~Peter_Bartlett1",
        "aff": ";University of California - Berkeley+University of California-Berkeley+University of California, Berkeley+University of California, Berkeley",
        "aff_domain": ";++berkeley.edu+berkeley",
        "position": ";Full Professor++Researcher+Professor",
        "bibtex": "@inproceedings{\nchakraborty2025statistical,\ntitle={Statistical Guarantees for Unpaired Image-to-Image Cross-Domain Analysis using {GAN}s},\nauthor={Saptarshi Chakraborty and Peter Bartlett},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=lUu0cJ2ifh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lUu0cJ2ifh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lblYg0GoCe",
        "title": "Optimal estimation of linear non-Gaussian structure equation models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Much of science involves discovering and modeling causal relationships in nature. Significant progress has been made in developing statistical methods for representing and identifying causal knowledge from data using Linear Non-Gaussian Acyclic Models (LiNGAMs). Despite successes in learning LiNGAMs across various sample settings, the optimal sample complexity for high-dimensional LiNGAMs remains unexplored. This study establishes the optimal sample complexity for learning the structure of LiNGAMs under a sub-Gaussianity assumption. Specifically, it introduces a structure recovery algorithm using distance covariance that achieves the optimal sample complexity, $n = \\Theta(d_{in} \\log \\frac{p}{d_{in}})$, without assuming faithfulness or a known indegree. The theoretical findings and superiority of the proposed algorithm compared to existing algorithms are validated through numerical experiments and real data analysis.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sunmin Oh;Seungsu Han;Gunwoong Park",
        "authorids": "~Sunmin_Oh1;~Seungsu_Han1;~Gunwoong_Park1",
        "gender": "F;M;M",
        "homepage": ";;https://sites.google.com/view/gwpark",
        "dblp": ";;",
        "google_scholar": "yxerWFcAAAAJ;;https://scholar.google.co.kr/citations?user=2nzrNCsAAAAJ",
        "orcid": ";;",
        "linkedin": ";seungsu-han-a92201330;",
        "or_profile": "~Sunmin_Oh1;~Seungsu_Han1;~Gunwoong_Park1",
        "aff": "Seoul National University;Seoul National University;Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "position": "PhD student;Undergrad student;Associate Professor",
        "bibtex": "@inproceedings{\noh2025optimal,\ntitle={Optimal estimation of linear non-Gaussian structure equation models},\nauthor={Sunmin Oh and Seungsu Han and Gunwoong Park},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=lblYg0GoCe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lblYg0GoCe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lgnGyZYcDM",
        "title": "Infinite-Horizon Reinforcement Learning with Multinomial Logit Function Approximation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study model-based reinforcement learning with non-linear function approximation where the transition function of the underlying Markov decision process (MDP) is given by a multinomial logit (MNL) model. We develop a provably efficient discounted value iteration-based algorithm that works for both infinite-horizon average-reward and discounted-reward settings. For average-reward communicating MDPs, the algorithm guarantees a regret upper bound of $\\tilde{\\mathcal{O}}(dD\\sqrt{T})$ where $d$ is the dimension of feature mapping, $D$ is the diameter of the underlying MDP, and $T$ is the horizon. For discounted-reward MDPs, our algorithm achieves $\\tilde{\\mathcal{O}}(d(1-\\gamma)^{-2}\\sqrt{T})$ regret where $\\gamma$ is the discount factor. Then we complement these upper bounds by providing several regret lower bounds. We prove a lower bound of $\\Omega(d\\sqrt{DT})$ for learning communicating MDPs of diameter $D$ and a lower bound of $\\Omega(d(1-\\gamma)^{-3/2}\\sqrt{T})$ for learning discounted-reward MDPs with discount factor $\\gamma$. Lastly, we show a regret lower bound of $\\Omega(dH^{3/2}\\sqrt{K})$ for learning $H$-horizon episodic MDPs with MNL function approximation where $K$ is the number of episodes, which improves upon the best-known lower bound for the finite-horizon setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jaehyun Park;Junyeop Kwon;Dabeen Lee",
        "authorids": "~Jaehyun_Park8;junyeopk@kaist.ac.kr;~Dabeen_Lee1",
        "gender": "M;;",
        "homepage": "https://github.com/uqpuark;;https://dabeenl.github.io",
        "dblp": ";;180/3115.html",
        "google_scholar": ";;8y38M00AAAAJ",
        "orcid": ";;0000-0002-3802-1371",
        "linkedin": "jhpark111;;dabeen-lee-071aa7220",
        "or_profile": "~Jaehyun_Park8;junyeopk@kaist.ac.kr;~Dabeen_Lee1",
        "aff": "Korea Advanced Institute of Science & Technology;;Seoul National University+KAIST",
        "aff_domain": "kaist.ac.kr;;snu.ac.kr+kaist.ac.kr",
        "position": "MS student;;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\npark2025infinitehorizon,\ntitle={Infinite-Horizon Reinforcement Learning with Multinomial Logistic Function Approximation},\nauthor={Jaehyun Park and Junyeop Kwon and Dabeen Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=lgnGyZYcDM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lgnGyZYcDM",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lq0VnDFztt",
        "title": "Conditional simulation via entropic optimal transport: Toward non-parametric estimation of conditional Brenier maps",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Conditional simulation is a fundamental task in statistical modeling: Generate samples from the conditionals given finitely many data points from a joint distribution. One promising approach is to construct conditional Brenier maps, where the components of the map pushforward a reference distribution to conditionals of the target. While many estimators exist, few, if any, come with statistical or algorithmic guarantees. To this end, we propose a non-parametric estimator for conditional Brenier maps based on the computational scalability of \\emph{entropic} optimal transport. Our estimator leverages a result of Carlier et al., (2010), which shows that optimal transport maps under a rescaled quadratic cost asymptotically converge to conditional Brenier maps; our estimator is precisely the entropic analogues of these converging maps. We provide heuristic justifications for how to choose the scaling parameter in the cost as a function of the number of samples by fully characterizing the Gaussian setting. We conclude by comparing the performance of the estimator to other machine learning and non-parametric approaches on benchmark datasets and Bayesian inference problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ricardo Baptista;Aram-Alexandre Pooladian;Michael Brennan;Youssef Marzouk;Jonathan Niles-Weed",
        "authorids": "~Ricardo_Baptista1;~Aram-Alexandre_Pooladian2;~Michael_Brennan2;~Youssef_Marzouk1;~Jonathan_Niles-Weed1",
        "gender": "M;M;M;M;",
        "homepage": ";http://www.arampooladian.com;;https://uqgroup.mit.edu;",
        "dblp": "136/6901;238/0532;;78/9757;",
        "google_scholar": ";6CNhsjoAAAAJ;phsTbmIAAAAJ;TwVbNZ4AAAAJ;",
        "orcid": ";;;0000-0001-8242-3290;",
        "linkedin": ";;;;",
        "or_profile": "~Ricardo_Baptista1;~Aram-Alexandre_Pooladian2;~Michael_Brennan2;~Youssef_Marzouk1;~Jonathan_Niles-Weed1",
        "aff": "Deparment of Computing + Mathematical Sciences, California Institute of Technology;New York University;;Massachusetts Institute of Technology;",
        "aff_domain": "cms.caltech.edu;nyu.edu;;mit.edu;",
        "position": "Instructor;PhD student;;Professor;",
        "bibtex": "@inproceedings{\npooladian2025conditional,\ntitle={Conditional simulation via entropic optimal transport: Toward non-parametric estimation of conditional Brenier maps},\nauthor={Aram-Alexandre Pooladian and Ricardo Baptista and Michael Brennan and Youssef Marzouk and Jonathan Niles-Weed},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=lq0VnDFztt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lq0VnDFztt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lqPXwN4ECf",
        "title": "Active Bipartite Ranking with Smooth Posterior Distributions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this article, bipartite ranking, a statistical learning problem involved in many applications and widely studied in the passive context, is approached in a much more general active setting than the discrete one previously considered in the literature. While the latter assumes that the conditional distribution is piece wise constant, the framework we develop permits in contrast to deal with continuous conditional distributions, provided that they fulfill a H\u00f6lder smoothness constraint. We first show that a naive approach based on discretisation at a uniform level, fixed a priori and consisting in applying next the active strategy designed for the discrete setting generally fails. Instead, we propose a novel algorithm, referred to as smooth-rank and designed for the continuous setting, which aims to minimise the distance between the ROC curve of the estimated ranking rule and the optimal one w.r.t. the $\\sup$ norm. We show that, for a fixed confidence level $\\epsilon>0$ and probability $\\delta\\in (0,1)$, smooth-rank is PAC$(\\epsilon,\\delta)$. In addition, we provide a problem dependent upper bound on the expected sampling time of smooth-rank and establish a problem dependent lower bound on the expected sampling time of any PAC$(\\epsilon,\\delta)$ algorithm. Beyond the theoretical analysis carried out, numerical results are presented, providing solid empirical evidence of the performance of the algorithm proposed, which compares favorably with alternative approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "James Cheshire;Stephan Cl\u00e9men\u00e7on",
        "authorids": "~James_Cheshire2;~Stephan_Cl\u00e9men\u00e7on1",
        "gender": "M;",
        "homepage": "https://sites.google.com/view/jamescheshireresearch;",
        "dblp": "267/9232;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~James_Cheshire2;~Stephan_Cl\u00e9men\u00e7on1",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\ncheshire2025active,\ntitle={Active Bipartite Ranking with Smooth Posterior Distributions},\nauthor={James Cheshire and Stephan Cl{\\'e}men{\\c{c}}on},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=lqPXwN4ECf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lqPXwN4ECf",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "m1KT9KefLp",
        "title": "Geometric Collaborative Filtering with Convergence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Latent variable collaborative filtering methods have been a standard approach to modelling user-click interactions due to their simplicity and effectiveness. However, there is limited work on analyzing the mathematical properties of these methods in particular on preventing the overfitting towards the identity, and such methods typically utilize loss functions that overlook the geometry between items. In this work, we introduce a notion of generalization gap in collaborative filtering and analyze this with respect to latent collaborative filtering models. We present a geometric upper bound that gives rise to loss functions, and a way to meaningfully utilize the geometry of the items to improve recommendations. We show how these losses can be minimized and gives the recipe to a new latent collaborative filtering algorithm, which we refer to as GeoCF, due to the geometric nature of our results. We then show experimentally that our proposed GeoCF algorithm can outperform other all existing methods on the Movielens20M and Netflix datasets, as well as two large-scale internal datasets. In summary, our work proposes a theoretically sound method which paves a way to better understand generalization of collaborative filtering at large.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hisham Husain;Julien Monteil",
        "authorids": "~Hisham_Husain1;~Julien_Monteil3",
        "gender": ";",
        "homepage": ";",
        "dblp": "222/3235;",
        "google_scholar": "bwq3crYAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Hisham_Husain1;~Julien_Monteil3",
        "aff": "Google;",
        "aff_domain": "google.com;",
        "position": "Researcher;",
        "bibtex": "@inproceedings{\nhusain2025geometric,\ntitle={Geometric Collaborative Filtering with Convergence},\nauthor={Hisham Husain and Julien Monteil},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=m1KT9KefLp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=m1KT9KefLp",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "mBbC5pzZxL",
        "title": "A graphical global optimization framework for parameter estimation of statistical models with nonconvex regularization functions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Optimization problems with norm-bounding constraints appear in various applications, from portfolio optimization to machine learning, feature selection, and beyond. A widely used variant of these problems relaxes the norm-bounding constraint through Lagrangian relaxation and moves it to the objective function as a form of penalty or regularization term. A challenging class of these models uses the zero-norm function to induce sparsity in statistical parameter estimation models. Most existing exact solution methods for these problems use additional binary variables together with artificial bounds on variables to formulate them as a mixed-integer program in a higher dimension, which is then solved by off-the-shelf solvers. Other exact methods utilize specific structural properties of the objective function to solve certain variants of these problems, making them non-generalizable to other problems with different structures. An alternative approach employs nonconvex penalties with desirable statistical properties, which are solved using heuristic or local methods due to the structural complexity of those terms. In this paper, we develop a novel graph-based method to globally solve optimization problems that contain a generalization of norm-bounding constraints. This includes standard $\\ell_p$-norms for $p \\in [0, \\infty)$ as well as nonconvex penalty terms, such as SCAD and MCP, as special cases. Our method uses decision diagrams to build strong convex relaxations for these constraints in the original space of variables without the need to introduce additional auxiliary variables or impose artificial variable bounds. We show that the resulting convexification method, when incorporated into a spatial branch-and-cut framework, converges to the global optimal value of the problem. To demonstrate the capabilities of the proposed framework, we conduct preliminary computational experiments on benchmark sparse linear regression problems with challenging nonconvex penalty terms that cannot be modeled or solved by existing global solvers.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Danial Davarnia;Mohammadreza Kiaghadi",
        "authorids": "~Danial_Davarnia1;~Mohammadreza_Kiaghadi1",
        "gender": ";M",
        "homepage": "https://sites.google.com/view/danialdavarnia/;",
        "dblp": ";",
        "google_scholar": ";1cihuZQAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Danial_Davarnia1;~Mohammadreza_Kiaghadi1",
        "aff": "Iowa State University;Iowa State University",
        "aff_domain": "iastate.edu;iastate.edu",
        "position": "Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\ndavarnia2025a,\ntitle={A graphical global optimization framework for parameter estimation of statistical models with nonconvex regularization functions},\nauthor={Danial Davarnia and Mohammadreza Kiaghadi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=mBbC5pzZxL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mBbC5pzZxL",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "mjnSTtc19o",
        "title": "Variational Schr\\\"odinger Momentum Diffusion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The Momentum Schr\u00f6dinger Bridge (mSB) (Chen et al., 2023c) has emerged as a leading method for accelerating generative diffusion processes and reducing transport costs. However, the lack of simulation-free properties inevitably results in high training costs and affects scalability. To obtain a trade-off between transport properties and scalability, we introduce variational Schr\\\"odinger momentum diffusion (VSMD), which employs linearized forward score functions (variational scores) to eliminate the dependence on simulated forward trajectories. Our approach leverages a multivariate diffusion process with adaptively transport-optimized variational scores. Additionally, we apply a critical-damping transform to stabilize training by removing the need for score estimations for both velocity and samples. Theoretically, we prove the convergence of samples generated with optimal variational scores and momentum diffusion. Empirical results demonstrate that VSMD efficiently generates anisotropic shapes while maintaining transport efficacy, outperforming overdamped alternatives, and avoiding complex denoising processes.  Our approach also scales effectively to real-world data, achieving competitive results in time series and image generation, both in unconditional and conditional settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kevin Rojas;Yixin Tan;Molei Tao;Yuriy Nevmyvaka;Wei Deng",
        "authorids": "~Kevin_Rojas1;~Yixin_Tan1;~Molei_Tao1;~Yuriy_Nevmyvaka1;~Wei_Deng1",
        "gender": "M;M;;;M",
        "homepage": "https://kevinrojas1499.github.io/;;http://people.math.gatech.edu/~mtao8/;;https://waynedw.github.io/",
        "dblp": "302/9492;;56/9263;92/1859;69/508-2",
        "google_scholar": "88YhA_QAAAAJ;3AGaybIAAAAJ;;https://scholar.google.com/citations?hl=en;IYiyxssAAAAJ",
        "orcid": ";;;;",
        "linkedin": "kevin-rojas-cisneros-045a20165/;yixin-tan-0b9b0a199/;;;",
        "or_profile": "~Kevin_Rojas1;~Yixin_Tan1;~Molei_Tao1;~Yuriy_Nevmyvaka1;~Wei_Deng1",
        "aff": "Georgia Institute of Technology;Duke University;Georgia Institute of Technology;Morgan Stanley;Morgan Stanley",
        "aff_domain": "gatech.edu;duke.edu;gatech.edu;morganstanley.com;morganstanley.com",
        "position": "PhD student;PhD student;Associate Professor;Principal Researcher;Researcher",
        "bibtex": "@inproceedings{\nrojas2025variational,\ntitle={Variational Schr{\\textbackslash}''odinger Momentum Diffusion},\nauthor={Kevin Rojas and Yixin Tan and Molei Tao and Yuriy Nevmyvaka and Wei Deng},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=mjnSTtc19o}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mjnSTtc19o",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "mkkFubLdNW",
        "title": "A Causal Framework for Evaluating Deferring Systems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this gap by evaluating deferring systems through a causal lens. We link the potential outcomes framework for causal inference with deferring systems, which allows to identify the causal impact of the deferring strategy on predictive accuracy. We distinguish two scenarios. In the first one, we have access to both the human and ML model predictions for the deferred instances. Here, we can identify the individual causal effects for deferred instances and the aggregates of them. In the second one, only human predictions are available for the deferred instances. Here, we can resort to regression discontinuity design to estimate a local causal effect. We evaluate our approach on synthetic and real datasets for seven deferring systems from the literature.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Filippo Palomba;Andrea Pugnana;Jose Manuel Alvarez;Salvatore Ruggieri",
        "authorids": "~Filippo_Palomba1;~Andrea_Pugnana1;~Jose_Manuel_Alvarez1;~Salvatore_Ruggieri1",
        "gender": "M;;M;M",
        "homepage": "https://filippopalomba.github.io/;;https://cc-jalvarez.github.io/;http://pages.di.unipi.it/ruggieri/",
        "dblp": ";;59/6703-2;86/3031",
        "google_scholar": "CleWL-8AAAAJ;;SD_BQEoAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-6400-3142;;0000-0001-9412-9013;0000-0002-1917-6087",
        "linkedin": ";;;salvatore-ruggieri-209759/",
        "or_profile": "~Filippo_Palomba1;~Andrea_Pugnana1;~Jose_Manuel_Alvarez1;~Salvatore_Ruggieri1",
        "aff": "Princeton University;;KU Leuven;University of Pisa",
        "aff_domain": "princeton.edu;;kuleuven.be;unipi.it",
        "position": "PhD student;;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\npalomba2025a,\ntitle={A Causal Framework for Evaluating Deferring Systems},\nauthor={Filippo Palomba and Andrea Pugnana and Jose Manuel Alvarez and Salvatore Ruggieri},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=mkkFubLdNW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mkkFubLdNW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "mkzKiQodnz",
        "title": "Noise-Aware Differentially Private Variational Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Differential privacy (DP) provides robust privacy guarantees for statistical inference, but this can lead to unreliable results and biases in downstream applications. While several noise-aware approaches have been proposed which integrate DP perturbation into the inference, they are limited to specific types of simple probabilistic models. In this work, we propose a novel method for noise-aware approximate Bayesian inference based on stochastic gradient variational inference which can also be applied to high-dimensional and non-conjugate models. We also propose a more accurate evaluation method for noise-aware posteriors. Empirically, our inference method has similar performance to existing methods in the domain where they are applicable. Outside this domain, we obtain accurate coverages on high-dimensional Bayesian linear regression and well-calibrated predictive probabilities on Bayesian logistic regression with the UCI Adult dataset.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Talal Alrawajfeh;Joonas J\u00e4lk\u00f6;Antti Honkela",
        "authorids": "~Talal_Alrawajfeh1;~Joonas_J\u00e4lk\u00f61;~Antti_Honkela1",
        "gender": "M;M;M",
        "homepage": ";;https://www.cs.helsinki.fi/u/ahonkela/",
        "dblp": ";188/5963;h/AnttiHonkela",
        "google_scholar": ";;XsyLs6AAAAAJ",
        "orcid": ";;0000-0001-9193-8093",
        "linkedin": "talal-alrawajfeh;;",
        "or_profile": "~Talal_Alrawajfeh1;~Joonas_J\u00e4lk\u00f61;~Antti_Honkela1",
        "aff": "University of Helsinki;University of Helsinki;University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi;helsinki.fi",
        "position": "PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nalrawajfeh2025noiseaware,\ntitle={Noise-aware Differentially Private Variational Inference},\nauthor={Talal Alrawajfeh and Joonas J{\\\"a}lk{\\\"o} and Antti Honkela},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=mkzKiQodnz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mkzKiQodnz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "n6hsIQJXSt",
        "title": "Decoupling epistemic and aleatoric uncertainties with possibility theory",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The special role of epistemic uncertainty in Machine Learning is now well recognised, and an increasing amount of research is focused on methods for dealing specifically with such a lack of knowledge. Yet, most often, a probabilistic representation is considered for both aleatoric and epistemic uncertainties, hence creating challenges in applications where decoupling these two types of uncertainty is necessary. In this work, we show that an alternative representation of epistemic uncertainty, based on possibility theory, maintains many of the convenient features of standard Bayesian inference while displaying specific behaviours and properties that closely match the ones of an intuitive notion of information. Our main contributions are: i) a general framework for jointly representing epistemic and aleatoric uncertainties, ii) a Bernstein-von Mises theorem for the analogue of Bayes' rule in possibility theory, iii) a version of the law of large numbers and of the central limit theorem for the associated variables, and iv) an analysis of the properties of the possibilistic maximum a posteriori. These results highlight that a dedicated and principled representation of epistemic uncertainty, that is compatible with standard Bayesian inference and preserves many of its strengths, is attainable.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nong Minh Hieu;Jeremie Houssineau;Neil K. Chada;Emmanuel Delande",
        "authorids": "~Nong_Minh_Hieu1;~Jeremie_Houssineau1;neilchada123@gmail.com;emmanuel.delande@cnes.fr",
        "gender": "M;M;;",
        "homepage": ";https://jeremiehoussineau.com/;;",
        "dblp": ";132/4855;;",
        "google_scholar": "6ZtPiA8AAAAJ;https://scholar.google.co.uk/citations?user=nvn5gbEAAAAJ;;",
        "orcid": ";0000-0001-8235-0239;;",
        "linkedin": "nong-minh-hieu-194318171/;j%C3%A9r%C3%A9mie-houssineau-394bba20/;;",
        "or_profile": "~Nong_Minh_Hieu1;~Jeremie_Houssineau1;neilchada123@gmail.com;emmanuel.delande@cnes.fr",
        "aff": "Singapore Management University;Nanyang Technological University;;",
        "aff_domain": "smu.edu.sg;ntu.edu.sg;;",
        "position": "PhD student;Assistant Professor;;",
        "bibtex": "@inproceedings{\nhieu2025decoupling,\ntitle={Decoupling epistemic and aleatoric uncertainties with possibility theory},\nauthor={Nong Minh Hieu and Jeremie Houssineau and Neil K. Chada and Emmanuel Delande},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=n6hsIQJXSt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=n6hsIQJXSt",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "n8KLQe9HlN",
        "title": "All models are wrong, some are useful: Model Selection with Limited Labels",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce MODEL SELECTOR, a framework for label-efficient selection of pretrained classifiers. Given a pool of unlabeled target data, MODEL SELECTOR samples a small subset of highly informative examples for labeling, in order to efficiently identify the best pretrained model for deployment on this target dataset. Through extensive experiments, we demonstrate that MODEL SELECTOR drastically reduces the need for labeled data while consistently picking the best or near-best performing model. Across 18 model collections on 16 different datasets, comprising over 1,500 pretrained models, MODEL SELECTOR reduces the labeling cost by up to 94.15% to identify the best model compared to the cost of the strongest baseline. Our results further highlight the robustness of MODEL SELECTOR in model selection, as it reduces the labeling cost by up to 72.41% when selecting a near-best model, whose accuracy is only within 1% of the best model.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Patrik Okanovic;Andreas Kirsch;Jannes Kasper;Torsten Hoefler;Andreas Krause;Nezihe Merve G\u00fcrel",
        "authorids": "~Patrik_Okanovic1;~Andreas_Kirsch1;~Jannes_Kasper1;~Torsten_Hoefler1;~Andreas_Krause1;~Nezihe_Merve_G\u00fcrel2",
        "gender": "M;;M;;M;Not Specified",
        "homepage": ";https://www.blackhc.net;;;https://las.inf.ethz.ch/krausea;https://nezihemervegurel.github.io/",
        "dblp": "323/5046;56/2914-2;;16/3869;87/1831-1.html;215/5003",
        "google_scholar": "ywvPTZ4AAAAJ;WYQVZpYAAAAJ;;;https://scholar.google.ch/citations?user=eDHv58AAAAAJ;5yYPHwYAAAAJ",
        "orcid": ";0000-0001-8244-7700;;;0000-0001-7260-9673;",
        "linkedin": "patrik-okanovic;blackhc;jannes-kasper-20944718a/;;krausea/;nezihemervegurel/",
        "or_profile": "~Patrik_Okanovic1;~Andreas_Kirsch1;~Jannes_Kasper1;~Torsten_Hoefler1;~Andreas_Krause1;~Nezihe_Merve_G\u00fcrel2",
        "aff": "Department of Computer Science, ETHZ - ETH Zurich;Google DeepMind ;;Swiss Federal Institute of Technology;ETH Zurich;Delft University of Technology",
        "aff_domain": "inf.ethz.ch;google.com;;ethz.ch;ethz.ch;tudelft.nl",
        "position": "PhD student;Researcher;;Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nokanovic2025all,\ntitle={All models are wrong, some are useful: Model Selection with Limited Labels},\nauthor={Patrik Okanovic and Andreas Kirsch and Jannes Kasper and Torsten Hoefler and Andreas Krause and Nezihe Merve G{\\\"u}rel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=n8KLQe9HlN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=n8KLQe9HlN",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nIF8XvtIr7",
        "title": "A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets, including differentially private synthetic data. Our theory yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice with several real datasets, downstream predictors and error metrics. As our theory predicts, multiple synthetic datasets often improve accuracy, while a single large synthetic dataset gives at best minimal improvement, showing that our insights are practically relevant.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ossi R\u00e4is\u00e4;Antti Honkela",
        "authorids": "~Ossi_R\u00e4is\u00e41;~Antti_Honkela1",
        "gender": "M;M",
        "homepage": ";https://www.cs.helsinki.fi/u/ahonkela/",
        "dblp": "296/0031;h/AnttiHonkela",
        "google_scholar": "https://scholar.google.fi/citations?user=FpmQ-jcAAAAJ;XsyLs6AAAAAJ",
        "orcid": ";0000-0001-9193-8093",
        "linkedin": "ossi-r%C3%A4is%C3%A4-749502139/;",
        "or_profile": "~Ossi_R\u00e4is\u00e41;~Antti_Honkela1",
        "aff": "University of Helsinki;University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nraisa2025a,\ntitle={A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets},\nauthor={Ossi R{\\\"a}is{\\\"a} and Antti Honkela},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nIF8XvtIr7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nIF8XvtIr7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nLClQpOUIh",
        "title": "The Uniformly Rotated Mondrian Kernel",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Random feature maps are used to decrease the computational cost of kernel machines in large-scale problems. The Mondrian kernel is one such example of a fast random feature approximation of the Laplace kernel, generated by a computationally efficient hierarchical random partition of the input space known as the Mondrian process. In this work, we study a variation of this random feature map by applying a uniform random rotation to the input space before running the Mondrian process to approximate a kernel that is invariant under rotations. We obtain a closed-form expression for the isotropic kernel that is approximated, as well as a uniform convergence rate of the uniformly rotated Mondrian kernel to this limit. To this end, we utilize techniques from the theory of stationary random tessellations in stochastic geometry and prove a new result on the geometry of the typical cell of the superposition of uniformly rotated Mondrian tessellations. Finally, we test the empirical performance of this random feature map on both synthetic and real-world datasets, demonstrating its improved performance over the Mondrian kernel on a dataset that is debiased from the standard coordinate axes.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Calvin Osborne;Eliza O'Reilly",
        "authorids": "~Calvin_Osborne1;~Eliza_O'Reilly1",
        "gender": "M;F",
        "homepage": ";https://sites.google.com/view/eliza-oreilly/home",
        "dblp": ";",
        "google_scholar": ";LptnkxkAAAAJ",
        "orcid": ";",
        "linkedin": "calvin-osborne-60b088221/;",
        "or_profile": "~Calvin_Osborne1;~Eliza_O'Reilly1",
        "aff": "Harvard University;Johns Hopkins University",
        "aff_domain": "harvard.edu;johnshopkins.edu",
        "position": "Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nosborne2025the,\ntitle={The Uniformly Rotated Mondrian Kernel},\nauthor={Calvin Osborne and Eliza O'Reilly},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nLClQpOUIh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nLClQpOUIh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ndWOLqVRHC",
        "title": "Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Online reinforcement learning (RL) typically requires online interaction data to learn a policy for a target task, but collecting such data can be high-stakes. This prompts interest in leveraging historical data to improve sample efficiency. The historical data may come from outdated or related source environments with different dynamics. It remains unclear how to effectively use such data in the target task to provably enhance learning and sample efficiency. To address this, we propose a hybrid transfer RL (HTRL) setting, where an agent learns in a target environment while accessing offline data from a source environment with shifted dynamics. We show that -- without information on the dynamics shift -- general shifted-dynamics data, even with subtle shifts, does not reduce sample complexity in the target environment.  However, focusing on HTRL with prior information on the degree of the dynamics shift, we design HySRL, a transfer algorithm that outperforms pure online RL with problem-dependent sample complexity guarantees. Finally, our experimental results demonstrate that HySRL surpasses the state-of-the-art pure online RL baseline.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chengrui Qu;Laixi Shi;Kishan Panaganti;Pengcheng You;Adam Wierman",
        "authorids": "~Chengrui_Qu1;~Laixi_Shi1;~Kishan_Panaganti1;~Pengcheng_You1;~Adam_Wierman1",
        "gender": "M;F;M;M;M",
        "homepage": "https://crqu.github.io;https://laixishi.github.io/;https://sites.google.com/a/tamu.edu/kpb;https://pengcheng-you.github.io/desires-lab/;https://adamwierman.com/",
        "dblp": ";211/7965;260/0365;;56/4447",
        "google_scholar": ";V8RkRr8AAAAJ;yTCoJdsAAAAJ;;4OvOdSgAAAAJ",
        "orcid": "0009-0002-6710-7358;;;;0000-0002-5923-0199",
        "linkedin": ";;;;adam-wierman-a529474/",
        "or_profile": "~Chengrui_Qu1;~Laixi_Shi1;~Kishan_Panaganti1;~Pengcheng_You1;~Adam_Wierman1",
        "aff": "Peking University;California Institute of Technology;California Institute of Technology;Peking University;California Institute of Technology",
        "aff_domain": "stu.pku.edu.cn;caltech.edu;caltech.edu;pku.edu.cn;caltech.edu",
        "position": "Undergrad student;Postdoc;Postdoc;Assistant Professor;Professor",
        "bibtex": "@inproceedings{\nqu2025hybrid,\ntitle={Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from Shifted-Dynamics Data},\nauthor={Chengrui Qu and Laixi Shi and Kishan Panaganti and Pengcheng You and Adam Wierman},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ndWOLqVRHC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ndWOLqVRHC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nffyNeZVHS",
        "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for aligning artificial intelligence systems with human values, achieving remarkable success in fine-tuning large language models. However, existing RLHF frameworks often assume that human preferences are relatively homogeneous and can be captured by a single, unified reward model. This assumption overlooks the inherent diversity and heterogeneity across individuals, limiting the adaptability of RLHF to personalized scenarios and risking misalignments that can diminish user satisfaction and trust in AI systems. In this paper, we address these challenges by introducing Low-Rank Adaptation (LoRA) into the personalized RLHF framework. We apply LoRA in the parameter space of the aggregation of all personalized reward functions, thereby enabling efficient learning of personalized reward models from potentially limited local datasets. Our approach exploits potential shared structures among the local ground-truth reward models while allowing for individual adaptation, without relying on restrictive assumptions about shared representations as in prior works. We further establish sample complexity guarantees for our method. Theoretical analysis demonstrates the effectiveness of the proposed approach in capturing both shared and individual-specific structures within heterogeneous human preferences, addressing the dual challenge of personalization requirements and practical data constraints. Experimental results on real-world datasets corroborate the efficiency of our algorithm in the personalized RLHF setting.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Renpu Liu;Peng Wang;Donghao Li;Cong Shen;Jing Yang",
        "authorids": "~Renpu_Liu1;~Peng_Wang40;~Donghao_Li3;~Cong_Shen1;~Jing_Yang3",
        "gender": ";M;;M;",
        "homepage": ";https://peter-peng-w.github.io/;;https://cshen317.github.io/;http://www.ee.psu.edu/yang",
        "dblp": ";;;79/6027-1.html;",
        "google_scholar": "RB_fv-kAAAAJ;HvK1Hp8AAAAJ;4vygbUIAAAAJ;70LBhKcAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-5699-3676;;0000-0002-3148-4453;",
        "linkedin": ";pengw96/;;cong-shen-3372404/;",
        "or_profile": "~Renpu_Liu1;~Peng_Wang40;~Donghao_Li3;~Cong_Shen1;~Jing_Yang3",
        "aff": "University of Virginia, Charlottesville;University of Virginia, Charlottesville;Pennsylvania State University;University of Virginia, Charlottesville;University of Virginia, Charlottesville",
        "aff_domain": "virginia.edu;virginia.edu;psu.edu;virginia.edu;virginia.edu",
        "position": "PhD student;PhD student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nliu2025a,\ntitle={A Shared Low-Rank Adaptation Approach to Personalized {RLHF}},\nauthor={Renpu Liu and Donghao Li and Peng Wang and Cong Shen and Jing Yang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nffyNeZVHS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nffyNeZVHS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nhGtq5s6GJ",
        "title": "Transformers are Provably Optimal In-context Estimators for Wireless Communications",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pre-trained transformers exhibit the capability of adapting to new tasks through in-context learning (ICL), where they efficiently utilize a limited set of prompts without explicit model optimization.\n The canonical communication problem of estimating transmitted symbols from received observations can be modeled as an in-context learning problem: Received observations are a noisy function of transmitted symbols, and this function can be represented by an unknown parameter whose statistics depend on an unknown latent context. This problem, which we term in-context estimation (ICE), has significantly greater complexity than the extensively studied linear regression problem.\n The optimal solution to the ICE problem is a non-linear function of the underlying context. In this paper, we prove that, for a subclass of such problems, a single-layer softmax attention transformer (SAT) computes the optimal solution of the above estimation problem in the limit of large prompt length. We also prove that the optimal configuration of such a transformer is indeed the minimizer of the corresponding training loss. Further, we empirically demonstrate the proficiency of multi-layer transformers in efficiently solving broader in-context estimation problems. Through extensive simulations, we show that solving ICE problems using transformers significantly outperforms standard approaches. Moreover, just with a few context examples, it achieves the same performance as an estimator with perfect knowledge of the latent context.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vishnu Teja Kunde;Vicram Rajagopalan;Chandra Shekhara Kaushik Valmeekam;Krishna Narayanan;Jean-Francois Chamberland;Dileep Kalathil;Srinivas Shakkottai",
        "authorids": "~Vishnu_Teja_Kunde1;~Vicram_Rajagopalan1;~Chandra_Shekhara_Kaushik_Valmeekam1;~Krishna_Narayanan1;~Jean-Francois_Chamberland1;~Dileep_Kalathil1;~Srinivas_Shakkottai1",
        "gender": "M;;M;M;M;M;",
        "homepage": ";;;https://krishnanarayanan.wikidot.com;https://people.engr.tamu.edu/chmbrlnd/index.html;http://people.tamu.edu/~dileep.kalathil/;https://cesg.tamu.edu/faculty/sshakkot/",
        "dblp": ";;;;;44/8356;03/353.html",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;oDivxXQAAAAJ;https://scholar.google.com/citations?hl=en;S24XFwwAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0002-2356-1682;;0000-0002-2983-9884;;0000-0002-5882-6433",
        "linkedin": "vishnu-teja-kunde-3a28951ab/;;;;chmbrlnd/;;",
        "or_profile": "~Vishnu_Teja_Kunde1;~Vicram_Rajagopalan1;~Chandra_Shekhara_Kaushik_Valmeekam1;~Krishna_Narayanan1;~Jean-Francois_Chamberland1;~Dileep_Kalathil1;~Srinivas_Shakkottai1",
        "aff": "Texas A&M University - College Station;;Texas A&M University - College Station;Texas A&M;Texas A&M University - College Station;Texas A&M University;Texas A&M",
        "aff_domain": "tamu.edu;;tamu.edu;tamu.edu;tamu.edu;tamu.edu;tamu.edu",
        "position": "PhD student;;PhD student;Full Professor;Full Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nkunde2025transformers,\ntitle={Transformers are Provably Optimal In-context Estimators for Wireless Communications},\nauthor={Vishnu Teja Kunde and Vicram Rajagopalan and Chandra Shekhara Kaushik Valmeekam and Krishna Narayanan and Jean-Francois Chamberland and Dileep Kalathil and Srinivas Shakkottai},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nhGtq5s6GJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nhGtq5s6GJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nntcvMKyb8",
        "title": "Logarithmic Neyman Regret for Adaptive Estimation of the Average Treatment Effect",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Estimation of the Average Treatment Effect (ATE) is a core problem in causal inference with strong connections to Off-Policy Evaluation in Reinforcement Learning. This paper considers the problem of adaptively selecting the treatment allocation probability in order to improve estimation of the ATE. The majority of prior work on adaptive ATE estimation focus on asymptotic guarantees, and in turn overlooks important practical considerations such as the difficulty of learning the optimal treatment allocation as well as hyper-parameter selection.  Existing non-asymptotic methods are limited by poor empirical performance and exponential dependence on problem parameters.  In order to address these gaps, we propose and analyze the Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an existing algorithm with strong asymptotic optimality guarantees, and provide finite sample bounds on its Neyman regret. Our analysis shows that, in the superpopulation setting, ClipSMT achieves exponential improvements in Neyman regret on two fronts: improving the dependence on $T$ from $O(\\sqrt{T})$ to $O(\\log T)$, as well as reducing the exponential dependence on problem parameters to a polynomial dependence---although the setting we consider is slightly less general. We conclude with simulations which show the marked improvement of ClipSMT over existing approaches.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ojash Neopane;Aaditya Ramdas;Aarti Singh",
        "authorids": "~Ojash_Neopane1;~Aaditya_Ramdas2;~Aarti_Singh2",
        "gender": "M;M;",
        "homepage": "https://oneopane.github.io/;http://stat.cmu.edu/~aramdas;",
        "dblp": "176/5399.html;117/3518;",
        "google_scholar": "lmAQ1l8AAAAJ;ZvFaPxUAAAAJ;",
        "orcid": ";0000-0003-0497-311X;",
        "linkedin": ";;",
        "or_profile": "~Ojash_Neopane1;~Aaditya_Ramdas2;~Aarti_Singh2",
        "aff": "Carnegie Mellon University;Carnegie Mellon University+Amazon;",
        "aff_domain": "cmu.edu;cmu.edu+amazon.com;",
        "position": "PhD student;Associate Professor+Researcher;",
        "bibtex": "@inproceedings{\nneopane2025logarithmic,\ntitle={Logarithmic Neyman Regret for Adaptive Estimation of the Average Treatment Effect},\nauthor={Ojash Neopane and Aaditya Ramdas and Aarti Singh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nntcvMKyb8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nntcvMKyb8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nuG0FYyWRT",
        "title": "From Gradient Clipping to Normalization for Heavy Tailed SGD",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent empirical evidence indicates that many machine learning applications involve heavy-tailed gradient noise, which challenges the standard assumptions of bounded variance in stochastic optimization. Gradient clipping has emerged as a popular tool to handle this heavy-tailed noise, as it achieves good performance both theoretically and practically. \nHowever, our current theoretical understanding of non-convex gradient clipping has three main shortcomings. First, the theory hinges on large, increasing clipping thresholds, which are in stark contrast to the small constant clipping thresholds employed in practice. Second, clipping thresholds require knowledge of problem-dependent parameters to guarantee convergence. Lastly, even with this knowledge, current sample complexity upper bounds for the method are sub-optimal in nearly all parameters. \nTo address these issues and motivated by practical observations, we make the connection of gradient clipping to its close relative --- Normalized SGD (NSGD) --- and study its convergence properties.\nFirst, we establish a parameter-free sample complexity for NSGD of $\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an $\\varepsilon$-stationary point, only assuming a finite $p$-th central moment of the noise, $p\\in(1,2]$. \nFurthermore, we prove the tightness of this result, by providing a matching algorithm-specific lower bound. In the setting where all problem parameters are known, we show this complexity is improved to $\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the previously known lower bound for all first-order methods in all problem dependent parameters. \nFinally, we establish high-probability convergence of NSGD with a mild logarithmic dependence on the failure probability. Our work complements the studies of gradient clipping under heavy-tailed noise, improving the sample complexities of existing algorithms and offering an alternative mechanism to achieve high-probability convergence.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Florian H\u00fcbler;Ilyas Fatkhullin;Niao He",
        "authorids": "~Florian_H\u00fcbler1;~Ilyas_Fatkhullin1;~Niao_He3",
        "gender": ";Not Specified;",
        "homepage": ";https://ai.ethz.ch/people/ilyas-fatkhullin.html;",
        "dblp": ";294/8711;",
        "google_scholar": ";UCOWHb4AAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Florian_H\u00fcbler1;~Ilyas_Fatkhullin1;~Niao_He3",
        "aff": ";ETHZ - ETH Zurich;",
        "aff_domain": ";ethz.ch;",
        "position": ";PhD student;",
        "bibtex": "@inproceedings{\nhubler2025from,\ntitle={From Gradient Clipping to Normalization for Heavy Tailed {SGD}},\nauthor={Florian H{\\\"u}bler and Ilyas Fatkhullin and Niao He},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=nuG0FYyWRT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nuG0FYyWRT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "o4v4Og831q",
        "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Prompt learning is a popular fine-tuning method for vision-language models due to its efficiency. It requires a small number of additional learnable parameters while significantly enhancing performance on target tasks. However, most existing methods suffer from overfitting to fine-tuning data, yielding poor generalizability. To address this, we propose a new training objective function based on a Bayesian learning principle to balance adaptability and generalizability. We derive a prior over the logits, where the mean function is parameterized by the pre-trained model, while the posterior corresponds to the fine-tuned model. This objective establishes a balance by allowing the fine-tuned model to adapt to downstream tasks while remaining close to the pre-trained model. To avoid the overfitting issues of the standard softmax function, we adopt the one-vs-each softmax approximation along with its P\\'olya-Gamma augmentation (OVE-PG). We evaluate our method on several benchmark datasets and demonstrate that using the Bayesian principle for prompt learning is indeed a sensible choice. Code is available at the https://github.com/ParkLabML/Bayesian_Principles_Improve_Prompt_Learning_In_Vision_Language_Models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mingyu Kim;Jongwoo Ko;Mijung Park",
        "authorids": "~Mingyu_Kim2;~Jongwoo_Ko1;~Mijung_Park3",
        "gender": ";M;",
        "homepage": ";https://sites.google.com/view/jongwooko;",
        "dblp": ";286/1503;",
        "google_scholar": ";l2jkwHwAAAAJ;",
        "orcid": ";;",
        "linkedin": ";jongwoo-ko-8b93051b4/;",
        "or_profile": "~Mingyu_Kim2;~Jongwoo_Ko1;~Mijung_Park3",
        "aff": ";Korea Advanced Institute of Science & Technology;",
        "aff_domain": ";kaist.ac.kr;",
        "position": ";PhD student;",
        "bibtex": "@inproceedings{\nkim2025bayesian,\ntitle={Bayesian Principles Improve Prompt Learning In Vision-Language Models},\nauthor={Mingyu Kim and Jongwoo Ko and Mijung Park},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=o4v4Og831q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o4v4Og831q",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "o7YQ3eAuor",
        "title": "Sampling in High-Dimensions using Stochastic Interpolants and Forward-Backward Stochastic Differential Equations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a class of diffusion-based algorithms to draw samples from high-dimensional probability distributions given their unnormalized densities. Ideally, our methods can transport samples from a Gaussian distribution to a specified target distribution in finite time. Our approach relies on the stochastic interpolants framework to define a time-indexed collection of probability densities that bridge a Gaussian distribution to the target distribution. Subsequently, we derive a diffusion process that obeys the aforementioned probability density at each time instant. Obtaining such a diffusion process involves solving certain Hamilton-Jacobi-Bellman PDEs. We solve these PDEs using the theory of forward-backward stochastic differential equations (FBSDE) together with machine learning-based methods. Through numerical experiments, we demonstrate that our algorithm can effectively draw samples from distributions that conventional methods struggle to handle.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anand Jerry George;Nicolas Macris",
        "authorids": "~Anand_Jerry_George1;~Nicolas_Macris1",
        "gender": "M;M",
        "homepage": "https://anandjez.github.io/;",
        "dblp": ";47/5851",
        "google_scholar": ";",
        "orcid": ";0000-0003-2189-7411",
        "linkedin": ";",
        "or_profile": "~Anand_Jerry_George1;~Nicolas_Macris1",
        "aff": "EPFL - EPF Lausanne;Ecole Polytechnique Federale Lausanne",
        "aff_domain": "epfl.ch;epfl.ch",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\ngeorge2025sampling,\ntitle={Sampling in High-Dimensions using Stochastic Interpolants and Forward-Backward Stochastic Differential Equations},\nauthor={Anand Jerry George and Nicolas Macris},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=o7YQ3eAuor}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o7YQ3eAuor",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oDEQLzpHwV",
        "title": "Information Transfer Across Clinical Tasks via Adaptive Parameter Optimisation",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "This paper presents Adaptive Parameter Optimisation (APO), a novel framework for optimising shared models across multiple clinical tasks, addressing the challenges of balancing strict parameter sharing\u2014often leading to task conflicts\u2014and soft parameter sharing, which may limit effective cross-task information exchange. The proposed APO framework leverages insights from the lazy behaviour observed in over-parameterised neural networks, where only a small subset of parameters undergo any substantial updates during training. APO dynamically identifies and updates task-specific parameters while treating parameters associated with other tasks as protected, limiting their modification to prevent interference. The remaining unassigned parameters remain unchanged, embodying the lazy training phenomenon. This dynamic management of task-specific, protected, and unclaimed parameters across tasks enables effective information sharing, preserves task-specific adaptability, and mitigates gradient conflicts without enforcing a uniform representation. Experimental results across diverse healthcare datasets demonstrate that APO surpasses traditional information-sharing approaches, such as multi-task learning and model-agnostic meta-learning, in improving task performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anshul Thakur;Elena Gal;Soheila Molaei;Xiao Gu;Patrick Schwab;Danielle Belgrave;Kim Branson;David A. Clifton",
        "authorids": "~Anshul_Thakur1;~Elena_Gal1;~Soheila_Molaei1;~Xiao_Gu1;~Patrick_Schwab1;danielle.x.belgrave@gsk.com;kim.m.branson@gsk.com;~David_A._Clifton1",
        "gender": "M;;F;M;;;;M",
        "homepage": ";;https://www.researchgate.net/profile/Soheila-Molaei-2;https://xiaogu.site;http://schwabpatrick.com;;;http://www.eng.ox.ac.uk/chi",
        "dblp": ";;236/6149;41/2698-3;152/9378;;;89/6424",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;iAq1AngAAAAJ;xpXBs0gAAAAJ;https://scholar.google.at/citations?hl=de;;;",
        "orcid": ";;;;;;;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Anshul_Thakur1;~Elena_Gal1;~Soheila_Molaei1;~Xiao_Gu1;~Patrick_Schwab1;danielle.x.belgrave@gsk.com;kim.m.branson@gsk.com;~David_A._Clifton1",
        "aff": "University of Oxford;;University of Oxford;University of Oxford;GlaxoSmithKline plc;;;University of Oxford",
        "aff_domain": "eng.ox.ac.uk;;ox.ac.uk;ox.ac.uk;gsk.com;;;ox.ac.uk",
        "position": "Lecturer;;Postdoc;Postdoc;Director;;;Full Professor",
        "bibtex": "@inproceedings{\nthakur2025information,\ntitle={Information Transfer Across Clinical Tasks via Adaptive Parameter Optimisation},\nauthor={Anshul Thakur and Elena Gal and Soheila Molaei and Xiao Gu and Patrick Schwab and Danielle Belgrave and Kim Branson and David A. Clifton},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oDEQLzpHwV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oDEQLzpHwV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oEWYNesvRJ",
        "title": "Computing high-dimensional optimal transport by flow neural networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Computing optimal transport (OT) for general high-dimensional data has been a long-standing challenge. Despite much progress, most of the efforts including neural network methods have been focused on the static formulation of the OT problem. The current work proposes to compute the dynamic OT between two arbitrary distributions $P$ and $Q$ by optimizing a flow model, where both distributions are only accessible via finite samples. Our method learns the dynamic OT by finding an invertible flow that minimizes the transport cost. The trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (DRE) and domain adaptation by interpolating distributions in the latent space. The effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on OT baselines, image-to-image translation, and high-dimensional DRE.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chen Xu;Xiuyuan Cheng;Yao Xie",
        "authorids": "~Chen_Xu12;~Xiuyuan_Cheng1;~Yao_Xie2",
        "gender": "M;;F",
        "homepage": "https://hamrel-cxu.github.io/;;http://www2.isye.gatech.edu/~yxie77",
        "dblp": ";79/9747;13/4242-2",
        "google_scholar": "https://scholar.google.com/citations?hl=en;I2gwdssAAAAJ;qvYp8ZQAAAAJ",
        "orcid": ";;",
        "linkedin": "chen-xu-92013714a/;;yaoxie/",
        "or_profile": "~Chen_Xu12;~Xiuyuan_Cheng1;~Yao_Xie2",
        "aff": "Toyota Research Institute (Large Behavior Models);Duke University;Georgia Institute of Technology",
        "aff_domain": "tri.global;duke.edu;gatech.edu",
        "position": "Researcher;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nxu2025computing,\ntitle={Computing high-dimensional optimal transport by flow neural networks},\nauthor={Chen Xu and Xiuyuan Cheng and Yao Xie},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oEWYNesvRJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oEWYNesvRJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oJdVh3hzCC",
        "title": "Collaborative non-parametric two-sample testing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multiple two-sample test problem in a graph-structured setting is a common scenario in fields such as Spatial Statistics and Neuroscience. Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions, $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. CTST integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning. We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the geometry of the problem.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alejandro David De la Concha Duarte;Nicolas Vayatis;Argyris Kalogeratos",
        "authorids": "~Alejandro_David_De_la_Concha_Duarte1;~Nicolas_Vayatis1;~Argyris_Kalogeratos1",
        "gender": "M;M;M",
        "homepage": ";;http://kalogeratos.com",
        "dblp": ";00/582;06/1545",
        "google_scholar": ";;oEcLSFEAAAAJ",
        "orcid": ";;0000-0003-1436-3593",
        "linkedin": "alejandro-david-de-la-concha-duarte-4a3819130/;;argyris-kalogeratos-9ab09026/",
        "or_profile": "~Alejandro_David_De_la_Concha_Duarte1;~Nicolas_Vayatis1;~Argyris_Kalogeratos1",
        "aff": ";Ecole Normale Superieure Paris-Saclay;Ecole Normale Superieure",
        "aff_domain": ";ens-paris-saclay.fr;ens-paris-saclay.fr",
        "position": ";Full Professor;Researcher",
        "bibtex": "@inproceedings{\nduarte2025collaborative,\ntitle={Collaborative non-parametric two-sample testing},\nauthor={Alejandro David De la Concha Duarte and Nicolas Vayatis and Argyris Kalogeratos},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oJdVh3hzCC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oJdVh3hzCC",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oMWasgN27d",
        "title": "Learning High-dimensional Gaussians from Censored Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We provide efficient algorithms for the problem of distribution learning from high-dimensional Gaussian data where in each sample, some of the variable values are missing. We suppose that the variables are {\\em missing not at random (MNAR)}.\n\nThe missingness model, denoted by $\\mathbb{S}(\\mathbf{y})$, is the function that maps any point $\\mathbf{y}\\in \\mathbb{R}^d$ to the subsets of its coordinates that are seen. In this work, we assume that it is known. We study the following two settings:\n\n- [**Self-censoring**] An observation $\\mathbf{x}$ is generated by first sampling the true value $\\mathbf{y}$ from a $d$-dimensional Gaussian $\\mathcal{N}(\\mathbf{\\mu}^*, \\Sigma^*)$ with unknown $\\mathbf{\\mu}^*$ and $\\Sigma^*$. \nFor each coordinate $i$, there exists a set $S_i\\subseteq \\mathbb{R}^d$ such that $x_i=y_i$ if and only if $y_i\\in S_i$. Otherwise, $x_i$ is missing and  takes a generic value (e.g ``?\"). \nWe design an algorithm that learns $\\mathcal{N}(\\mathbf{\\mu}^*, \\Sigma^*)$ up to TV distance $\\varepsilon$, using $\\textup{poly}(d, 1/\\varepsilon)$ samples, assuming only that each pair of coordinates is observed with sufficiently high probability. \n\n- [**Linear thresholding**] An observation $\\mathbf{x}$ is generated by first sampling $\\mathbf{y}$ from a $d$-dimensional Gaussian $\\mathcal{N}(\\mathbf{\\mu}^*, \\Sigma)$ with unknown $\\mathbf{\\mu}^*$ and known $\\Sigma$, and then applying the missingness model $\\mathbb{S}$ where $\\mathbb{S}(\\mathbf{y}) = \\{i \\in [d]: \\mathbf{v}_i^T \\mathbf{y} \\leq b_i\\}$ for some $\\mathbf{v}_1, \\dots, \\mathbf{v}_d \\in \\mathbb{R}^d$ and $b_1, \\dots, b_d \\in \\mathbb{R}$. \nWe design an efficient mean estimation algorithm, assuming that none of the possible missingness patterns is very rare conditioned on the values of the observed coordinates and that any small subset of coordinates is observed with sufficiently high probability.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arnab Bhattacharyya;Constantinos Costis Daskalakis;Themis Gouleakis;Yuhao Wang",
        "authorids": "~Arnab_Bhattacharyya1;~Constantinos_Costis_Daskalakis1;~Themis_Gouleakis2;~Yuhao_Wang2",
        "gender": "M;M;;F",
        "homepage": "https://warwick.ac.uk/fac/sci/dcs/people/arnab_bhattacharyya/;http://people.csail.mit.edu/costis/;;https://www.yohannawang.com/",
        "dblp": "64/574.html;;;54/518.html",
        "google_scholar": "eECXWqUAAAAJ;iTv2cOgAAAAJ;;LHHyKJAAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Arnab_Bhattacharyya1;~Constantinos_Costis_Daskalakis1;~Themis_Gouleakis2;~Yuhao_Wang2",
        "aff": "The University of Warwick;Massachusetts Institute of Technology+Massachusetts Institute of Technology;;National University of Singapore",
        "aff_domain": "warwick.ac.uk;mit.edu+mit.edu;;nus.edu.sg",
        "position": "Associate Professor;+Full Professor;;PhD student",
        "bibtex": "@inproceedings{\nbhattacharyya2025learning,\ntitle={Learning High-dimensional Gaussians from Censored Data},\nauthor={Arnab Bhattacharyya and Constantinos Costis Daskalakis and Themis Gouleakis and Yuhao Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oMWasgN27d}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oMWasgN27d",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oYOzz3Fm7H",
        "title": "Minimum Empirical Divergence for Sub-Gaussian Linear Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a novel linear bandit algorithm called LinMED (Linear Minimum Empirical Divergence), which is a linear extension of the MED algorithm that was originally designed for multi-armed bandits.\nLinMED is a randomized algorithm that admits a closed-form computation of the arm sampling probabilities, unlike the popular randomized algorithm called linear Thompson sampling.\nSuch a feature proves useful for off-policy evaluation where the unbiased evaluation requires accurately computing the sampling probability.\nWe prove that LinMED enjoys a near-optimal regret bound of $d\\sqrt{n}$ up to logarithmic factors where $d$ is the dimension and $n$ is the time horizon.\nWe further show that LinMED enjoys a $\\frac{d^2}{\\Delta}\\left(\\log^2(n)\\right)\\log\\left(\\log(n)\\right)$ problem-dependent regret where $\\Delta$ is the smallest suboptimality gap.\nOur empirical study shows that LinMED has a competitive performance with the state-of-the-art algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kapilan Balagopalan;Kwang-Sung Jun",
        "authorids": "~Kapilan_Balagopalan1;~Kwang-Sung_Jun1",
        "gender": "M;M",
        "homepage": "https://kapilan-balagopalan.github.io/;http://kwangsungjun.github.io",
        "dblp": ";88/8411",
        "google_scholar": "3SKpK9oAAAAJ;VgvC7o8AAAAJ",
        "orcid": ";",
        "linkedin": "kapilan-balagopalan-5a1832126/;",
        "or_profile": "~Kapilan_Balagopalan1;~Kwang-Sung_Jun1",
        "aff": "University of Arizona;University of Arizona",
        "aff_domain": "arizona.edu;cs.arizona.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nbalagopalan2025minimum,\ntitle={Minimum Empirical Divergence for Sub-Gaussian Linear Bandits},\nauthor={Kapilan Balagopalan and Kwang-Sung Jun},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oYOzz3Fm7H}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oYOzz3Fm7H",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oYqZvaMkmJ",
        "title": "Signature Isolation Forest",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work aims to target such challenges by introducing Signature Isolation Forest, a novel class of AD algorithm leveraging the signature transform arising from rough path theory. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark showing the relevance of our methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Marta Campi;Guillaume Staerman;Gareth W. Peters;Tomoko Masui",
        "authorids": "~Marta_Campi1;~Guillaume_Staerman1;~Gareth_W._Peters1;~Tomoko_Masui1",
        "gender": "F;Not Specified;M;",
        "homepage": ";https://guillaumestaermanml.github.io/;https://www.qrslab.com;https://www.ism.ac.jp/~tmatsui/",
        "dblp": ";;;",
        "google_scholar": "JCC0JToAAAAJ;Zb2ax0wAAAAJ;https://scholar.google.com.au/citations?user=lsb_nJoAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Marta_Campi1;~Guillaume_Staerman1;~Gareth_W._Peters1;~Tomoko_Masui1",
        "aff": "Institut Pasteur;;;The Institute of Statistical Mathematics, Japan, Tokyo Institute of Technology",
        "aff_domain": "pasteur.fr;;;ism.ac.jp",
        "position": "Postdoc;;;Full Professor",
        "bibtex": "@inproceedings{\ncampi2025signature,\ntitle={Signature Isolation Forest},\nauthor={Marta Campi and Guillaume Staerman and Gareth W. Peters and Tomoko Masui},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=oYqZvaMkmJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oYqZvaMkmJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ofoxdvlzAZ",
        "title": "MING: A Functional Approach to Learning Molecular Generative Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Traditional molecule generation methods often rely on sequence- or graph-based representations, which can limit their expressive power or require complex permutation-equivariant architectures. This paper introduces a novel paradigm for learning molecule generative models based on functional representations. Specifically, we propose Molecular Implicit Neural Generation (MING), a diffusion-based model that learns molecular distributions in the function space. Unlike standard diffusion processes in the data space, MING employs a novel functional denoising probabilistic process, which jointly denoises information in both the function's input and output spaces by leveraging an expectation-maximization procedure for latent implicit neural representations of data. This approach enables a simple yet effective model design that accurately captures underlying function distributions. Experimental results on molecule-related datasets demonstrate MING's superior performance and ability to generate plausible molecular samples, surpassing state-of-the-art data-space methods while offering a more streamlined architecture and significantly faster generation times. The code is available at https://github.com/v18nguye/MING.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Van Khoa Nguyen;Maciej Falkiewicz;Giangiacomo Mercatali;Alexandros Kalousis",
        "authorids": "~Van_Khoa_Nguyen2;~Maciej_Falkiewicz1;~Giangiacomo_Mercatali1;~Alexandros_Kalousis1",
        "gender": "M;;;M",
        "homepage": ";;;http://dmml.ch/alexandros-kalousis/",
        "dblp": "372/5727;225/0679;;68/6004",
        "google_scholar": "UDVkJKcAAAAJ;https://scholar.google.ch/citations?user=08jtE7MAAAAJ;;uVkn9UEAAAAJ",
        "orcid": ";;;",
        "linkedin": "khoa-nguyen-139b9b15b/;;;",
        "or_profile": "~Van_Khoa_Nguyen2;~Maciej_Falkiewicz1;~Giangiacomo_Mercatali1;~Alexandros_Kalousis1",
        "aff": "University of Geneva;Geneva School of Business Administration, HES-SO University of Applied Sciences of Western Switzerland;+University of Geneva;;University of Applied Sciences Western Switzerland",
        "aff_domain": "unige.ch;hesge.ch+unige.ch;;hesge.ch",
        "position": "PhD student;Researcher+PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nnguyen2025ming,\ntitle={{MING}: A Functional Approach to Learning Molecular Generative Models},\nauthor={Van Khoa Nguyen and Maciej Falkiewicz and Giangiacomo Mercatali and Alexandros Kalousis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ofoxdvlzAZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ofoxdvlzAZ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "olOU8aEfvV",
        "title": "The Pivoting Framework: Frank-Wolfe Algorithms with Active Set Size Control",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We propose the pivoting meta algorithm (PM) to enhance optimization algorithms that generate iterates as convex combinations of vertices of a feasible region $C\\subseteq \\mathbb{R}^n$, including Frank-Wolfe (FW) variants. PM guarantees that the active set (the set of vertices in the convex combination) of the modified algorithm remains as small as $dim(C)+1$ as stipulated by Carath\u00e9odory's theorem. PM achieves this by reformulating the active set expansion task into an equivalent linear program, which can be efficiently solved using a single pivot step akin to the primal simplex algorithm; the convergence rate of the original algorithms are maintained. Furthermore, we establish the connection between PM and active set identification, in particular showing under mild assumptions that PM applied to the away-step Frank-Wolfe algorithm or the blended pairwise Frank-Wolfe algorithm bounds the active set size by the dimension of the optimal face plus $1$. We provide numerical experiments to illustrate practicality and efficacy on active set size reduction.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mathieu Besan\u00e7on;Sebastian Pokutta;Elias Samuel Wirth",
        "authorids": "~Mathieu_Besan\u00e7on1;~Sebastian_Pokutta1;~Elias_Samuel_Wirth1",
        "gender": "M;M;M",
        "homepage": "https://matbesancon.xyz;http://www.pokutta.com;https://elwirth.github.io/",
        "dblp": "245/2718;75/7718;293/8082.html",
        "google_scholar": "https://scholar.google.ca/citations?user=-xStCAIAAAAJ;;Sz2JU6oAAAAJ",
        "orcid": "0000-0002-6284-3033;;0009-0008-8957-8736",
        "linkedin": ";;",
        "or_profile": "~Mathieu_Besan\u00e7on1;~Sebastian_Pokutta1;~Elias_Samuel_Wirth1",
        "aff": "INRIA;ZIB+TU Berlin;TU Berlin",
        "aff_domain": "inria.fr;zib.de+tu-berlin.de;tu-berlin.de",
        "position": "Researcher;Vice President+Full Professor;PhD student",
        "bibtex": "@inproceedings{\nbesancon2025the,\ntitle={The Pivoting Framework: Frank-Wolfe Algorithms with Active Set Size Control},\nauthor={Mathieu Besan{\\c{c}}on and Sebastian Pokutta and Elias Samuel Wirth},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=olOU8aEfvV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=olOU8aEfvV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ou8tQ1jmQJ",
        "title": "Clustered Invariant Risk Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This study extends the problem settings of Invariant Risk Minimization(IRM) for Out-of-Distribution generalization problems to unknown clustered environments settings. In this scenario, where a given set of environments exhibits an unknown clustered structure, our objective is to identify a single invariant feature extractor and per-cluster regressors (or classifiers) built on top of the feature extractor. To achieve this, we propose a new framework called Clustered IRM for simultaneously identifying the cluster structure and the invariant features. Our theoretical analysis demonstrates that the required number of training environments for such identification is only $O(d_\\mathrm{sp} + K^2)$, where $d_\\mathrm{sp}$ represents the dimensionality of the spurious features, and $K$ is the number of clusters. Numerical experiments validate the effectiveness of our proposed framework.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tomoya Murata;Atsushi Nitanda;Taiji Suzuki",
        "authorids": "~Tomoya_Murata1;~Atsushi_Nitanda1;~Taiji_Suzuki1",
        "gender": "M;M;M",
        "homepage": ";https://sites.google.com/site/atsushinitanda;http://ibis.t.u-tokyo.ac.jp/suzuki/",
        "dblp": "151/5035;155/1884;08/312",
        "google_scholar": "hH5pbMIAAAAJ;https://scholar.google.co.jp/citations?user=LyVvaf8AAAAJ;x8osrBsAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Tomoya_Murata1;~Atsushi_Nitanda1;~Taiji_Suzuki1",
        "aff": "NTT DATA Mathematical Systems Inc.;Nanyang Technological University+A*STAR;The University of Tokyo",
        "aff_domain": "msi.co.jp;ntu.edu.sg+a-star.edu.sg;u-tokyo.ac.jp",
        "position": "Researcher;Associate Professor+Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nmurata2025clustered,\ntitle={Clustered Invariant Risk Minimization},\nauthor={Tomoya Murata and Atsushi Nitanda and Taiji Suzuki},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ou8tQ1jmQJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ou8tQ1jmQJ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "p4CHBlYxYj",
        "title": "Additive Model Boosting: New Insights and Path(ologie)s",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Additive models (AMs) have sparked a lot of interest in machine learning recently, allowing the incorporation of interpretable structures into a wide range of model classes. Many commonly used approaches to fit a wide variety of potentially complex additive models build on the idea of boosting additive models. While boosted additive models (BAMs) work well in practice, certain theoretical aspects are still poorly understood, including general convergence behavior and what optimization problem is being solved when accounting for the implicit regularizing nature of boosting. In this work, we study the solution paths of BAMs and establish connections with other approaches for certain classes of problems. Along these lines, we derive novel convergence results for BAMs, which yield crucial insights into the inner workings of the method. While our results generally provide reassuring theoretical evidence for the practical use of BAMs, they also uncover some \"pathologies\" of boosting for certain additive model classes concerning their convergence behavior that require caution in practice. We empirically validate our theoretical findings through several numerical experiments.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rickmer Schulte;David R\u00fcgamer",
        "authorids": "~Rickmer_Schulte1;~David_R\u00fcgamer1",
        "gender": ";M",
        "homepage": ";https://davidruegamer.github.io/",
        "dblp": ";220/5560",
        "google_scholar": "ja8BlE0AAAAJ;https://scholar.google.de/citations?user=_DYguksAAAAJ",
        "orcid": "0009-0004-7820-0018;",
        "linkedin": "rickmer-schulte;",
        "or_profile": "~Rickmer_Schulte1;~David_R\u00fcgamer1",
        "aff": "Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;LMU Munich",
        "aff_domain": "lmu.de;lmu.de",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nschulte2025additive,\ntitle={Additive Model Boosting: New Insights and Path(ologie)s},\nauthor={Rickmer Schulte and David R{\\\"u}gamer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=p4CHBlYxYj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=p4CHBlYxYj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "pCwFjmTNyp",
        "title": "Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a scalable kinetic Langevin dynamics algorithm for sampling parameter spaces of big data and AI applications.  Our scheme combines a symmetric forward/backward sweep over minibatches with a symmetric discretization of Langevin dynamics. For a particular Langevin splitting method (UBU), we show that the resulting Symmetric Minibatch Splitting-UBU (SMS-UBU) integrator has bias $\\mathcal{O}(h^2 d^{1/2})$ in dimension $d>0$ with stepsize $h>0$, despite only using one minibatch per iteration, thus providing excellent control of the sampling bias as a function of the stepsize. We apply the algorithm to explore local modes of the posterior distribution of Bayesian neural networks (BNNs) and evaluate the calibration performance of the posterior predictive probabilities for neural networks with convolutional neural network architectures for classification problems on three different datasets (Fashion-MNIST, Celeb-A and chest X-ray). Our results indicate that BNNs sampled with SMS-UBU can offer significantly better calibration performance compared to standard methods of training and stochastic weight averaging.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Paulin;Peter A. Whalley;Neil K. Chada;Benedict J. Leimkuhler",
        "authorids": "~Daniel_Paulin1;~Peter_A._Whalley1;~Neil_K._Chada1;~Benedict_J._Leimkuhler1",
        "gender": ";M;M;M",
        "homepage": ";https://people.math.ethz.ch/~pwhalley/;https://sites.google.com/view/neilc;https://www.maths.ed.ac.uk/~bleimkuh",
        "dblp": ";;;52/4339",
        "google_scholar": ";da2B_TkAAAAJ;;https://scholar.google.co.uk/citations?user=SAJrT0wAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Daniel_Paulin1;~Peter_A._Whalley1;~Neil_K._Chada1;~Benedict_J._Leimkuhler1",
        "aff": ";ETHZ - ETH Zurich;City University of Hong Kong;University of Edinburgh",
        "aff_domain": ";ethz.ch;cityu.edu.hk;ed.ac.uk",
        "position": ";Postdoc;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\npaulin2025sampling,\ntitle={Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics},\nauthor={Daniel Paulin and Peter A. Whalley and Neil K. Chada and Benedict J. Leimkuhler},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=pCwFjmTNyp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pCwFjmTNyp",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "pW9UW8uDlR",
        "title": "Sequential Kernelized Stein Discrepancy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a sequential version of the kernelized Stein discrepancy goodness-of-fit test, which allows for conducting goodness-of-fit tests for unnormalized densities that are continuously monitored and adaptively stopped. That is, the sample size need not be fixed prior to data collection; the practitioner can choose whether to stop the test or continue to gather evidence at any time while controlling the false discovery rate. In stark contrast to related literature, we do not impose uniform boundedness on the Stein kernel. Instead, we exploit the potential boundedness of the Stein kernel at arbitrary point evaluations to define test martingales, that give way to the subsequent novel sequential tests. We prove the validity of the test, as well as an asymptotic lower bound for the logarithmic growth of the wealth process under the alternative.  We further illustrate the empirical performance of the test with a variety of distributions, including restricted Boltzmann machines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Diego Martinez-Taboada;Aaditya Ramdas",
        "authorids": "~Diego_Martinez-Taboada1;~Aaditya_Ramdas2",
        "gender": "M;M",
        "homepage": ";http://stat.cmu.edu/~aramdas",
        "dblp": "331/8680;117/3518",
        "google_scholar": "9-p1MCsAAAAJ;ZvFaPxUAAAAJ",
        "orcid": ";0000-0003-0497-311X",
        "linkedin": "diego-martinez-taboada/;",
        "or_profile": "~Diego_Martinez-Taboada1;~Aaditya_Ramdas2",
        "aff": "Carnegie Mellon University;Carnegie Mellon University+Amazon",
        "aff_domain": "cmu.edu;cmu.edu+amazon.com",
        "position": "PhD student;Associate Professor+Researcher",
        "bibtex": "@inproceedings{\nmartinez-taboada2025sequential,\ntitle={Sequential Kernelized Stein Discrepancy},\nauthor={Diego Martinez-Taboada and Aaditya Ramdas},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=pW9UW8uDlR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pW9UW8uDlR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "q4hCSrCtcn",
        "title": "Differentially Private Graph Data Release: Inefficiencies & Unfairness",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Networks in sectors like telecommunications and transportation often contain sensitive user data, requiring privacy enhancing technologies during data release to ensure privacy. While Differential Privacy (DP) is recognized as the leading standard for privacy preservation, its use comes with new challenges, as the noise added for privacy introduces inaccuracies or biases. DP techniques have also been found to distribute these biases disproportionately across different populations, inducing fairness issues.\nThis paper investigates the effects of DP on bias and fairness when releasing network edge weights. We specifically examine how these privacy measures affect decision-making tasks, such as computing shortest paths, which are crucial for routing in transportation and communications networks, and provide both theoretical insights and empirical evidence on the inherent trade-offs between privacy, accuracy, and fairness for network data release.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ferdinando Fioretto;Diptangshu Sen;Juba Ziani",
        "authorids": "~Ferdinando_Fioretto1;~Diptangshu_Sen1;~Juba_Ziani1",
        "gender": "M;M;M",
        "homepage": "http://nandofioretto.com;https://sites.google.com/view/dsen;http://www.juba-ziani.com",
        "dblp": "119/6404;258/9671.html;157/3784",
        "google_scholar": "ASf9Q04AAAAJ;jVTm3LoAAAAJ;https://scholar.google.co.in/citations?user=1bwPKXpo97YC",
        "orcid": ";0000-0003-2804-3208;0000-0002-3324-4349",
        "linkedin": ";diptangshu-sen-a4292914a/;",
        "or_profile": "~Ferdinando_Fioretto1;~Diptangshu_Sen1;~Juba_Ziani1",
        "aff": "University of Virginia, Charlottesville;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "virginia.edu;gatech.edu;gatech.edu",
        "position": "Assistant Professor;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nfioretto2025differentially,\ntitle={Differentially Private Graph Data Release: Inefficiencies \\& Unfairness},\nauthor={Ferdinando Fioretto and Diptangshu Sen and Juba Ziani},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=q4hCSrCtcn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=q4hCSrCtcn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qDExOTe1Bn",
        "title": "Symmetry-Based Structured Matrices for Efficient Approximately Equivariant Networks",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "There has been much recent interest in designing neural networks (NNs) with relaxed equivariance, which interpolate between exact equivariance and full flexibility for consistent performance gains. In a separate line of work, structured parameter matrices with low displacement rank (LDR)---which permit fast function and gradient evaluation---have been used to create compact NNs, though primarily benefiting classical convolutional neural networks (CNNs). In this work, we propose a framework based on symmetry-based structured matrices to build approximately equivariant NNs with fewer parameters. Our approach unifies the aforementioned areas using Group Matrices (GMs), a forgotten precursor to the modern notion of regular representations of finite groups. GMs allow the design of structured matrices similar to LDR matrices, which can generalize all the elementary operations of a CNN from cyclic groups to arbitrary finite groups. We show GMs can also generalize classical LDR theory to general discrete groups, enabling a natural formalism for approximate equivariance. We test GM-based architectures on various tasks with relaxed symmetry and find that our framework performs competitively with approximately equivariant NNs and other structured matrix-based methods, often with one to two orders of magnitude fewer parameters.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ashwin Samudre;Mircea Petrache;Brian Nord;Shubhendu Trivedi",
        "authorids": "~Ashwin_Samudre1;~Mircea_Petrache1;~Brian_Nord1;~Shubhendu_Trivedi2",
        "gender": "M;M;;",
        "homepage": "https://kiryteo.github.io/;https://sites.google.com/site/mircpetrache/home;https://iamstarnord.com;",
        "dblp": ";;;",
        "google_scholar": "Btu380oAAAAJ;HiYZ-6MAAAAJ;;",
        "orcid": "0000-0001-8136-7645;0000-0003-2181-169X;;",
        "linkedin": "ashwin-samudre-370273107/;mircea-petrache-4983a4104/;;",
        "or_profile": "~Ashwin_Samudre1;~Mircea_Petrache1;~Brian_Nord1;~Shubhendu_Trivedi2",
        "aff": "Allen Institute;Pontificia Universidad Catolica de Chile;;",
        "aff_domain": "alleninstitute.org;puc.cl;;",
        "position": "Researcher;Assistant Professor;;",
        "bibtex": "@inproceedings{\nsamudre2025symmetrybased,\ntitle={Symmetry-Based Structured Matrices for Efficient Approximately Equivariant Networks},\nauthor={Ashwin Samudre and Mircea Petrache and Brian Nord and Shubhendu Trivedi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qDExOTe1Bn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qDExOTe1Bn",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qJgs9o7ZDE",
        "title": "Nonparametric Factor Analysis and Beyond",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis, and causal representation learning, rely on assumptions of additive independent noise or noiseless regimes. In contrast, we study the more general case where noise can take arbitrary forms, depend on latent variables, and be non-invertibly entangled within a nonlinear function. We propose a general framework for identifying latent variables in the nonparametric noisy settings. We first show that, under suitable conditions, the generative model is identifiable up to certain submanifold indeterminacies even in the presence of non-negligible noise. Furthermore, under the structural or distributional variability conditions, we prove that latent variables of the general nonlinear models are identifiable up to trivial indeterminacies. Based on the proposed theoretical framework, we have also developed corresponding estimation methods and validated them in various synthetic and real-world settings. Interestingly, our estimate of the true GDP growth from alternative measurements suggests more insightful information on the economies than official reports. We expect our framework to provide new insight into how both researchers and practitioners deal with latent variables in real-world scenarios.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yujia Zheng;Yang Liu;Jiaxiong Yao;Yingyao Hu;Kun Zhang",
        "authorids": "~Yujia_Zheng1;~Yang_Liu91;~Jiaxiong_Yao1;~Yingyao_Hu1;~Kun_Zhang1",
        "gender": "M;M;M;;M",
        "homepage": "https://yjzheng.com;;https://sites.google.com/site/jiaxiongyao16/home;http://www.econ2.jhu.edu/people/hu/;http://www.andrew.cmu.edu/user/kunz1/",
        "dblp": "245/6109-1.html;;;;96/3115-1",
        "google_scholar": "https://scholar.google.co.uk/citations?user=ioiW248AAAAJ;fsMKtN8AAAAJ;GUZvyXQAAAAJ#:~:text=M%20Newiak,%20R%20Ouedraogo,%20B%20Tenison,%20J%20Yao,;;RGoypN4AAAAJ",
        "orcid": "0009-0003-5225-6366;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Yujia_Zheng1;~Yang_Liu91;~Jiaxiong_Yao1;~Yingyao_Hu1;~Kun_Zhang1",
        "aff": "Carnegie Mellon University;International Monetary Fund;IMF;Johns Hopkins University;Mohamed bin Zayed University of Artificial Intelligence+Carnegie Mellon University",
        "aff_domain": "cmu.edu;imf.org;imf.org;jhu.edu;mbzuai.ac.ae+cmu.edu",
        "position": "PhD student;Researcher;Researcher;Full Professor;Professor+Associate Professor",
        "bibtex": "@inproceedings{\nhu2025nonparametric,\ntitle={Nonparametric Factor Analysis and Beyond},\nauthor={Yingyao Hu and Yang Liu and Jiaxiong Yao and Kun Zhang and Yujia Zheng},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qJgs9o7ZDE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qJgs9o7ZDE",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qVDnA4BdCc",
        "title": "Evidential Uncertainty Probes for Graph Neural Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accurate quantification of both aleatoric and epistemic uncertainties is essential when deploying Graph Neural Networks (GNNs) in high-stakes applications such as drug discovery and financial fraud detection, where reliable predictions are critical. Although Evidential Deep Learning (EDL) efficiently quantifies uncertainty using a Dirichlet distribution over predictive probabilities, existing EDL-based GNN (EGNN) models require modifications to the network architecture and retraining, failing to take advantage of pre-trained models.\nWe propose a plug-and-play framework for uncertainty quantification in GNNs that works with pre-trained models without the need for retraining.  Our Evidential Probing Network (EPN) uses a lightweight Multi-Layer-Perceptron (MLP) head to extract evidence from learned representations, allowing efficient integration with various GNN architectures.  We further introduce evidence-based regularization techniques, referred to as EPN-reg, to enhance the estimation of epistemic uncertainty with theoretical justifications. \nExtensive experiments demonstrate that the proposed EPN-reg achieves state-of-the-art performance in accurate and efficient uncertainty quantification, making it suitable for real-world deployment.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Linlin Yu;Kangshuo Li;Pritom Kumar Saha;Yifei Lou;Feng Chen",
        "authorids": "~Linlin_Yu1;~Kangshuo_Li1;~Pritom_Kumar_Saha1;~Yifei_Lou2;~Feng_Chen7",
        "gender": "F;M;M;F;M",
        "homepage": ";https://www.linkedin.com/in/kangshuo-li-10359a218/;;https://sites.google.com/site/louyifei/;https://personal.utdallas.edu/~fxc190007/",
        "dblp": "204/9716;;;;21/3047-1",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;wD1PVDMAAAAJ;iCiUflEAAAAJ;KOQ-SSYAAAAJ",
        "orcid": "0009-0001-5690-9905;;0000-0003-1646-7804;0000-0003-1973-5704;",
        "linkedin": "linlin-yu-723884249/;;pritom-kun/;;",
        "or_profile": "~Linlin_Yu1;~Kangshuo_Li1;~Pritom_Kumar_Saha1;~Yifei_Lou2;~Feng_Chen7",
        "aff": "The University of Texas at Dallas;UT-Dallas;University of Texas at Dallas;University of North Carolina at Chapel Hill;University of Texas, Dallas",
        "aff_domain": "cs.utdallas.edu;cs.utdallas.edu;utdallas.edu;unc.edu;utdallas.edu",
        "position": "PhD student;PhD student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nyu2025evidential,\ntitle={Evidential Uncertainty Probes for Graph Neural Networks},\nauthor={Linlin Yu and Kangshuo Li and Pritom Kumar Saha and Yifei Lou and Feng Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qVDnA4BdCc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qVDnA4BdCc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qXV0ryFFfz",
        "title": "Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales Inventory Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning (RL) has proven to be well-performed and versatile in inventory control (IC). However, further improvement of RL algorithms in the IC domain is impeded by two limitations of online experience. First, online experience is expensive to acquire in real-world applications. With the low sample efficiency nature of RL algorithms, it would take extensive time to collect enough data and train the RL policy to convergence. Second, online experience may not reflect the true demand due to the lost-sales phenomenon typical in IC, which makes the learning process more challenging. To address the above challenges, we propose a training framework that combines reinforcement learning with feedback graph (RLFG) and intrinsically motivated exploration (IME) to boost sample efficiency. In particular, we first leverage the MDP structure inherent in lost-sales IC problems and design the feedback graph (FG) tailored to lost-sales IC problems to generate abundant side experiences aiding in RL updates. Then we conduct a rigorous theoretical analysis of how the designed FG reduces the sample complexity of RL methods. Guided by these insights, we design an intrinsic reward to direct the RL agent to explore to the state-action space with more side experiences, further exploiting FG\u2019s capability. Experimental results on single-item, multi-item, and multi-echelon environments demonstrate that our method greatly improves the sample efficiency of applying RL in IC. \nOur code is available at https://github.com/Ziffer-byakuya/RLIMFG4IC",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zifan LIU;Xinran Li;Shibo Chen;Gen Li;Jiashuo Jiang;Jun Zhang",
        "authorids": "~Zifan_LIU1;~Xinran_Li3;~Shibo_Chen2;~Gen_Li2;~Jiashuo_Jiang1;~Jun_Zhang25",
        "gender": "M;F;M;M;;",
        "homepage": "https://ziffer-byakuya.github.io/zifferliu.github.io/;https://lxxxxr.github.io/;http://faculty.sustech.edu.cn/profiles/chensb;;https://jiashuo3.github.io/;https://eejzhang.people.ust.hk/",
        "dblp": ";;;28/538-5.html;281/6676;z/JunZhang4",
        "google_scholar": ";6fYlKXgAAAAJ;;https://scholar.google.com/citations?view_op=list_works;;1Is687QAAAAJ",
        "orcid": "0000-0001-7948-5124;0000-0003-0245-9459;0000-0001-7329-428X;0000-0002-3078-9191;0000-0001-5230-4231;0000-0002-5222-1898",
        "linkedin": ";;;;;",
        "or_profile": "~Zifan_LIU1;~Xinran_Li3;~Shibo_Chen2;~Gen_Li2;~Jiashuo_Jiang1;~Jun_Zhang25",
        "aff": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;;The Chinese University of Hong Kong;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology",
        "aff_domain": "hkust.edu.hk;hkust.edu;;cuhk.edu.hk;ust.hk;ust.hk",
        "position": "PhD student;PhD student;;Assistant Professor;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nliu2025reinforcement,\ntitle={Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales Inventory Control},\nauthor={Zifan LIU and Xinran Li and Shibo Chen and Gen Li and Jiashuo Jiang and Jun Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qXV0ryFFfz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qXV0ryFFfz",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qmZwsWnMi7",
        "title": "Distance Estimation for High-Dimensional Discrete Distributions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Given two distributions $\\mathcal{P}$ and $\\mathcal{Q}$ over a high-dimensional domain $\\{0,1\\}^n$, and a parameter $\\varepsilon$, the goal of distance estimation is to determine the statistical distance between $\\mathcal{P}$ and $\\mathcal{Q}$, up to an additive tolerance $\\pm \\varepsilon$. Since exponential lower bounds (in $n$) are known for the problem in the standard sampling model, research has focused on richer query models where one can draw conditional samples. This paper presents the first polynomial query distance estimator in the conditional sampling model ($\\mathsf{COND}$). \n\nWe base our algorithm on the relatively weaker \\textit{subcube conditional} sampling ($\\mathsf{SUBCOND}$) oracle, which draws samples from the distribution conditioned on some of the dimensions. $\\mathsf{SUBCOND}$ is a promising model for widespread practical use because it captures the natural behavior of discrete samplers. Our algorithm makes $\\tilde{\\mathcal{O}}(n^3/\\varepsilon^5)$ queries to $\\mathsf{SUBCOND}$.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kuldeep S. Meel;Gunjan Kumar;Yash Pote",
        "authorids": "~Kuldeep_S._Meel2;~Gunjan_Kumar3;~Yash_Pote1",
        "gender": "M;;M",
        "homepage": "https://www.kuldeepmeel.com;https://sites.google.com/view/gunjankumar;",
        "dblp": "129/1623;127/7421.html;246/3083.html",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Kuldeep_S._Meel2;~Gunjan_Kumar3;~Yash_Pote1",
        "aff": "Georgia Institute of Technology+Department of Computer Science;;National University of Singapore",
        "aff_domain": "gatech.edu+cs.toronto.edu;;u.nus.edu",
        "position": "Associate Professor+Associate Professor;;PhD student",
        "bibtex": "@inproceedings{\nmeel2025distance,\ntitle={Distance Estimation for High-Dimensional Discrete Distributions},\nauthor={Kuldeep S. Meel and Gunjan Kumar and Yash Pote},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qmZwsWnMi7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qmZwsWnMi7",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qtppWP3gXl",
        "title": "Certifiably Quantisation-Robust training and inference of Neural Networks",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We tackle the problem of computing guarantees for the robustness of neural networks against quantisation of their inputs, parameters and activation values. In particular, we pose the problem of bounding the worst-case discrepancy between the original neural network and all possible quantised ones parametrised by a given maximum quantisation diameter $\\epsilon > 0$ over a finite dataset. To achieve this, we first reformulate the problem  in terms of bilinear optimisation, which can be solved for provable bounds on the robustness guarantee. We then show how a quick scheme based on interval bound propagation can be developed and implemented during training so to allow for the learning of neural networks robust against a continuous family of quantisation techniques. We evaluated our methodology on a variety of architectures on datasets such as MNIST, F-MNIST and CIFAR10. We demonstrate how non-trivial bounds on guaranteed accuracy can be obtained on several architectures and how quantisation robustness can be significantly improved through robust training.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hue Dang;Matthew Robert Wicker;Goetz Botterweck;Andrea Patane",
        "authorids": "~Hue_Dang2;~Matthew_Robert_Wicker1;~Goetz_Botterweck1;~Andrea_Patane1",
        "gender": "F;M;M;",
        "homepage": ";https://www.matthewwicker.org;https://www.botterweck.de;",
        "dblp": ";207/7909.html;60/2897.html;",
        "google_scholar": "bimX9ZsAAAAJ;_0qEDNIAAAAJ;xsIAZAkAAAAJ;xRzKYP0AAAAJ",
        "orcid": "0009-0008-1378-9545;;0000-0002-5556-1660;",
        "linkedin": ";;;",
        "or_profile": "~Hue_Dang2;~Matthew_Robert_Wicker1;~Goetz_Botterweck1;~Andrea_Patane1",
        "aff": "University of Dublin, Trinity College;Imperial College London;University of Dublin, Trinity College+University of Limerick+Lero;University of Dublin, Trinity College",
        "aff_domain": "tcd.ie;imperial.ac.uk;tcd.ie+ul.ie+lero.ie;tcd.ie",
        "position": "PhD student;Assistant Professor;Associate Professor+Researcher+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\ndang2025certifiably,\ntitle={Certifiably Quantisation-Robust training and inference of Neural Networks},\nauthor={Hue Dang and Matthew Robert Wicker and Goetz Botterweck and Andrea Patane},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=qtppWP3gXl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qtppWP3gXl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "quw6Wxhu4B",
        "title": "Class Imbalance in Anomaly Detection: Learning from an Exactly Solvable Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Class imbalance (CI) is a longstanding problem in machine learning, slowing down training and reducing performances. Although empirical remedies exist, it is often unclear which ones work best and when, due to the lack of an overarching theory. We address a common case of imbalance, that of anomaly (or outlier) detection. We provide a theoretical framework to analyze, interpret and address CI. It is based on an exact solution of the teacher-student perceptron model, through replica theory. Within this framework, one can distinguish several sources of CI: either intrinsic, train or test imbalance. Our analysis reveals that, depending on the specific problem setting, one source or another might dominate. We further find that the optimal train imbalance is generally different from 50%, with a non trivial dependence on the intrinsic imbalance, the abundance of data and on the noise in the learning. Moreover, there is a crossover between a small noise training regime where results are independent of the noise level to a high noise regime where performances quickly degrade with noise. Our results challenge some of the conventional wisdom on CI and pave the way for integrated approaches to the topic.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Francesco Saverio Pezzicoli;Valentina Ros;Fran\u00e7ois P. Landes;Marco Baity-Jesi",
        "authorids": "~Francesco_Saverio_Pezzicoli1;valentina.ros@cnrs.fr;~Fran\u00e7ois_P._Landes1;~Marco_Baity-Jesi1",
        "gender": "M;;;M",
        "homepage": ";;;https://mbaityje.github.io/",
        "dblp": ";;;52/11265",
        "google_scholar": "rewoVXUAAAAJ;;;",
        "orcid": ";;;0000-0002-8723-906X",
        "linkedin": ";;;",
        "or_profile": "~Francesco_Saverio_Pezzicoli1;valentina.ros@cnrs.fr;~Fran\u00e7ois_P._Landes1;~Marco_Baity-Jesi1",
        "aff": ";;;Eawag",
        "aff_domain": ";;;eawag.ch",
        "position": ";;;Group Leader",
        "bibtex": "@inproceedings{\npezzicoli2025anomaly,\ntitle={Anomaly Detection with Class Imbalance: learning from Exactly Solvable Models},\nauthor={Francesco Saverio Pezzicoli and Valentina Ros and Fran{\\c{c}}ois P. Landes and Marco Baity-Jesi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=quw6Wxhu4B}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=quw6Wxhu4B",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "r5F7Z8s0Qk",
        "title": "Implicit Diffusion: Efficient optimization through stochastic sampling",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Sampling and automatic differentiation are both ubiquitous in modern machine learning. At its intersection, differentiating through a sampling operation, with respect to the parameters of the sampling process, is a problem that is both challenging and broadly applicable. We introduce a general framework and a new algorithm for first-order optimization of parameterized stochastic diffusions, performing jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical and experimental results showcasing the performance of our method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pierre Marion;Anna Korba;Peter Bartlett;Mathieu Blondel;Valentin De Bortoli;Arnaud Doucet;Felipe Llinares-L\u00f3pez;Courtney Paquette;Quentin Berthet",
        "authorids": "~Pierre_Marion1;~Anna_Korba2;~Peter_Bartlett1;~Mathieu_Blondel1;~Valentin_De_Bortoli1;~Arnaud_Doucet2;~Felipe_Llinares-L\u00f3pez1;~Courtney_Paquette1;~Quentin_Berthet2",
        "gender": "M;;M;;;;;;M",
        "homepage": "https://pierremarion23.github.io/;;https://www.stat.berkeley.edu/~bartlett/;http://www.mblondel.org;https://vdeborto.github.io/;;;;http://q-berthet.github.io/",
        "dblp": "250/2318;182/8959.html;https://dblp.org/pers/hd/b/Bartlett:Peter_L=;05/8614.html;224/9338;;;;129/1262",
        "google_scholar": "https://scholar.google.fr/citations?user=Q8H5LgIAAAAJ;https://scholar.google.fr/citations?user=dbH6E3kAAAAJ;yQNhFGUAAAAJ;C0EKzrUAAAAJ;;;;;bHwGZjcAAAAJ",
        "orcid": ";;;;;;;;",
        "linkedin": "pierre-marion-816474130/;;;;;;;;",
        "or_profile": "~Pierre_Marion1;~Anna_Korba2;~Peter_Bartlett1;~Mathieu_Blondel1;~Valentin_De_Bortoli1;~Arnaud_Doucet2;~Felipe_Llinares-L\u00f3pez1;~Courtney_Paquette1;~Quentin_Berthet2",
        "aff": "EPFL - EPF Lausanne;Ensae ParisTech;University of California - Berkeley+University of California-Berkeley+University of California, Berkeley+University of California, Berkeley;Google;University of Oxford;;;;Google DeepMind",
        "aff_domain": "epfl.ch;ensae.fr;++berkeley.edu+berkeley;google.com;ox.ac.uk;;;;google.com",
        "position": "Postdoc;Assistant Professor;Full Professor++Researcher+Professor;Research scientist;Postdoc;;;;Researcher",
        "bibtex": "@inproceedings{\nmarion2025implicit,\ntitle={Implicit Diffusion: Efficient optimization through stochastic sampling},\nauthor={Pierre Marion and Anna Korba and Peter Bartlett and Mathieu Blondel and Valentin De Bortoli and Arnaud Doucet and Felipe Llinares-L{\\'o}pez and Courtney Paquette and Quentin Berthet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=r5F7Z8s0Qk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=r5F7Z8s0Qk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rAvvANgQoh",
        "title": "Counting Graphlets of Size k under Local Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The problem of counting subgraphs or graphlets under local differential privacy is an important challenge that has attracted significant attention from researchers. However, much of the existing work focuses on small graphlets like triangles or $k$-stars. In this paper, we propose a non-interactive, locally differentially private algorithm capable of counting graphlets of any size $k$. When $n$ is the number of nodes in the input graph, we show that the expected $\\ell_2$ error of our algorithm is $O(n^{k - 1})$. Additionally, we prove that there exists a class of input graphs and graphlets of size $k$ for which any non-interactive counting algorithm incurs an expected $\\ell_2$ error of $\\Omega(n^{k - 1})$, demonstrating the optimality of our result. Furthermore, we establish that for certain input graphs and graphlets, any locally differentially private algorithm must have an expected $\\ell_2$ error of $\\Omega(n^{k - 1.5})$. Our experimental results show that our algorithm is more accurate than the classical randomized response method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vorapong Suppakitpaisarn;Donlapark Ponnoprat;Nicha Hirankarn;Quentin Hillebrand",
        "authorids": "~Vorapong_Suppakitpaisarn1;~Donlapark_Ponnoprat1;myenicha@gmail.com;~Quentin_Hillebrand1",
        "gender": "M;M;;M",
        "homepage": "http://vorapong-sup.net/;https://donlapark.pages.dev;;https://quentin-hillebrand.com",
        "dblp": "72/10310.html;274/2677;;353/7714",
        "google_scholar": "lo0TdfoAAAAJ;l4ldeeUAAAAJ;;https://scholar.google.fr/citations?user=2FIuSQUAAAAJ",
        "orcid": ";0000-0001-5679-4426;;0000-0002-7747-4998",
        "linkedin": ";;;quentin-hillebrand",
        "or_profile": "~Vorapong_Suppakitpaisarn1;~Donlapark_Ponnoprat1;myenicha@gmail.com;~Quentin_Hillebrand1",
        "aff": "The University of Tokyo;Chiang Mai University;;The University of Tokyo",
        "aff_domain": "u-tokyo.ac.jp;cmu.ac.th;;g.ecc.u-tokyo.ac.jp",
        "position": "Associate Professor;Lecturer;;PhD student",
        "bibtex": "@inproceedings{\nsuppakitpaisarn2025counting,\ntitle={Counting Graphlets of Size k under Local Differential Privacy},\nauthor={Vorapong Suppakitpaisarn and Donlapark Ponnoprat and Nicha Hirankarn and Quentin Hillebrand},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rAvvANgQoh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rAvvANgQoh",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rFfNuzzXXW",
        "title": "DDEQs: Distributional Deep Equilibrium Models through Wasserstein Gradient Flows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deep Equilibrium Models (DEQs) are a class of implicit neural networks that solve for a fixed point of a neural network in their forward pass. Traditionally, DEQs take sequences as inputs, but have since been applied to a variety of data. In this work, we present Distributional Deep Equilibrium Models (DDEQs), extending DEQs to discrete measure inputs, such as sets or point clouds. We provide a theoretically grounded framework for DDEQs. Leveraging Wasserstein gradient flows, we show how the forward pass of the DEQ can be adapted to find fixed points of discrete measures under permutation-invariance, and derive adequate network architectures for DDEQs. In experiments, we show that they can compete with state-of-the-art models in tasks such as point cloud classification and point cloud completion, while being significantly more parameter-efficient.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jonathan Geuter;Cl\u00e9ment Bonet;Anna Korba;David Alvarez-Melis",
        "authorids": "~Jonathan_Geuter1;~Cl\u00e9ment_Bonet1;~Anna_Korba2;~David_Alvarez-Melis1",
        "gender": "M;M;;M",
        "homepage": ";https://clbonet.github.io;;https://dmelis.github.io/",
        "dblp": ";304/8220;182/8959.html;168/8255",
        "google_scholar": "5CTwTC0AAAAJ;wjCPk5kAAAAJ;https://scholar.google.fr/citations?user=dbH6E3kAAAAJ;XsxZrYYAAAAJ",
        "orcid": ";0000-0002-3390-1169;;0000-0002-9591-8986",
        "linkedin": "jonathan-geuter/;cl\u00e9ment-bonet-2840a9153;;",
        "or_profile": "~Jonathan_Geuter1;~Cl\u00e9ment_Bonet1;~Anna_Korba2;~David_Alvarez-Melis1",
        "aff": "Harvard University;Ecole Nationale de la Statistique et de l'Administration Economique;Ensae ParisTech;School of Engineering and Applied Sciences, Harvard University+Microsoft",
        "aff_domain": "g.harvard.edu;ensae.fr;ensae.fr;seas.harvard.edu+microsoft.com",
        "position": "PhD student;Postdoc;Assistant Professor;Assistant Professor+Senior Researcher",
        "bibtex": "@inproceedings{\ngeuter2025ddeqs,\ntitle={{DDEQ}s: Distributional Deep Equilibrium Models through Wasserstein Gradient Flows},\nauthor={Jonathan Geuter and Cl{\\'e}ment Bonet and Anna Korba and David Alvarez-Melis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rFfNuzzXXW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rFfNuzzXXW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rJcXQnGbVb",
        "title": "HR-Bandit: Human-AI Collaborated Linear Recourse Bandit",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Human doctors frequently recommend actionable recourses that allow patients to modify their conditions to access more effective treatments. Inspired by such healthcare scenarios, we propose the Recourse Linear UCB (\\textsf{RLinUCB}) algorithm, which optimizes both action selection and feature modifications by balancing exploration and exploitation. We further extend this to the Human-AI Linear Recourse Bandit (\\textsf{HR-Bandit}), which integrates human expertise to enhance performance. \\textsf{HR-Bandit} offers three key guarantees: (i) a warm-start guarantee for improved initial performance, (ii) a human-effort guarantee to minimize required human interactions, and (iii) a robustness guarantee that ensures sublinear regret even when human decisions are suboptimal. Empirical results, including a healthcare case study, validate its superior performance against existing benchmarks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junyu Cao;Ruijiang Gao;Esmaeil Keyvanshokooh",
        "authorids": "~Junyu_Cao1;~Ruijiang_Gao2;~Esmaeil_Keyvanshokooh1",
        "gender": "F;;Not Specified",
        "homepage": "https://junyucao.com/;https://github.com/ruijiang81;http://ekshokooh.github.io/",
        "dblp": "198/0859;;",
        "google_scholar": "rWNjzJsAAAAJ;iRHyFa4AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Junyu_Cao1;~Ruijiang_Gao2;~Esmaeil_Keyvanshokooh1",
        "aff": "University of Texas, Austin;University of Texas at Dallas;Texas A&M University - College Station",
        "aff_domain": "utexas.edu;utdallas.edu;tamu.edu",
        "position": "Assistant Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncao2025hrbandit,\ntitle={{HR}-Bandit: Human-{AI} Collaborated Linear Recourse Bandit},\nauthor={Junyu Cao and Ruijiang Gao and Esmaeil Keyvanshokooh},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rJcXQnGbVb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rJcXQnGbVb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rOKQGZjwhq",
        "title": "Statistical Learning of Distributionally Robust Stochastic Control in Continuous State Spaces",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We explore the control of stochastic systems with potentially continuous state and action spaces, characterized by the state dynamics $X_{t+1} = f(X_t, A_t, W_t)$. Here, $X$, $A$, and $W$ represent the state, action, and exogenous random noise processes, respectively, with $f$ denoting a known function that describes state transitions. Traditionally, the noise process $(W_t)_{t \\geq 0}$ is assumed to be independent and identically distributed, with a distribution that is either fully known or can be consistently estimated. However, the occurrence of distributional shifts, typical in engineering settings, necessitates the consideration of the robustness of the policy. This paper introduces a distributionally robust stochastic control paradigm that accommodates possibly adaptive adversarial perturbation to the noise distribution within a prescribed ambiguity set. We examine two adversary models: current-action-aware and current-action-unaware, leading to different dynamic programming equations. Furthermore, we characterize the optimal finite sample minimax rates for achieving uniform learning of the robust value function across continuum states under both adversary types, considering ambiguity sets defined by $f_k$-divergence and Wasserstein distance. Finally, we demonstrate the applicability of our framework across various real-world settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shengbo Wang;Nian Si;Jose Blanchet;Zhengyuan Zhou",
        "authorids": "~Shengbo_Wang1;~Nian_Si1;~Jose_Blanchet1;~Zhengyuan_Zhou2",
        "gender": "M;M;M;M",
        "homepage": "https://shengbo-wang.github.io/;http://niansi.me;https://web.stanford.edu/~jblanche/;https://scholar.google.com/citations?user=hiGI9v0AAAAJ&hl=en",
        "dblp": ";254/2589;75/5093.html;125/5270",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;;https://scholar.google.co.in/citations?user=O24CcQQAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;jose-blanchet;",
        "or_profile": "~Shengbo_Wang1;~Nian_Si1;~Jose_Blanchet1;~Zhengyuan_Zhou2",
        "aff": "University of Southern California+Stanford University;Hong Kong University of Science and Technology;Stanford University;New York University",
        "aff_domain": "usc.edu+stanford.edu;hkust.edu.hk;stanford.edu;nyu.edu",
        "position": "Assistant Professor+PhD student;Assistant Professor;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025statistical,\ntitle={Statistical Learning of Distributionally Robust Stochastic Control in Continuous State Spaces},\nauthor={Shengbo Wang and Nian Si and Jose Blanchet and Zhengyuan Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rOKQGZjwhq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rOKQGZjwhq",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rVEVn0MaVF",
        "title": "Learning the Distribution Map in Reverse Causal Performative Prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening system. Such shifts in distribution are particularly prevalent in social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learning the distribution shift. Our method is predicated on a \\emph{reverse causal model}, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to effectively minimize the performative prediction risk.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniele Bracale;Subha Maity;Yuekai Sun;Moulinath Banerjee",
        "authorids": "~Daniele_Bracale1;~Subha_Maity1;~Yuekai_Sun1;~Moulinath_Banerjee1",
        "gender": "M;M;;M",
        "homepage": "https://m.facebook.com/daniele.bracale.9?ref=bookmarks;https://lsa.umich.edu/stats/people/phd-students/smaity.html;https://yuekai.github.io/;https://lsa.umich.edu/stats/people/faculty/moulib.html",
        "dblp": ";278/2922;;",
        "google_scholar": ";eD9vCGMAAAAJ;6T1XtW8AAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Daniele_Bracale1;~Subha_Maity1;~Yuekai_Sun1;~Moulinath_Banerjee1",
        "aff": "University of Michigan;University of Waterloo;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor",
        "aff_domain": "umich.edu;uwaterloo.ca;umich.edu;umich.edu",
        "position": "PhD student;Assistant Professor;Assistant \u2192 Associate Professor of Statistics;Full Professor",
        "bibtex": "@inproceedings{\nbracale2025learning,\ntitle={Learning the Distribution Map in Reverse Causal Performative Prediction},\nauthor={Daniele Bracale and Subha Maity and Yuekai Sun and Moulinath Banerjee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rVEVn0MaVF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rVEVn0MaVF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rlmg3gN0fr",
        "title": "Risk-sensitive Bandits: Arm Mixture Optimality and Regret-efficient Algorithms",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces a general framework for risk-sensitive bandits that integrates the notions of risk-sensitive objectives by adopting a rich class of {\\em distortion riskmetrics}. The introduced framework subsumes the various existing risk-sensitive models. An important and hitherto unknown observation is that for a wide range of riskmetrics, the optimal bandit policy involves selecting a \\emph{mixture} of arms. This is in sharp contrast to the convention in the multi-arm bandit algorithms that there is generally a \\emph{solitary} arm that maximizes the utility, whether purely reward-centric or risk-sensitive. This creates a major departure from the principles for designing bandit algorithms since there are uncountable mixture possibilities. The contributions of the paper are as follows: (i) it formalizes a general framework for risk-sensitive bandits, (ii) identifies standard risk-sensitive bandit models for which solitary arm selections is not optimal, (iii) and designs regret-efficient algorithms whose sampling strategies can accurately track optimal arm mixtures (when mixture is optimal) or the solitary arms (when solitary is optimal). The algorithms are shown to achieve a regret that scales according to $O((\\log T/T )^{\\nu})$, \nwhere $T$ is the horizon, and $\\nu>0$ is a riskmetric-specific constant.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Meltem Tatl\u0131;Arpan Mukherjee;Prashanth L. A.;Karthikeyan Shanmugam;Ali Tajer",
        "authorids": "~Meltem_Tatl\u01311;~Arpan_Mukherjee1;~Prashanth_L._A.1;~Karthikeyan_Shanmugam1;~Ali_Tajer1",
        "gender": "F;M;;M;M",
        "homepage": ";;;https://sites.google.com/corp/view/karthikeyan-shanmugam/;https://www.isg-rpi.com/",
        "dblp": ";;;;65/2830",
        "google_scholar": ";;;https://scholar.google.ca/citations?user=m4DyPcUAAAAJ;",
        "orcid": ";;;0009-0008-2879-5868;",
        "linkedin": "meltem-tatli;arpan-mukherjee-75b287b6/;;;",
        "or_profile": "~Meltem_Tatl\u01311;~Arpan_Mukherjee1;~Prashanth_L._A.1;~Karthikeyan_Shanmugam1;~Ali_Tajer1",
        "aff": "Rensselaer Polytechnic Institute;Imperial College London;;Google Deepmind;Rensselaer Polytechnic Institute",
        "aff_domain": "rpi.edu;ic.ac.uk;;google.com;rpi.edu",
        "position": "PhD student;Postdoc;;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\ntatl2025risksensitive,\ntitle={Risk-sensitive Bandits: Arm Mixture Optimality and Regret-efficient Algorithms},\nauthor={Meltem Tatl{\\i} and Arpan Mukherjee and Prashanth L. A. and Karthikeyan Shanmugam and Ali Tajer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=rlmg3gN0fr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rlmg3gN0fr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "s6ljg6uX9C",
        "title": "Learning Laplacian Positional Encodings for Heterophilous Graphs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we theoretically demonstrate that current graph positional encodings (PEs) are not beneficial and could potentially hurt performance in tasks involving heterophilous graphs, where nodes that are close tend to have different labels. This limitation is critical as many real-world networks exhibit heterophily, and even highly homophilous graphs can contain local regions of strong heterophily. To address this limitation, we propose Learnable Laplacian Positional Encodings (LLPE), a new PE that leverages the full spectrum of the graph Laplacian, enabling them to capture graph structure on both homophilous and heterophilous graphs. Theoretically, we prove LLPE's ability to approximate a general class of graph distances and demonstrate its generalization properties. Empirically, our evaluation on 12 benchmarks demonstrates that LLPE improves accuracy across a variety of GNNs, including graph transformers, by up to 35% and 14% on synthetic and real-world graphs, respectively. Going forward, our work represents a significant step towards developing PEs that effectively capture complex structures in heterophilous graphs.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Ito;Jiong Zhu;Dexiong Chen;Danai Koutra;Jenna Wiens",
        "authorids": "~Michael_Ito1;~Jiong_Zhu1;~Dexiong_Chen1;~Danai_Koutra1;~Jenna_Wiens1",
        "gender": "M;M;M;F;F",
        "homepage": "https://www.michaelito-ai.com/;https://www.jiongzhu.net;https://dexiong.me;http://web.eecs.umich.edu/~dkoutra/;http://www-personal.umich.edu/~wiensj/",
        "dblp": ";51/8525;240/6347;91/9987;63/10451",
        "google_scholar": ";KjGFQ0QAAAAJ;goM0yAIAAAAJ;https://scholar.google.com.tw/citations?user=bDrA1-8AAAAJ;fvEfKxkAAAAJ",
        "orcid": ";0000-0002-6145-3295;;0000-0002-3206-8179;0000-0002-1057-7722",
        "linkedin": ";;;;",
        "or_profile": "~Michael_Ito1;~Jiong_Zhu1;~Dexiong_Chen1;~Danai_Koutra1;~Jenna_Wiens1",
        "aff": "University of Michigan - Ann Arbor;Meta Facebook+Amazon;Max-Planck Institute;University of Michigan - Ann Arbor+Amazon;University of Michigan Ann Arbor",
        "aff_domain": "umich.edu;meta.com+amazon.com;mpg.de;umich.edu+amazon.com;umich.edu",
        "position": "PhD student;Research Scientist+Applied Scientist;Project leader;Associate Professor+Scholar;Associate Professor",
        "bibtex": "@inproceedings{\nito2025learning,\ntitle={Learning Laplacian Positional Encodings for Heterophilous Graphs},\nauthor={Michael Ito and Jiong Zhu and Dexiong Chen and Danai Koutra and Jenna Wiens},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=s6ljg6uX9C}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=s6ljg6uX9C",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "s74HpEDH4a",
        "title": "Personalized Convolutional Dictionary Learning of Physiological Time Series",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Human physiological signals tend to exhibit both global and local structures: the former are shared across a population, while the latter reflect inter-individual variability. \nFor instance, kinetic measurements of the gait cycle during locomotion present common characteristics, although idiosyncrasies may be observed due to biomechanical disposition or pathology.\nTo better represent datasets with local-global structure, this work extends Convolutional Dictionary Learning (CDL), a popular method for learning interpretable representations, or dictionaries, of time-series data.\nIn particular, we propose Personalized CDL (PerCDL), in which a local dictionary models local information as a personalized spatiotemporal transformation of a global dictionary.\nThe transformation is learnable and can combine operations such as time-warping and rotation.\nFormal computational and statistical guarantees for PerCDL are provided and its effectiveness on synthetic and real human locomotion data is demonstrated.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Axel Roques;Samuel Gruffaz;Kyurae Kim;Alain Oliviero Durmus;Laurent Oudre",
        "authorids": "~Axel_Roques1;~Samuel_Gruffaz1;~Kyurae_Kim1;~Alain_Oliviero_Durmus1;~Laurent_Oudre2",
        "gender": "M;M;;;",
        "homepage": ";https://www.linkedin.com/in/samuel-gruffaz;;;",
        "dblp": ";;;;",
        "google_scholar": ";Ndn1EvEAAAAJ;;;",
        "orcid": "0000-0001-6964-4435;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Axel_Roques1;~Samuel_Gruffaz1;~Kyurae_Kim1;~Alain_Oliviero_Durmus1;~Laurent_Oudre2",
        "aff": "Ecole Normale Superieure;Ecole Normale Superieure;;;",
        "aff_domain": "ens-paris-saclay.fr;ens-paris-saclay.fr;;;",
        "position": "PhD student;PhD student;;;",
        "bibtex": "@inproceedings{\nroques2025personalized,\ntitle={Personalized Convolutional Dictionary Learning of Physiological Time Series},\nauthor={Axel Roques and Samuel Gruffaz and Kyurae Kim and Alain Oliviero Durmus and Laurent Oudre},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=s74HpEDH4a}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=s74HpEDH4a",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sG1FnbNziT",
        "title": "TRADE: Transfer of Distributions between External Conditions with Normalizing Flows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modeling distributions that depend on external control parameters is a common scenario in diverse applications like molecular simulations, where system properties like temperature affect molecular configurations. Despite the relevance of these applications, existing solutions are unsatisfactory as they require severely restricted model architectures or rely on energy-based training, which is prone to instability. We introduce TRADE, which overcomes these limitations by formulating the learning process as a boundary value problem. By initially training the model for a specific condition using either i.i.d.~samples or backward KL training, we establish a boundary distribution. We then propagate this information across other conditions using the gradient of the unnormalized density with respect to the external parameter. This formulation, akin to the principles of physics-informed neural networks, allows us to efficiently learn parameter-dependent distributions without restrictive assumptions.\nExperimentally, we demonstrate that TRADE achieves excellent results in a wide range of applications, ranging from Bayesian inference and molecular simulations to physical lattice models.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Stefan Wahl;Armand Rousselot;Felix Draxler;Ullrich Koethe",
        "authorids": "~Stefan_Wahl1;~Armand_Rousselot1;~Felix_Draxler1;~Ullrich_Koethe1",
        "gender": "M;M;M;M",
        "homepage": ";;;https://hci.iwr.uni-heidelberg.de/vislearn/people/ullrich-koethe/",
        "dblp": ";304/8323;242/9148;15/809",
        "google_scholar": ";FqvjidYAAAAJ;rFbxDSAAAAAJ;gt-yaNMAAAAJ",
        "orcid": ";;0000-0003-0978-1539;0000-0001-6036-1287",
        "linkedin": "wahlstefan/;;felix-draxler/;",
        "or_profile": "~Stefan_Wahl1;~Armand_Rousselot1;~Felix_Draxler1;~Ullrich_Koethe1",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen+Ruprecht-Karls-Universit\u00e4t Heidelberg;Heidelberg University, Ruprecht-Karls-Universit\u00e4t Heidelberg;University of California, Irvine;Heidelberg University",
        "aff_domain": "uni-tuebingen.de+uni-heidelberg.de;iwr.uni-heidelberg.de;uci.edu;uni-heidelberg.de",
        "position": "PhD student+MS student;PhD student;Postdoc;Adjunct Professor",
        "bibtex": "@inproceedings{\nwahl2025trade,\ntitle={{TRADE}: Transfer of Distributions between External Conditions with Normalizing Flows},\nauthor={Stefan Wahl and Armand Rousselot and Felix Draxler and Ullrich Koethe},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sG1FnbNziT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sG1FnbNziT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sKm0wq8Rnb",
        "title": "On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction.\nBayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand.\nOver the past years, several papers have introduced sampling algorithms with corresponding theorems stating that they achieve this property. \nIn this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size. \nFurthermore, for most of the algorithms, we show that convergence to the correct distribution can be restored with a simple fix at the cost of increasing computational demand.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tim Rensmeyer;Oliver Niggemann",
        "authorids": "~Tim_Rensmeyer1;~Oliver_Niggemann2",
        "gender": "M;M",
        "homepage": "https://www.researchgate.net/profile/Tim-Rensmeyer;https://www.hsu-hh.de/imb/",
        "dblp": ";",
        "google_scholar": ";https://scholar.google.de/citations?user=RLEd5GgAAAAJ",
        "orcid": ";0000-0001-8747-3596",
        "linkedin": ";",
        "or_profile": "~Tim_Rensmeyer1;~Oliver_Niggemann2",
        "aff": ";Helmut Schmidt University / University of the Federal Armed Forces Hamburg ",
        "aff_domain": ";hsu-hh.de",
        "position": ";Full Professor",
        "bibtex": "@inproceedings{\nrensmeyer2025on,\ntitle={On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors},\nauthor={Tim Rensmeyer and Oliver Niggemann},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sKm0wq8Rnb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sKm0wq8Rnb",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sNiBbGKDDB",
        "title": "Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Autoencoders based on Graph Neural Networks (GNNs) have garnered significant attention in recent years for their ability to learn informative latent representations of complex topologies, such as graphs. Despite the prevalence of Graph Autoencoders, there has been limited focus on developing and evaluating explainable neural-based graph generative models specifically designed for signed networks. To address this gap, we propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE extracts node-level representations that express node memberships over distinct extreme profiles, referred to as archetypes, within the network. This is achieved by projecting the graph onto a learned polytope, which governs its polarization. The framework employs the Skellam distribution for analyzing signed networks combined with relational archetypal analysis and GNNs. Our experimental evaluation demonstrates the SGAAEs' capability to successfully infer node memberships over underlying latent structures while extracting competing communities. Additionally, we introduce the 2-level network polarization problem and show how SGAAE is able to characterize such a setting. The proposed model achieves high performance in different tasks of signed link prediction across four real-world datasets, outperforming several baseline models. Finally, SGAAE allows for interpretable visualizations in the polytope space, revealing the distinct aspects of the network, as well as, how nodes are expressing them.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nikolaos Nakis;Chrysoula Kosma;Giannis Nikolentzos;Michail Chatzianastasis;Iakovos Evdaimon;Michalis Vazirgiannis",
        "authorids": "~Nikolaos_Nakis1;~Chrysoula_Kosma1;~Giannis_Nikolentzos1;~Michail_Chatzianastasis1;~Iakovos_Evdaimon1;~Michalis_Vazirgiannis1",
        "gender": ";F;M;M;M;M",
        "homepage": "https://nicknakis.github.io/nnaknik/;;http://users.uop.gr/~nikolentzos/;https://michailchatzianastasis.github.io/;https://iakovosevdaimon.github.io/cv/;",
        "dblp": "318/2791.html;325/4681;163/6278;292/4296;;v/MVazirgiannis",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?view_op=list_works;bdom4I8AAAAJ;e0HbE2YAAAAJ;Gkh4ulEAAAAJ;https://scholar.google.gr/citations?user=aWGJYcMAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "nikolaos-nakis-67a07a147/;chrykosma/;;michail-chatzianastasis-799ab3153/;;",
        "or_profile": "~Nikolaos_Nakis1;~Chrysoula_Kosma1;~Giannis_Nikolentzos1;~Michail_Chatzianastasis1;~Iakovos_Evdaimon1;~Michalis_Vazirgiannis1",
        "aff": "Yale University;Paris-Saclay;University of Peloponnese;;Ecole Polytechnique;Ecole Polytechnique, France",
        "aff_domain": "yale.edu;ens-paris-saclay.fr;uop.gr;;polytechnique.edu;polytechnique.fr",
        "position": "Postdoc;Postdoc;Assistant Professor;;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nnakis2025signed,\ntitle={Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings},\nauthor={Nikolaos Nakis and Chrysoula Kosma and Giannis Nikolentzos and Michail Chatzianastasis and Iakovos Evdaimon and Michalis Vazirgiannis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sNiBbGKDDB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sNiBbGKDDB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sQwftWSlmI",
        "title": "Inverse Optimization with Prediction Market: A Characterization of Scoring Rules for Elciting System States",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Inverse optimization aims to recover the unknown state in forward optimization after observing a state-outcome pair. This is relevant when we want to identify the underlying state of a system or to design a system with desirable outcomes. Whereas inverse optimization has been investigated in the algorithmic perspective during past two decades, its formulation intimately tied with the principal's subjective choice of a desirable state---indeed, this is crucial to make the inverse problem well-posed. We go beyond the conventional inverse optimization by building upon prediction market, where multiple agents submit their beliefs until converging to market equilibria. The market equilibria express the crowd consensus on a desirable state, effectively eschewing the subjective design. To this end, we derive a proper scoring rule for prediction market design in the context of inverse optimization.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Han Bao;Shinsaku Sakaue",
        "authorids": "~Han_Bao2;~Shinsaku_Sakaue1",
        "gender": "M;M",
        "homepage": "https://hermite.jp/;https://ssakaue.github.io/",
        "dblp": "120/1444-2;183/6350",
        "google_scholar": "MqMzjeMAAAAJ;https://scholar.google.co.jp/citations?user=9oTbrmEAAAAJ",
        "orcid": "0000-0002-4473-2604;",
        "linkedin": ";",
        "or_profile": "~Han_Bao2;~Shinsaku_Sakaue1",
        "aff": "The Institute of Statistical Mathematics;CyberAgent, Inc.+The University of Tokyo+NTT",
        "aff_domain": "ism.ac.jp;cyberagent.co.jp+u-tokyo.ac.jp+ntt.co.jp",
        "position": "Associate Professor;Researcher+Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\nbao2025inverse,\ntitle={Inverse Optimization with Prediction Market: A Characterization of Scoring Rules for Elciting System States},\nauthor={Han Bao and Shinsaku Sakaue},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sQwftWSlmI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sQwftWSlmI",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sWLeL16ahT",
        "title": "A Subquadratic Time Approximation Algorithm for Individually Fair k-Center",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the $k$-center problem in the context of individual fairness.\nLet $P$ be a set of $n$  points in a metric space and $r_x$ be the distance between $x \\in P$ and its $\\lceil n/k \\rceil$-th nearest neighbor. The problem asks to optimize the $k$-center objective under the constraint that, for every point $x$, there is a center within distance $r_x$. \nWe give bicriteria $(\\beta,\\gamma)$-approximation algorithms that compute clusterings such that every point $x \\in P$ has a center within distance $\\beta r_x$ and the clustering cost is at most $\\gamma$ times the optimal cost.\nOur main contributions are a deterministic $O(n^2+ kn \\log n)$ \ntime $(2,2)$-approximation algorithm and a randomized $O(nk\\log(n/\\delta)+k^2/\\varepsilon)$ time $(10,2+\\varepsilon)$-approximation algorithm, where $\\delta$ denotes the failure probability. For the latter, we develop a randomized sampling procedure to compute constant factor approximations for the values $r_x$ for all $x\\in P$ in subquadratic time; we believe this procedure to be of independent interest within the context of individual fairness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Matthijs Ebbens;Nicole Funk;Jan H\u00f6ckendorff;Christian Sohler;Vera Weil",
        "authorids": "ymebbens@gmail.com;~Nicole_Funk1;~Jan_H\u00f6ckendorff1;~Christian_Sohler1;weil@cs.uni-koeln.de",
        "gender": ";F;;M;",
        "homepage": ";;;;",
        "dblp": ";;;47/2482;",
        "google_scholar": ";;;;",
        "orcid": ";0000-0002-7826-9291;0000-0002-7513-8501;;",
        "linkedin": ";;;;",
        "or_profile": "ymebbens@gmail.com;~Nicole_Funk1;~Jan_H\u00f6ckendorff1;~Christian_Sohler1;weil@cs.uni-koeln.de",
        "aff": ";Universit\u00e4t K\u00f6ln;Universit\u00e4t K\u00f6ln;University of Cologne;",
        "aff_domain": ";uni-koeln.de;uni-koeln.de;uni-koeln.de;",
        "position": ";PhD student;PhD student;Professor;",
        "bibtex": "@inproceedings{\nhockendorff2025a,\ntitle={A Subquadratic Time Approximation Algorithm for Individually Fair k-Center},\nauthor={Jan H{\\\"o}ckendorff and Christian Sohler and Nicole Funk and Vera Weil and Matthijs Ebbens},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sWLeL16ahT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sWLeL16ahT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sWg9NuypBm",
        "title": "Weighted Sum of Gaussian Process Latent Variable Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. \nOur key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) for the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations.\nOur framework allows arbitrary non-linear variations in the signals while being able to incorporate useful priors for the linear weights, such as summing-to-one. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample.\nTo demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy\ndataset with varying temperatures, a simulated dataset for identifying flow configuration\nthrough a pipe, and a dataset for determining the type of rock from its reflectance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "James A C Odgers;Ruby Sedgwick;Chrysoula Dimitra Kappatou;Ruth Misener;Sarah Lucie Filippi",
        "authorids": "~James_A_C_Odgers1;~Ruby_Sedgwick1;~Chrysoula_Dimitra_Kappatou1;~Ruth_Misener1;~Sarah_Lucie_Filippi1",
        "gender": "M;F;F;F;F",
        "homepage": "https://jamesacodgers.github.io/;https://rsedgwick.github.io/;;https://wp.doc.ic.ac.uk/rmisener/;",
        "dblp": "362/5903;;;04/8800;",
        "google_scholar": ";940HguIAAAAJ;wJO-SVEAAAAJ;AQxtWHoAAAAJ;HhMJevQAAAAJ",
        "orcid": "0000-0002-6436-9267;;;0000-0001-5612-5417;",
        "linkedin": ";;https://linkedin.com/in/cdkappatou;ruth-misener/;",
        "or_profile": "~James_A_C_Odgers1;~Ruby_Sedgwick1;~Chrysoula_Dimitra_Kappatou1;~Ruth_Misener1;~Sarah_Lucie_Filippi1",
        "aff": "Helmholtz Zentrum M\u00fcnchen+Imperial College London;Imperial College London;Imperial College London;Amazon+Imperial College London;Imperial College London",
        "aff_domain": "helmholtz-munich.de+ic.ac.uk;imperial.ac.uk;imperial.ac.uk;amazon.co.uk+imperial.ac.uk;imperial.ac.uk",
        "position": "Postdoc+PhD student;Postdoc;Postdoc;Researcher+Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nodgers2025weighted,\ntitle={Weighted Sum of Gaussian Process Latent Variable Models},\nauthor={James A C Odgers and Ruby Sedgwick and Chrysoula Dimitra Kappatou and Ruth Misener and Sarah Lucie Filippi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sWg9NuypBm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sWg9NuypBm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sky7bMXCaB",
        "title": "SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Community detection is a fundamental task in graph analysis, with methods often relying on fitting models like the Stochastic Block Model (SBM) to observed networks. While many algorithms can accurately estimate SBM parameters when the input graph is a perfect sample from the model, real-world graphs rarely conform to such idealized assumptions. Therefore, robust algorithms are crucial---ones that can recover model parameters even when the data deviates from the assumed distribution. In this work, we propose SubSearch, an algorithm for robustly estimating SBM parameters by exploring the space of subgraphs in search of one that closely aligns with the model's assumptions. Our approach also functions as an outlier detection method, properly identifying nodes responsible for the graph's deviation from the model and going beyond simple techniques like pruning high-degree nodes. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Leonardo Bianco;Christine Keribin;Zacharie Naulet",
        "authorids": "~Leonardo_Bianco1;christine.keribin.psud@gmail.com;~Zacharie_Naulet1",
        "gender": "M;;M",
        "homepage": ";;https://www.imo.universite-paris-saclay.fr/~naulet/",
        "dblp": ";;",
        "google_scholar": "_A3mDioAAAAJ;;",
        "orcid": ";;",
        "linkedin": "leonardo-bianco/;;",
        "or_profile": "~Leonardo_Bianco1;christine.keribin.psud@gmail.com;~Zacharie_Naulet1",
        "aff": "Universit\u00e9 Paris-Saclay;;Universite Paris Saclay",
        "aff_domain": "universite-paris-saclay.fr;;universite-paris-saclay.fr",
        "position": "PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nbianco2025subsearch,\ntitle={SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search},\nauthor={Leonardo Bianco and Christine Keribin and Zacharie Naulet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=sky7bMXCaB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sky7bMXCaB",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "suKXusb4MS",
        "title": "FLIPHAT: Joint Differential Privacy for High Dimensional Linear Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant. Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data. First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting. To further address the problem, we design a computationally efficient bandit algorithm, **F**orgetfu**L** **I**terative **P**rivate **HA**rd **T**hresholding (FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality. We show that FLIPHAT achieves optimal regret in terms of privacy parameters, context dimension, and time horizon up to a linear factor in model sparsity in the problem independent case. We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Saptarshi Roy;Sunrit Chakraborty;Debabrota Basu",
        "authorids": "~Saptarshi_Roy1;~Sunrit_Chakraborty1;~Debabrota_Basu1",
        "gender": "M;M;",
        "homepage": "https://sites.google.com/umich.edu/saptarshi-roys-home-page/home?authuser=1;https://lsa.umich.edu/stats/people/phd-students/sunritc.html;https://debabrota-basu.github.io/",
        "dblp": ";;126/2209",
        "google_scholar": "Ywix3OUAAAAJ;p6QwiP4AAAAJ;https://scholar.google.co.in/citations?user=e26Maa4AAAAJ",
        "orcid": "0000-0003-4183-205X;;",
        "linkedin": ";;",
        "or_profile": "~Saptarshi_Roy1;~Sunrit_Chakraborty1;~Debabrota_Basu1",
        "aff": "University of Texas at Austin;University of Michigan - Ann Arbor;INRIA",
        "aff_domain": "utexas.edu;umich.edu;inria.fr",
        "position": "Postdoc;PhD student;Faculty",
        "bibtex": "@inproceedings{\nroy2025fliphat,\ntitle={{FLIPHAT}: Joint Differential Privacy for High Dimensional Linear Bandits},\nauthor={Saptarshi Roy and Sunrit Chakraborty and Debabrota Basu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=suKXusb4MS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=suKXusb4MS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "t1HteHSSfe",
        "title": "Restructuring Tractable Probabilistic Circuits",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "Probabilistic circuits (PCs) are a unifying representation for probabilistic models that support tractable inference. Numerous applications of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algorithms require that the circuits respect the same structure, i.e. variable scopes decomposes according to the same vtree. In this work, we propose and study the task of restructuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We propose a generic approach for this problem and show that it leads to novel polynomial-time algorithms for multiplying circuits respecting different vtrees, as well as a practical depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling efficient inference by changing their structures at inference time.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Honghua Zhang;Benjie Wang;Marcelo Arenas;Guy Van den Broeck",
        "authorids": "~Honghua_Zhang1;~Benjie_Wang1;~Marcelo_Arenas1;~Guy_Van_den_Broeck1",
        "gender": "M;;M;",
        "homepage": "http://web.cs.ucla.edu/~hzhang19/;https://web.cs.ucla.edu/~benjiewang/;https://marceloarenas.cl/;",
        "dblp": "65/6130;255/7169;76/6735;",
        "google_scholar": "2qxBYJUAAAAJ;https://scholar.google.co.uk/citations?user=tOeUlJoAAAAJ;YHR0wkkAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Honghua_Zhang1;~Benjie_Wang1;~Marcelo_Arenas1;~Guy_Van_den_Broeck1",
        "aff": "University of California, Los Angeles;University of California, Los Angeles;RelationalAI+Pontificia Universidad Catolica de Chile+Pontificia Universidad Catolica de Chile;",
        "aff_domain": "cs.ucla.edu;ucla.edu;relational.ai+uc.cl+puc.cl;",
        "position": "PhD student;Postdoc;Researcher+Full Professor+Full Professor;",
        "bibtex": "@inproceedings{\nzhang2025restructuring,\ntitle={Restructuring Tractable Probabilistic Circuits},\nauthor={Honghua Zhang and Benjie Wang and Marcelo Arenas and Guy Van den Broeck},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=t1HteHSSfe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=t1HteHSSfe",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "t2pBdWt99n",
        "title": "General Staircase Mechanisms for Optimal Differential Privacy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We derive the optimal differentially private additive noise mechanism for queries in $\\mathbb{R}^d$ when sensitivity and error are defined by an arbitrary norm $||\\cdot||_K$. The optimal mechanism is a generalization of the staircase mechanism, which is known to be optimal under the $\\ell_1$ norm when $d \\leq 2$; we extend the mechanism and its guarantee to arbitrary norms and dimensions, proving a conjecture of Geng et al. [2015] along the way. The generalized staircase mechanism we derive can be viewed as a refinement of the $K$-norm mechanism of Hardt and Talwar [2010] , with improvements particularly evident in the low-privacy regime as $\\epsilon \\to \\infty$. We show how to implement the generalized staircase mechanism efficiently, given an efficient algorithm for sampling the unit $K$-norm ball, and demonstrate that it significantly reduces error in realistic settings, including under non-standard norms that are common in practice, and across a range of error metrics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alex Kulesza;Ananda Theertha Suresh;Yuyan Wang",
        "authorids": "~Alex_Kulesza2;~Ananda_Theertha_Suresh1;~Yuyan_Wang1",
        "gender": ";M;F",
        "homepage": "http://www.alexkulesza.com/;https://theertha.info;",
        "dblp": "61/4512;119/3884;",
        "google_scholar": "2OUGYFAAAAAJ;K6ef57QAAAAJ;JvSO3Q0AAAAJ",
        "orcid": ";;",
        "linkedin": ";;yuyan-wang-670a55199/",
        "or_profile": "~Alex_Kulesza2;~Ananda_Theertha_Suresh1;~Yuyan_Wang1",
        "aff": "Google;Google;Google",
        "aff_domain": "google.com;google.com;google.com",
        "position": "Research Scientist;Research Scientist;Researcher",
        "bibtex": "@inproceedings{\nkulesza2025general,\ntitle={General Staircase Mechanisms for Optimal Differential Privacy},\nauthor={Alex Kulesza and Ananda Theertha Suresh and Yuyan Wang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=t2pBdWt99n}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=t2pBdWt99n",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "t6TgZYdWFF",
        "title": "Flexible and Efficient Probabilistic PDE Solvers through Gaussian Markov Random Fields",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mechanistic knowledge about the physical world is virtually always expressed via partial differential equations (PDEs).\n  Recently, there has been a surge of interest in probabilistic PDE solvers---Bayesian statistical models mostly based on Gaussian process (GP) priors which seamlessly combine empirical measurements and mechanistic knowledge.\n  As such, they quantify uncertainties arising from e.g. noisy or missing data, unknown PDE parameters or discretization error by design.\n  Prior work has established connections to classical PDE solvers and provided solid theoretical guarantees.\n  However, scaling such methods to large-scale problems remains a fundamental challenge primarily due to dense covariance matrices.\n  Our approach addresses the scalability issues by leveraging the Markov property of many commonly used GP priors.\n  It has been shown that such priors are solutions to stochastic PDEs (SPDEs) which when discretized allow for highly efficient GP regression through sparse linear algebra.\n  In this work, we show how to leverage this prior class to make probabilistic PDE solvers practical, even for large-scale nonlinear PDEs, through greatly accelerated inference mechanisms.\n  Additionally, our approach also allows for flexible and physically meaningful priors beyond what can be modeled with covariance functions.\n  Experiments confirm substantial speedups and accelerated convergence of our physics-informed priors in nonlinear settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tim Weiland;Marvin Pf\u00f6rtner;Philipp Hennig",
        "authorids": "~Tim_Weiland1;~Marvin_Pf\u00f6rtner1;~Philipp_Hennig1",
        "gender": "M;;M",
        "homepage": "https://timwei.land;;http://mml.inf.uni-tuebingen.de",
        "dblp": ";;08/9077",
        "google_scholar": ";;https://scholar.google.de/citations?user=UeG5w08AAAAJ",
        "orcid": ";;0000-0001-7293-6092",
        "linkedin": ";;",
        "or_profile": "~Tim_Weiland1;~Marvin_Pf\u00f6rtner1;~Philipp_Hennig1",
        "aff": ";;University of T\u00fcbingen",
        "aff_domain": ";;uni-tuebingen.de",
        "position": ";;Full Professor",
        "bibtex": "@inproceedings{\nweiland2025flexible,\ntitle={Flexible and Efficient Probabilistic {PDE} Solvers through Gaussian Markov Random Fields},\nauthor={Tim Weiland and Marvin Pf{\\\"o}rtner and Philipp Hennig},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=t6TgZYdWFF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=t6TgZYdWFF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "t7J3l7RwGA",
        "title": "Legitimate ground-truth-free metrics for deep uncertainty classification scoring",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite the increasing demand for safer machine learning practices, the use of Uncertainty Quantification (UQ) methods in production remains limited. This limitation is exacerbated by the challenge of validating UQ methods in absence of UQ ground truth. \nIn classification tasks, when only a usual set of test data is at hand, several authors suggested different metrics that can be computed from such test points while assessing the quality of quantified uncertainties. \nThis paper investigates such metrics and proves that they are theoretically well-behaved and actually tied to some uncertainty ground truth which is easily interpretable in terms of model prediction trustworthiness ranking. \nEquipped with those new results, and given the applicability of those metrics in the usual supervised paradigm, we argue that our contributions will help promoting a broader use of UQ in deep learning.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arthur Pignet;Chiara Regniez;John Klein",
        "authorids": "~Arthur_Pignet1;~Chiara_Regniez1;~John_Klein1",
        "gender": "M;F;",
        "homepage": ";;https://john-klein.github.io",
        "dblp": ";;https://dblp2.uni-trier.de/search/publ?q=author%3AJohn_Klein%3Acolot%7Cdesterscke%7Cpasquier%7Cbas%7Clecomte",
        "google_scholar": "ASYImeMAAAAJ;;2KarUKwAAAAJ",
        "orcid": ";;0000-0002-0179-1145",
        "linkedin": ";chiara-r%C3%A9gniez-6a8375170/;",
        "or_profile": "~Arthur_Pignet1;~Chiara_Regniez1;~John_Klein1",
        "aff": "Owkin Inc.;Columbia University;University of Lille",
        "aff_domain": "owkin.com;columbia.edu;univ-lille.fr",
        "position": "Researcher;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\npignet2025legitimate,\ntitle={Legitimate ground-truth-free metrics for deep uncertainty classification scoring},\nauthor={Arthur Pignet and Chiara Regniez and John Klein},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=t7J3l7RwGA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=t7J3l7RwGA",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tAoR5gUqeX",
        "title": "TempTest: Local Normalization Distortion and the Detection of Machine-generated Text",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing methods for the zero-shot detection of machine-generated text are dominated by three statistical quantities: log-likelihood, log rank, and entropy. As language models mimic the distribution of human text ever closer, this will limit our ability to build effective detection algorithms. To combat this, we introduce a method for detecting machine-generated text that is entirely agnostic of the generating language model. This is achieved by targeting a defect in the way that decoding strategies, such as temperature or top-k sampling, normalize conditional probability measures. This method can be rigorously theoretically justified, is easily explainable, and is conceptually distinct from existing methods for detecting machine-generated text. We evaluate our detector in the white and black box settings across various language models, datasets, and passage lengths. We also study the effect of paraphrasing attacks on our detector and the extent to which it is biased against non-native speakers. In each of these settings, the performance of our test is at least comparable to that of other state-of-the-art text detectors, and in some cases, we strongly outperform these baselines.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tom Kempton;Stuart Burrell;Connor J Cheverall",
        "authorids": "~Tom_Kempton1;~Stuart_Burrell1;~Connor_J_Cheverall1",
        "gender": "M;M;",
        "homepage": "https://personalpages.manchester.ac.uk/staff/thomas.kempton/;https://stuartburrell.github.io/;",
        "dblp": "402/4148;361/5101;",
        "google_scholar": "https://scholar.google.nl/citations?user=wOstJyUAAAAJ;https://scholar.google.co.uk/citations?user=DJ74QYQAAAAJ;XzBI0oYAAAAJ",
        "orcid": "0000-0003-3326-2472;0000-0002-6333-1750;0009-0005-4330-7197",
        "linkedin": "tom-kempton-41244822a/;stuart-burrell/;",
        "or_profile": "~Tom_Kempton1;~Stuart_Burrell1;~Connor_J_Cheverall1",
        "aff": "University of Manchester;Visa Inc.+Featurespace;University of Cambridge",
        "aff_domain": "manchester.ac.uk;visa.com+featurespace.co.uk;cam.ac.uk",
        "position": "Associate Professor;Manager, AI Research and Innovation+Lead Research Engineer;PhD student",
        "bibtex": "@inproceedings{\nkempton2025temptest,\ntitle={TempTest: Local Normalisation Distortion and the Detection of Machine Generated Text},\nauthor={Tom Kempton and Stuart Burrell and Connor J Cheverall},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tAoR5gUqeX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tAoR5gUqeX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tBcxoC05At",
        "title": "Regularity in Canonicalized Models: A Theoretical Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In learning with invariances (or symmetries), canonicalization is a widely used technique that projects data onto a smaller subset of the input space to reduce associated redundancies. The transformed dataset is then processed through a function from a designated function class to obtain the final invariant representation.\nAlthough canonicalization is often simple and flexible, both theoretical and empirical evidence suggests that the projection map can be discontinuous and unstable, which poses challenges for machine learning applications. However, the overall end-to-end representation can still remain continuous.\nFocusing on the importance of end-to-end regularity rather than the projection mapping itself, this paper explores the continuity and regularity of canonicalized models from a theoretical perspective. In a broad setting of input spaces and group actions, we establish necessary and sufficient conditions for the continuity or regularity of canonicalized models of any order, thereby characterizing the minimal conditions required for stability.\nTo our knowledge, this represents the first comprehensive investigation into the end-to-end regularity of canonicalized models, offering critical insights into their design and application, as well as guidance for enhancing stability in practical settings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Behrooz Tahmasebi;Stefanie Jegelka",
        "authorids": "~Behrooz_Tahmasebi1;~Stefanie_Jegelka3",
        "gender": "M;F",
        "homepage": "https://people.csail.mit.edu/bzt/;http://people.csail.mit.edu/stefje/",
        "dblp": "223/0884;38/7003",
        "google_scholar": "ZXCO3DMAAAAJ;gTWUZlsAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Behrooz_Tahmasebi1;~Stefanie_Jegelka3",
        "aff": "School of Engineering and Applied Sciences, Harvard University+Massachusetts Institute of Technology;Technische Universit\u00e4t M\u00fcnchen+Massachusetts Institute of Technology",
        "aff_domain": "seas.harvard.edu+mit.edu;tum.de+mit.edu",
        "position": "Postdoc+PhD student;Full Professor+Associate Professor",
        "bibtex": "@inproceedings{\ntahmasebi2025regularity,\ntitle={Regularity in Canonicalized Models: A Theoretical Perspective},\nauthor={Behrooz Tahmasebi and Stefanie Jegelka},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tBcxoC05At}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tBcxoC05At",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tYVDkoymXT",
        "title": "Optimal Stochastic Trace Estimation in Generative Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Hutchinson estimators are widely employed in training divergence-based likelihoods for diffusion models to ensure optimal transport (OT) properties. However, this estimator often suffers from high variance and scalability concerns. To address these challenges, we investigate Hutch++, an optimal stochastic trace estimator for generative models, designed to minimize training variance while maintaining transport optimality. Hutch++ is particularly effective for handling ill-conditioned matrices with large condition numbers, which commonly arise when high-dimensional data exhibits a low-dimensional structure. To mitigate the need for frequent and costly QR decompositions, we propose practical schemes that balance frequency and accuracy, backed by theoretical guarantees. Our analysis demonstrates that Hutch++ leads to generations of higher quality. Furthermore, this method exhibits effective variance reduction in various applications, including simulations, conditional time series forecasts, and image generation.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinyang Liu;Hengrong Du;Wei Deng;Ruqi Zhang",
        "authorids": "~Xinyang_Liu4;~Hengrong_Du1;~Wei_Deng1;~Ruqi_Zhang1",
        "gender": "M;M;M;F",
        "homepage": "https://xinyangatk.github.io;https://hengrongdu.netlify.app/;https://waynedw.github.io/;https://ruqizhang.github.io/",
        "dblp": ";366/8373;69/508-2;",
        "google_scholar": "https://scholar.google.com.hk/citations?hl=zh-CN;OtzsCPcAAAAJ;IYiyxssAAAAJ;4ojpmc8AAAAJ",
        "orcid": ";0000-0003-2392-8963;;",
        "linkedin": ";;;",
        "or_profile": "~Xinyang_Liu4;~Hengrong_Du1;~Wei_Deng1;~Ruqi_Zhang1",
        "aff": "University of Texas at Austin;University of California, Irvine;Morgan Stanley;Purdue University",
        "aff_domain": "utexas.edu;uci.edu;morganstanley.com;purdue.edu",
        "position": "PhD student;Assistant Professor;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025optimal,\ntitle={Optimal Stochastic Trace Estimation in Generative Modeling},\nauthor={Xinyang Liu and Hengrong Du and Wei Deng and Ruqi Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tYVDkoymXT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tYVDkoymXT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tnLuSQIlJY",
        "title": "Causal Discovery on Dependent Binary Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The assumption of independence between observations (units) in a dataset is prevalent across various methodologies for learning causal graphical models. However, this assumption often finds itself in conflict with real-world data, posing challenges to accurate structure learning. We propose a decorrelation-based approach for causal graph learning on dependent binary data, where the local conditional distribution is defined by a latent utility model with dependent errors across units. We develop a pairwise maximum likelihood method to estimate the covariance matrix for the dependence among the units. Then, leveraging the estimated covariance matrix, we develop an EM-like iterative algorithm to generate and de-correlate samples of the latent utility variables, which serve as de-correlated data. Any standard causal discovery method can be applied on the de-correlated data to learn the underlying causal graph. We demonstrate that the proposed de-correlation approach significantly improves the accuracy in causal graph learning, through numerical experiments on both synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alex Chen;Qing Zhou",
        "authorids": "~Alex_Chen1;~Qing_Zhou1",
        "gender": "M;",
        "homepage": ";https://faculty.stat.ucla.edu/zhou/",
        "dblp": ";",
        "google_scholar": ";lS1d6mwAAAAJ",
        "orcid": ";",
        "linkedin": "alex-chen-86513714a/;",
        "or_profile": "~Alex_Chen1;~Qing_Zhou1",
        "aff": "University of California, Los Angeles;University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nchen2025causal,\ntitle={Causal Discovery on Dependent Binary Data},\nauthor={Alex Chen and Qing Zhou},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tnLuSQIlJY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tnLuSQIlJY",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tp3Aw6t0QF",
        "title": "Unifying Feature-Based Explanations with Functional ANOVA and Cooperative Game Theory",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Feature-based explanations, using perturbations or gradients, are a prevalent tool to understand decisions of black box machine learning models. Yet, differences between these methods still remain mostly unknown, which limits their applicability for practitioners. In this work, we introduce a unified framework for local and global feature-based explanations using two well-established concepts: functional ANOVA (fANOVA) from statistics, and the notion of value and interaction from cooperative game theory. We introduce three fANOVA decompositions that determine the influence of feature distributions, and use game-theoretic measures, such as the Shapley value and interactions, to specify the influence of higher-order interactions. Our framework combines these two dimensions to uncover similarities and differences between a wide range of explanation techniques for features and groups of features. We then empirically showcase the usefulness of our framework on synthetic and real-world datasets.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fabian Fumagalli;Maximilian Muschalik;Eyke H\u00fcllermeier;Barbara Hammer;Julia Herbinger",
        "authorids": "~Fabian_Fumagalli1;~Maximilian_Muschalik1;~Eyke_H\u00fcllermeier1;~Barbara_Hammer4;~Julia_Herbinger1",
        "gender": "M;M;M;F;F",
        "homepage": "https://hammer-lab.techfak.uni-bielefeld.de/people/316634936/;https://maxmuschalik.com/;https://cs.uni-paderborn.de/index.php?id=60202;https://www.techfak.uni-bielefeld.de/~bhammer/;https://www.slds.stat.uni-muenchen.de/people/herbinger/",
        "dblp": "329/4508;329/4090;h/EykeHullermeier;h/BarbaraHammer;",
        "google_scholar": "anUMB08AAAAJ;https://scholar.google.de/citations?user=jJBCW74AAAAJ;https://scholar.google.de/citations?user=usVJeNN3xFAC;1d3OxaUAAAAJ;",
        "orcid": "0000-0003-3955-3510;0000-0002-6921-0204;0000-0002-9944-4108;0000-0002-2615-8151;",
        "linkedin": "fabian-fumagalli/;maximilian-muschalik/;;;",
        "or_profile": "~Fabian_Fumagalli1;~Maximilian_Muschalik1;~Eyke_H\u00fcllermeier1;~Barbara_Hammer4;~Julia_Herbinger1",
        "aff": "Universit\u00e4t Bielefeld;Institute of Computer Science, Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen;Universit\u00e4t Bielefeld;ATB Potsdam",
        "aff_domain": "uni-bielefeld.de;ifi.lmu.de;lmu.de;uni-bielefeld.de;atb-potsdam.de",
        "position": "PhD student;PhD student;Full Professor;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nfumagalli2025unifying,\ntitle={Unifying Feature-Based Explanations with Functional {ANOVA} and Cooperative Game Theory},\nauthor={Fabian Fumagalli and Maximilian Muschalik and Eyke H{\\\"u}llermeier and Barbara Hammer and Julia Herbinger},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tp3Aw6t0QF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tp3Aw6t0QF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tpndZWB0Hj",
        "title": "Best-Arm Identification in Unimodal Bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the fixed-confidence best-arm identification problem in unimodal bandits, in which the means of the arms increase with the index of the arm up to their maximum, then decrease. We derive two lower bounds on the stopping time of any algorithm. The instance-dependent lower bound suggests that due to the unimodal structure, only three arms contribute to the leading confidence-dependent cost. However, a worst-case lower bound shows that a linear dependence on the number of arms is unavoidable in the confidence-independent cost. We propose modifications of Track-and-Stop and a Top Two algorithm that leverage the unimodal structure. Both versions of Track-and-Stop are asymptotically optimal for one-parameter exponential families. The Top Two algorithm is asymptotically near-optimal for Gaussian distributions and we prove a non-asymptotic guarantee matching the worse-case lower bound. The algorithms can be implemented efficiently and we demonstrate their competitive empirical performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Riccardo Poiani;Marc Jourdan;Emilie Kaufmann;R\u00e9my Degenne",
        "authorids": "~Riccardo_Poiani3;~Marc_Jourdan1;~Emilie_Kaufmann1;~R\u00e9my_Degenne1",
        "gender": "M;M;F;M",
        "homepage": ";https://marcjourdan.github.io;https://emiliekaufmann.github.io/;https://remydegenne.github.io/",
        "dblp": "268/8198;228/8157;67/11350;157/1070",
        "google_scholar": "WQWOAkkAAAAJ;BOXGjhgAAAAJ;9GE1vx4AAAAJ;https://scholar.google.fr/citations?user=H-uIBOwAAAAJ",
        "orcid": ";0000-0002-2449-4549;;",
        "linkedin": ";marc-jourdan/;;",
        "or_profile": "~Riccardo_Poiani3;~Marc_Jourdan1;~Emilie_Kaufmann1;~R\u00e9my_Degenne1",
        "aff": "Bocconi University;EPFL - EPF Lausanne;CNRS;INRIA",
        "aff_domain": "unibocconi.it;epfl.ch;cnrs.fr;inria.fr",
        "position": "Postdoc;Postdoc;Researcher;Researcher",
        "bibtex": "@inproceedings{\npoiani2025bestarm,\ntitle={Best-Arm Identification in Unimodal Bandits},\nauthor={Riccardo Poiani and Marc Jourdan and Emilie Kaufmann and R{\\'e}my Degenne},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tpndZWB0Hj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tpndZWB0Hj",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tpxxNenfDT",
        "title": "TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose TVineSynth, a vine copula based synthetic tabular data generator, which is designed to balance privacy and utility, using the vine tree structure and its truncation to do the trade-off. Contrary to synthetic data generators that achieve DP by globally adding noise, TVineSynth performs a controlled approximation of the estimated data generating distribution, so that it does not suffer from poor utility of the resulting synthetic data for downstream prediction tasks. TVineSynth introduces a targeted bias into the vine copula model that, combined with the specific tree structure of the vine, causes the model to zero out privacy-leaking dependencies while relying on those that are beneficial for utility. Privacy is here measured with membership (MIA) and attribute inference attacks (AIA). Further, we theoretically justify how the construction of TVineSynth ensures AIA privacy under a natural privacy measure for continuous sensitive attributes. When compared to competitor models, with and without DP, on simulated and on real-world data, TVineSynth achieves a superior privacy-utility balance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Elisabeth Griesbauer;Claudia Czado;Arnoldo Frigessi;Ingrid Hob\u00e6k Haff",
        "authorids": "~Elisabeth_Griesbauer1;~Claudia_Czado1;~Arnoldo_Frigessi1;~Ingrid_Hob\u00e6k_Haff1",
        "gender": "Not Specified;;M;F",
        "homepage": "https://www.med.uio.no/imb/?vrtx=person-view&uid=elismg&lang=en;https://www.math.cit.tum.de/en/math/people/professors/czado-claudia/;https://www.med.uio.no/imb/english/people/aca/frigessi/;",
        "dblp": ";;;",
        "google_scholar": ";https://scholar.google.com/citations?hl=de;;",
        "orcid": "0009-0002-8268-775X;;;0009-0005-5396-9696",
        "linkedin": "elisabeth-griesbauer-38479028a;;;",
        "or_profile": "~Elisabeth_Griesbauer1;~Claudia_Czado1;~Arnoldo_Frigessi1;~Ingrid_Hob\u00e6k_Haff1",
        "aff": "University of Oslo;Technische Universit\u00e4t M\u00fcnchen;University of Oslo;University of Oslo",
        "aff_domain": "uio.no;tum.de;uio.no;uio.no",
        "position": "PhD student;Associate Professor;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\ngriesbauer2025tvinesynth,\ntitle={{TV}ineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular Data to Balance Privacy and Utility},\nauthor={Elisabeth Griesbauer and Claudia Czado and Arnoldo Frigessi and Ingrid Hob{\\ae}k Haff},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tpxxNenfDT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tpxxNenfDT",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tuHVCBN5fw",
        "title": "Spectral Differential Network Analysis for High-Dimensional Time Series",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Spectral networks derived from multivariate time series data arise in many domains, from brain science to Earth science. Often, it is of interest to study how these networks change under different conditions. For instance, to better understand epilepsy, it would be interesting to capture the changes in the brain connectivity network as a patient experiences a seizure, using electroencephalography data. A common approach relies on estimating the networks in each condition and calculating their difference. Such estimates may behave poorly in high dimensions as the networks themselves may not be sparse in structure while their difference may be. We build upon this observation to develop an estimator of the difference in inverse spectral densities across two conditions. Using an l1 penalty on the difference, consistency is established by only requiring the difference to be sparse. We illustrate the method on synthetic data experiments and on experiments with electroencephalography data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Hellstern;Byol Kim;Zaid Harchaoui;Ali Shojaie",
        "authorids": "~Michael_Hellstern1;~Byol_Kim1;~Zaid_Harchaoui1;~Ali_Shojaie1",
        "gender": "M;F;;M",
        "homepage": "https://www.biostat.washington.edu/people/mike-hellstern;;;http://faculty.washington.edu/ashojaie/",
        "dblp": ";;;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-8993-4509;;0000-0001-8846-3533",
        "linkedin": ";byolkim/;;",
        "or_profile": "~Michael_Hellstern1;~Byol_Kim1;~Zaid_Harchaoui1;~Ali_Shojaie1",
        "aff": "University of Washington;Sookmyung Women's University;;University of Washington",
        "aff_domain": "uw.edu;sookmyung.ac.kr;;u.washington.edu",
        "position": "PhD student;Assistant Professor;;Full Professor",
        "bibtex": "@inproceedings{\nhellstern2025spectral,\ntitle={Spectral Differential Network Analysis for High-dimensional Time Series},\nauthor={Michael Hellstern and Byol Kim and Zaid Harchaoui and Ali Shojaie},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tuHVCBN5fw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tuHVCBN5fw",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tvHsQIe8LO",
        "title": "The VampPrior Mixture Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Widely used deep latent variable models (DLVMs), in particular Variational Autoencoders (VAEs), employ overly simplistic priors on the latent space. To achieve strong clustering performance, existing methods that replace the standard normal prior with a Gaussian mixture model (GMM) require defining the number of clusters to be close to the number of expected ground truth classes a-priori and are susceptible to poor initializations. We leverage VampPrior concepts (Tomczak and Welling, 2018) to fit a Bayesian GMM prior, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. In a VAE, the VMM attains highly competitive clustering performance on benchmark datasets. Integrating the VMM into scVI (Lopez et al., 2018), a popular scRNA-seq integration method, significantly improves its performance and automatically arranges cells into clusters with similar biological characteristics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andrew A. Stirn;David A. Knowles",
        "authorids": "~Andrew_A._Stirn1;~David_A._Knowles1",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Andrew_A._Stirn1;~David_A._Knowles1",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\nstirn2025the,\ntitle={The VampPrior Mixture Model},\nauthor={Andrew A. Stirn and David A. Knowles},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=tvHsQIe8LO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tvHsQIe8LO",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "txOzscP6Sr",
        "title": "Adaptive RKHS Fourier Features for Compositional Gaussian Process Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deep Gaussian Processes (DGPs) leverage a compositional structure to model non-stationary processes. DGPs typically rely on local inducing point approximations across intermediate GP layers. Recent advances in DGP inference have shown that incorporating global Fourier features from the Reproducing Kernel Hilbert Space (RKHS) can enhance the DGPs' capability to capture complex non-stationary patterns. This paper extends the use of these features to compositional GPs involving linear transformations. In particular, we introduce Ordinary Differential Equation(ODE)--based RKHS Fourier features that allow for adaptive amplitude and phase modulation through convolution operations. This convolutional formulation relates our work to recently proposed deep latent force models, a multi-layer structure designed for modelling nonlinear dynamical systems. By embedding these adjustable RKHS Fourier features within a doubly stochastic variational inference framework, our model exhibits improved predictive performance across various regression tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinxing Shi;Thomas Baldwin-McDonald;Mauricio A \u00c1lvarez",
        "authorids": "~Xinxing_Shi1;~Thomas_Baldwin-McDonald1;~Mauricio_A_\u00c1lvarez1",
        "gender": ";;",
        "homepage": ";;",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": ";;",
        "aff": ";;",
        "aff_domain": ";;",
        "position": ";;",
        "bibtex": "@inproceedings{\nshi2025adaptive,\ntitle={Adaptive {RKHS} Fourier Features for Compositional Gaussian Process Models},\nauthor={Xinxing Shi and Thomas Baldwin-McDonald and Mauricio A {\\'A}lvarez},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=txOzscP6Sr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=txOzscP6Sr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "u1ZmPMu3yD",
        "title": "Learning Infinite-Horizon Average-Reward Linear Mixture MDPs of Bounded Span",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper proposes a computationally tractable algorithm for learning infinite-horizon average-reward linear mixture Markov decision processes (MDPs) under the Bellman optimality condition. Our algorithm for linear mixture MDPs achieves a nearly minimax optimal regret upper bound of $\\widetilde{\\mathcal{O}}(d\\sqrt{\\mathrm{sp}(v^*)T})$ over $T$ time steps where $\\mathrm{sp}(v^*)$ is the span of the optimal bias function $v^*$ and $d$ is the dimension of the feature mapping. Our algorithm applies the recently developed technique of running value iteration on a discounted-reward MDP approximation with clipping by the span. We prove that the value iteration procedure, even with the clipping operation, converges. Moreover, we show that the associated variance term due to random transitions can be bounded even under clipping. Combined with the weighted ridge regression-based parameter estimation scheme, this leads to the nearly minimax optimal regret guarantee.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Woojin Chae;Kihyuk Hong;Yufan Zhang;Ambuj Tewari;Dabeen Lee",
        "authorids": "~Woojin_Chae2;~Kihyuk_Hong1;~Yufan_Zhang3;~Ambuj_Tewari1;~Dabeen_Lee1",
        "gender": "M;M;;M;",
        "homepage": ";;;https://www.ambujtewari.com;https://dabeenl.github.io",
        "dblp": ";;;24/567;180/3115.html",
        "google_scholar": ";;;ttbl4FsAAAAJ;8y38M00AAAAJ",
        "orcid": ";;;0000-0001-6969-7844;0000-0002-3802-1371",
        "linkedin": "chaewoojin/;hominot/;yufanzh/;;dabeen-lee-071aa7220",
        "or_profile": "~Woojin_Chae2;~Kihyuk_Hong1;~Yufan_Zhang3;~Ambuj_Tewari1;~Dabeen_Lee1",
        "aff": "Korea Advanced Institute of Science & Technology;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;Seoul National University+KAIST",
        "aff_domain": "kaist.ac.kr;umich.edu;umich.edu;umich.edu;snu.ac.kr+kaist.ac.kr",
        "position": "Undergrad student;PhD student;Undergrad student;Full Professor;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nchae2025learning,\ntitle={Learning Infinite-Horizon Average-Reward Linear Mixture {MDP}s of Bounded Span},\nauthor={Woojin Chae and Kihyuk Hong and Yufan Zhang and Ambuj Tewari and Dabeen Lee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=u1ZmPMu3yD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=u1ZmPMu3yD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "u2hlwh88vd",
        "title": "Conditional diffusions for amortized neural posterior estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Neural posterior estimation (NPE), a simulation-based computational approach for Bayesian inference, has shown great success in approximating complex posterior distributions. Existing NPE methods typically rely on normalizing flows, which approximate a distribution by composing many simple, invertible transformations. But flow-based models, while state of the art for NPE, are known to suffer from several limitations, including training instability and sharp trade-offs between representational power and computational cost. In this work, we demonstrate the effectiveness of conditional diffusions coupled with high-capacity summary networks for amortized NPE. Conditional diffusions address many of the challenges faced by flow-based methods.  Our results show that, across a highly varied suite of benchmarking problems for NPE architectures, diffusions offer improved stability, superior accuracy, and faster training times, even with simpler, shallower models. Building on prior work on diffusions for NPE, we show that these gains persist across a variety of different summary network architectures. Code is available at https://github.com/TianyuCodings/cDiff.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tianyu Chen;Vansh Bansal;James G. Scott",
        "authorids": "~Tianyu_Chen3;~Vansh_Bansal2;~James_G._Scott1",
        "gender": "M;;M",
        "homepage": ";;https://jgscott.github.io",
        "dblp": ";;82/9380",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": "tianyu-chen-1a056a160/;;",
        "or_profile": "~Tianyu_Chen3;~Vansh_Bansal2;~James_G._Scott1",
        "aff": "University of Texas at Austin;;University of Texas at Austin",
        "aff_domain": "utexas.edu;;utexas.edu",
        "position": "PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nchen2025conditional,\ntitle={Conditional diffusions for neural posterior estimation},\nauthor={Tianyu Chen and Vansh Bansal and James G. Scott},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=u2hlwh88vd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=u2hlwh88vd",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "u4eUg5lE9l",
        "title": "A Multi-Task Learning Approach to Linear Multivariate Forecasting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accurate forecasting of multivariate time series data is important in many engineering and scientific applications. Recent state-of-the-art works ignore the inter-relations between variates, using their model on each variate independently. This raises several research questions related to proper modeling of multivariate data. In this work, we propose to view multivariate forecasting as a multi-task learning problem, facilitating the analysis of forecasting by considering the angle between task gradients and their balance. To do so, we analyze linear models to characterize the behavior of tasks. Our analysis suggests that tasks can be defined by grouping similar variates together, which we achieve via a simple clustering that depends on correlation-based similarities. Moreover, to balance tasks, we scale gradients with respect to their prediction error. Then, each task is solved with a linear model within our MTLinear framework. We evaluate our approach on challenging benchmarks in comparison to strong baselines, and we show it obtains on-par or better results on multivariate forecasting problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Liran Nochumsohn;Hedi Zisling;Omri Azencot",
        "authorids": "~Liran_Nochumsohn1;~Hedi_Zisling1;~Omri_Azencot1",
        "gender": "M;M;Unspecified",
        "homepage": ";;http://omriazencot.com",
        "dblp": ";320/8352;132/3985.html",
        "google_scholar": ";;https://scholar.google.co.il/citations?user=MEGuRmAAAAAJ",
        "orcid": ";0009-0009-5212-9926;",
        "linkedin": "liran-nochumsohn-200ab91b3/;hedi-zisling/;omri-azencot-a8812417/",
        "or_profile": "~Liran_Nochumsohn1;~Hedi_Zisling1;~Omri_Azencot1",
        "aff": "Ben Gurion University of the Negev;Ben-Gurion University of the Negev;Ben-Gurion University of the Negev",
        "aff_domain": "bgu.ac.il;bgu.ac.il;bgu.ac.il",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nnochumsohn2025a,\ntitle={A Multi-Task Learning Approach to Linear Multivariate Forecasting},\nauthor={Liran Nochumsohn and Hedi Zisling and Omri Azencot},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=u4eUg5lE9l}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=u4eUg5lE9l",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uCs5QJRFSS",
        "title": "Model selection for behavioral learning data and applications to contextual bandits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Learning for animals or humans is the process that leads to behaviors better adapted to the environment. This process highly depends on the individual that learns and is usually observed only through the individual's actions. This article presents ways to use this individual behavioral data to find the model that best explains how the individual learns. We propose two model selection methods: a general hold-out procedure and an AIC-type criterion, both adapted to non-stationary dependent data. We provide theoretical error bounds for these methods that are close to those of the standard i.i.d. case. To compare these approaches, we apply them to contextual bandit models and illustrate their use on both synthetic and experimental learning data in a human categorization task.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Julien Aubert;Louis K\u00f6hler;Luc Leh\u00e9ricy;Giulia Mezzadri;Patricia Reynaud-Bouret",
        "authorids": "~Julien_Aubert1;~Louis_K\u00f6hler1;~Luc_Leh\u00e9ricy2;~Giulia_Mezzadri1;~Patricia_Reynaud-Bouret1",
        "gender": "M;;M;Non-Binary;F",
        "homepage": "https://julienaubert3.github.io/;;https://www.normalesup.org/~llehericy/;https://www.giuliamezzadri.com/;https://math.unice.fr/~reynaudb/",
        "dblp": ";;;;",
        "google_scholar": "3QvF7bIAAAAJ;;;;",
        "orcid": ";0000-0001-8015-8067;;0000-0001-6453-9070;",
        "linkedin": ";;;giulia-mezzadri/;",
        "or_profile": "~Julien_Aubert1;~Louis_K\u00f6hler1;~Luc_Leh\u00e9ricy2;~Giulia_Mezzadri1;~Patricia_Reynaud-Bouret1",
        "aff": "Universit\u00e9 Paris-Saclay+Universit\u00e9 C\u00f4te d'Azur;Universit\u00e9 de Nice-Sophia Antipolis;Universit\u00e9 C\u00f4te d'Azur;;CNRS",
        "aff_domain": "universite-paris-saclay.fr+univ-cotedazur.fr;unice.fr;unice.fr;;cnrs.fr",
        "position": "Postdoc+PhD student;PhD student;Researcher;;Principal Researcher",
        "bibtex": "@inproceedings{\naubert2025model,\ntitle={Model selection for behavioral learning data and applications to contextual bandits},\nauthor={Julien Aubert and Louis K{\\\"o}hler and Luc Leh{\\'e}ricy and Patricia Reynaud-Bouret and Giulia Mezzadri},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=uCs5QJRFSS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uCs5QJRFSS",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uNepBdsZyc",
        "title": "Microfoundation inference for strategic prediction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Often in prediction tasks, the predictive model itself can influence the distribution of the target variable, a phenomenon termed *performative prediction*. Generally, this influence stems from strategic actions taken by stakeholders with a vested interest in predictive models.\nA key challenge that hinders the widespread adaptation of performative prediction in machine learning is that practitioners are generally unaware of the social impacts of their predictions. To address this gap, we propose a methodology for learning the distribution map that encapsulates the long-term impacts of predictive models on the population. Specifically, we model agents' responses as a cost-adjusted utility maximization problem and propose estimates for said cost. Our approach leverages optimal transport to align pre-model exposure (*ex ante*) and post-model exposure (*ex post*) distributions. We provide a rate of convergence for this proposed estimate and assess its quality through empirical demonstrations on a credit scoring dataset.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniele Bracale;Subha Maity;Felipe Maia Polo;Seamus Somerstep;Moulinath Banerjee;Yuekai Sun",
        "authorids": "~Daniele_Bracale1;~Subha_Maity1;~Felipe_Maia_Polo1;~Seamus_Somerstep1;~Moulinath_Banerjee1;~Yuekai_Sun1",
        "gender": "M;M;M;;M;",
        "homepage": "https://m.facebook.com/daniele.bracale.9?ref=bookmarks;https://lsa.umich.edu/stats/people/phd-students/smaity.html;https://felipemaiapolo.github.io/;https://somerstep.github.io;https://lsa.umich.edu/stats/people/faculty/moulib.html;https://yuekai.github.io/",
        "dblp": ";278/2922;261/9581;;;",
        "google_scholar": ";eD9vCGMAAAAJ;CJbgmnkAAAAJ;;;6T1XtW8AAAAJ",
        "orcid": ";;0000-0002-4950-2795;;;",
        "linkedin": ";;;seamus-somerstep-a1a217194;;",
        "or_profile": "~Daniele_Bracale1;~Subha_Maity1;~Felipe_Maia_Polo1;~Seamus_Somerstep1;~Moulinath_Banerjee1;~Yuekai_Sun1",
        "aff": "University of Michigan;University of Waterloo;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor",
        "aff_domain": "umich.edu;uwaterloo.ca;umich.edu;umich.edu;umich.edu;umich.edu",
        "position": "PhD student;Assistant Professor;PhD student;PhD student;Full Professor;Assistant \u2192 Associate Professor of Statistics",
        "bibtex": "@inproceedings{\nbracale2025microfoundation,\ntitle={Microfoundation inference for strategic prediction},\nauthor={Daniele Bracale and Subha Maity and Felipe Maia Polo and Seamus Somerstep and Moulinath Banerjee and Yuekai Sun},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=uNepBdsZyc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uNepBdsZyc",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uPgGW67dSX",
        "title": "A Robust Kernel Statistical Test of Invariance: Detecting Subtle Asymmetries",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "While invariances naturally arise in almost any type of real-world data, no efficient and robust test exists for detecting them in observational data under arbitrarily given group actions. We tackle this problem by studying measures of invariance that can capture even negligible underlying patterns. Our first contribution is to show that, while detecting subtle asymmetries is computationally intractable, a randomized method can be used to robustly estimate closeness measures to invariance within constant factors. This provides a general framework for robust statistical tests of invariance. Despite the extensive and well-established literature, our methodology, to the best of our knowledge, is the first to provide statistical tests for general group invariances with finite-sample guarantees on Type II errors. In addition, we focus on kernel methods and propose deterministic algorithms for robust testing with respect to both finite and infinite groups, accompanied by a rigorous analysis of their convergence rates and sample complexity. Finally, we revisit the general framework in the specific case of kernel methods, showing that recent closeness measures to invariance, defined via group averaging, are provably robust, leading to powerful randomized algorithms.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ashkan Soleymani;Behrooz Tahmasebi;Stefanie Jegelka;Patrick Jaillet",
        "authorids": "~Ashkan_Soleymani1;~Behrooz_Tahmasebi1;~Stefanie_Jegelka3;~Patrick_Jaillet1",
        "gender": "M;M;F;M",
        "homepage": "https://ashkansoleymani.lids.mit.edu/;https://people.csail.mit.edu/bzt/;http://people.csail.mit.edu/stefje/;http://web.mit.edu/jaillet/www/",
        "dblp": "270/3353.html;223/0884;38/7003;https://dblp.uni-trier.de/pers/hd/j/Jaillet:Patrick",
        "google_scholar": "omHTV3MAAAAJ;ZXCO3DMAAAAJ;gTWUZlsAAAAJ;ND0FM6EAAAAJ",
        "orcid": ";;;0000-0002-8585-6566",
        "linkedin": ";;;patrick-jaillet-1260445/",
        "or_profile": "~Ashkan_Soleymani1;~Behrooz_Tahmasebi1;~Stefanie_Jegelka3;~Patrick_Jaillet1",
        "aff": "Massachusetts Institute of Technology;School of Engineering and Applied Sciences, Harvard University+Massachusetts Institute of Technology;Technische Universit\u00e4t M\u00fcnchen+Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;seas.harvard.edu+mit.edu;tum.de+mit.edu;mit.edu",
        "position": "PhD student;Postdoc+PhD student;Full Professor+Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nsoleymani2025a,\ntitle={A Robust Kernel Statistical Test of Invariance: Detecting Subtle Asymmetries},\nauthor={Ashkan Soleymani and Behrooz Tahmasebi and Stefanie Jegelka and Patrick Jaillet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=uPgGW67dSX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uPgGW67dSX",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ucyKTM7lO5",
        "title": "Efficient Trajectory Inference in Wasserstein Space Using Consecutive Averaging",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Capturing data from dynamic processes through cross-sectional measurements is seen in many fields, such as computational biology. Trajectory inference deals with the challenge of reconstructing continuous processes from such observations. In this work, we propose methods for B-spline approximation and interpolation of point clouds through consecutive averaging that is intrinsic to the Wasserstein space. Combining subdivision schemes with optimal transport-based geodesic, our methods carry out trajectory inference at a chosen level of precision and smoothness, and can automatically handle scenarios where particles undergo division over time. We prove linear convergence rates and rigorously evaluate our method on cell data characterized by bifurcations, merges, and trajectory splitting scenarios like *supercells*, comparing its performance against state-of-the-art trajectory inference and interpolation methods. The results not only underscore the effectiveness of our method in inferring trajectories but also highlight the benefit of performing interpolation and approximation that respect the inherent geometric properties of the data.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amartya Banerjee;Harlin Lee;Nir Sharon;Caroline Moosm\u00fcller",
        "authorids": "~Amartya_Banerjee1;~Harlin_Lee1;~Nir_Sharon1;~Caroline_Moosm\u00fcller1",
        "gender": "M;;;F",
        "homepage": "https://amartya21.github.io/;;https://www.tau.ac.il/~nsharon/;https://tarheels.live/cmoosm/",
        "dblp": ";;;",
        "google_scholar": "3ZItW_UAAAAJ;;https://scholar.google.com/citations?view_op=search_authors;DIYjPzgAAAAJ",
        "orcid": "0000-0002-7998-3390;;;0000-0002-7728-0261",
        "linkedin": "amartya1;;;",
        "or_profile": "~Amartya_Banerjee1;~Harlin_Lee1;~Nir_Sharon1;~Caroline_Moosm\u00fcller1",
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill;;;University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;;;unc.edu",
        "position": "PhD student;;;Assistant Professor",
        "bibtex": "@inproceedings{\nbanerjee2025efficient,\ntitle={Efficient Trajectory Inference in Wasserstein Space Using Consecutive Averaging},\nauthor={Amartya Banerjee and Harlin Lee and Nir Sharon and Caroline Moosm{\\\"u}ller},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ucyKTM7lO5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ucyKTM7lO5",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uhsjJe4Cx2",
        "title": "Cross-modality Matching and Prediction of Perturbation Responses with Labeled Gromov-Wasserstein Optimal Transport",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "It is now possible to conduct large scale perturbation screens with complex readout modalities, such as different molecular profiles or high content cell images. While these open the way for systematic dissection of causal cell circuits, integrating such data across screens to maximize our ability to predict circuits poses substantial computational challenges, which have not been addressed. Here, we extend two Gromov-Wasserstein optimal transport methods to incorporate the perturbation label for cross-modality alignment. The obtained alignment is then employed to train a predictive model that estimates cellular responses to perturbations observed with only one measurement modality. We validate our method for the tasks of cross-modality alignment and cross-modality prediction in a recent multi-modal single-cell perturbation dataset. Our approach opens the way to unified causal models of cell biology.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jayoung Ryu;Charlotte Bunne;Luca Pinello;Aviv Regev;Romain Lopez",
        "authorids": "~Jayoung_Ryu1;~Charlotte_Bunne1;~Luca_Pinello1;~Aviv_Regev1;~Romain_Lopez1",
        "gender": "F;F;M;F;",
        "homepage": ";https://aimm.epfl.ch;http://pinellolab.org;https://www.roche.com/about/governance/executive_committee/aviv-regev.htm;https://romain-lopez.github.io/",
        "dblp": ";217/2348;;;132/4587",
        "google_scholar": "CdDPlREAAAAJ;https://scholar.google.com/citations?hl=en;DPdrI94AAAAJ;;https://scholar.google.fr/citations?user=Z8RR17oAAAAJ",
        "orcid": "0000-0001-8710-8898;0000-0003-1431-103X;0000-0003-1109-3823;;0000-0003-0495-738X",
        "linkedin": "jayoung-ryu-5152281b8/;bunnech/;luca-pinello-7a1b0118/;;",
        "or_profile": "~Jayoung_Ryu1;~Charlotte_Bunne1;~Luca_Pinello1;~Aviv_Regev1;~Romain_Lopez1",
        "aff": "Harvard University;EPFL - EPF Lausanne;Massachusetts General Hospital, Harvard University;;New York University+Stanford University",
        "aff_domain": "g.harvard.edu;epfl.ch;mgh.harvard.edu;;nyu.edu+stanford.edu",
        "position": "PhD student;Assistant Professor;Associate Professor;;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nryu2025crossmodality,\ntitle={Cross-modality Matching and Prediction of Perturbation Responses with Labeled Gromov-Wasserstein Optimal Transport},\nauthor={Jayoung Ryu and Charlotte Bunne and Luca Pinello and Aviv Regev and Romain Lopez},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=uhsjJe4Cx2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uhsjJe4Cx2",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "v13muX4Q3i",
        "title": "Density Ratio Estimation via Sampling along Generalized Geodesics on Statistical Manifolds",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The density ratio of two probability distributions is one of the fundamental tools in mathematical and computational statistics and machine learning, and it has a variety of known applications. Therefore, density ratio estimation from finite samples is a very important task, but it is known to be unstable when the distributions are distant from each other. One approach to address this problem is density ratio estimation using incremental mixtures of the two distributions. We geometrically reinterpret existing methods for density ratio estimation based on incremental mixtures. We show that these methods can be regarded as iterating on the Riemannian manifold along a particular curve between the two probability distributions. Making use of the geometry of the manifold, we propose to consider incremental density ratio estimation along generalized geodesics on this manifold. To achieve such a method requires Monte Carlo sampling along geodesics via transformations of the two distributions. We show how to implement an iterative algorithm to sample along these geodesics and show how changing the distances along the geodesic affect the variance and accuracy of the estimation of the density ratio.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Masanari Kimura;Howard Bondell",
        "authorids": "~Masanari_Kimura2;~Howard_Bondell2",
        "gender": ";",
        "homepage": "https://nocotan.github.io/;",
        "dblp": ";",
        "google_scholar": "https://scholar.google.co.jp/citations?user=bPRGC8gAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Masanari_Kimura2;~Howard_Bondell2",
        "aff": "University of Melbourne;",
        "aff_domain": "unimelb.edu.au;",
        "position": "Postdoc;",
        "bibtex": "@inproceedings{\nkimura2025density,\ntitle={Density Ratio Estimation via Sampling along Generalized Geodesics on Statistical Manifolds},\nauthor={Masanari Kimura and Howard Bondell},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=v13muX4Q3i}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=v13muX4Q3i",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "v6hI4jxD1b",
        "title": "Bayesian Gaussian Process ODEs via Double Normalizing Flows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Gaussian processes have been used to model the vector field of continuous dynamical systems, which are characterized by a probabilistic ordinary differential equation (GP-ODE). Bayesian inference for these models has been extensively studied and applied in tasks such as time series prediction. However, the use of standard GPs with basic kernels like squared exponential  kernels has been common in GP-ODE research, limiting the model's ability to represent complex scenarios. To address this limitation, we introduce normalizing flows to reparameterize the ODE vector field, resulting in a data-driven prior distribution, thereby increasing flexibility and expressive power. We develop a variational inference algorithm that utilizes analytically tractable probability density functions of normalizing flows. Additionally, we also apply normalizing flows to the posterior inference of GP-ODEs to resolve the issue of strong mean-field assumptions. By applying normalizing flows in these ways, our model improves accuracy and uncertainty estimates for Bayesian GP-ODEs. We validate the effectiveness of our approach on simulated dynamical systems and real-world human motion data, including time series prediction and missing data recovery tasks.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "JIAN XU;Shian Du;Junmei Yang;Xinghao Ding;Delu Zeng;John Paisley",
        "authorids": "~JIAN_XU5;~Shian_Du1;~Junmei_Yang1;~Xinghao_Ding1;~Delu_Zeng4;~John_Paisley1",
        "gender": "M;M;F;M;;M",
        "homepage": ";;http://www2.scut.edu.cn/eemd/2013/1231/c4581a76497/page.htm;https://xmu-smartdsp.github.io/teamindex/xhding.html;;http://www.columbia.edu/~jwp2128/",
        "dblp": "73/1149-21;317/1383;157/9330.html;80/7693;;97/7035",
        "google_scholar": "https://scholar.google.com.hk/citations?user=DublkSoAAAAJ;SUgR5VAAAAAJ;;;;r31_fYQAAAAJ",
        "orcid": ";;;0000-0003-2288-5287;;",
        "linkedin": ";;;;;",
        "or_profile": "~JIAN_XU5;~Shian_Du1;~Junmei_Yang1;~Xinghao_Ding1;~Delu_Zeng4;~John_Paisley1",
        "aff": "South China University of Technology;Tsinghua University;South China University of Technology;Xiamen University;;Columbia University",
        "aff_domain": "scut.edu.cn;tsinghua.edu.cn;scut.edu.cn;xmu.edu.cn;;columbia.edu",
        "position": "PhD student;MS student;Associate Professor;Full Professor;;Associate Professor",
        "bibtex": "@inproceedings{\nxu2025bayesian,\ntitle={Bayesian Gaussian Process {ODE}s via Double Normalizing Flows},\nauthor={JIAN XU and Shian Du and Junmei Yang and Xinghao Ding and Delu Zeng and John Paisley},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=v6hI4jxD1b}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=v6hI4jxD1b",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "v9XFxkTl7m",
        "title": "Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayes Theory",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Lifelong reinforcement learning (RL) has been developed as a paradigm for extending single-task RL to more realistic, dynamic settings. In lifelong RL, the \"life\" of an RL agent is modeled as a stream of tasks drawn from a task distribution.\nWe propose EPIC (Empirical PAC-Bayes that Improves Continuously),  a novel algorithm designed for lifelong RL using PAC-Bayes theory. EPIC learns a shared policy distribution, referred to as the world policy, which enables rapid adaptation to new tasks while retaining valuable knowledge from previous experiences. Our theoretical analysis establishes a relationship between the algorithm's generalization performance and the number of prior tasks preserved in memory. We also derive the sample complexity of EPIC in terms of RL regret. Extensive experiments on a variety of environments demonstrate that EPIC significantly outperforms existing methods in lifelong RL, offering both theoretical guarantees and practical efficacy through the use of the world policy.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhi Zhang;Chris Chow;Yasi Zhang;Yanchao Sun;Haochen Zhang;Eric Hanchen Jiang;Han Liu;Furong Huang;Yuchen Cui;OSCAR HERNAN MADRID PADILLA",
        "authorids": "~Zhi_Zhang1;~Chris_Chow1;~Yasi_Zhang1;~Yanchao_Sun1;~Haochen_Zhang5;~Eric_Hanchen_Jiang1;~Han_Liu4;~Furong_Huang1;~Yuchen_Cui1;~OSCAR_HERNAN_MADRID_PADILLA2",
        "gender": ";M;Not Specified;F;;M;;F;F;",
        "homepage": ";;https://yasminzhang.github.io/;https://ycsun2017.github.io/home/index.html;;https://www.ericjiang.info/;;https://furong-huang.com;https://yuchencui.cc;https://hernanmp.github.io/",
        "dblp": ";;317/8966;132/6840;;363/7444;;72/8513;201/5416.html;",
        "google_scholar": "O__axAoAAAAJ;rEQgw_QAAAAJ;6hu7hXkAAAAJ;bloBY_QAAAAJ;;https://scholar.google.com/citations?hl=zh-CN;;13yyuCcAAAAJ;qQz2cm8AAAAJ;",
        "orcid": ";;;0000-0002-1137-9939;;;;;0000-0001-7417-1222;",
        "linkedin": ";;yasi-zhang-8737b3250/;;;eric-jiang-713ba5183/;;;;",
        "or_profile": "~Zhi_Zhang1;~Chris_Chow1;~Yasi_Zhang1;~Yanchao_Sun1;~Haochen_Zhang5;~Eric_Hanchen_Jiang1;~Han_Liu4;~Furong_Huang1;~Yuchen_Cui1;~OSCAR_HERNAN_MADRID_PADILLA2",
        "aff": "University of California, Los Angeles;Personal;University of California, Los Angeles;Apple AI/ML;;University of California, Los Angeles+University of California, Los Angeles;Northwestern University;University of Maryland;University of California, Los Angeles;University of California, Los Angeles",
        "aff_domain": "ucla.edu;gmail.com;ucla.edu;apple.com;;ucla.edu+ucla.edu;u.northwestern.edu;umd.edu;ucla.edu;ucla.edu",
        "position": "PhD student;Independent researcher;PhD student;Researcher;;PhD student+Undergrad student;Associate Professor;Associate Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025statistical,\ntitle={Statistical Guarantees for Lifelong Reinforcement Learning using {PAC}-Bayesian Theory},\nauthor={Zhi Zhang and Chris Chow and Yasi Zhang and Yanchao Sun and Haochen Zhang and Eric Hanchen Jiang and Han Liu and Furong Huang and Yuchen Cui and OSCAR HERNAN MADRID PADILLA},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=v9XFxkTl7m}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=v9XFxkTl7m",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "vHdKiqxZNk",
        "title": "A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a general framework for deriving generalization bounds for parallel positively \nhomogeneous neural networks--a class of neural networks whose input-output map decomposes as \nthe sum of positively homogeneous maps. Examples of such networks include matrix \nfactorization and sensing, single-layer multi-head attention mechanisms, tensor \nfactorization, deep linear and ReLU networks, and more. Our general framework is based on \nlinking the non-convex empirical risk minimization (ERM) problem to a closely related convex \noptimization problem over prediction functions, which provides a global, achievable lower-bound\nto the ERM problem. We exploit this convex lower-bound to perform generalization \nanalysis in the convex space while controlling the discrepancy between the convex model and \nits non-convex counterpart. We apply our general framework to a wide variety of models \nranging from low-rank matrix sensing, to structured matrix sensing, two-layer linear \nnetworks, two-layer ReLU networks, and single-layer multi-head attention mechanisms, \nachieving generalization bounds with a sample complexity that scales almost linearly with the \nnetwork width.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Uday Kiran Reddy Tadipatri;Benjamin David Haeffele;Joshua Agterberg;Rene Vidal",
        "authorids": "~Uday_Kiran_Reddy_Tadipatri1;~Benjamin_David_Haeffele1;~Joshua_Agterberg1;~Rene_Vidal1",
        "gender": "M;;M;",
        "homepage": "http://tudaykiranreddy.github.io/;;https://jagterberg.github.io/;http://www.vision.jhu.edu",
        "dblp": ";;241/6911;v/ReneVidal",
        "google_scholar": "_D9ydAUAAAAJ;;wk3svSEAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-2043-8427;;;",
        "linkedin": "tadipatri-uday-kiran-reddy/;;;rene-vidal-74844928/",
        "or_profile": "~Uday_Kiran_Reddy_Tadipatri1;~Benjamin_David_Haeffele1;~Joshua_Agterberg1;~Rene_Vidal1",
        "aff": "University of Pennsylvania;;University of Illinois, Urbana Champaign;University of Pennsylvania+Amazon",
        "aff_domain": "seas.upenn.edu;;illinois.edu;upenn.edu+amazon.com",
        "position": "PhD student;;Assistant Professor;Full Professor+Principal Researcher",
        "bibtex": "@inproceedings{\ntadipatri2025a,\ntitle={A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks},\nauthor={Uday Kiran Reddy Tadipatri and Benjamin David Haeffele and Joshua Agterberg and Rene Vidal},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=vHdKiqxZNk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vHdKiqxZNk",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "vI5mej4Mho",
        "title": "Factor Analysis with Correlated Topic Model for Multi-Modal Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Integrating various data modalities brings valuable insights into underlying phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation underlying different simple data modalities, where each sample is  represented by a vector of features. However, FA is not suited for structured data modalities, such as text or single cell sequencing data, where multiple data points are measured per each sample and exhibit a clustering structure. To overcome this challenge, we introduce FACTM, a novel, multi-view and multi-structure Bayesian model that combines FA with correlated topic modeling and is optimized using variational inference. Additionally, we introduce a method for rotating latent factors to enhance interpretability with respect to binary features. On text and video benchmarks as well as real-world music and COVID-19 datasets, we demonstrate that FACTM outperforms other methods in identifying clusters in structured data, and integrating them with simple modalities via the inference of shared, interpretable factors.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ma\u0142gorzata \u0141az\u0119cka;Ewa Maria Szczurek",
        "authorids": "~Ma\u0142gorzata_\u0141az\u0119cka1;~Ewa_Maria_Szczurek1",
        "gender": ";F",
        "homepage": ";https://www.mimuw.edu.pl/~szczurek/",
        "dblp": ";48/1715",
        "google_scholar": ";https://scholar.google.pl/citations?user=hltmGf0AAAAJ",
        "orcid": ";0000-0002-1320-6695",
        "linkedin": ";",
        "or_profile": "~Ma\u0142gorzata_\u0141az\u0119cka1;~Ewa_Maria_Szczurek1",
        "aff": ";Helmholtz Zentrum M\u00fcnchen+University of Warsaw",
        "aff_domain": ";helmholtz-munich.de+uw.edu.pl",
        "position": ";Principal Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nazecka2025factor,\ntitle={Factor Analysis with Correlated Topic Model for Multi-Modal Data},\nauthor={Ma{\\l}gorzata {\\L}az{\\k{e}}cka and Ewa Maria Szczurek},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=vI5mej4Mho}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vI5mej4Mho",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "vkvJDRmOLs",
        "title": "On the Consistent Recovery of Joint Distributions from Conditionals",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Self-supervised learning methods that mask parts of the input data and train models to predict the missing components have led to significant advances in machine learning. These approaches learn conditional distributions $p(x_T \\mid x_S)$ simultaneously, where $x_S$ and $x_T$ are subsets of the observed variables. In this paper, we examine the core problem of when all these conditional distributions are consistent with some joint distribution, and whether common models used in practice can learn consistent conditionals. We explore this problem in two settings. First, for the complementary conditioning sets where $S \\cup T$ is the complete set of variables, we introduce the concept of path consistency, a necessary condition for a consistent joint. Second, we consider the case where we have access to $p(x_T \\mid x_S)$ for all subsets $S$ and $T$. In this case, we propose the concepts of autoregressive and swap consistency, which we show are necessary and sufficient conditions for a consistent joint. For both settings, we analyze when these consistency conditions hold and show that standard discriminative models \\emph{may fail to satisfy them}. Finally, we corroborate via experiments that proposed consistency measures can be used as proxies for evaluating the consistency of conditionals $p(x_T \\mid x_S)$, and common parameterizations may find it hard to learn true conditionals.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mahbod Majid;Rattana Pukdee;Vishwajeet Agrawal;Burak Var\u0131c\u0131;Pradeep Kumar Ravikumar",
        "authorids": "~Mahbod_Majid1;~Rattana_Pukdee1;~Vishwajeet_Agrawal1;~Burak_Var\u0131c\u01311;~Pradeep_Kumar_Ravikumar1",
        "gender": "M;M;M;;M",
        "homepage": "https://www.mahbodmajid.com/;;;;http://www.cs.cmu.edu/~pradeepr/",
        "dblp": "307/5441;;;;94/3594",
        "google_scholar": "https://scholar.google.com/citations?hl=en;KhnQ8zoAAAAJ;tUmJiQcAAAAJ;;https://scholar.google.com.tw/citations?user=Q4DTPw4AAAAJ",
        "orcid": "0000-0001-9304-2872;;;;",
        "linkedin": ";rattana-pukdee/;vishwajeet-a-bab306116/;;",
        "or_profile": "~Mahbod_Majid1;~Rattana_Pukdee1;~Vishwajeet_Agrawal1;~Burak_Var\u0131c\u01311;~Pradeep_Kumar_Ravikumar1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;;;Carnegie Mellon University",
        "aff_domain": "cmu.edu;andrew.cmu.edu;;;cmu.edu",
        "position": "PhD student;PhD student;;;Full Professor",
        "bibtex": "@inproceedings{\nmajid2025on,\ntitle={On the Consistent Recovery of Joint Distributions from Conditionals},\nauthor={Mahbod Majid and Rattana Pukdee and Vishwajeet Agrawal and Burak Var{\\i}c{\\i} and Pradeep Kumar Ravikumar},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=vkvJDRmOLs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vkvJDRmOLs",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "vz7EADbj4t",
        "title": "Stochastic Gradient Descent for B\u00e9zier Simplex Representation of Pareto Set in Multi-Objective Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-objective optimization aims to find a set of solutions that achieve the best trade-off among multiple conflicting objective functions. While various multi-objective optimization algorithms have been proposed so far, most of them aim to find finite solutions as an approximation of the Pareto set, which may not adequately capture the entire structure of the Pareto set, especially when the number of variables is large. To overcome this limitation, we propose a method to obtain a parametric hypersurface representing the entire Pareto set instead of a finite set of points. Since the Pareto set of an $M$-objective optimization problem typically forms an $(M-1)$-dimensional simplex, we use a B\u00e9zier simplex as a model to express the Pareto set. We then develop a stochastic gradient descent-based algorithm that updates the B\u00e9zier simplex model toward the Pareto set, introducing a preconditioning matrix to enhance convergence. Our convergence analysis demonstrated that the proposed algorithm outperforms naive stochastic gradient descent in terms of convergence rate. Furthermore, we validate the effectiveness of our method through various multi-objective optimization problem instances, including real-world problems.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yasunari Hikima;Ken Kobayashi;Akinori Tanaka;Akiyoshi Sannai;Naoki Hamada",
        "authorids": "~Yasunari_Hikima1;~Ken_Kobayashi1;~Akinori_Tanaka1;~Akiyoshi_Sannai1;~Naoki_Hamada1",
        "gender": "M;;M;M;M",
        "homepage": ";https://kenkoba2119.github.io/;;https://sites.google.com/view/akiyoshisannai/%E3%83%9B%E3%83%BC%E3%83%A0;",
        "dblp": ";73/3956.html;243/2791;220/5533;25/2721",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=fyMWmOMAAAAJ;tj5TiyMAAAAJ;https://scholar.google.com/citations?hl=ja;https://scholar.google.co.jp/citations?user=JcNej3UAAAAJ",
        "orcid": "0000-0002-6636-1592;;;;0000-0002-3630-5987",
        "linkedin": ";;;;",
        "or_profile": "~Yasunari_Hikima1;~Ken_Kobayashi1;~Akinori_Tanaka1;~Akiyoshi_Sannai1;~Naoki_Hamada1",
        "aff": "Kyushu University+Fujitsu Limited;Institute of Science Tokyo;RIKEN;Kyoto University;KLab Inc.",
        "aff_domain": "kyushu-u.ac.jp+fujitsu.com;m.isct.ac.jp;riken.jp;kyoto-u.ac.jp;klab.com",
        "position": "PhD student+Researcher;Associate Professor;Associate Professor;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nhikima2025stochastic,\ntitle={Stochastic Gradient Descent for B\\'ezier Simplex Representation of Pareto Set in Multi-Objective Optimization},\nauthor={Yasunari Hikima and Ken Kobayashi and Akinori Tanaka and Akiyoshi Sannai and Naoki Hamada},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=vz7EADbj4t}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vz7EADbj4t",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wHDQx822qF",
        "title": "Ordered $\\mathcal{V}$-information Growth: A Fresh Perspective on Shared Information",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mutual information (MI) is widely employed as a measure of shared information between random variables. However, MI assumes unbounded computational resources\u2014a condition rarely met in practice, where predicting a random variable $Y$ from $X$ must rely on finite resources.  $\\mathcal{V}$-information addresses this limitation by employing a predictive family $\\mathcal{V}$ to emulate computational constraints, yielding a directed measure of shared information. Focusing on the mixed setting (continuous $X$ and discrete $Y$), here we highlight the upward bias of empirical $\\mathcal{V}$-information, $\\hat I_{\\mathcal{V}}(X \\rightarrow Y)$, even when $\\mathcal{V}$ is low-complexity (e.g., shallow neural networks). To mitigate this bias, we introduce $\\mathcal{V}$-Information Growth (VI-Growth), defined as $\\\\hat I_{\\mathcal{V}}(X \\rightarrow Y) - \\hat I_{\\mathcal{V}}(X' \\rightarrow Y')$, where $X', Y' \\sim P_X P_Y$ represent independent variables. While VI-Growth effectively counters over-estimation, more complex predictive families may lead to under-estimation. To address this, we construct a sequence of predictive families $\\mathcal{V}_1, \\mathcal{V}_2, \\ldots, \\mathcal{V}$ of increasing complexity and compute the maximum of VI-Growth across these families, yielding the ordered VI-Growth (O-VIG). We provide theoretical results that justify this approach, showing that O-VIG is a provably tighter lower bound for the true $\\mathcal{V}$-Information than empirical $\\mathcal{V}$-Information itself, and exhibits stronger convergence properties than $\\mathcal{V}$-Information. Empirically, O-VIG alleviates bias and consistently outperforms state-of-the-art methods in both MI estimation and dataset complexity estimation, demonstrating its practical utility.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rohan Ghosh;Mehul Motani",
        "authorids": "~Rohan_Ghosh1;~Mehul_Motani1",
        "gender": ";M",
        "homepage": ";https://mehulmotani.github.io/",
        "dblp": "171/8274;83/4035",
        "google_scholar": "https://scholar.google.com.sg/citations?user=xfZ2FRAAAAAJ;https://scholar.google.com.sg/citations?user=Bm9BwEQAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Rohan_Ghosh1;~Mehul_Motani1",
        "aff": "National University of Singapore;National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "position": "Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nghosh2025ordered,\ntitle={Ordered \\${\\textbackslash}mathcal\\{V\\}\\$-information Growth: A New Perspective on Shared Information},\nauthor={Rohan Ghosh and Mehul Motani},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wHDQx822qF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wHDQx822qF",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wV6smUoQg9",
        "title": "Towards Fair Graph Learning without Demographic Information",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fair Graph Neural Networks (GNNs) have been extensively studied in graph-based applications. However, most approaches to fair GNNs assume the full availability of demographic information by default, which is often unrealistic due to legal restrictions or privacy concerns, leaving a noticeable gap in methods for addressing bias under such constraints. To this end, we propose a novel method for fair graph learning without demographic information. Our approach leverages a Bayesian variational autoencoder to infer missing demographic information and uses disentangled latent variables to separately capture demographics-related and label-related information, reducing interference when inferring demographic proxies. Additionally, we incorporate a fairness regularizer that enables measuring model fairness without demographics while optimizing the fairness objective. Extensive experiments on three real-world graph datasets demonstrate the proposed method's effectiveness in improving both fairness and utility.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zichong Wang;Nhat Hoang;Xingyu Zhang;Kevin Bello;Xiangliang Zhang;Sundararaja Sitharama Iyengar;Wenbin Zhang",
        "authorids": "~Zichong_Wang1;~Nhat_Hoang1;xiz261@pitt.edu;~Kevin_Bello1;~Xiangliang_Zhang1;iyengar@cs.fiu.edu;~Wenbin_Zhang1",
        "gender": "M;;;M;F;;",
        "homepage": "https://zichongwang.com;;;https://www.cs.cmu.edu/~kbello;https://sites.nd.edu/xiangliang-zhang/;;https://users.cs.fiu.edu/~wbzhang/",
        "dblp": ";;;202/2531;74/1890-1;;35/4073-2",
        "google_scholar": "NgxMqeEAAAAJ;;;pCS09UsAAAAJ;BhRJe4wAAAAJ;;M802p54AAAAJ",
        "orcid": ";;;;0000-0002-3574-5665;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Zichong_Wang1;~Nhat_Hoang1;xiz261@pitt.edu;~Kevin_Bello1;~Xiangliang_Zhang1;iyengar@cs.fiu.edu;~Wenbin_Zhang1",
        "aff": "Florida International University;;;Soroco;University of Notre Dame;;Florida International University",
        "aff_domain": "fiu.edu;;;soroco.com;nd.edu;;fiu.edu",
        "position": "PhD student;;;Researcher;Associate Professor;;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025towards,\ntitle={Towards Fair Graph Learning without Demographic Information},\nauthor={Zichong Wang and Nhat Hoang and Xingyu Zhang and Kevin Bello and Xiangliang Zhang and Sundararaja Sitharama Iyengar and Wenbin Zhang},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wV6smUoQg9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wV6smUoQg9",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wYfOdkKnsr",
        "title": "Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work focuses on the gradient flow dynamics of a neural network model that uses correlation loss to approximate a multi-index function on high-dimensional standard Gaussian data. \nSpecifically, the multi-index function we consider is a sum of neurons $f^*(x) = \\sum_{j=1}^k \\sigma^*(v_j^T x)$ where $v_1, ..., v_k$ are unit vectors, and $\\sigma^*$ lacks the first and second Hermite polynomials in its Hermite expansion. \nIt is known that, for the single-index case ($k=1$), overcoming the search phase requires polynomial time complexity. \nWe first generalize this result to multi-index functions characterized by vectors in arbitrary directions. \nAfter the search phase, it is not clear whether the network neurons converge to the index vectors, or get stuck at a sub-optimal solution.\nWhen the index vectors are orthogonal, we give a complete characterization of the fixed points and prove that neurons converge to the nearest index vectors. \nTherefore, using $n \\asymp k \\log k$ neurons ensures finding the full set of index vectors with gradient flow with high probability over random initialization.\nWhen $v_i^T v_j = \\beta \\geq 0$ for all $i \\neq j$, we prove the existence of a sharp threshold $\\beta_c = c/(c+k)$ at which the fixed point that computes the average of the index vectors transitions from a saddle point to a minimum. \n  Numerical simulations show that using a correlation loss and a mild overparameterization suffices to learn all of the index vectors when they are nearly orthogonal, however, the correlation loss fails when the dot product between the index vectors exceeds a certain threshold.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Berfin Simsek;Amire Bendjeddou;Daniel Hsu",
        "authorids": "~Berfin_Simsek1;~Amire_Bendjeddou1;~Daniel_Hsu1",
        "gender": "F;;M",
        "homepage": "https://www.bsimsek.com/;;https://www.cs.columbia.edu/~djhsu/",
        "dblp": "244/2455;;h/DanielHsu.html",
        "google_scholar": "Ysi38KIAAAAJ;;Bp6tvy0AAAAJ",
        "orcid": ";;0000-0002-3495-7113",
        "linkedin": ";amire-bendjeddou-89195215b;",
        "or_profile": "~Berfin_Simsek1;~Amire_Bendjeddou1;~Daniel_Hsu1",
        "aff": "Flatiron Institute;EPFL;Columbia University",
        "aff_domain": "flatironinstitute.org;epfl.ch;columbia.edu",
        "position": "Researcher;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nsimsek2025learning,\ntitle={Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence},\nauthor={Berfin Simsek and Amire Bendjeddou and Daniel Hsu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wYfOdkKnsr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wYfOdkKnsr",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wkLO3Q1it8",
        "title": "Score matching for bridges without learning time-reversals",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a new algorithm for learning a bridged diffusion process using score-matching methods. \nOur method relies on reversing the dynamics of the forward process and using this to learn a score function, which, via Doob's $h$-transform, gives us a bridged diffusion process; that is, a process conditioned on an endpoint. \nIn contrast to prior methods, ours learns the score term $\\nabla_x \\log p(t, x; T, y)$, for given $t, y$ directly, completely avoiding the need for first learning a time-reversal. \nWe compare the performance of our algorithm with existing methods and see that it outperforms using the (learned) time-reversals to learn the score term. The code can be found at https://github.com/libbylbaker/forward_bridge.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Elizabeth Louise Baker;Moritz Schauer;Stefan Sommer",
        "authorids": "~Elizabeth_Louise_Baker1;~Moritz_Schauer2;~Stefan_Sommer1",
        "gender": ";;",
        "homepage": ";https://mschauer.eu;",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";0000-0003-3310-7915;",
        "linkedin": ";;",
        "or_profile": "~Elizabeth_Louise_Baker1;~Moritz_Schauer2;~Stefan_Sommer1",
        "aff": ";Chalmers University of Technology+G\u00f6teborg University;",
        "aff_domain": ";chalmers.se+gu.se;",
        "position": ";Associate Professor+Associate Professor;",
        "bibtex": "@inproceedings{\nbaker2025score,\ntitle={Score matching for bridges without time-reversals},\nauthor={Elizabeth Louise Baker and Moritz Schauer and Stefan Sommer},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wkLO3Q1it8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wkLO3Q1it8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wmUvA272M3",
        "title": "$f$-PO: Generalizing Preference Optimization with $f$-divergence Minimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Preference optimization has made significant progress recently, with numerous methods developed to align language models with human preferences. This paper introduces $f$-divergence Preference Optimization ($f$-PO), a novel framework that generalizes and extends existing approaches. $f$-PO minimizes $f$-divergences between the optimized policy and the optimal policy, encompassing a broad family of alignment methods using various divergences. Our approach unifies previous algorithms like DPO and EXO, while offering new variants through different choices of $f$-divergences. We provide theoretical analysis of $f$-PO's properties and conduct extensive experiments on state-of-the-art language models using benchmark datasets. Results demonstrate $f$-PO's effectiveness across various tasks, achieving superior performance compared to existing methods on popular benchmarks such as AlpacaEval 2, Arena-Hard, MT-Bench, and Open LLM Leaderboard v2. Additionally, we present ablation studies exploring the impact of different $f$-divergences, offering insights into the trade-offs between regularization and performance in offline preference optimization. Our work contributes both practical algorithms and theoretical understanding to the field of language model alignment. Code is available at https://github.com/MinkaiXu/fPO.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiaqi Han;Mingjian Jiang;Yuxuan Song;Stefano Ermon;Minkai Xu",
        "authorids": "~Jiaqi_Han2;~Mingjian_Jiang1;~Yuxuan_Song2;~Stefano_Ermon1;~Minkai_Xu1",
        "gender": "M;M;M;M;Not Specified",
        "homepage": "https://hanjq17.github.io;https://www.cs.toronto.edu/~mjjiang/;https://yuxuansong.com;http://cs.stanford.edu/~ermon/;https://minkaixu.com",
        "dblp": "235/0412;;;47/8135;257/3355",
        "google_scholar": "AKppgMAAAAAJ;;xlnZ1OIAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;0009-0007-9735-3767",
        "linkedin": ";mingjian-jiang-279318201/;;;",
        "or_profile": "~Jiaqi_Han2;~Mingjian_Jiang1;~Yuxuan_Song2;~Stefano_Ermon1;~Minkai_Xu1",
        "aff": "Computer Science Department, Stanford University;Stanford University;ByteDance Inc.+Tsinghua University;Stanford University;Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu;bytedance.com+tsinghua.edu.cn;stanford.edu;stanford.edu",
        "position": "PhD student;MS student;Intern+PhD student;Associate Professor;PhD student",
        "bibtex": "@inproceedings{\nxu2025fpo,\ntitle={\\$f\\$-{PO}: Generalizing Preference Optimization with \\$f\\$-divergence Minimization},\nauthor={Minkai Xu and Jiaqi Han and Mingjian Jiang and Yuxuan Song and Stefano Ermon},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wmUvA272M3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wmUvA272M3",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wvifBIPnuV",
        "title": "$\\mathcal{I}$-trustworthy Models. A framework for trustworthiness evaluation of probabilistic classifiers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As probabilistic models continue to permeate various facets of our society and contribute to scientific advancements, it becomes a necessity to go beyond traditional metrics such as predictive accuracy and error rates and assess their trustworthiness. Grounded in the competence-based theory of trust, this work formalizes $\\mathcal{I}$-trustworthy framework -- a novel framework for assessing the trustworthiness of probabilistic classifiers for inference tasks by linking conditional calibration to trustworthiness. To assess $\\mathcal{I}$-trustworthiness, we use the local calibration error (LCE) and develop a method of hypothesis-testing. This method utilizes a kernel-based test statistic, Kernel Local Calibration Error (KLCE), to test local calibration of a probabilistic classifier. This study provides theoretical guarantees by offering convergence bounds for an unbiased estimator of KLCE. Additionally, we present a diagnostic tool designed to identify and measure biases in cases of miscalibration. The effectiveness of the proposed test statistic is demonstrated through its application to both simulated and real-world datasets. Finally, LCE of related recalibration methods is studied, and we provide evidence of insufficiency of existing methods to achieve I-trustworthiness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ritwik Vashistha;Arya Farahi",
        "authorids": "~Ritwik_Vashistha1;~Arya_Farahi1",
        "gender": "M;M",
        "homepage": "https://ritwik.super.site/;https://afarahi.github.io/",
        "dblp": ";188/6368",
        "google_scholar": "kSat02cAAAAJ;TFLWMfQAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ritwik_Vashistha1;~Arya_Farahi1",
        "aff": "University of Texas at Austin;University of Texas, Austin",
        "aff_domain": "utexas.edu;utexas.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nvashistha2025mathcalitrustworthy,\ntitle={\\${\\textbackslash}mathcal\\{I\\}\\$-trustworthy Models. A Framework for trustworthiness evaluation of probabilistic classifiers},\nauthor={Ritwik Vashistha and Arya Farahi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=wvifBIPnuV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wvifBIPnuV",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xJU2GjcC1U",
        "title": "An Iterative Algorithm for Rescaled Hyperbolic Functions Regression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have numerous real-life applications across various domains, such as natural language translation, sentiment analysis, language modeling, chatbots and conversational agents, creative writing, text classification, summarization, and generation. LLMs have shown great promise in improving the accuracy and efficiency of these tasks, and have the potential to revolutionize the field of natural language processing (NLP) in the years to come.\nExponential function based attention unit is a fundamental element in LLMs. Several previous works have studied the convergence of exponential regression and softmax regression.\n\nIn this paper, we propose an iterative algorithm to solve a rescaled version of the slightly different formulation of the softmax regression problem that arises in attention mechanisms of large language models. Specifically, we consider minimizing the squared loss between a certain function, which can be either the exponential function, hyperbolic sine function, or hyperbolic cosine function, and its inner product with a target $n$-dimensional vector $b$, scaled by the normalization term. This ``rescaled softmax regression'' differs from classical softmax regression in the location of the normalization factor.\n\nThe efficiency and generalizability of this framework to multiple hyperbolic functions make it relevant for optimizing attention mechanisms. The analysis also leads to a corollary bounding solution changes under small perturbations for in-context learning. Limitations and societal impact are discussed.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yeqi Gao;Zhao Song;Junze Yin",
        "authorids": "~Yeqi_Gao1;~Zhao_Song3;~Junze_Yin1",
        "gender": "M;M;M",
        "homepage": ";https://www.youtube.com/@simapofang;https://yinj66.github.io/",
        "dblp": "289/6991;76/4051-2;339/6793",
        "google_scholar": ";yDZct7UAAAAJ;4qdW3UQAAAAJ",
        "orcid": ";0000-0003-4589-5234;0009-0005-1548-8058",
        "linkedin": ";;junze-yin-8416651bb/",
        "or_profile": "~Yeqi_Gao1;~Zhao_Song3;~Junze_Yin1",
        "aff": "Tsinghua University;University of California, Berkeley;Rice University",
        "aff_domain": "thu.edu.cn;berkeley.edu;rice.edu",
        "position": "MS student;Associate Professor;PhD student",
        "bibtex": "@inproceedings{\ngao2025an,\ntitle={An Iterative Algorithm for Rescaled Hyperbolic Functions Regression},\nauthor={Yeqi Gao and Zhao Song and Junze Yin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=xJU2GjcC1U}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xJU2GjcC1U",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xit2wOU09f",
        "title": "On the Computational Tractability of the (Many) Shapley Values",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent studies have examined the computational complexity of computing Shapley additive explanations (also known as SHAP) across various models and distributions, revealing their tractability or intractability in different settings. However, these studies primarily focused on a specific variant called Conditional SHAP, though many other variants exist and address different limitations. In this work, we analyze the complexity of computing a much broader range of such variants, including Conditional, Interventional, and Baseline SHAP, while exploring both local and global computations. We show that both local and global Interventional and Baseline SHAP can be computed in polynomial time for various ML models under Hidden Markov Model distributions, extending popular algorithms such as TreeSHAP beyond empirical distributions. On the downside, we prove intractability results for these variants over a wide range of neural networks and tree ensembles. We believe that our results emphasize the intricate diversity of computing Shapley values, demonstrating how their complexity is substantially shaped by both the specific SHAP variant, the model type, and the distribution.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Reda Marzouk;Shahaf Bassan;Guy Katz;De la Higuera",
        "authorids": "~Reda_Marzouk1;~Shahaf_Bassan1;~Guy_Katz1;~De_la_Higuera1",
        "gender": "M;;M;M",
        "homepage": "https://www.univ-nantes.fr/mohamed-reda-marzouk;;http://www.katz-lab.com;http://pagesperso.lina.univ-nantes.fr/~cdlh/",
        "dblp": "262/3782.html;;23/10321;",
        "google_scholar": ";;https://scholar.google.com.tw/citations?user=3nYG5BMAAAAJ;https://scholar.google.com/scholar?hl=fr",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Reda_Marzouk1;~Shahaf_Bassan1;~Guy_Katz1;~De_la_Higuera1",
        "aff": "Independent;;Hebrew University of Jerusalem;Universit\u00e9 de Nantes",
        "aff_domain": "independent.com;;huji.ac.il;univ-nantes.fr",
        "position": "Researcher;;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nmarzouk2025on,\ntitle={On the Computational Tractability of the (Many) Shapley Values},\nauthor={Reda Marzouk and Shahaf Bassan and Guy Katz and De la Higuera},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=xit2wOU09f}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xit2wOU09f",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xmZaLoKHdQ",
        "title": "Calm Composite Losses: Being Improper Yet Proper Composite",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Strict proper losses are fundamental loss functions inducing classifiers capable of estimating class probabilities.\nWhile practitioners have devised many loss functions, their properness is often unverified.\nIn this paper, we identify several losses as improper, calling into question the validity of class probability estimates derived from their simplex-projected outputs.\nNevertheless, we show that these losses are strictly proper composite with appropriate link functions, allowing predictions to be mapped into true class probabilities.\nWe invent the calmness condition, which we prove suffices to identify that a loss has a strictly proper composite representation, and provide the general form of the inverse link.\nTo further understand proper composite losses, we explore proper composite losses through the framework of property elicitation, revealing a connection between inverse link functions and Bregman projections.\nNumerical simulations are provided to demonstrate the behavior of proper composite losses and the effectiveness of the inverse link function.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Han Bao;Nontawat Charoenphakdee",
        "authorids": "~Han_Bao2;~Nontawat_Charoenphakdee1",
        "gender": "M;M",
        "homepage": "https://hermite.jp/;https://nolfwin.github.io/",
        "dblp": "120/1444-2;227/3074",
        "google_scholar": "MqMzjeMAAAAJ;https://scholar.google.co.jp/citations?user=sEFoFbgAAAAJ",
        "orcid": "0000-0002-4473-2604;0000-0002-0214-4943",
        "linkedin": ";nontawat-charoenphakdee-b07b7385/",
        "or_profile": "~Han_Bao2;~Nontawat_Charoenphakdee1",
        "aff": "The Institute of Statistical Mathematics;Preferred Networks, Inc.",
        "aff_domain": "ism.ac.jp;preferred.jp",
        "position": "Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nbao2025being,\ntitle={Being Improper Yet Proper Composite: Revisiting Loss Functions from Perspectives of Calm Composite Loss and Property Elicitation},\nauthor={Han Bao and Nontawat Charoenphakdee},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=xmZaLoKHdQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xmZaLoKHdQ",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xmllRK5QWi",
        "title": "Elastic Representation: Mitigating Spurious Correlations for Group Robustness",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deep learning models can suffer from severe performance degradation when relying on spurious correlations between input features and labels, making the models perform well on training data but have poor prediction accuracy for minority groups. This problem arises especially when training data are limited or imbalanced. While most prior work focuses on learning invariant features (with consistent correlations to y), it overlooks the potential harm of spurious correlations between features. We hereby propose Elastic Representation (ElRep) to learn features by imposing Nuclear- and Frobenius-norm penalties on the representation from the last layer of a neural network. Similar to the elastic net, ElRep enjoys the benefits of learning important features without losing feature diversity. The proposed method is simple yet effective. It can be integrated into many deep learning approaches to mitigate spurious correlations and improve group robustness. Moreover, we theoretically show that ElRep has minimum negative impacts on in-distribution predictions. This is a remarkable advantage over approaches that prioritize minority groups at the cost of overall performance.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tao Wen;Zihan Wang;Quan Zhang;Qi Lei",
        "authorids": "~Tao_Wen1;~Zihan_Wang20;~Quan_Zhang1;~Qi_Lei1",
        "gender": "M;M;M;F",
        "homepage": "https://taowen0309.github.io/;;http://zhangquan-ut.github.io;https://cecilialeiqi.github.io/",
        "dblp": ";;06/2243;",
        "google_scholar": "mbYJLZYAAAAJ;ZBF2zKMAAAAJ;;kGOgaowAAAAJ",
        "orcid": "0009-0009-1587-9971;;;",
        "linkedin": "tao-wen/;zihan-wang-3b0050249/;;",
        "or_profile": "~Tao_Wen1;~Zihan_Wang20;~Quan_Zhang1;~Qi_Lei1",
        "aff": "Dartmouth College;New York University;Michigan State University;New York University",
        "aff_domain": "dartmouth.edu;nyu.edu;msu.edu;nyu.edu",
        "position": "PhD student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwen2025elastic,\ntitle={Elastic Representation: Mitigating Spurious Correlations for Group Robustness},\nauthor={Tao Wen and Zihan Wang and Quan Zhang and Qi Lei},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=xmllRK5QWi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xmllRK5QWi",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xzLgVKWmz6",
        "title": "Quantifying Knowledge Distillation using Partial Information Decomposition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Knowledge distillation deploys complex machine learning models in resource-constrained environments by training a smaller student model to emulate internal representations of a complex teacher model. However, the teacher's representations can also encode nuisance or additional information not relevant to the downstream task. Distilling such irrelevant information can actually impede the performance of a capacity-limited student model. This observation motivates our primary question: What are the information-theoretic limits of knowledge distillation? To this end, we leverage Partial Information Decomposition to quantify and explain the transferred knowledge and knowledge left to distill for a downstream task. We theoretically demonstrate that the task-relevant transferred knowledge is succinctly captured by the measure of redundant information about the task between the teacher and student. We propose a novel multi-level optimization to incorporate redundant information as a regularizer, leading to our framework of Redundant Information Distillation (RID). RID leads to more resilient and effective distillation under nuisance teachers as it succinctly quantifies task-relevant knowledge rather than simply aligning student and teacher representations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pasan Dissanayake;Faisal Hamman;Barproda Halder;Ilia Sucholutsky;Qiuyi Zhang;Sanghamitra Dutta",
        "authorids": "~Pasan_Dissanayake1;~Faisal_Hamman1;~Barproda_Halder1;~Ilia_Sucholutsky1;~Qiuyi_Zhang1;~Sanghamitra_Dutta2",
        "gender": "M;M;F;M;M;F",
        "homepage": ";https://www.faisalhamman.com/;;https://ilia10000.github.io/;https://qiuyiz.github.io;https://sites.google.com/site/sanghamitraweb/",
        "dblp": "292/8397;332/3468;342/3043.html;239/5108;133/8559;154/6653",
        "google_scholar": "isO0bMwAAAAJ;Zeoc1A8AAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=6MfHyuMAAAAJ;mE11hO8AAAAJ;BgaqaXwAAAAJ",
        "orcid": "0000-0003-0997-332X;;;0000-0003-4121-7479;;0000-0002-6500-2627",
        "linkedin": "pasandissanayake;;;iliasu/;;",
        "or_profile": "~Pasan_Dissanayake1;~Faisal_Hamman1;~Barproda_Halder1;~Ilia_Sucholutsky1;~Qiuyi_Zhang1;~Sanghamitra_Dutta2",
        "aff": "University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;New York University;Google;University of Maryland, College Park",
        "aff_domain": "umd.edu;umd.edu;umd.edu;nyu.edu;google.com;umd.edu",
        "position": "PhD student;PhD student;PhD student;Faculty Fellow/Assistant Professor;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\ndissanayake2025quantifying,\ntitle={Quantifying Knowledge Distillation using Partial Information Decomposition},\nauthor={Pasan Dissanayake and Faisal Hamman and Barproda Halder and Qiuyi Zhang and Ilia Sucholutsky and Sanghamitra Dutta},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=xzLgVKWmz6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xzLgVKWmz6",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "y7wGcBog9X",
        "title": "MDP Geometry, Normalization and Reward Balancing Solvers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a new geometric interpretation of Markov Decision Processes (MDPs) with a natural normalization procedure that allows us to adjust the value function at each state without altering the advantage of any action with respect to any policy. This advantage-preserving transformation of the MDP motivates a class of algorithms which we call *Reward Balancing*, which solve MDPs by iterating through these transformations, until an approximately optimal policy can be trivially found.  We provide a convergence analysis of several algorithms in this class, in particular showing that for MDPs for unknown transition probabilities we can improve upon state-of-the-art sample complexity results.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arsenii Mustafin;Aleksei Pakharev;Alex Olshevsky;Ioannis Paschalidis",
        "authorids": "~Arsenii_Mustafin1;pakhara@mskcc.org;~Alex_Olshevsky1;~Ioannis_Paschalidis1",
        "gender": ";;;M",
        "homepage": ";;;http://sites.bu.edu/paschalidis/",
        "dblp": ";;;44/2060",
        "google_scholar": ";;;Es_hZ0QAAAAJ",
        "orcid": ";;;0000-0002-3343-2913",
        "linkedin": ";;;yannis-paschalidis-75a921/",
        "or_profile": "~Arsenii_Mustafin1;pakhara@mskcc.org;~Alex_Olshevsky1;~Ioannis_Paschalidis1",
        "aff": ";;;Boston University",
        "aff_domain": ";;;bu.edu",
        "position": ";;;Full Professor",
        "bibtex": "@inproceedings{\nmustafin2025mdp,\ntitle={{MDP} Geometry, Normalization and Reward Balancing Solvers},\nauthor={Arsenii Mustafin and Aleksei Pakharev and Alex Olshevsky and Ioannis Paschalidis},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=y7wGcBog9X}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=y7wGcBog9X",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yC1qOOzicM",
        "title": "Credal Two-Sample Tests of Epistemic Uncertainty",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce credal two-sample testing, a new hypothesis testing framework for comparing credal sets---convex sets of probability measures where each element captures aleatoric uncertainty and the set itself represents epistemic uncertainty that arises from the modeller's partial ignorance. Compared to classical two-sample tests, which focus on comparing precise distributions, the proposed framework provides a broader and more versatile set of hypotheses. This approach enables the direct integration of epistemic uncertainty, effectively addressing the challenges arising from partial ignorance in hypothesis testing. By generalising two-sample test to compare credal sets, our framework enables reasoning for equality, inclusion, intersection, and mutual exclusivity, each offering unique insights into the modeller's epistemic beliefs. As the first work on nonparametric hypothesis testing for comparing credal sets, we focus on finitely generated credal sets derived from i.i.d. samples from multiple distributions\u2014referred to as \\emph{credal samples}.\nWe formalise these tests as two-sample tests with nuisance parameters and introduce the first permutation-based solution for this class of problems, significantly improving upon existing methods. Our approach properly incorporates the modeller's epistemic uncertainty into hypothesis testing, leading to more robust and credible conclusions, with kernel-based implementations for real-world applications.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siu Lun Chau;Antonin Schrab;Arthur Gretton;Dino Sejdinovic;Krikamol Muandet",
        "authorids": "~Siu_Lun_Chau1;~Antonin_Schrab1;~Arthur_Gretton1;~Dino_Sejdinovic1;~Krikamol_Muandet1",
        "gender": "M;;M;M;M",
        "homepage": "https://chau999.github.io/;;http://www.gatsby.ucl.ac.uk/~gretton/;https://sejdino.github.io/;http://krikamol.org",
        "dblp": "264/9823;;56/2574;31/1783;34/1240",
        "google_scholar": "e7ZBlIsAAAAJ;;OUv7J6QAAAAJ;v8Dg1lIAAAAJ;E2z5uYsAAAAJ",
        "orcid": ";;;0000-0001-5547-9213;0000-0002-4182-5282",
        "linkedin": ";;;https://linkedin.com/in/dinosejdinovic;krikamol-muandet/",
        "or_profile": "~Siu_Lun_Chau1;~Antonin_Schrab1;~Arthur_Gretton1;~Dino_Sejdinovic1;~Krikamol_Muandet1",
        "aff": "Nanyang Technological University+CISPA \u2013 Helmholtz Center for Information Security;;Google+University College London;University of Adelaide;CISPA Helmholtz Center for Information Security",
        "aff_domain": "ntu.sg+cispa.de;;deepmind.com+ucl.ac.uk;adelaide.edu.au;cispa.saarland",
        "position": "Assistant Professor+Postdoc;;Researcher+Professor;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nchau2025credal,\ntitle={Credal Two-Sample Tests of Epistemic Ignorance},\nauthor={Siu Lun Chau and Antonin Schrab and Arthur Gretton and Dino Sejdinovic and Krikamol Muandet},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yC1qOOzicM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yC1qOOzicM",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yGL5KBqnl4",
        "title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Prior-Fitted Networks (PFNs) have recently been proposed to efficiently perform tabular classification tasks. Although they achieve good performance on small datasets, they encounter limitations with larger datasets. These limitations include significant memory consumption and increased computational complexity, primarily due to the impracticality of incorporating all training samples as inputs within these networks. To address these challenges, we investigate the fitting assumption for PFNs and input samples. Building on this understanding, we propose *BoostPFN* designed to enhance the performance of these networks, especially for large-scale datasets. We also theoretically validate the convergence of BoostPFN and our empirical results demonstrate that the BoostPFN method can outperform standard PFNs with the same size of training samples in large datasets and achieve a significant acceleration in training times compared to other established baselines in the field, including widely-used Gradient Boosting Decision Trees (GBDTs), deep learning methods and AutoML systems. High performance is maintained for up to 50x of the pre-training size of PFNs, substantially extending the limit of training samples. Through this work, we address the challenges of efficiently handling large datasets via PFN-based models, paving the way for faster and more effective tabular data classification training and prediction process.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuxin Wang;Botian Jiang;YiranGuo;Quan Gan;David Wipf;Xuanjing Huang;Xipeng Qiu",
        "authorids": "~Yuxin_Wang3;~Botian_Jiang1;~YiranGuo1;~Quan_Gan1;~David_Wipf1;~Xuanjing_Huang1;~Xipeng_Qiu1",
        "gender": "M;M;M;M;M;F;M",
        "homepage": ";;;;http://www.davidwipf.com/;https://xuanjing-huang.github.io/;https://xpqiu.github.io/",
        "dblp": "68/1041;;;;81/6421;05/6735-1;69/1395",
        "google_scholar": "bTo8CT0AAAAJ;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.cz/citations?hl=zh-CN;;YJx1WSgAAAAJ;RGsMgZA4H78C;Pq4Yp_kAAAAJ",
        "orcid": ";;;0009-0002-0986-457X;;0000-0001-9197-9426;0000-0001-7163-5247",
        "linkedin": ";%E5%8D%9A%E5%A4%A9-%E5%A7%9C-01a120227?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BDQvpyioVTMKEM8AgLhbJKQ%3D%3D;;quan-gan-231992136/;;;",
        "or_profile": "~Yuxin_Wang3;~Botian_Jiang1;~YiranGuo1;~Quan_Gan1;~David_Wipf1;~Xuanjing_Huang1;~Xipeng_Qiu1",
        "aff": "Fudan University;Fudan University;Fudan University;Amazon;Amazon AI Research Lab;Fudan University;Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;amazon.com;amazon.com;fudan.edu.cn;fudan.edu.cn",
        "position": "PhD student;MS student;MS student;Researcher;Principal Research Scientist;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwang2025priorfitted,\ntitle={Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners},\nauthor={Yuxin Wang and Botian Jiang and YiranGuo and Quan Gan and David Wipf and Xuanjing Huang and Xipeng Qiu},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yGL5KBqnl4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yGL5KBqnl4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yMh0z4kTw8",
        "title": "Understanding Expert Structures on Minimax Parameter Estimation in Contaminated Mixture of Experts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We conduct the convergence analysis of parameter estimation in the contaminated mixture of experts. This model is motivated from the prompt learning problem where ones utilize prompts, which can be formulated as experts, to fine-tune a large-scale pre-trained model for learning downstream tasks. There are two fundamental challenges emerging from the analysis: (i) the proportion in the mixture of the pre-trained model and the prompt may converge to zero during the training, leading to the prompt vanishing issue; (ii) the algebraic interaction among parameters of the pre-trained model and the prompt can occur via some partial differential equations and decelerate the prompt learning. In response, we introduce a distinguishability condition to control the previous parameter interaction. Additionally, we also investigate various types of expert structure to understand their effects on the convergence behavior of parameter estimation. In each scenario, we provide comprehensive convergence rates of parameter estimation along with the corresponding minimax lower bounds. Finally, we run several numerical experiments to empirically justify our theoretical findings.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fanqi Yan;Huy Nguyen;Le Quang Dung;Pedram Akbarian;Nhat Ho",
        "authorids": "~Fanqi_Yan1;~Huy_Nguyen5;~Le_Quang_Dung1;~Pedram_Akbarian1;~Nhat_Ho1",
        "gender": "F;M;M;M;M",
        "homepage": "https://franziskayan.github.io/;https://huynm99.github.io/;https://quangdung0110.github.io;https://pedakb.github.io/;https://nhatptnk8912.github.io/",
        "dblp": "249/2741;48/6075;34/5055;358/2800;203/4479",
        "google_scholar": ";_YYwzhQAAAAJ;cc5JeMIAAAAJ;eg68QWIAAAAJ;https://scholar.google.ca/citations?user=Xs7cKMwAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";huy-nguyen-081199/;;;nhat-pham-minh-ho-267b8164/",
        "or_profile": "~Fanqi_Yan1;~Huy_Nguyen5;~Le_Quang_Dung1;~Pedram_Akbarian1;~Nhat_Ho1",
        "aff": "University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas, Austin",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nyan2025understanding,\ntitle={Understanding Expert Structures on Minimax Parameter Estimation in Contaminated Mixture of Experts},\nauthor={Fanqi Yan and Huy Nguyen and Le Quang Dung and Pedram Akbarian and Nhat Ho},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yMh0z4kTw8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yMh0z4kTw8",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yNeoBG7WOP",
        "title": "Reinforcement Learning for Infinite-Horizon Average-Reward Linear MDPs via Approximation by Discounted-Reward MDPs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of infinite-horizon average-reward reinforcement learning with linear Markov decision processes (MDPs). The associated Bellman operator of the problem not being a contraction makes the algorithm design challenging. Previous approaches either suffer from computational inefficiency or require strong assumptions on dynamics, such as ergodicity, for achieving a regret bound of $\\widetilde{\\mathcal{O}}(\\sqrt{T})$. In this paper, we propose the first algorithm that achieves $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret with computational complexity polynomial in the problem parameters, without making strong assumptions on dynamics. Our approach approximates the average-reward setting by a discounted MDP with a carefully chosen discounting factor, and then applies an optimistic value iteration. We propose an algorithmic structure that plans for a nonstationary policy through optimistic value iteration and follows that policy until a specified information metric in the collected data doubles. Additionally, we introduce a value function clipping procedure for limiting the span of the value function for sample efficiency.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kihyuk Hong;Woojin Chae;Yufan Zhang;Dabeen Lee;Ambuj Tewari",
        "authorids": "~Kihyuk_Hong1;~Woojin_Chae2;~Yufan_Zhang3;~Dabeen_Lee1;~Ambuj_Tewari1",
        "gender": "M;M;;;M",
        "homepage": ";;;https://dabeenl.github.io;https://www.ambujtewari.com",
        "dblp": ";;;180/3115.html;24/567",
        "google_scholar": ";;;8y38M00AAAAJ;ttbl4FsAAAAJ",
        "orcid": ";;;0000-0002-3802-1371;0000-0001-6969-7844",
        "linkedin": "hominot/;chaewoojin/;yufanzh/;dabeen-lee-071aa7220;",
        "or_profile": "~Kihyuk_Hong1;~Woojin_Chae2;~Yufan_Zhang3;~Dabeen_Lee1;~Ambuj_Tewari1",
        "aff": "University of Michigan - Ann Arbor;Korea Advanced Institute of Science & Technology;University of Michigan - Ann Arbor;Seoul National University+KAIST;University of Michigan - Ann Arbor",
        "aff_domain": "umich.edu;kaist.ac.kr;umich.edu;snu.ac.kr+kaist.ac.kr;umich.edu",
        "position": "PhD student;Undergrad student;Undergrad student;Assistant Professor+Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nhong2025reinforcement,\ntitle={Reinforcement Learning for Infinite-Horizon Average-Reward Linear {MDP}s via Approximation by Discounted-Reward {MDP}s},\nauthor={Kihyuk Hong and Woojin Chae and Yufan Zhang and Dabeen Lee and Ambuj Tewari},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yNeoBG7WOP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yNeoBG7WOP",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yQNhQP3s1o",
        "title": "Prediction-Centric Uncertainty Quantification via MMD",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Deterministic mathematical models, such as those specified via differential equations, are a powerful tool to communicate scientific insight.      However, such models are necessarily simplified descriptions of the real world.  Generalised Bayesian methodologies have been proposed for inference with misspecified models, but these are typically associated with vanishing parameter uncertainty as more data are observed.  In the context of a misspecified deterministic mathematical model, this has the undesirable consequence that posterior predictions become deterministic and certain, while being incorrect.  Taking this observation as a starting point, we propose *Prediction-Centric Uncertainty Quantification*, where a mixture distribution based on the deterministic model confers improved uncertainty quantification in the predictive context.  Computation of the mixing distribution is cast as a (regularised) gradient flow of the maximum mean discrepancy (MMD), enabling consistent numerical approximations to be obtained.  Results are reported on both a toy model from population ecology and a real model of protein signalling in cell biology.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zheyang Shen;Jeremias Knoblauch;Samuel Power;Chris J. Oates",
        "authorids": "~Zheyang_Shen1;~Jeremias_Knoblauch1;~Samuel_Power1;~Chris_J._Oates1",
        "gender": "M;M;M;",
        "homepage": ";https://jeremiasknoblauch.github.io/;https://sites.google.com/view/sp-monte-carlo/;https://oates.work",
        "dblp": "228/9233.html;220/5462;308/7223.html;118/6076",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=4TPsxlsAAAAJ;ePQTKrEAAAAJ;W_Ul5jMAAAAJ",
        "orcid": ";;0000-0001-8644-8014;",
        "linkedin": "zheyang-shen-64662787/;;samuel-power-6308b02b/;",
        "or_profile": "~Zheyang_Shen1;~Jeremias_Knoblauch1;~Samuel_Power1;~Chris_J._Oates1",
        "aff": "Newcastle University, UK;;University of Bristol;Newcastle University",
        "aff_domain": "newcastle.ac.uk;;bristol.ac.uk;ncl.ac.uk",
        "position": "Research associate;;Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nshen2025predictioncentric,\ntitle={Prediction-Centric Uncertainty Quantification via {MMD}},\nauthor={Zheyang Shen and Jeremias Knoblauch and Samuel Power and Chris J. Oates},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yQNhQP3s1o}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yQNhQP3s1o",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ybpHHnBf7x",
        "title": "Pure Exploration with Feedback Graphs",
        "track": "main",
        "status": "Oral",
        "tldr": "",
        "abstract": "We study the sample complexity of pure exploration in an online learning problem with a feedback graph. This graph dictates the feedback available to the learner, covering scenarios between full-information, pure bandit feedback, and settings with no feedback on the chosen action. While variants of this problem have been investigated for regret minimization, no prior work has addressed the pure exploration setting, which is the focus of our study. We derive an instance-specific lower bound on the sample complexity of learning the best action with fixed confidence, even when the feedback graph is unknown and stochastic, and present unidentifiability results for Bernoulli rewards. Additionally, our findings reveal how the sample complexity scales with key graph-dependent quantities. Lastly, we introduce TaS-FG (Track and Stop for Feedback Graphs), an asymptotically optimal algorithm, and demonstrate its efficiency across different graph configurations.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alessio Russo;Yichen Song;Aldo Pacchiano",
        "authorids": "~Alessio_Russo1;~Yichen_Song2;~Aldo_Pacchiano1",
        "gender": ";M;M",
        "homepage": ";;https://www.aldopacchiano.ai",
        "dblp": ";;129/6338",
        "google_scholar": ";;no_BfYgAAAAJ",
        "orcid": ";;",
        "linkedin": ";yichensong/;",
        "or_profile": "~Alessio_Russo1;~Yichen_Song2;~Aldo_Pacchiano1",
        "aff": ";Boston University;Boston University",
        "aff_domain": ";bu.edu;bu.edu",
        "position": ";PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nrusso2025pure,\ntitle={Pure Exploration with Feedback Graphs},\nauthor={Alessio Russo and Yichen Song and Aldo Pacchiano},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ybpHHnBf7x}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ybpHHnBf7x",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yhQKdKSio0",
        "title": "Robust Multi-fidelity Bayesian Optimization with Deep Kernel and Partition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-fidelity Bayesian optimization (MFBO) is a powerful approach that utilizes low-fidelity, cost-effective sources to expedite the exploration and exploitation of a high-fidelity objective function. Existing MFBO methods with theoretical foundations either lack justification for performance improvements over single-fidelity optimization or rely on strong assumptions about the relationships between fidelity sources to construct surrogate models and direct queries to low-fidelity sources. To mitigate the dependency on cross-fidelity assumptions while maintaining the advantages of low-fidelity queries, we introduce a random sampling and partition-based MFBO framework with deep kernel learning. This framework is robust to cross-fidelity model misspecification and explicitly illustrates the benefits of low-fidelity queries. Our results demonstrate that the proposed algorithm effectively manages complex cross-fidelity relationships and efficiently optimizes the target fidelity function.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fengxue Zhang;Thomas Desautels;Yuxin Chen",
        "authorids": "~Fengxue_Zhang1;~Thomas_Desautels1;~Yuxin_Chen1",
        "gender": "M;;",
        "homepage": ";;http://yuxinchen.org/",
        "dblp": ";124/8980;11/5123-1",
        "google_scholar": ";;-k1N7HAAAAAJ",
        "orcid": ";;",
        "linkedin": "fengxue-zhang-18b205146/;;",
        "or_profile": "~Fengxue_Zhang1;~Thomas_Desautels1;~Yuxin_Chen1",
        "aff": "University of Chicago;Lawrence Livermore National Labs;University of Chicago",
        "aff_domain": "uchicago.edu;llnl.gov;uchicago.edu",
        "position": "Ph.D. student;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025robust,\ntitle={Robust Multi-fidelity Bayesian Optimization with Deep Kernel and Partition},\nauthor={Fengxue Zhang and Thomas Desautels and Yuxin Chen},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=yhQKdKSio0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yhQKdKSio0",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ynLilizbOW",
        "title": "Improved dependence on coherence in eigenvector and eigenvalue estimation error bounds",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Spectral estimators are fundamental in low-rank matrix models and arise throughout machine learning and statistics, with applications including network analysis, matrix completion and PCA. These estimators aim to recover the leading eigenvalues and eigenvectors of an unknown signal matrix observed subject to noise. While extensive research has addressed the statistical accuracy of spectral estimators under a variety of conditions, most previous work has assumed that the signal eigenvectors are incoherent with respect to the standard basis. This assumption typically arises because of suboptimal dependence on coherence in one or more concentration inequalities.\nUsing a new matrix concentration result that may be of independent interest, we establish estimation error bounds for eigenvector and eigenvalue recovery whose dependence on coherence significantly improves upon prior work. Our results imply that coherence-free bounds can be achieved when the standard deviation of the noise is comparable to its Orlicz 1-norm (i.e., its subexponential norm). This matches known minimax lower bounds under Gaussian noise up to logarithmic factors.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hao Yan;Keith Levin",
        "authorids": "~Hao_Yan12;~Keith_Levin2",
        "gender": "M;",
        "homepage": "https://ezyhdxm.github.io/academic-site/;https://pages.stat.wisc.edu/~kdlevin/",
        "dblp": ";",
        "google_scholar": "fzYfS8sAAAAJ;qumjO10AAAAJ",
        "orcid": ";",
        "linkedin": "hao-yan-79aa0b169/;",
        "or_profile": "~Hao_Yan12;~Keith_Levin2",
        "aff": "University of Wisconsin - Madison;University of Wisconsin - Madison",
        "aff_domain": "wisc.edu;wisc.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nyan2025improved,\ntitle={Improved dependence on coherence in eigenvector and eigenvalue estimation error bounds},\nauthor={Hao Yan and Keith Levin},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=ynLilizbOW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ynLilizbOW",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "z9RDCfRZHw",
        "title": "ADEPT: Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction (ADEPT-PCA and ADEPT-AE) and personalized diffusion models (ADEPT-DGM). We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size). We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity. We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaan Ozkara;Bruce Huang;Ruida Zhou;Suhas Diggavi",
        "authorids": "~Kaan_Ozkara1;~Bruce_Huang1;~Ruida_Zhou1;~Suhas_Diggavi1",
        "gender": ";M;M;",
        "homepage": ";;https://sites.google.com/view/ruida-zhou;https://www.ee.ucla.edu/suhas-diggavi/",
        "dblp": ";;215/2026;d/SNDiggavi.html#j15",
        "google_scholar": "W-JoHj0AAAAJ;;kXbo1twAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";bruce-huang-106670170;;",
        "or_profile": "~Kaan_Ozkara1;~Bruce_Huang1;~Ruida_Zhou1;~Suhas_Diggavi1",
        "aff": "University of California, Los Angeles;University of California, Los Angeles;Amazon;University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu;amazon.com;ucla.edu",
        "position": "PhD student;PhD student;Applied Scientist;Professor",
        "bibtex": "@inproceedings{\nozkara2025adept,\ntitle={{ADEPT}: Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning},\nauthor={Kaan Ozkara and Bruce Huang and Ruida Zhou and Suhas Diggavi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=z9RDCfRZHw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=z9RDCfRZHw",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zO46aYZqrG",
        "title": "Automatically Adaptive Conformal Risk Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Science and technology have a growing need for effective mechanisms that ensure reliable, controlled performance from black-box machine learning algorithms. These performance guarantees should ideally hold conditionally on the input\u2014that is the performance guarantees should hold, at least approximately, no matter what the input. However, beyond stylized discrete groupings such as ethnicity and gender, the right notion of conditioning can be difficult to define. For example, in problems such as image segmentation, we want the uncertainty to reflect the intrinsic difficulty of the test sample, but this may be difficult to capture via a conditioning event. Building on the recent work of Gibbs et al. [2023], we propose a methodology for achieving approximate conditional control of statistical risks\u2014the expected value of loss functions\u2014by adapting to the difficulty of test samples. Our framework goes beyond traditional conditional risk \ncontrol based on user-provided conditioning events to the algorithmic, data-driven determination of appropriate function classes for conditioning. We apply this framework to various regression and segmentation tasks, enabling finer-grained control over model performance and demonstrating that by continuously monitoring and adjusting these parameters, we can achieve superior precision compared to conventional risk-control methods.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Vincent Blot;Anastasios Nikolas Angelopoulos;Michael Jordan;Nicolas J-B. Brunel",
        "authorids": "~Vincent_Blot1;~Anastasios_Nikolas_Angelopoulos1;~Michael_Jordan1;~Nicolas_J-B._Brunel1",
        "gender": "M;M;M;M",
        "homepage": "https://vincentblot28.github.io;http://angelopoulos.ai;http://www.cs.berkeley.edu/~jordan/;http://www.math-evry.cnrs.fr/members/nbrunel/welcome",
        "dblp": ";;j/MichaelIJordan;",
        "google_scholar": ";nfX25MMAAAAJ;https://scholar.google.com.tw/citations?user=yxUduqMAAAAJ;https://scholar.google.com/citations?hl=fr",
        "orcid": ";;0000-0001-8935-817X;0000-0002-2840-8484",
        "linkedin": ";anastasiosa/;;nicolasbrunel/",
        "or_profile": "~Vincent_Blot1;~Anastasios_Nikolas_Angelopoulos1;~Michael_Jordan1;~Nicolas_J-B._Brunel1",
        "aff": "Universit\u00e9 Paris-Saclay;University of California, Berkeley+University of California, Berkeley;University of California, Berkeley;Capgemini+ENSIIE",
        "aff_domain": "univ-paris-saclay.fr;berkeley.edu+berkeley.edu;berkeley.edu;capgemini.com+ensiie.fr",
        "position": "PhD student;Postdoc+PhD student;Full Professor;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nblot2025automatically,\ntitle={Automatically Adaptive Conformal Risk Control},\nauthor={Vincent Blot and Anastasios Nikolas Angelopoulos and Michael Jordan and Nicolas J-B. Brunel},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zO46aYZqrG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zO46aYZqrG",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zVVu6yFPbD",
        "title": "Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "One of the most basic problems in reinforcement learning (RL) is policy evaluation: estimating the long-term return, i.e., value function, corresponding to a given fixed policy. The celebrated Temporal Difference (TD) learning algorithm addresses this problem, and recent work has investigated finite-time convergence guarantees for this algorithm and variants thereof. However, these guarantees hinge on the reward observations being always generated from a well-behaved (e.g., sub-Gaussian) true reward distribution. Motivated by harsh, real-world environments where such an idealistic assumption may no longer hold, we revisit the policy evaluation problem from the perspective of *adversarial robustness*. In particular, we consider a Huber-contaminated reward model where an adversary can arbitrarily corrupt each reward sample with a small probability $\\epsilon$. Under this observation model, we first show that the adversary can cause the vanilla TD algorithm to converge to any arbitrary value function. We then develop a novel algorithm called *Robust-TD* and prove that its finite-time guarantees match that of vanilla TD with linear function approximation up to a small $O(\\epsilon)$ term that captures the effect of corruption. We complement this result with a minimax lower bound, revealing that such an additive corruption-induced term is unavoidable. To our knowledge, these results are the first of their kind in the context of adversarial robustness of stochastic approximation schemes driven by Markov noise. The key new technical tool that enables our results is an analysis of the Median-of-Means estimator with corrupted, time-correlated data that might be of independent interest to the literature on robust statistics.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sreejeet Maity;Aritra Mitra",
        "authorids": "~Sreejeet_Maity1;~Aritra_Mitra1",
        "gender": "M;M",
        "homepage": "https://smaity1729rl.wordpress.ncsu.edu/;https://amitra2.wordpress.ncsu.edu/",
        "dblp": "329/5043.html;169/2015.html",
        "google_scholar": "tpNoQ3AAAAAJ;5aUntRIAAAAJ",
        "orcid": "0009-0006-0209-6693;",
        "linkedin": "%F0%9D%9A%82%F0%9D%9A%9B%F0%9D%9A%8E%F0%9D%9A%8E%F0%9D%9A%93%F0%9D%9A%8E%F0%9D%9A%8E%F0%9D%9A%9D-%F0%9D%99%BC%F0%9D%9A%8A%F0%9D%9A%92%F0%9D%9A%9D%F0%9D%9A%A2-a3729b158/;aritra-mitra-695108b9/",
        "or_profile": "~Sreejeet_Maity1;~Aritra_Mitra1",
        "aff": "North Carolina State University;North Carolina State University",
        "aff_domain": "ncsu.edu;ncsu.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nmaity2025adversariallyrobust,\ntitle={Adversarially-Robust {TD} Learning with Markovian Data: Finite-Time Rates and Fundamental Limits},\nauthor={Sreejeet Maity and Aritra Mitra},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zVVu6yFPbD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zVVu6yFPbD",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zkTdiuQSnR",
        "title": "Cross Validation for Correlated Data in Classification Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a methodology for model evaluation and selection in binary classification models with the presence of correlations in the data, where the sampling mechanism violates the i.i.d. assumption. Our methodology involves a formulation of the bias term between the standard Cross-Validation (CV) estimator and the mean generalization error, and practical data-based procedures to estimate this term. Consequently, we present the bias-corrected CV estimator, which is the standard CV estimate supplemented by an estimate of the bias term. This concept was introduced in the literature only in the context of a linear model with squared error loss as the criterion for prediction performance. Our suggested bias-corrected CV estimator can be applied to any learning model, including deep neural networks, and to standard criteria for prediction performance for classification tasks, including misclassification rate, cross-entropy and hinge loss. We demonstrate the applicability of the proposed methodology in various scenarios where the data contains complex correlation structures (such as clustered and spatial relationships) with synthetic data and real-world datasets, providing evidence that the bias-corrected CV estimator is better than the standard CV estimator.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Oren Yuval;Saharon Rosset",
        "authorids": "~Oren_Yuval1;~Saharon_Rosset1",
        "gender": "M;",
        "homepage": "https://www.tau.ac.il/~saharon/;",
        "dblp": ";09/6597",
        "google_scholar": ";",
        "orcid": "0000-0002-3836-4403;",
        "linkedin": ";",
        "or_profile": "~Oren_Yuval1;~Saharon_Rosset1",
        "aff": "Tel Aviv University;Tel Aviv University",
        "aff_domain": "tau.ac.il;tau.ac.il",
        "position": "PhD student;Professor",
        "bibtex": "@inproceedings{\nyuval2025cross,\ntitle={Cross Validation for correlated data in Classification models},\nauthor={Oren Yuval and Saharon Rosset},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zkTdiuQSnR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zkTdiuQSnR",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zl7hfa06u4",
        "title": "Empirical Error Estimates for Graph Sparsification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graph sparsification is a well-established technique for accelerating graph-based learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such results are typically impractical at a numerical level. Taking an alternative approach, we propose to address these issues from a data-driven perspective by computing empirical error estimates. The proposed error estimates are highly versatile, and we demonstrate this in four use cases: Laplacian matrix approximation, graph cut queries, graph-structured regression, and spectral clustering. Moreover, we provide two theoretical guarantees for the error estimates, and explain why the cost of computing them is manageable in comparison to the overall cost of a typical graph sparsification workflow.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siyao Wang;Miles E. Lopes",
        "authorids": "~Siyao_Wang1;~Miles_E._Lopes1",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": "siyao-wang-441164176;",
        "or_profile": "~Siyao_Wang1;~Miles_E._Lopes1",
        "aff": "University of California, Davis;",
        "aff_domain": "ucdavis.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nwang2025empirical,\ntitle={Empirical Error Estimates for Graph Sparsification},\nauthor={Siyao Wang and Miles E. Lopes},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zl7hfa06u4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zl7hfa06u4",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zoIXnmw2sm",
        "title": "Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we propose a novel model called Recurrent Explicit Duration Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent explicit duration variables into the rSLDS model. We also propose an inference and learning scheme that involves the use of P\u00f3lya-gamma augmentation. We demonstrate the improved segmentation capabilities of our model on three benchmark datasets, including two quantitative datasets and one qualitative dataset.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Miko\u0142aj S\u0142upi\u0144ski",
        "authorids": "~Miko\u0142aj_S\u0142upi\u0144ski1",
        "gender": "M",
        "homepage": "",
        "dblp": "361/5168",
        "google_scholar": "d8FMaA0AAAAJ",
        "orcid": "0000-0001-7324-5357",
        "linkedin": "miko%C5%82aj-s%C5%82upi%C5%84ski-839509109/",
        "or_profile": "~Miko\u0142aj_S\u0142upi\u0144ski1",
        "aff": "University of Wroclaw",
        "aff_domain": "uni.wroc.pl",
        "position": "PhD student",
        "bibtex": "@inproceedings{\nsupinski2025bayesian,\ntitle={Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems},\nauthor={Miko{\\l}aj S{\\l}upi{\\'n}ski},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zoIXnmw2sm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zoIXnmw2sm",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zoou4Qa6lu",
        "title": "Learning the Pareto Front Using Bootstrapped Observation Samples",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We consider Pareto front identification~(PFI) for linear bandits (PFILin), i.e., the goal is to identify a set of arms with undominated \nmean reward vectors when the mean reward vector is a linear function of the context. \nPFILin includes the best arm identification problem and multi-objective active learning as special cases.   \nThe sample complexity of our proposed algorithm is optimal up to a logarithmic factor.   \nIn addition, the regret incurred by our algorithm during the estimation is within a logarithmic factor of the optimal regret among all algorithms that identify the Pareto front.\nOur key contribution is a new estimator that in every round updates the estimate for the unknown parameter along \\emph{multiple} context directions -- in contrast to the conventional estimator that only updates\nthe parameter estimate along the chosen context. \nThis allows us to use low-regret arms to collect information about Pareto optimal arms. \nOur key innovation is to reuse the exploration\nsamples multiple times; in contrast to conventional estimators that use each sample only once. \nNumerical experiments demonstrate that the proposed algorithm successfully identifies the Pareto front while controlling the regret.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wonyoung Kim;Garud Iyengar;assaf zeevi",
        "authorids": "~Wonyoung_Kim2;~Garud_Iyengar1;~assaf_zeevi2",
        "gender": "M;M;M",
        "homepage": ";http://www.columbia.edu/~gi10/;",
        "dblp": "32/680;i/GarudIyengar.html;85/2086",
        "google_scholar": "YRuPBq4AAAAJ;;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Wonyoung_Kim2;~Garud_Iyengar1;~assaf_zeevi2",
        "aff": ";Columbia University;Columbia University",
        "aff_domain": ";columbia.edu;columbia.edu",
        "position": ";Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nkim2025learning,\ntitle={Learning the Pareto Front Using Bootstrapped Observation Samples},\nauthor={Wonyoung Kim and Garud Iyengar and assaf zeevi},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zoou4Qa6lu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zoou4Qa6lu",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zpfWwq0Ytl",
        "title": "Variational Adversarial Training Towards Policies with Improved Robustness",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning (RL), while being the benchmark for policy formulation, often struggles to deliver robust solutions across varying scenarios, leading to marked performance drops under environmental perturbations.~Traditional adversarial training, based on a two-player max-min game, is known to bolster the robustness of RL agents, but it faces challenges: first, the complexity of the worst-case optimization problem may induce *over-optimism*, and second, the choice of a specific set of potential adversaries might lead to *over-pessimism* by considering implausible scenarios. In this work, we first observe that these two challenges do not balance out each other. Thus, we propose to apply variational optimization to optimize over the worst-case distribution of the adversary instead of a single worst-case adversary. Moreover, to counteract over-optimism, we train the RL agent to maximize the lower quantile of the cumulative rewards under worst-case adversary distribution. Our novel algorithm demonstrates a significant advancement over existing robust RL methods, corroborating the importance of the identified challenges and the effectiveness of our approach. To alleviate computational overhead associated with the proposed approach, we also propose a simplified version with lower computational burden and only minimal performance degradation. Extensive experiments validate that our approaches consistently yield policies with superior robustness.",
        "keywords": "",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juncheng Dong;Hao-Lun Hsu;Qitong Gao;Vahid Tarokh;Miroslav Pajic",
        "authorids": "~Juncheng_Dong1;~Hao-Lun_Hsu1;~Qitong_Gao1;~Vahid_Tarokh1;~Miroslav_Pajic2",
        "gender": ";M;M;;M",
        "homepage": ";https://hlhsu.github.io/;http://qitonggao.com;;http://people.duke.edu/~mp275/",
        "dblp": ";303/0321;238/5422;;74/7446.html",
        "google_scholar": ";h9qf9vUAAAAJ;Flv4SrsAAAAJ;;Fbn21-8AAAAJ",
        "orcid": ";;;;",
        "linkedin": ";hlhsu/;qitong-gao;;",
        "or_profile": "~Juncheng_Dong1;~Hao-Lun_Hsu1;~Qitong_Gao1;~Vahid_Tarokh1;~Miroslav_Pajic2",
        "aff": ";Duke University;Netflix;;Duke University",
        "aff_domain": ";duke.edu;netflix.com;;duke.edu",
        "position": ";PhD student;Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\ndong2025variational,\ntitle={Variational Adversarial Training Towards Policies with Improved Robustness},\nauthor={Juncheng Dong and Hao-Lun Hsu and Qitong Gao and Vahid Tarokh and Miroslav Pajic},\nbooktitle={The 28th International Conference on Artificial Intelligence and Statistics},\nyear={2025},\nurl={https://openreview.net/forum?id=zpfWwq0Ytl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zpfWwq0Ytl",
        "pdf_size": 0,
        "wc_review": "",
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    }
]