[
    {
        "id": "05T81ScPFb",
        "title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a 'foundation' model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input. Code, models, demo, and more results are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.",
        "keywords": "Novel View Synthesis;3D Gaussian Splatting;Monocular 3D Reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/a177979219657cb8bd5cf30b2b5b8d1b83074102.pdf",
        "author": "Stanislaw Szymanowicz;Eldar Insafutdinov;Chuanxia Zheng;Dylan Campbell;Joao F. Henriques;Christian Rupprecht;Andrea Vedaldi",
        "authorids": "~Stanislaw_Szymanowicz1;~Eldar_Insafutdinov3;~Chuanxia_Zheng1;~Dylan_Campbell1;~Joao_F._Henriques1;~Christian_Rupprecht1;~Andrea_Vedaldi1",
        "gender": ";M;M;M;M;M;M",
        "homepage": ";https://eldar.insafutdinov.com/;http://www.chuanxiaz.com/;https://sites.google.com/view/djcampbell;http://www.robots.ox.ac.uk/~joao/;http://chrirupp.github.io;https://www.robots.ox.ac.uk/~vedaldi/",
        "dblp": "295/8991.html;172/1246;195/8988;139/6663;31/8617.html;https://dblp.uni-trier.de/pid/76/744-1;99/2825",
        "google_scholar": "kPJKnRIAAAAJ;u4unGhAAAAAJ;mvpE6bIAAAAJ;https://scholar.google.com.au/citations?user=FayBF1AAAAAJ;aCQjyp0AAAAJ;https://scholar.google.de/citations?user=IrYlproAAAAJ;bRT7t28AAAAJ",
        "orcid": ";0000-0003-1447-121X;;0000-0002-4717-6850;;;0000-0003-1374-2858",
        "linkedin": ";eldar-insafutdinov-18845315/;chuanxia-zheng-80a3b8110/;;;;",
        "or_profile": "~Stanislaw_Szymanowicz1;~Eldar_Insafutdinov3;~Chuanxia_Zheng1;~Dylan_Campbell1;~Joao_F._Henriques1;~Christian_Rupprecht1;~Andrea_Vedaldi1",
        "aff": "University of Oxford+Google;Independent Researcher;University of Oxford;Australian National University;University of Oxford;University of Oxford;University of Oxford+Meta",
        "aff_domain": "robots.ox.ac.uk+google.com;robots.ox.ac.uk;ox.ac.uk;anu.edu.au;ox.ac.uk;ox.ac.uk;ox.ac.uk+meta.com",
        "position": "PhD student+Intern;Researcher;Postdoc;Lecturer;Principal Researcher;Associate Professor;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nszymanowicz2025flashd,\ntitle={Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image},\nauthor={Stanislaw Szymanowicz and Eldar Insafutdinov and Chuanxia Zheng and Dylan Campbell and Joao F. Henriques and Christian Rupprecht and Andrea Vedaldi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=05T81ScPFb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=05T81ScPFb",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "0CNSbBa85A",
        "title": "LoopSplat: Loop Closure by Registering 3D Gaussian Splats",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code will be made available.",
        "keywords": "RGBD-SLAM;Dense Reconstruction;Loop Closure;3DGS Registration",
        "primary_area": "",
        "supplementary_material": "/attachment/3d953d0f320018ac62f1c624f99343dcf093e027.zip",
        "author": "Liyuan Zhu;Yue Li;Erik Sandstr\u00f6m;Shengyu Huang;Konrad Schindler;Iro Armeni",
        "authorids": "~Liyuan_Zhu1;~Yue_Li12;~Erik_Sandstr\u00f6m1;~Shengyu_Huang1;~Konrad_Schindler1;~Iro_Armeni1",
        "gender": "M;;M;M;M;",
        "homepage": "https://www.zhuliyuan.net/;;https://eriksandstroem.github.io/;https://shengyuh.github.io;https://igp.ethz.ch/personen/person-detail.html?persid=143986;https://ir0.github.io/",
        "dblp": "320/6433;;192/9251;;73/488;132/5216",
        "google_scholar": "YQR0iBwAAAAJ;;https://scholar.google.com/citations?view_op=list_works;bBn1d1oAAAAJ;FZuNgqIAAAAJ;m2oTZkIAAAAJ",
        "orcid": ";;;;0000-0002-3172-9246;",
        "linkedin": "liyuan-zhu-7a6364194;;;shengyu-huang-2a50b2158/;konrad-schindler-5b0b22153/;",
        "or_profile": "~Liyuan_Zhu1;~Yue_Li12;~Erik_Sandstr\u00f6m1;~Shengyu_Huang1;~Konrad_Schindler1;~Iro_Armeni1",
        "aff": "Stanford University;;ETHZ - ETH Zurich;;Swiss Federal Institute of Technology;Stanford University",
        "aff_domain": "stanford.edu;;ethz.ch;;ethz.ch;stanford.edu",
        "position": "PhD student;;Postdoc;;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhu2025loopsplat,\ntitle={LoopSplat: Loop Closure by Registering 3D Gaussian Splats},\nauthor={Liyuan Zhu and Yue Li and Erik Sandstr{\\\"o}m and Shengyu Huang and Konrad Schindler and Iro Armeni},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=0CNSbBa85A}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0CNSbBa85A",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "18oW1dWMgp",
        "title": "Geometric Correspondence Consistency in RGB-D Relative Pose Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Relative pose estimation for RGB-D cameras is crucial in a number of applications. A typical approach relies on RANSAC to find a triplet pair of 3D point correspondences from which relative pose can be derived. A key aspect to this work ensures the geometric consistency of the triplet, {\\em i.e.}, pairwise distances between 3D points are preserved between the two views. Observe, however, that depth values are typically an order of magnitude less precise than feature locations, leading to large distance thresholds and admission of numerous false positives. This paper proposes that the constraint of 3D distance can be cast as a 2D constraint which we refer to as the {\\em Geometric Correspondence Constraint (GCC)}. This constraint states that given one pair of correspondences, the two images are partitioned into a family nested curves such that corresponding points must lie on corresponding curves. This can act as a filter in the RANSAC process with significant savings in computation and with increased robustness and accuracy as demonstrated in experiments using TUM, ICL-NUIM, and RGBD Scene v2 datasets.",
        "keywords": "geometric consistency;relative pose;RGB-D;multiview geometry",
        "primary_area": "",
        "supplementary_material": "/attachment/69da1667f49b3ab3b7ef7f2b47aadbf6832770f1.pdf",
        "author": "Sourav Kumar;Chiang-Heng Chien;Benjamin Kimia",
        "authorids": "~Sourav_Kumar1;~Chiang-Heng_Chien1;~Benjamin_Kimia1",
        "gender": "M;M;M",
        "homepage": ";;",
        "dblp": ";;90/2917",
        "google_scholar": ";bZ5gYbUAAAAJ;",
        "orcid": "0000-0003-0352-8989;;",
        "linkedin": ";;",
        "or_profile": "~Sourav_Kumar1;~Chiang-Heng_Chien1;~Benjamin_Kimia1",
        "aff": ";Brown University;Brown University",
        "aff_domain": ";brown.edu;brown.edu",
        "position": ";PhD student;Full Professor",
        "bibtex": "@inproceedings{\nkumar2025geometric,\ntitle={Geometric Correspondence Consistency in {RGB}-D Relative Pose Estimation},\nauthor={Sourav Kumar and Chiang-Heng Chien and Benjamin Kimia},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=18oW1dWMgp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=18oW1dWMgp",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1NhnG9BvQB",
        "title": "SMORE: Simultaneous Map and Object REconstruction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a method for dynamic surface reconstruction of large-scale urban scenes from LiDAR. Depth-based reconstructions tend to focus on small-scale objects or large-scale SLAM reconstructions that treat moving objects as outliers. We take a holistic perspective and optimize a compositional model of a dynamic scene that decomposes the world into rigidly-moving objects and the background. To achieve this, we take inspiration from recent novel view synthesis methods and frame the reconstruction problem as a global optimization over neural surfaces, ego poses, and object poses, which minimizes the error between composed spacetime surfaces and input LiDAR scans. In contrast to view synthesis methods, which typically minimize 2D errors with gradient descent, we minimize a 3D point-to-surface error by coordinate descent, which we decompose into registration and surface reconstruction steps. Each step can be handled well by off-the-shelf methods without any re-training. We analyze the surface reconstruction step for rolling-shutter LiDARs, and show that deskewing operations common in continuous time SLAM can be applied to dynamic objects as well, improving results over prior art by 10X. Beyond pursuing dynamic reconstruction as a goal in and of itself, we propose that such a system can be used to auto-label partially annotated sequences and produce ground truth annotation for hard-to-label problems such as depth completion and scene flow.",
        "keywords": "LiDAR scene reconstruction;surface reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/623fcbd867e03e56cb177b80dc1287d17d978b23.pdf",
        "author": "Nathaniel Eliot Chodosh;Anish Madan;Simon Lucey;Deva Ramanan",
        "authorids": "~Nathaniel_Eliot_Chodosh1;~Anish_Madan1;~Simon_Lucey2;~Deva_Ramanan1",
        "gender": "M;M;M;M",
        "homepage": "https://nchodosh.github.io;;https://www.adelaide.edu.au/directory/simon.lucey;https://www.cs.cmu.edu/~deva/",
        "dblp": ";265/6058;01/3542;49/488",
        "google_scholar": "b4qKr7gAAAAJ;eZ4WZmIAAAAJ;vmAe35UAAAAJ;9B8PoXUAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Nathaniel_Eliot_Chodosh1;~Anish_Madan1;~Simon_Lucey2;~Deva_Ramanan1",
        "aff": "Villanova University;Carnegie Mellon University;University of Adelaide;School of Computer Science, Carnegie Mellon University",
        "aff_domain": "villanova.edu;andrew.cmu.edu;adelaide.edu.au;cs.cmu.edu",
        "position": "Assistant Professor;Researcher;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nchodosh2025smore,\ntitle={{SMORE}: Simultaneous Map and Object {RE}construction},\nauthor={Nathaniel Eliot Chodosh and Anish Madan and Simon Lucey and Deva Ramanan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=1NhnG9BvQB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1NhnG9BvQB",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "1xLfRYErqk",
        "title": "UniMotion: Unifying 3D Human Motion Synthesis and Understanding",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We introduce UniMotion, the first unified multi-task human motion model capable of both flexible motion control and frame-level motion understanding. \nWhile existing works control avatar motion with global text conditioning, or with fine-grained per frame scripts, none can do both at once.\nIn addition, none of the existing works can output frame-level text paired with the generated poses.\nIn contrast, UniMotion allows to control motion with global text, local frame-level text, or both at once, providing more flexible control for users.\nImportantly, UniMotion is the first model which by design outputs local text paired with the generated poses, allowing users to know what motion happens and when, which is necessary for a wide range of applications.\nWe show UniMotion opens up new applications: 1.) hierarchical control, allowing users to specify motion at different levels of detail,\n2.) obtaining motion text descriptions for existing MoCap data or YouTube videos\n3.) allowing for editability, generating motion from text, and editing the motion via text edits. \nMoreover, UniMotion attains state-of-the-art results for the frame-level text-to-motion task on the established HumanML3D dataset.\nThe pre-trained model and code will be made public upon publication.\nWe urge readers to see our results in motion in the supplementary video.",
        "keywords": "3D Human Motion Synthesis;3D Human Motion Understanding",
        "primary_area": "",
        "supplementary_material": "/attachment/1f845ca16a48ab0c0d2d21771af2a8e61f48b264.zip",
        "author": "Chuqiao Li;Julian Chibane;Yannan He;Naama Pearl;Andreas Geiger;Gerard Pons-Moll",
        "authorids": "~Chuqiao_Li1;~Julian_Chibane1;~Yannan_He1;~Naama_Pearl1;~Andreas_Geiger3;~Gerard_Pons-Moll2",
        "gender": "F;;M;;M;",
        "homepage": "https://coral79.github.io/;https://virtualhumans.mpi-inf.mpg.de/people/Chibane.html;https://virtualhumans.mpi-inf.mpg.de/people/He.html;;http://www.cvlibs.net;",
        "dblp": "320/3506;260/0133;287/4831;;40/5825-1;",
        "google_scholar": "oPGWc44AAAAJ;;GoGwj4YAAAAJ;;https://scholar.google.ca/citations?hl=en;",
        "orcid": ";;;;0000-0002-8151-3726;",
        "linkedin": ";;;naama-pearl-3112b5188/;;",
        "or_profile": "~Chuqiao_Li1;~Julian_Chibane1;~Yannan_He1;~Naama_Pearl1;~Andreas_Geiger3;~Gerard_Pons-Moll2",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Saarland Informatics Campus, Max-Planck Institute;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;University of Haifa;University of Tuebingen;",
        "aff_domain": "uni-tuebingen.de;mpi-inf.mpg.de;uni-tuebingen.de;haifa.ac.il;uni-tuebingen.de;",
        "position": "PhD student;PhD student;PhD student;MS student;Professor;",
        "bibtex": "@inproceedings{\nli2025unimotion,\ntitle={UniMotion: Unifying 3D Human Motion Synthesis and Understanding},\nauthor={Chuqiao Li and Julian Chibane and Yannan He and Naama Pearl and Andreas Geiger and Gerard Pons-Moll},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=1xLfRYErqk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1xLfRYErqk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "22Pr4VxcTL",
        "title": "Approximate 2D-3D Shape Matching for Interactive Applications",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Matching a 2D contour to a non-rigidly deformed 3D mesh is a challenging problem due to ambiguities arising from dimensionality differences. In the past, product graph based methods were only able to either produce fast but noisy solutions, or smooth but slow solutions (the latter enabled by higher-order costs computed in the conjugate product graph). In this work, we propose an approximation of these higher-order costs so that they can be computed in the ordinary product graph. This leads to an efficient algorithm for high-quality 2D-3D shape matching and enables novel applications, like an interactive user interface which allows to refine the solution gradually. We show theoretically that our method is efficient, and we experimentally validate that the accuracy gap of our approximation to the optimum is small in practice. Our code is available.",
        "keywords": "shape matching;2D-to-3D;Dijkstra;product graph;approximation",
        "primary_area": "",
        "supplementary_material": "/attachment/69b3568796f12351955a68a96db2d8709eba0f79.zip",
        "author": "Christoph Petzsch;Paul Roetzer;Zorah L\u00e4hner;Florian Bernard",
        "authorids": "~Christoph_Petzsch1;~Paul_Roetzer1;~Zorah_L\u00e4hner1;~Florian_Bernard3",
        "gender": "M;M;F;",
        "homepage": ";https://paulroetzer.github.io/;https://geometryinml.cs.uni-bonn.de;https://florianbernard.net",
        "dblp": ";313/2161;175/1635;134/8112",
        "google_scholar": ";UtPc6sYAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.de/citations?user=9GrQ2KYAAAAJ",
        "orcid": ";;0000-0003-0599-094X;",
        "linkedin": "christoph-petzsch-45597b31a/;paul-roetzer/;;",
        "or_profile": "~Christoph_Petzsch1;~Paul_Roetzer1;~Zorah_L\u00e4hner1;~Florian_Bernard3",
        "aff": "Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn+Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn;Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn, Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn;Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn;Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn",
        "aff_domain": "uni-bonn.de+uni-bonn.de;cs.uni-bonn.de;uni-bonn.de;uni-bonn.de",
        "position": "MS student+Undergrad student;PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\npetzsch2025approximate,\ntitle={Approximate 2D-3D Shape Matching for Interactive Applications},\nauthor={Christoph Petzsch and Paul Roetzer and Zorah L{\\\"a}hner and Florian Bernard},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=22Pr4VxcTL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=22Pr4VxcTL",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "2hrkfLpkLp",
        "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper targets interactive object-level editing (\\textit{e.g.}, deletion, recoloring, transformation, composition) in dynamic scenes.\nRecently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited.\nTo solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame.\nSpecifically, we extend the original dynamic NeRF by incorporating Hybrid Semantic Feature Distillation to maintain spatial-temporal consistency after editing.\nIn addition, a Recursive Selection Refinement module is presented \n to significantly boost object segmentation accuracy within a dynamic NeRF to aid the editing process.\nMoreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing.\nExtensive quantitative and qualitative experiments on real application scenarios demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. \nProject page: \\href{https://patrickddj.github.io/4D-Editor}{https://patrickddj.github.io/4D-Editor}",
        "keywords": "Dynamic NeRF Editing;Computer Vision",
        "primary_area": "",
        "supplementary_material": "/attachment/86e8729213805d0c9ac76c7735200519d2ecf6d2.pdf",
        "author": "DaDong Jiang;Zhihui Ke;Xiaobo Zhou;Tie Qiu;Xidong Shi;Hao Yan",
        "authorids": "~DaDong_Jiang1;~Zhihui_Ke1;~Xiaobo_Zhou5;~Tie_Qiu2;~Xidong_Shi1;~Hao_Yan10",
        "gender": "M;;;M;M;M",
        "homepage": "https://github.com/PatrickDDj?tab=repositories;;;https://www.tieqiu.net;https://github.com/twtsuif;https://github.com/haoyan14",
        "dblp": ";276/4518;;;;",
        "google_scholar": ";;;;;",
        "orcid": ";0009-0002-4042-7044;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~DaDong_Jiang1;~Zhihui_Ke1;~Xiaobo_Zhou5;~Tie_Qiu2;~Xidong_Shi1;~Hao_Yan10",
        "aff": "Tianjin University;Tianjin University;;Tianjin University;Tianjin University;Tianjin University",
        "aff_domain": "tju.edu.cn;tju.edu.cn;;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "position": "MS student;PhD student;;Full Professor;Undergrad student;MS student",
        "bibtex": "@inproceedings{\njiang2025deditor,\ntitle={4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation},\nauthor={DaDong Jiang and Zhihui Ke and Xiaobo Zhou and Tie Qiu and Xidong Shi and Hao Yan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=2hrkfLpkLp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2hrkfLpkLp",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "347Ack86Q5",
        "title": "ARC-Flow : Articulated, Resolution-Agnostic, Correspondence-Free Matching and Interpolation of 3D Shapes under Flow Fields",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "This work presents a unified framework for the unsupervised prediction of physically plausible interpolations between two 3D articulated shapes and the automatic estimation of dense correspondence between them. \n\nInterpolation is modelled as a diffeomorphic transformation using a smooth, time-varying flow field governed by Neural Ordinary Differential Equations (ODEs). This ensures topological consistency and non-intersecting trajectories while accommodating hard constraints, such as volume preservation, and soft constraints, e.g physical priors. \n\nCorrespondence is recovered using an efficient Varifold formulation, that is effective on high-fidelity surfaces with differing parameterizations. \n\nA simple skeleton structure augments the source shape, imposing physically motivated constraints on the deformation field and aiding in resolving symmetric ambiguities, without requiring skinning weights or prior knowledge of the skeleton's target pose configuration.\n\nQualitative and quantitative results demonstrate competitive or superior performance over existing state-of-the-art approaches in both shape correspondence and interpolation tasks across standard datasets.",
        "keywords": "Shape Interpolation;Shape Registration;3D Articulated Shapes;Diffeomorphic Transformations;Neural Ordinary Differential Equations (NODEs);Geometric Measure Theory",
        "primary_area": "",
        "supplementary_material": "/attachment/593cc7fc5983e5a9b191554cdcfeabc6525a9c1b.pdf",
        "author": "Adam Hartshorne;Allen Paul;Tony Shardlow;Neill D. F. Campbell",
        "authorids": "~Adam_Hartshorne1;~Allen_Paul1;~Tony_Shardlow1;~Neill_D._F._Campbell1",
        "gender": "M;M;;",
        "homepage": "https://www.camera.ac.uk/;https://researchportal.bath.ac.uk/en/persons/allen-paul;https://people.bath.ac.uk/tjs42/;",
        "dblp": "305/0185.html;380/0344;;",
        "google_scholar": "Gkzm2IIAAAAJ;;zReZ_jsAAAAJ;",
        "orcid": "0009-0005-4679-6096;0009-0000-7962-3080;0000-0003-1154-855X;",
        "linkedin": ";;;",
        "or_profile": "~Adam_Hartshorne1;~Allen_Paul1;~Tony_Shardlow1;~Neill_D._F._Campbell1",
        "aff": "University of Bath;University of Bath;University of Bath;",
        "aff_domain": "bath.ac.uk;bath.ac.uk;bath.ac.uk;",
        "position": "Postdoc;PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nhartshorne2025arcflow,\ntitle={{ARC}-Flow : Articulated, Resolution-Agnostic, Correspondence-Free Matching and Interpolation of 3D Shapes under Flow Fields},\nauthor={Adam Hartshorne and Allen Paul and Tony Shardlow and Neill D. F. Campbell},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=347Ack86Q5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=347Ack86Q5",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "3D353zoRwU",
        "title": "ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D Part Segmentation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Zero-shot 3D part segmentation is a challenging and fundamental task. In this work, we propose a novel pipeline, ZeroPS, which achieves high-quality knowledge transfer from 2D pretrained foundation models (FMs), SAM and GLIP, to 3D object point clouds. We aim to explore the natural relationship between multi-view correspondence and the FMs' prompt mechanism and build bridges on it. In ZeroPS, the relationship manifests as follows: 1) lifting 2D to 3D by leveraging co-viewed regions and SAM's prompt mechanism, 2) relating 1D classes to 3D parts by leveraging 2D-3D view projection and GLIP's prompt mechanism, and 3) enhancing prediction performance by leveraging multi-view observations. Extensive evaluations on the PartNetE and AKBSeg benchmarks demonstrate that ZeroPS significantly outperforms the SOTA method across zero-shot unlabeled and instance segmentation tasks. ZeroPS does not require additional training or fine-tuning for the FMs. ZeroPS applies to both simulated and real-world data. It is hardly affected by domain shift. The project page is available at https://luis2088.github.io/ZeroPS_page/.",
        "keywords": "3D part segmentation;zero-shot;foundation modal",
        "primary_area": "",
        "supplementary_material": "/attachment/8a2acc7b8bc3f0382cf126f797d92e07ad3788ba.pdf",
        "author": "Yuheng Xue;Nenglun Chen;Jun Liu;Wenyun Sun",
        "authorids": "~Yuheng_Xue1;~Nenglun_Chen1;~Jun_Liu8;~Wenyun_Sun1",
        "gender": ";M;M;M",
        "homepage": ";https://scholar.google.com/citations?user=UhjTC7AAAAAJ;;https://faculty.nuist.edu.cn/sunwenyun/en",
        "dblp": ";230/7699.html;95/3736-36;196/3722",
        "google_scholar": ";UhjTC7AAAAAJ;Q5Ild8UAAAAJ;U3GWBDwAAAAJ",
        "orcid": ";;;0000-0002-2049-3960",
        "linkedin": ";;;wenyun-sun-\u5b59\u6587\u8d5f-108417101",
        "or_profile": "~Yuheng_Xue1;~Nenglun_Chen1;~Jun_Liu8;~Wenyun_Sun1",
        "aff": ";Nanjing University of Information Science and Technology;Lancaster University;Nanjing University of Information Science and Technology",
        "aff_domain": ";nuist.edu.cn;lancaster.ac.uk;nuist.edu.cn",
        "position": ";Lecturer;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nxue2025zerops,\ntitle={Zero{PS}: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D Part Segmentation},\nauthor={Yuheng Xue and Nenglun Chen and Jun Liu and Wenyun Sun},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=3D353zoRwU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3D353zoRwU",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "45FxQkTEP3",
        "title": "Fully-Geometric Cross-attention for Point Cloud Registration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Point cloud registration approaches often fail when the overlap between point clouds is low due to noisy point correspondences. This work introduces a novel cross-attention mechanism tailored for Transformer-based architectures that tackles this problem, by fusing information from coordinates and features at the super-point level between point clouds. This formulation has remained unexplored primarily because it must guarantee rotation and translation invariance since point clouds reside in different and independent reference frames. We integrate the Gromov\u2013Wasserstein distance into the cross-attention formulation to jointly compute distances between points across different point clouds\nand account for their geometric structure. By doing so, points from two distinct point clouds can attend to each other under arbitrary rigid transformations. At the point level, we also devise a self-attention mechanism that aggregates the local geometric structure information into point features for fine matching. Our formulation boosts the number of inlier correspondences, thereby yielding more precise registration results compared to state-of-the-art approaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch, KITTI, and 3DCSR datasets. Project page: https://github.com/twowwj/FLAT.",
        "keywords": "3D point cloud; Registration;",
        "primary_area": "",
        "supplementary_material": "/attachment/7968dae90701f72c0742e8bbef7e5ce2dc926e90.pdf",
        "author": "Weijie Wang;Guofeng Mei;Jian Zhang;Nicu Sebe;Bruno Lepri;Fabio Poiesi",
        "authorids": "~Weijie_Wang1;~Guofeng_Mei1;~Jian_Zhang3;~Nicu_Sebe1;~Bruno_Lepri1;~Fabio_Poiesi3",
        "gender": "M;M;M;M;M;M",
        "homepage": ";https://gfmei.github.io/;https://www.uts.edu.au/staff/jian.zhang;http://disi.unitn.it/~sebe/;;https://fabiopoiesi.github.io/",
        "dblp": ";197/7875;07/314-2;20/3519;99/6489;87/8843",
        "google_scholar": "https://scholar.google.it/citations?user=AtB0KFsAAAAJ;VsmIGqsAAAAJ;https://scholar.google.com.au/citations?hl=en;https://scholar.google.it/citations?user=stFCYOAAAAAJ;JfcopG0AAAAJ;https://scholar.google.co.uk/citations?user=BQ7li6AAAAAJ",
        "orcid": "0000-0002-1168-3527;0000-0002-0494-5031;0000-0002-7240-3541;0000-0002-6597-7248;0000-0003-1275-2333;0000-0002-9769-1279",
        "linkedin": ";;;;brunolepri/?originalSubdomain=it;",
        "or_profile": "~Weijie_Wang1;~Guofeng_Mei1;~Jian_Zhang3;~Nicu_Sebe1;~Bruno_Lepri1;~Fabio_Poiesi3",
        "aff": "University of Trento;Fondazione Bruno Kessler;University of Technology Sydney;University of Trento;Fondazione Bruno Kessler;Fondazione Bruno Kessler",
        "aff_domain": "unitn.it;fbk.eu;uts.edu.au;unitn.it;fbk.eu;fbk.eu",
        "position": "PhD student;Researcher;Full Professor;Full Professor;Principal Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nwang2025fullygeometric,\ntitle={Fully-Geometric Cross-attention for Point Cloud Registration},\nauthor={Weijie Wang and Guofeng Mei and Jian Zhang and Nicu Sebe and Bruno Lepri and Fabio Poiesi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=45FxQkTEP3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=45FxQkTEP3",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "470WxVD1L3",
        "title": "LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The rise of Extended Reality (XR) requires efficient streaming of 3D online worlds, challenging current 3DGS representations to adapt to bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS that supports adaptive streaming and progressive rendering. Our method constructs a layered structure for cumulative representation, incorporates dynamic opacity optimization to maintain visual fidelity, and utilizes occupancy maps to efficiently manage Gaussian splats. This proposed model offers a progressive representation supporting a continuous rendering quality adapted for bandwidth-aware streaming. Extensive experiments validate the effectiveness of our approach in balancing visual fidelity with the compactness of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in LPIPS with 23% of the original model size, and shows its potential for bandwidth-adapted 3D streaming and rendering applications.",
        "keywords": "Adaptive Streaming;3D Gaussian Splatting;Level-of-Detail",
        "primary_area": "",
        "supplementary_material": "/attachment/df46a6b04ba101210f7faea88744f714dbb4681e.pdf",
        "author": "Yuang Shi;G\u00e9raldine Morin;Simone Gasparini;Wei Tsang Ooi",
        "authorids": "~Yuang_Shi1;~G\u00e9raldine_Morin2;~Simone_Gasparini3;~Wei_Tsang_Ooi1",
        "gender": "M;F;M;",
        "homepage": "https://yuang-ian.github.io/;;;",
        "dblp": ";14/1943;;",
        "google_scholar": "ytwibHUAAAAJ;;;",
        "orcid": "0000-0002-7893-8512;;0000-0001-8239-8005;",
        "linkedin": ";;;",
        "or_profile": "~Yuang_Shi1;~G\u00e9raldine_Morin2;~Simone_Gasparini3;~Wei_Tsang_Ooi1",
        "aff": "National University of Singapore;University of Toulouse;IRIT;",
        "aff_domain": "nus.edu.sg;irit.fr;irit.fr;",
        "position": "PhD student;Full Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nshi2025lapisgs,\ntitle={Lapis{GS}: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming},\nauthor={Yuang Shi and G{\\'e}raldine Morin and Simone Gasparini and Wei Tsang Ooi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=470WxVD1L3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=470WxVD1L3",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "5uw1GRBFoT",
        "title": "MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Structure-from-Motion (SfM), a task aiming at jointly recovering camera poses and 3D geometry of a scene given a set of images, remains a hard problem with still many open challenges despite decades of significant progress. The traditional solution for SfM consists of a complex pipeline of minimal solvers which tends to propagate errors and fails when images do not sufficiently overlap, have too little motion, etc. Recent methods have attempted to revisit this paradigm, but we empirically show that they fall short of fixing these core issues. In this paper, we propose instead to build upon a recently released foundation model for 3D vision that can robustly produce local 3D reconstructions and accurate matches. We introduce a low-memory approach to accurately align these local reconstructions in a global coordinate system. We further show that such foundation models can serve as efficient image retrievers without any overhead, reducing the overall complexity from quadratic to linear. Overall, our novel SfM pipeline is simple, scalable, fast and truly unconstrained, i.e. it can handle any collection of images, ordered or not. Extensive experiments on multiple benchmarks show that our method provides steady performance across diverse settings, especially outperforming existing methods in small- and medium-scale settings.",
        "keywords": "large-scale structure-from-motion (SfM);multiview 3D reconstruction;camera pose estimation",
        "primary_area": "",
        "supplementary_material": "/attachment/88c94c556001e708693418d6782acaeac5b3b5a0.pdf",
        "author": "Bardienus Pieter Duisterhof;Lojze Zust;Philippe Weinzaepfel;Vincent Leroy;Yohann Cabon;Jerome Revaud",
        "authorids": "~Bardienus_Pieter_Duisterhof1;~Lojze_Zust1;~Philippe_Weinzaepfel1;~Vincent_Leroy3;~Yohann_Cabon1;~Jerome_Revaud1",
        "gender": "M;M;M;M;;M",
        "homepage": "https://bart-ai.com;https://vicos.si/people/lojze_zust/;https://europe.naverlabs.com/people_user/Philippe-Weinzaepfel/;https://europe.naverlabs.com/people_user/vincent-leroy/;https://europe.naverlabs.com/people_user/yohann-cabon/;https://europe.naverlabs.com/people_user/Jerome-Revaud/",
        "dblp": "243/5766;298/8098.html;29/9989;02/7933-2;180/5563;17/6506",
        "google_scholar": "LLsYMFYAAAAJ;fqJskugAAAAJ;https://scholar.google.fr/citations?user=LSxIJ5cAAAAJ;https://scholar.google.fr/citations?user=HKFj2wkAAAAJ;;https://scholar.google.fr/citations?user=asmBzogAAAAJ",
        "orcid": ";0000-0002-6171-2920;;;0009-0005-6295-2167;",
        "linkedin": ";lojze-zust/;;;;",
        "or_profile": "~Bardienus_Pieter_Duisterhof1;~Lojze_Zust1;~Philippe_Weinzaepfel1;~Vincent_Leroy3;~Yohann_Cabon1;~Jerome_Revaud1",
        "aff": "Carnegie Mellon University;Naver Labs Europe+Naver Labs Europe+University of Ljubljana;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe;Naver Labs Europe",
        "aff_domain": "cmu.edu;naverlabs.com+naverlabs.com+uni-lj.si;naverlabs.com;naverlabs.com;naverlabs.com;naverlabs.com",
        "position": "PhD student;Researcher+Intern+PhD student;Research Scientist;Research Scientist;Research Engineer;Principal Researcher",
        "bibtex": "@inproceedings{\nduisterhof2025mastrsfm,\ntitle={{MAS}t3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion},\nauthor={Bardienus Pieter Duisterhof and Lojze Zust and Philippe Weinzaepfel and Vincent Leroy and Yohann Cabon and Jerome Revaud},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=5uw1GRBFoT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5uw1GRBFoT",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "70PWK4XgxQ",
        "title": "SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging. Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description. We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning. SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. These programs are generalized into compact, editable meta-programs. When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects. Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples. Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.",
        "keywords": "scene generation;procedural modeling;neurosymbolic methods;visual programs",
        "primary_area": "",
        "supplementary_material": "/attachment/6e812782435a71ce3e169fb8295a77a23262c121.zip",
        "author": "Hou In Ivan Tam;Hou In Derek Pun;Austin Wang;Angel X Chang;Manolis Savva",
        "authorids": "~Hou_In_Ivan_Tam1;~Hou_In_Derek_Pun1;~Austin_Wang2;~Angel_X_Chang1;~Manolis_Savva1",
        "gender": "M;;M;F;M",
        "homepage": "https://iv-t.github.io/;https://houip.github.io/;https://atwang16.github.io;https://angelxuanchang.github.io;http://msavva.github.io/",
        "dblp": "364/7339;384/9610;359/9540.html;46/10489;21/9924",
        "google_scholar": "https://scholar.google.ca/citations?user=Qlxk_ZYAAAAJ;https://scholar.google.com/citations?hl=en;5bZFMK8AAAAJ;8gfs8XIAAAAJ;4D2vsdYAAAAJ",
        "orcid": ";;;0009-0003-5055-6437;0000-0001-6132-8964",
        "linkedin": "ivantamh73i;houinpun/;austin-wang-234b3b162/;;manolis-savva-39591a2b/",
        "or_profile": "~Hou_In_Ivan_Tam1;~Hou_In_Derek_Pun1;~Austin_Wang2;~Angel_X_Chang1;~Manolis_Savva1",
        "aff": "Simon Fraser University+Simon Fraser University;Simon Fraser University;Simon Fraser University;Simon Fraser University;Simon Fraser University",
        "aff_domain": "sfu.ca+sfu.ca;sfu.ca;sfu.ca;sfu.ca;sfu.ca",
        "position": "PhD student+MS student;MS student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\ntam2025scenemotifcoder,\ntitle={SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements},\nauthor={Hou In Ivan Tam and Hou In Derek Pun and Austin Wang and Angel X Chang and Manolis Savva},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=70PWK4XgxQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=70PWK4XgxQ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7KykU750WQ",
        "title": "DEGAS: Detailed Expressions on Full-body Gaussian Avatars",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Although neural rendering has made significant advances in creating lifelike, animatable full-body and head avatars, incorporating detailed expressions into full-body avatars remains largely unexplored.\nWe present DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for full-body avatars with rich facial expressions.\nTrained on multiview videos of a given subject, our method learns a conditional variational autoencoder that takes both the body motion and facial expression as driving signals to generate Gaussian maps in the UV layout.\nTo drive the facial expressions, instead of the commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression latent space trained solely on 2D portrait images, bridging the gap between 2D talking faces and 3D avatars. \nLeveraging the rendering capability of 3DGS and the rich expressiveness of the expression latent space, the learned avatars can be reenacted to reproduce photorealistic rendering images with subtle and accurate facial expressions.\nExperiments on an existing dataset and our newly proposed dataset of full-body talking avatars demonstrate the efficacy of our method. \nWe also propose an audio-driven extension of our method with the help of 2D talking faces, opening new possibilities for interactive AI agents.",
        "keywords": "Animatable Avatar;Human Reconstruction;View Synthesis;Animation;Tele-presentation",
        "primary_area": "",
        "supplementary_material": "/attachment/69efbf2c974a9d9699c9a44f377735a72ad9ba11.zip",
        "author": "Zhijing Shao;Duotun Wang;Qing-Yao Tian;Yao-Dong Yang;Hengyu Meng;Zeyu Cai;BO DONG;Yu Zhang;Kang Zhang;Zeyu Wang",
        "authorids": "~Zhijing_Shao1;~Duotun_Wang1;~Qing-Yao_Tian1;~Yao-Dong_Yang2;~Hengyu_Meng1;~Zeyu_Cai2;~BO_DONG15;~Yu_Zhang48;~Kang_Zhang9;~Zeyu_Wang15",
        "gender": "M;M;;;M;M;F;M;M;M",
        "homepage": "https://initialneil.github.io/;https://www.duotun-wang.co.uk/;;;https://hengyumeng.github.io/;https://github.com/zcai0612;https://github.com/BoDong1006/BO-DONG.git;https://github.com/yuzhangiot/yuzhangiot/tree/main;https://cma.hkust-gz.edu.cn/people/kang-zhang/;https://zachzeyuwang.github.io/",
        "dblp": "359/4184;225/8349;;;;;;;29/177-1;132/7882-3.html",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;;;;;zNGYoA8AAAAJ;dasJlA4AAAAJ;q7NLPG0AAAAJ",
        "orcid": "0009-0008-3204-3271;;;;;0009-0006-5422-4044;;;0000-0003-3802-7535;0000-0001-5374-6330",
        "linkedin": ";;;;;;;;;zachzeyuwang/",
        "or_profile": "~Zhijing_Shao1;~Duotun_Wang1;~Qing-Yao_Tian1;~Yao-Dong_Yang2;~Hengyu_Meng1;~Zeyu_Cai2;~BO_DONG15;~Yu_Zhang48;~Kang_Zhang9;~Zeyu_Wang15",
        "aff": "The Hong Kong University of Science and Technology (Guangzhou)+Prometheus Vision Technology Co., Ltd.;Hong Kong University of Science and Technology (Guangzhou);;;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Swinburne University of Technology, Sarawak Campus+Swinburne University of Technology, Sarawak Campus;Prometheus Vision Technology Co., Ltd.;Hong Kong University of Science and Technology (Guangzhou);The Hong Kong University of Science and Technology (Guangzhou)",
        "aff_domain": "connect.hkust-gz.edu.cn+prometh.xyz;hkust.edu;;;hkust-gz.edu.cn;hkust.edu;swinburne.edu.my+swinburne.edu.my;prometh.xyz;hkust-gz.edu.cn;ust.hk",
        "position": "PhD student+Researcher;PhD student;;;MS student;MS student;PhD student+PhD student;Researcher;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nshao2025degas,\ntitle={{DEGAS}: Detailed Expressions on Full-body Gaussian Avatars},\nauthor={Zhijing Shao and Duotun Wang and Qing-Yao Tian and Yao-Dong Yang and Hengyu Meng and Zeyu Cai and BO DONG and Yu Zhang and Kang Zhang and Zeyu Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=7KykU750WQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7KykU750WQ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "7Otoa15gsA",
        "title": "Denoising Monte Carlo Renders with Diffusion Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Physically-based renderings contain Monte Carlo noise, with variance that increases as the number of rays per pixel decreases. This noise, while zero-mean for good modern renderers, can have heavy tails (most notably, for scenes containing specular or refractive objects). Learned methods for restoring low fidelity renders are highly developed, because suppressing render noise means one can save compute and use fast renders with few rays per pixel. We demonstrate that a diffusion model can denoise low fidelity renders successfully. Furthermore, our method can be conditioned on a variety of natural render information, and this conditioning helps performance. Quantitative experiments show that our method is competitive with SOTA across a range of sampling rates. Qualitative examination of the reconstructions suggests that the image prior applied by a diffusion method strongly favors reconstructions that are like real images -- so have straight shadow boundaries, curved specularities and no fireflies.",
        "keywords": "Denoising;Diffusion Models;Ray Traced Renders",
        "primary_area": "",
        "supplementary_material": "/attachment/810975b7d0536e3b5a7765e85de8192d1c06a8b8.pdf",
        "author": "Vaibhav Vavilala;Rahul Vasanth;David Forsyth",
        "authorids": "~Vaibhav_Vavilala1;~Rahul_Vasanth1;~David_Forsyth1",
        "gender": "M;;M",
        "homepage": ";;https://cs.illinois.edu/directory/profile/daf",
        "dblp": "176/5565;;f/DavidAForsyth",
        "google_scholar": "5UnQgEUAAAAJ;;https://scholar.google.com.tw/citations?user=5H0arvkAAAAJ",
        "orcid": ";;0000-0002-2278-0752",
        "linkedin": "vaibhav-vavilala;;",
        "or_profile": "~Vaibhav_Vavilala1;~Rahul_Vasanth1;~David_Forsyth1",
        "aff": "University of Illinois, Urbana Champaign;;University of Illinois, Urbana-Champaign",
        "aff_domain": "illinois.edu;;uiuc.edu",
        "position": "PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nvavilala2025denoising,\ntitle={Denoising Monte Carlo Renders with Diffusion Models},\nauthor={Vaibhav Vavilala and Rahul Vasanth and David Forsyth},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=7Otoa15gsA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7Otoa15gsA",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8qpjYG1x8I",
        "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Creating personalized 3D animations with precise control and realistic head motions remains challenging for current speech-driven 3D facial animation methods. Editing these animations is especially complex and time consuming, requires precise control and typically handled by highly skilled animators. Most existing works focus on controlling style or emotion of the synthesized animation and cannot edit/regenerate parts of an input animation. They also overlook the fact that multiple plausible lip and head movements can match the same audio input. To address these challenges, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation. Our approach produces diverse plausible lip and head motions for a single audio input and  allows for editing via keyframing and interpolation. Specifically, we propose a fully-convolutional diffusion model that can leverage the viseme-level diversity in our training corpus. Additionally, we employ a speaking-style personalization and a novel sparsely-guided motion diffusion to enable precise control and editing. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity.",
        "keywords": "3D Facial motion synthesis and editing;3D facial motion editing;motion synthesis",
        "primary_area": "",
        "supplementary_material": "/attachment/111f1d5c7a4a2983bae765bd05cd4e519573de6a.zip",
        "author": "Balamurugan Thambiraja;Malte Prinzler;Sadegh Aliakbarian;Darren Cosker;Justus Thies",
        "authorids": "~Balamurugan_Thambiraja1;~Malte_Prinzler1;~Sadegh_Aliakbarian1;~Darren_Cosker3;~Justus_Thies1",
        "gender": "M;M;M;M;M",
        "homepage": "https://balamuruganthambiraja.github.io/;https://www.linkedin.com/in/malte-prinzler/;https://sadegh-aa.github.io/;http://www.cs.bath.ac.uk/~dpc;https://justusthies.github.io/",
        "dblp": "337/1338;;185/7885;79/608;145/9981",
        "google_scholar": "IiZ10SYAAAAJ;;1qXJQ7cAAAAJ;https://scholar.google.co.uk/citations?user=n0rYM4kAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "balamurugan-thambiraja/;malte-prinzler/;;darren-cosker-61271413/;",
        "or_profile": "~Balamurugan_Thambiraja1;~Malte_Prinzler1;~Sadegh_Aliakbarian1;~Darren_Cosker3;~Justus_Thies1",
        "aff": ";Max Planck Institute for Intelligent Systems, Max-Planck Institute;Microsoft;Microsoft;Technische Universit\u00e4t Darmstadt",
        "aff_domain": ";tuebingen.mpg.de;microsoft.com;microsoft.com;tu-darmstadt.de",
        "position": ";PhD student;Researcher;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nthambiraja2025diface,\ntitle={3Di{FACE}: Synthesizing and Editing Holistic 3D Facial Animation},\nauthor={Balamurugan Thambiraja and Malte Prinzler and Sadegh Aliakbarian and Darren Cosker and Justus Thies},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=8qpjYG1x8I}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8qpjYG1x8I",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "8xHWWXZ9pq",
        "title": "GS-Pose: Generalizable Segmentation-Based 6D Object Pose Estimation With 3D Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces GS-Pose, a unified framework for localizing and estimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB images of a previously unseen object and builds three distinct representations stored in a database. At inference, GS-Pose operates sequentially by locating the object in the input image, estimating its initial 6D pose using a retrieval approach, and refining the pose with a render-and-compare method. The key insight is the application of the appropriate object representation at each stage of the process. In particular, for the refinement step, we leverage 3D Gaussian splatting, a novel differentiable rendering technique that offers high rendering speed and relatively low optimization time. Off-the-shelf toolchains and commodity hardware, such as mobile phones, can be used to capture new objects to be added to the database. Extensive evaluations on the LINEMOD and OnePose-LowTexture datasets demonstrate excellent performance, establishing the new state-of-the-art. The source code is publicly available at https://github.com/dingdingcai/GSPose.",
        "keywords": "6D Object Pose Estimation;Segmentation;3D Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/da396cde87cee6ae9b6e153e7dfa5476bfcdd992.zip",
        "author": "Dingding Cai;Janne Heikkila;Esa Rahtu",
        "authorids": "~Dingding_Cai1;~Janne_Heikkil\u00e41;~Esa_Rahtu1",
        "gender": "M;M;",
        "homepage": "https://dingdingcai.github.io/;https://www.oulu.fi/university/researcher/janne-heikkila;",
        "dblp": "198/1127;25/4802;",
        "google_scholar": "rDG33HAAAAAJ;https://scholar.google.com/citations?hl=en;",
        "orcid": ";0000-0003-0073-0866;",
        "linkedin": ";janne-heikkil%C3%A4-12ba9133/;",
        "or_profile": "~Dingding_Cai1;~Janne_Heikkil\u00e41;~Esa_Rahtu1",
        "aff": "Tampere University;University of Oulu;",
        "aff_domain": "tuni.fi;oulu.fi;",
        "position": "PhD student;Full Professor;",
        "bibtex": "@inproceedings{\ncai2025gspose,\ntitle={{GS}-Pose: Generalizable Segmentation-Based 6D Object Pose Estimation With 3D Gaussian Splatting},\nauthor={Dingding Cai and Janne Heikkila and Esa Rahtu},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=8xHWWXZ9pq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8xHWWXZ9pq",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9Hyk9hkYiE",
        "title": "CameraHMR: Aligning People with Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we address the challenge of accurate 3D human pose and shape (HPS) estimation from monocular images. The key to accuracy and robustness lies in high-quality training data. Existing training datasets containing real images with pseudo ground truth (pGT) use SMPLify to fit SMPL to sparse 2D joint locations, assuming a simplified camera with default intrinsics. We make two contributions that improve pGT accuracy.\nFirst, to estimate camera intrinsics, we develop a field-of-view prediction model HumanFoV trained on a dataset of images containing people. We use the estimated intrinsics to enhance the 4D-Humans dataset by incorporating a full perspective camera model during SMPLify fitting.\nSecond, 2D joints provide limited constraints on 3D body shape, often resulting in average-looking bodies. To address this, we use the BEDLAM dataset to train a dense surface keypoint detector. We apply this detector to the 4D-Humans dataset and modify SMPLify to fit the detected keypoints, resulting in significantly more realistic body shapes.\nFinally, we enhance the HMR2.0 architecture to include the estimated camera parameters. We iterate the process of model training and SMPLify fitting initialized with the previously trained model. This leads to more accurate pGT and significant performance gains. Our method, CameraHMR, achieves state-of-the-art 3D accuracy on HPS benchmarks. Code will be available for research purposes.",
        "keywords": "3D Human pose and shape;camera calibration;SMPLify fitting",
        "primary_area": "",
        "supplementary_material": "/attachment/2c3ddfdb55c27eedf4a872929d75d9e054d52e1a.zip",
        "author": "Priyanka Patel;Michael J. Black",
        "authorids": "~Priyanka_Patel1;~Michael_J._Black1",
        "gender": "F;",
        "homepage": "https://pixelite1201.github.io/;",
        "dblp": ";",
        "google_scholar": "hO4Z13gAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Priyanka_Patel1;~Michael_J._Black1",
        "aff": "Meshcapade;",
        "aff_domain": "gmail.com;",
        "position": "MLOps Engineer;",
        "bibtex": "@inproceedings{\npatel2025camerahmr,\ntitle={Camera{HMR}: Aligning People with Perspective},\nauthor={Priyanka Patel and Michael J. Black},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=9Hyk9hkYiE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9Hyk9hkYiE",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9SmBIwDtYf",
        "title": "Vocabulary-Free 3D Instance Segmentation with Vision-Language Assistant",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Most recent 3D instance segmentation methods are open vocabulary, offering a greater flexibility than closed-vocabulary methods. Yet, they are limited to reasoning within a specific set of concepts, i.e. the vocabulary, prompted by the user at test time. \nIn essence, these models cannot reason in an open-ended fashion, i.e., answering ``What are the objects in the scene?''.\nWe introduce the first method to address 3D instance segmentation in a setting that is void of any vocabulary prior, namely a vocabulary-free setting.\nWe leverage a large vision-language assistant and an open-vocabulary 2D instance segmenter to discover and ground semantic categories on the posed images. \nTo form 3D instance mask, we first partition the input point cloud into dense superpoints, which are then merged into 3D instance masks. We propose a novel superpoint merging strategy via spectral clustering, accounting for both mask coherence and semantic coherence that are estimated from the 2D object instance masks.\nWe evaluate our method using ScanNet200 and Replica, outperforming existing methods in both vocabulary-free and open-vocabulary settings.  \nCode will be made available.",
        "keywords": "Vocabulary-Free;Instance segmentation;Vision and Language Assistant;2D segmentor",
        "primary_area": "",
        "supplementary_material": "/attachment/606004b7a65acefe21a1ddce4d171a7e2dd8fb1b.pdf",
        "author": "Guofeng Mei;Luigi Riz;Yiming Wang;Fabio Poiesi",
        "authorids": "~Guofeng_Mei1;~Luigi_Riz1;~Yiming_Wang2;~Fabio_Poiesi3",
        "gender": "M;;F;M",
        "homepage": "https://gfmei.github.io/;;https://www.yimingwang.it/;https://fabiopoiesi.github.io/",
        "dblp": "197/7875;;71/3182-2;87/8843",
        "google_scholar": "VsmIGqsAAAAJ;;https://scholar.google.co.uk/citations?user=KBZ3zrEAAAAJ;https://scholar.google.co.uk/citations?user=BQ7li6AAAAAJ",
        "orcid": "0000-0002-0494-5031;;0000-0002-5932-4371;0000-0002-9769-1279",
        "linkedin": ";;yiming-wang-8b878685/;",
        "or_profile": "~Guofeng_Mei1;~Luigi_Riz1;~Yiming_Wang2;~Fabio_Poiesi3",
        "aff": "Fondazione Bruno Kessler;;Fondazione Bruno Kessler;Fondazione Bruno Kessler",
        "aff_domain": "fbk.eu;;fbk.eu;fbk.eu",
        "position": "Researcher;;Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nmei2025vocabularyfree,\ntitle={Vocabulary-Free 3D Instance Segmentation with Vision-Language Assistant},\nauthor={Guofeng Mei and Luigi Riz and Yiming Wang and Fabio Poiesi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=9SmBIwDtYf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9SmBIwDtYf",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "9abfUtE6iQ",
        "title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Scene flow, which provides the 3D motion field of the first frame from two consecutive point clouds, is vital for dynamic scene perception. However, contemporary scene flow methods face three major challenges. Firstly, they lack global flow embedding or only consider the context of individual point clouds before embedding, leading to embedded points struggling to perceive the consistent semantic relationship of another frame. To address this issue, we propose a novel approach called Dual Cross Attentive (DCA) for the latent fusion and alignment between two frames based on semantic contexts. This is then integrated into Global Fusion Flow Embedding (GF) to initialize flow embedding based on global correlations in both contextual and Euclidean spaces. Secondly, deformations exist in non-rigid objects after the warping layer, which distorts the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow at next-level, the Spatial Temporal Re-embedding (STR) module is devised to update the point sequence features at current-level. Lastly, poor generalization is often observed due to the significant domain gap between synthetic and LiDAR-scanned datasets. We leverage novel domain adaptive losses to effectively bridge the gap of motion inference from synthetic to real-world. Experiments demonstrate that our approach achieves state-of-the-art (SOTA) performance across various datasets, with particularly outstanding results in real-world LiDAR-scanned situations.",
        "keywords": "Point Cloud;Scene Flow;Attentive Mechanism;Deep Learning;3D Vision",
        "primary_area": "",
        "supplementary_material": "/attachment/9234fc76a90de26280dce9806fba4fc1090f2162.pdf",
        "author": "Zhiyang Lu;Qinghan Chen;Zhimin Yuan;Chenglu Wen;Ming Cheng;Cheng Wang",
        "authorids": "~Zhiyang_Lu1;~Qinghan_Chen1;~Zhimin_Yuan1;~Chenglu_Wen1;~Ming_Cheng4;~Cheng_Wang2",
        "gender": ";;M;;M;M",
        "homepage": "https://github.com/O-VIGIA;;;;;https://chwang.xmu.edu.cn/index_en.htm",
        "dblp": "260/7699;;72/1060;140/4398;82/104-2;54/2062-3",
        "google_scholar": ";;https://scholar.google.com.hk/citations?user=L214A8gAAAAJ;;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0000-8161-428X;;;;0000-0001-6480-6482;0000-0001-6075-796X",
        "linkedin": ";;;;;",
        "or_profile": "~Zhiyang_Lu1;~Qinghan_Chen1;~Zhimin_Yuan1;~Chenglu_Wen1;~Ming_Cheng4;~Cheng_Wang2",
        "aff": "Xiamen University;;Nanyang Normal University;Xiamen University;Xiamen University;Xiamen University",
        "aff_domain": "xmu.edu.cn;;nynu.edu.cn;xmu.edu.cn;xmu.edu.cn;xmu.edu.cn",
        "position": "PhD student;;Assistant Professor;Full Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nlu2025ssrflow,\ntitle={{SSRF}low: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow},\nauthor={Zhiyang Lu and Qinghan Chen and Zhimin Yuan and Chenglu Wen and Ming Cheng and Cheng Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=9abfUtE6iQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9abfUtE6iQ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "A33sBLz9WR",
        "title": "MLI-NeRF: Multi-Light Intrinsic-Aware Neural Radiance Fields",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current methods for extracting intrinsic image components, such as reflectance and shading, primarily rely on statistical priors. These methods focus mainly on simple synthetic scenes and isolated objects and struggle to perform well on challenging real-world data. To address this issue, we propose MLI-NeRF, which integrates Multiple Light information in Intrinsic-aware Neural Radiance Fields. By leveraging scene information provided by different light source positions complementing the multi-view information, we generate pseudo-label images for reflectance and shading to guide intrinsic image decomposition without the need for ground truth data. Our method introduces straightforward supervision for intrinsic component separation and ensures robustness across diverse scene types. We validate our approach on both synthetic and real-world datasets, outperforming existing state-of-the-art methods. Additionally, we demonstrate its applicability to various image editing tasks.",
        "keywords": "intrinsic decomposition;neural radiance fields(NeRF);multiple lights",
        "primary_area": "",
        "supplementary_material": "/attachment/8927080cbe6a21bc1770d057875cf111ce25eab0.pdf",
        "author": "Yixiong Yang;Shilin Hu;Haoyu Wu;Ramon Baldrich;Dimitris Samaras;Maria Vanrell",
        "authorids": "~Yixiong_Yang1;~Shilin_Hu1;~Haoyu_Wu2;~Ramon_Baldrich1;~Dimitris_Samaras3;~Maria_Vanrell1",
        "gender": ";M;;M;M;F",
        "homepage": ";;https://hao-yu-wu.github.io;https://cvc.uab.cat;https://www.cs.stonybrook.edu/~samaras/;",
        "dblp": ";298/4910;;;s/DimitrisSamaras;",
        "google_scholar": "IIUoUDQAAAAJ;;8AypXfoAAAAJ;;https://scholar.google.com/citations?hl=en;",
        "orcid": ";;;;0000-0002-1373-0294;",
        "linkedin": ";shilin-hu/;;;;",
        "or_profile": "~Yixiong_Yang1;~Shilin_Hu1;~Haoyu_Wu2;~Ramon_Baldrich1;~Dimitris_Samaras3;~Maria_Vanrell1",
        "aff": "Universitat Aut\u00f3noma de Barcelona;State University of New York at Stony Brook;Stony Brook University;Computer Vision Center, Universitat Aut\u00f3noma de Barcelona;Stony Brook University;",
        "aff_domain": "cvc.uab.es;stonybrook.edu;cs.stonybrook.edu;cvc.uab.es;cs.stonybrook.edu;",
        "position": "PhD student;PhD student;PhD student;Associate Professor;Full Professor;",
        "bibtex": "@inproceedings{\nyang2025mlinerf,\ntitle={{MLI}-Ne{RF}: Multi-Light Intrinsic-Aware Neural Radiance Fields},\nauthor={Yixiong Yang and Shilin Hu and Haoyu Wu and Ramon Baldrich and Dimitris Samaras and Maria Vanrell},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=A33sBLz9WR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=A33sBLz9WR",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "A34Vt92UMG",
        "title": "Joker: Conditional 3D Head Synthesis With Extreme Facial Expressions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Joker, a new method for the conditional synthesis of 3D human heads with extreme expressions. Given a single reference image of a person, we synthesize a volumetric human head with the reference\u2019s identity and a new expression. We offer control over the expression via a 3D morphable model (3DMM) and textual inputs. This multi-modal conditioning signal is essential since 3DMMs alone fail to define subtle emotional changes and extreme expressions, including those involving the mouth cavity and tongue articulation. Our method is built upon a 2D diffusion-based prior that generalizes well to out-of-domain samples, such as sculptures, heavy makeup, and paintings while achieving high levels of expressiveness. To improve view consistency, we propose a new 3D distillation technique that converts predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D prior and our distillation technique produce state-of-the-art results, which are confirmed by our extensive evaluations. Also, to the best of our knowledge, our method is the first to achieve view-consistent extreme tongue articulation.",
        "keywords": "Head Avatars;Expression;Diffusion Models;3D Distillation",
        "primary_area": "",
        "supplementary_material": "/attachment/5fe3d65c439b2c97a9ceccbb7519fc55b152cc16.zip",
        "author": "Malte Prinzler;Egor Zakharov;Vanessa Skliarova;Berna Kabadayi;Justus Thies",
        "authorids": "~Malte_Prinzler1;~Egor_Zakharov1;~Vanessa_Skliarova1;~Berna_Kabadayi1;~Justus_Thies1",
        "gender": "M;M;F;;M",
        "homepage": "https://www.linkedin.com/in/malte-prinzler/;https://egorzakharov.github.io/;https://github.com/Vanessik;https://bernakabadayi.github.io/;https://justusthies.github.io/",
        "dblp": ";227/2698;322/5225;;145/9981",
        "google_scholar": ";wyF-PxIAAAAJ;;;",
        "orcid": ";0000-0002-9880-9531;;;",
        "linkedin": "malte-prinzler/;egor-zakharov-881126118/;;;",
        "or_profile": "~Malte_Prinzler1;~Egor_Zakharov1;~Vanessa_Skliarova1;~Berna_Kabadayi1;~Justus_Thies1",
        "aff": "Max Planck Institute for Intelligent Systems, Max-Planck Institute;Meta+Department of Computer Science, ETHZ - ETH Zurich;Max Planck Institute for Intelligent Systems, Max-Planck Institute+ETHZ - ETH Zurich;Max Planck Institute for Intelligent Systems, Max-Planck Institute;Technische Universit\u00e4t Darmstadt",
        "aff_domain": "tuebingen.mpg.de;meta.com+inf.ethz.ch;tuebingen.mpg.de+ethz.ch;tuebingen.mpg.de;tu-darmstadt.de",
        "position": "PhD student;Research Scientist+Postdoc;PhD student+PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nprinzler2025joker,\ntitle={Joker: Conditional 3D Head Synthesis With Extreme Facial Expressions},\nauthor={Malte Prinzler and Egor Zakharov and Vanessa Skliarova and Berna Kabadayi and Justus Thies},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=A34Vt92UMG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=A34Vt92UMG",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "AmrIeQyMHL",
        "title": "EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Human activities are inherently complex, and even simple household tasks involve numerous object interactions. To better understand these activities and behaviors, it is crucial to model their dynamic interactions with the environment. The recent availability of affordable head-mounted cameras and egocentric data offers a more accessible and efficient means to understand dynamic human-object interactions in 3D environments. However, most existing methods for human activity modeling either focus on reconstructing 3D models of hand-object or human-scene interactions or on mapping 3D scenes, neglecting dynamic interactions with objects. The few existing solutions often require inputs from multiple sources, including multi-camera setups, depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the first method capable of simultaneously reconstructing 3D scenes and dynamically tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely discrete nature of Gaussian Splatting and segment dynamic interactions from the background. Our approach employs a clip-level online learning pipeline that leverages the dynamic nature of human activities, allowing us to reconstruct the temporal evolution of the scene in chronological order and track rigid object motion. Additionally, our method automatically segments object and background Gaussians, providing 3D representations for both static scenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic Gaussian methods in challenging in-the-wild videos and we also qualitatively demonstrate the high quality of the reconstructed models.",
        "keywords": "Gaussian Splatting;3D Reconstruction;Egocentric Videos",
        "primary_area": "",
        "supplementary_material": "/attachment/c7b07b53c39b9c897a26a7da1568aca90df7a0a3.zip",
        "author": "Daiwei Zhang;Gengyan Li;Jiajie Li;Micka\u00ebl Bressieux;Otmar Hilliges;Marc Pollefeys;Luc Van Gool;Xi Wang",
        "authorids": "~Daiwei_Zhang2;~Gengyan_Li1;~Jiajie_Li4;~Micka\u00ebl_Bressieux1;~Otmar_Hilliges1;~Marc_Pollefeys2;~Luc_Van_Gool1;~Xi_Wang10",
        "gender": "M;M;;M;M;M;;F",
        "homepage": ";;;;https://ait.ethz.ch/people/hilliges/;;;https://xiwang1212.github.io/homepage/",
        "dblp": ";290/1267;;;82/2289;p/MarcPollefeys;61/5017;",
        "google_scholar": ";;;;-epU9OsAAAAJ;YYH0BjEAAAAJ;https://scholar.google.be/citations?user=TwMib_QAAAAJ;IYUPj9MAAAAJ",
        "orcid": ";0000-0002-1427-7612;;;0000-0002-5068-3474;;;0000-0001-5442-1116",
        "linkedin": "daiwei-z-b172b9149/;;;mickael-bressieux/;;marc-pollefeys-30a7075/;;",
        "or_profile": "~Daiwei_Zhang2;~Gengyan_Li1;~Jiajie_Li4;~Micka\u00ebl_Bressieux1;~Otmar_Hilliges1;~Marc_Pollefeys2;~Luc_Van_Gool1;~Xi_Wang10",
        "aff": ";ETHZ - ETH Zurich+Google;;;ETHZ - ETH Zurich;Microsoft+ETHZ - ETH Zurich+Swiss Federal Institute of Technology;Sofia Un. St. Kliment Ohridski;ETHZ - ETH Zurich",
        "aff_domain": ";ethz.ch+google.com;;;ethz.ch;microsoft.com+ethz.ch+ethz.ch;insait.ai;ethz.ch",
        "position": ";PhD student+PhD student;;;Full Professor;Director+Professor+Full Professor;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nzhang2025egogaussian,\ntitle={EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting},\nauthor={Daiwei Zhang and Gengyan Li and Jiajie Li and Micka{\\\"e}l Bressieux and Otmar Hilliges and Marc Pollefeys and Luc Van Gool and Xi Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=AmrIeQyMHL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AmrIeQyMHL",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "BWJoaUetKj",
        "title": "Betsu-Betsu: Separable 3D Reconstruction of Two Interacting Objects from Multiple Views",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Separable 3D reconstruction of multiple objects from multi-view RGB images\u2014resulting in two different 3D shapes for the two objects with a clear separation between them\u2014remains a sparely researched problem. It is challenging due to severe mutual occlusions and ambiguities\nalong the objects\u2019 interaction boundaries. This paper investigates the setting and introduces a new neuro-implicit method that can reconstruct the geometry and appearance of two objects undergoing close interactions while disjoining both in 3D, avoiding surface inter-penetrations and enabling novel-view synthesis of the observed scene. In our approach, the objects in the scene are first encoded using a shared multi-resolution hash grid. Next, its features are decoded into two neural SDFs for the respective objects. The framework is end-to-end trainable and supervised using a novel alpha-blending regularization that ensures that the two geometries are well separated even under extreme occlusions. Our reconstruction method is markerless and can be applied to rigid as well as articulated objects. We introduce a new dataset consisting of close interactions between a human and an object and also evaluate on two scenes of humans performing martial arts. The experiments confirm the effectiveness of our framework and substantial improvements using 3D and novel view synthesis metrics compared to several existing approaches applicable in our setting.",
        "keywords": "SDF;multi-view geometry",
        "primary_area": "",
        "supplementary_material": "/attachment/b14f21ff59b3b3a6b6dfe63d08f82f2d3558109c.zip",
        "author": "Suhas Gopal;Rishabh Dabral;Vladislav Golyanik;Christian Theobalt",
        "authorids": "~Suhas_Gopal1;~Rishabh_Dabral1;~Vladislav_Golyanik1;~Christian_Theobalt2",
        "gender": "M;;M;M",
        "homepage": "https://suhasg.in/;;https://people.mpi-inf.mpg.de/~golyanik/;https://www.mpi-inf.mpg.de/~theobalt/",
        "dblp": ";;180/6438;55/3346",
        "google_scholar": ";;https://scholar.google.co.uk/citations?user=we9LnVcAAAAJ;https://scholar.google.com.tw/citations?user=eIWg8NMAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Suhas_Gopal1;~Rishabh_Dabral1;~Vladislav_Golyanik1;~Christian_Theobalt2",
        "aff": "Universit\u00e4t des Saarlandes;;Saarland Informatics Campus, Max-Planck Institute for Informatics;Saarbruecken Research Center for Visual Computing, Interaction, and Artificial Intellligence+Max-Planck-Institute for Informatics, Saarland Informatics Campus",
        "aff_domain": "uni-saarland.de;;mpi-inf.mpg.de;via-center.science+mpi-inf.mpg.de",
        "position": "MS student;;Principal Researcher;Director+Director",
        "bibtex": "@inproceedings{\ngopal2025betsubetsu,\ntitle={Betsu-Betsu: Separable 3D Reconstruction of Two Interacting Objects from Multiple Views},\nauthor={Suhas Gopal and Rishabh Dabral and Vladislav Golyanik and Christian Theobalt},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=BWJoaUetKj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BWJoaUetKj",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CE0x3i6MiH",
        "title": "E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Novel view synthesis techniques predominantly utilize RGB cameras, inheriting their limitations such as the need for sufficient lighting, susceptibility to motion blur, and restricted dynamic range. In contrast, event cameras are significantly more resilient to these limitations but have been less explored in this domain, particularly in large-scale settings. Current methodologies primarily focus on front-facing or object-oriented (360-degree view) scenarios. For the first time, we introduce 3D Gaussians for event-based novel view synthesis. Our method reconstructs large and unbounded scenes with high visual quality. We contribute the first real and synthetic event datasets tailored for this setting. Our method demonstrates superior novel view synthesis and consistently outperforms the baseline EventNeRF by a margin of 11\u221225% in PSNR (dB) while being orders of magnitude faster in reconstruction and rendering.",
        "keywords": "event-based vision;E-3DGS;novel view rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/522478e9e676b7a3c1060b103d8a02525ea00381.zip",
        "author": "Sohaib Zahid;Viktor Rudnev;Eddy Ilg;Vladislav Golyanik",
        "authorids": "~Sohaib_Zahid1;~Viktor_Rudnev1;~Eddy_Ilg3;~Vladislav_Golyanik1",
        "gender": ";M;M;M",
        "homepage": ";https://people.mpi-inf.mpg.de/~vrudnev/;https://www.utn.de/departments/department-engineering/cvmp-lab/;https://people.mpi-inf.mpg.de/~golyanik/",
        "dblp": ";281/6598;151/9307;180/6438",
        "google_scholar": "Tu8QDt8AAAAJ;qT6PqycAAAAJ;MYvSvGsAAAAJ;https://scholar.google.co.uk/citations?user=we9LnVcAAAAJ",
        "orcid": ";0000-0002-8608-8394;;",
        "linkedin": "sohaib023/;;eddy-ilg/;",
        "or_profile": "~Sohaib_Zahid1;~Viktor_Rudnev1;~Eddy_Ilg3;~Vladislav_Golyanik1",
        "aff": ";Max-Planck Institute for Informatics, SIC+Universit\u00e4t des Saarlandes;University of Technology Nuremberg;Saarland Informatics Campus, Max-Planck Institute for Informatics",
        "aff_domain": ";mpi-inf.mpg.de+uni-saarland.de;utn.de;mpi-inf.mpg.de",
        "position": ";PhD student+PhD student;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nzahid2025edgs,\ntitle={E-3{DGS}: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting},\nauthor={Sohaib Zahid and Viktor Rudnev and Eddy Ilg and Vladislav Golyanik},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=CE0x3i6MiH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CE0x3i6MiH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CcCgioqFM9",
        "title": "GVP: Generative Volumetric Primitives",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Advances in 3D-aware generative models have pushed the boundary of image synthesis with explicit camera control.\n   To achieve high-resolution image synthesis, several attempts have been made to design efficient generators, such as hybrid architectures with both 3D and 2D components.\n   However, such a design compromises multiview consistency, and the design of a pure 3D generator with high resolution is still an open problem.\n   In this work, we present Generative Volumetric Primitives (GVP), the first pure 3D volumetric generative model that can sample and render 512-resolution images in real-time.\n   GVP jointly models a number of volumetric primitives and their spatial information, both of which can be efficiently generated via a 2D convolutional network.\n   The mixture of these primitives naturally captures the sparsity in the 3D volume.\n   The training of such a generator with a high degree of freedom is made possible through a combination of adversarial and knowledge distillation training.\n   The learned model exhibits dense 3D correspondences between samples. We provide exhaustive qualitative and qualitative evaluations for dense correspondences.\n   Experiments on several datasets demonstrate superior efficiency, 3D consistency, and the emergence of dense correspondences of GVP over the state-of-the-art.",
        "keywords": "3D Generative Model;Face Modelling;Morphable Models",
        "primary_area": "",
        "supplementary_material": "/attachment/501341f5bd5f74fbee6947bc7f075ed4445c43a9.zip",
        "author": "MALLIKARJUN BYRASANDRA RAMALINGA REDDY;Xingang Pan;Mohamed Elgharib;Christian Theobalt",
        "authorids": "~MALLIKARJUN_BYRASANDRA_RAMALINGA_REDDY1;~Xingang_Pan1;~Mohamed_Elgharib1;~Christian_Theobalt2",
        "gender": "M;M;M;M",
        "homepage": "https://people.mpi-inf.mpg.de/~mbr/;https://xingangpan.github.io/;http://people.mpi-inf.mpg.de/~elgharib/;https://www.mpi-inf.mpg.de/~theobalt/",
        "dblp": "241/9493.html;211/7940;232/2002.html;55/3346",
        "google_scholar": "NlAB4EsAAAAJ;https://scholar.google.com.hk/citations?user=uo0q9WgAAAAJ;e1WLgm8AAAAJ;https://scholar.google.com.tw/citations?user=eIWg8NMAAAAJ",
        "orcid": ";0000-0002-5825-9467;;",
        "linkedin": ";;;",
        "or_profile": "~MALLIKARJUN_BYRASANDRA_RAMALINGA_REDDY1;~Xingang_Pan1;~Mohamed_Elgharib1;~Christian_Theobalt2",
        "aff": "Stability AI+Saarland Informatics Campus, Max-Planck Institute;Nanyang Technological University;;Saarbruecken Research Center for Visual Computing, Interaction, and Artificial Intellligence+Max-Planck-Institute for Informatics, Saarland Informatics Campus",
        "aff_domain": "stability.ai+mpi-inf.mpg.de;ntu.edu.sg;;via-center.science+mpi-inf.mpg.de",
        "position": "research scientist +PhD student;Assistant Professor;;Director+Director",
        "bibtex": "@inproceedings{\nreddy2025gvp,\ntitle={{GVP}: Generative Volumetric Primitives},\nauthor={MALLIKARJUN BYRASANDRA RAMALINGA REDDY and Xingang Pan and Mohamed Elgharib and Christian Theobalt},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=CcCgioqFM9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CcCgioqFM9",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "CmqNia1tLK",
        "title": "FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense Correspondences",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Surface reconstruction from multiple, calibrated images is a challenging task - often requiring a large number of collected images with significant overlap. We look at the specific case of human foot reconstruction. As with previous successful foot reconstruction work, we seek to extract rich per-pixel geometry cues from multi-view RGB images, and fuse these into a final 3D object. Our method, FOCUS, tackles this problem with 3 main contributions: (i) SynFoot2, an extension of an existing synthetic foot dataset to include a new data type: dense correspondence with the parameterized foot model FIND; (ii) an uncertainty-aware dense correspondence predictor trained on our synthetic dataset; (iii) two methods for reconstructing a 3D surface from dense correspondence predictions: one inspired by Structure-from-Motion, and one optimization-based using the FIND model. We show that our reconstruction achieves state-of-the-art reconstruction quality in a few-view setting, performing comparably to state-of-the-art when many views are available, and runs substantially faster. We release our synthetic dataset to the research community. Code is available at: https://github.com/OllieBoyne/FOCUS",
        "keywords": "foot;optimization;synthetic;dense correspondences;surface reconstruction;photogrammetry;parameterized model",
        "primary_area": "",
        "supplementary_material": "/attachment/17116673b4887399b3da60e7347d91ce153a596d.zip",
        "author": "Oliver Boyne;Roberto Cipolla",
        "authorids": "~Oliver_Boyne1;~Roberto_Cipolla1",
        "gender": "M;M",
        "homepage": "https://ollieboyne.github.io;https://mi.eng.cam.ac.uk/~cipolla/",
        "dblp": "270/8048;c/RobertoCipolla",
        "google_scholar": "m69FDD4AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-8999-2151",
        "linkedin": "ollie-boyne/;",
        "or_profile": "~Oliver_Boyne1;~Roberto_Cipolla1",
        "aff": "University of Cambridge;University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nboyne2025focus,\ntitle={{FOCUS} - Multi-View Foot Reconstruction From Synthetically Trained Dense Correspondences},\nauthor={Oliver Boyne and Roberto Cipolla},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=CmqNia1tLK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CmqNia1tLK",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "DA2jgGDPkW",
        "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While radiance field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS.",
        "keywords": "computer vision;machine learning;view synthesis",
        "primary_area": "",
        "supplementary_material": "/attachment/2719ba948b85eaad95de01de86aa787c148145c0.pdf",
        "author": "Michael Niemeyer;Fabian Manhardt;Marie-Julie Rakotosaona;Michael Oechsle;Daniel Duckworth;Rama Gosula;Keisuke Tateno;John Bates;Dominik Kaeser;Federico Tombari",
        "authorids": "~Michael_Niemeyer1;~Fabian_Manhardt1;~Marie-Julie_Rakotosaona1;~Michael_Oechsle1;~Daniel_Duckworth1;~Rama_Gosula1;~Keisuke_Tateno1;~John_Bates2;~Dominik_Kaeser1;~Federico_Tombari1",
        "gender": "M;M;;;M;M;M;;;M",
        "homepage": "https://m-niemeyer.github.io/;http://campar.in.tum.de/Main/FabianManhardt;;https://moechsle.github.io/;;;;;;https://federicotombari.github.io/",
        "dblp": "232/1712;173/9271;234/6243;232/2468;10/8371.html;;25/1526;;;16/3539",
        "google_scholar": "https://scholar.google.de/citations?user=v1O7i_0AAAAJ;https://scholar.google.de/citations?user=bERItx8AAAAJ;eQ0om98AAAAJ;vMcsUNAAAAAJ;2fWmq-4AAAAJ;;https://scholar.google.com/citations?hl=en;;;TFsE4BIAAAAJ",
        "orcid": ";0000-0002-4577-4590;;;;;;;;0000-0001-5598-5212",
        "linkedin": ";;;;dduckworth/;ramagosula/;;john-bates-8225963/;dominikkaeser/;fedet/",
        "or_profile": "~Michael_Niemeyer1;~Fabian_Manhardt1;~Marie-Julie_Rakotosaona1;~Michael_Oechsle1;~Daniel_Duckworth1;~Rama_Gosula1;~Keisuke_Tateno1;~John_Bates2;~Dominik_Kaeser1;~Federico_Tombari1",
        "aff": "Google;Google;Google;Google;Google;;Google;Google;;Google+Technical University Munich (TUM)",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;;google.com;google.com;;google.com+in.tum.de",
        "position": "Researcher;Researcher;Researcher;Researcher;Researcher;;Researcher;Researcher;;Research Director+Lecturer",
        "bibtex": "@inproceedings{\nniemeyer2025radsplat,\ntitle={RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ {FPS}},\nauthor={Michael Niemeyer and Fabian Manhardt and Marie-Julie Rakotosaona and Michael Oechsle and Daniel Duckworth and Rama Gosula and Keisuke Tateno and John Bates and Dominik Kaeser and Federico Tombari},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=DA2jgGDPkW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DA2jgGDPkW",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Dwzwe5Me51",
        "title": "Learning assisted Interactive Modelling with Rough Freehand 3D Sketch Strokes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Freehand 3D sketches are a great medium to ideate and create visual content. However, generating 3D models from such rough sketches remains an unsolved, non-trivial task. We present a complete end-to-end interactive framework for rapid, incremental modelling from sparse, irregular 3D sketches. At the core of our solution, is 'sketchTransformer', a two-staged transformer network architecture, that fits parametric surface patches to a set of sketch strokes. We devise a novel pseudo height field representation that enables sketchTransformer to handle the noise and sparseness in the input strokes. Our method interactively evolves the surface model while maintaining smooth joins to nearby patches. We show two frontends for our framework, one on the desktop and as a mobile AR application, to illustrate how our method complements a standard 3D modelling pipelines. We can robustly handle a large variety of input 3D strokes, that competing methods cannot parse adequately.",
        "keywords": "3D Sketches;Freeform Modelling;AR;Interactive;Supervised Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/b1408275f0797ea047c8ff08cb148a019d69f6be.zip",
        "author": "Sukanya Bhattacharjee;Parag Chaudhuri",
        "authorids": "~Sukanya_Bhattacharjee1;~Parag_Chaudhuri1",
        "gender": "F;Unspecified",
        "homepage": "http://www.cse.iitb.ac.in/bhatsukanya;https://www.cse.iitb.ac.in/~paragc/",
        "dblp": ";57/5346.html",
        "google_scholar": ";kOkSgaMAAAAJ",
        "orcid": ";0000-0002-1706-5703",
        "linkedin": ";",
        "or_profile": "~Sukanya_Bhattacharjee1;~Parag_Chaudhuri1",
        "aff": "Indian Institute of Technology Bombay;Indian Institute of Technology Bombay",
        "aff_domain": "iitb.ac.in;iitb.ac.in",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nbhattacharjee2025learning,\ntitle={Learning assisted Interactive Modelling with Rough Freehand 3D Sketch Strokes},\nauthor={Sukanya Bhattacharjee and Parag Chaudhuri},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Dwzwe5Me51}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Dwzwe5Me51",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "E3tNxq8eHh",
        "title": "Geometry-Aware Feature Matching for Large-Scale Structure from Motion",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.",
        "keywords": "Feature Matching;Structure from Motion",
        "primary_area": "",
        "supplementary_material": "/attachment/0f08f258e78ddfb11aba0735196e7ea17313491e.pdf",
        "author": "Gonglin Chen;Jinsen Wu;Haiwei Chen;Wenbin Teng;Zhiyuan Gao;Andrew Feng;Rongjun Qin;Yajie Zhao",
        "authorids": "~Gonglin_Chen1;~Jinsen_Wu1;~Haiwei_Chen1;~Wenbin_Teng1;~Zhiyuan_Gao1;~Andrew_Feng2;~Rongjun_Qin1;~Yajie_Zhao1",
        "gender": "M;M;M;M;;M;M;F",
        "homepage": "http://www.xtcpete.com;;https://github.com/nintendops;https://wbteng9526.github.io/;;;https://u.osu.edu/qin.324/;https://www.yajie-zhao.com/",
        "dblp": ";;194/4223;308/6056.html;;183/0621;;54/7467",
        "google_scholar": "kY9gogcAAAAJ;;;HKXO9CEAAAAJ;;JKWxGfsAAAAJ;https://scholar.google.com.sg/citations?user=9bB9WWsAAAAJ;",
        "orcid": ";;0000-0002-4918-0917;;;;;",
        "linkedin": ";jinsen-wu/;;wenbin-teng/;;;rongjun-qin-b8333674/;",
        "or_profile": "~Gonglin_Chen1;~Jinsen_Wu1;~Haiwei_Chen1;~Wenbin_Teng1;~Zhiyuan_Gao1;~Andrew_Feng2;~Rongjun_Qin1;~Yajie_Zhao1",
        "aff": "USC Institute for Creative Technologies, University of Southern California+University of Southern California;University of Southern California;University of Southern California;University of Southern California+USC Institute for Creative Technologies, University of Southern California;;Institute for Creative Technologies, University of Southern California;Ohio State University, Columbus;University of Southern California+USC Institute for Creative Technologies, University of Southern California",
        "aff_domain": "ict.usc.edu+usc.edu;usc.edu;usc.edu;usc.edu+ict.usc.edu;;ict.usc.edu;osu.edu;usc.edu+ict.usc.edu",
        "position": "PhD student+PhD student;MS student;Postdoc;PhD student+PhD student;;Research Scientist;Associate Professor;Assistant Professor+Director",
        "bibtex": "@inproceedings{\nchen2025geometryaware,\ntitle={Geometry-Aware Feature Matching for Large-Scale Structure from Motion},\nauthor={Gonglin Chen and Jinsen Wu and Haiwei Chen and Wenbin Teng and Zhiyuan Gao and Andrew Feng and Rongjun Qin and Yajie Zhao},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=E3tNxq8eHh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=E3tNxq8eHh",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FFlGtmV1Co",
        "title": "Gen3DSR: Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors. However, real-world scenarios are far more complex and exceed the capabilities of these methods. We therefore propose a hybrid method following a divide-and-conquer strategy. We first process the scene holistically, extracting depth and semantic information, and then leverage an object-level method for the detailed reconstruction of individual components. By splitting the problem into simpler tasks, our system is able to generalize to various types of scenes without retraining or fine-tuning. We purposely design our pipeline to be highly modular with independent, self-contained modules, to avoid the need for end-to-end training of the whole system. This enables the pipeline to naturally improve as future methods can replace the individual modules. We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works. Project page: https://andreeadogaru.github.io/Gen3DSR",
        "keywords": "3D scene reconstruction;single-view;compositional",
        "primary_area": "",
        "supplementary_material": "/attachment/ff39c2f0fa81fd969b747e77e0039410bf72a660.zip",
        "author": "Andreea Dogaru;Mert \u00d6zer;Bernhard Egger",
        "authorids": "~Andreea_Dogaru1;~Mert_\u00d6zer1;~Bernhard_Egger1",
        "gender": "F;M;M",
        "homepage": "https://andreeadogaru.github.io/;https://github.com/mert-o;",
        "dblp": "330/2959;;40/2471-1",
        "google_scholar": "HF1pWaAAAAAJ;;W9Kjud4AAAAJ",
        "orcid": "0000-0003-3250-0213;;",
        "linkedin": "andreea-dogaru/;;",
        "or_profile": "~Andreea_Dogaru1;~Mert_\u00d6zer1;~Bernhard_Egger1",
        "aff": "Friedrich-Alexander Universit\u00e4t Erlangen-N\u00fcrnberg;Friedrich-Alexander Universit\u00e4t Erlangen-N\u00fcrnberg;Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg",
        "aff_domain": "fau.de;fau.de;fau.de",
        "position": "PhD student;MS student;Professor",
        "bibtex": "@inproceedings{\ndogaru2025gendsr,\ntitle={Gen3{DSR}: Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View},\nauthor={Andreea Dogaru and Mert {\\\"O}zer and Bernhard Egger},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=FFlGtmV1Co}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FFlGtmV1Co",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "FdIshoUrqU",
        "title": "Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents a novel approach for cross-view synthesis aimed at generating plausible ground-level images from corresponding satellite imagery or vice versa. We refer to these tasks as satellite-to-ground (Sat2Grd) and ground-to-satellite (Grd2Sat) synthesis, respectively. Unlike previous works that typically focus on one-to-one generation, producing a single output image from a single input image, our approach acknowledges the inherent one-to-many nature of the problem. This recognition stems from the challenges posed by differences in illumination, weather conditions, and occlusions between the two views. To effectively model this uncertainty, we leverage recent advancements in diffusion models. Specifically, we exploit random Gaussian noise to represent the diverse possibilities learnt from the target view data. Additionally, we introduce a Geometry-guided Cross-view Condition (GCC) strategy to establish explicit geometric correspondences between satellite and street-view features. This enables us to resolve the geometry ambiguity introduced by camera pose between image pairs, boosting the performance of cross-view image synthesis. Through extensive quantitative and qualitative analyses on three benchmark cross-view datasets, we demonstrate the superiority of our proposed geometry-guided cross-view condition over baseline methods, including recent state-of-the-art approaches in cross-view image synthesis. Our method generates images of higher quality, fidelity, and diversity than other state-of-the-art approaches.",
        "keywords": "Image Synthesis;Satellite Imagery;Cross-view",
        "primary_area": "",
        "supplementary_material": "/attachment/75258494cb48aafef978157b3d3090d50c36aa6c.zip",
        "author": "Tao Jun Lin;Wenqing wang;Yujiao Shi;Akhil Perincherry;Ankit Vora;Hongdong Li",
        "authorids": "~Tao_Jun_Lin1;~Wenqing_wang5;~Yujiao_Shi1;~Akhil_Perincherry1;~Ankit_Vora1;~Hongdong_Li1",
        "gender": ";F;F;;M;M",
        "homepage": ";;https://shiyujiao.github.io/;;https://ankitvora19.wixsite.com/portfolio;http://users.cecs.anu.edu.au/~hongdong/",
        "dblp": ";03/4982-2;159/2546;;242/8412;59/4859.html",
        "google_scholar": ";kH-E48MAAAAJ;rVsRpZEAAAAJ;;EUS0qnEAAAAJ;https://scholar.google.com.tw/citations?hl=en",
        "orcid": ";;0000-0001-6028-9051;;0000-0001-7976-8730;",
        "linkedin": ";;yujiao-shi-053a12198/;;https://linkedin.com/in/ankitvora1;",
        "or_profile": "~Tao_Jun_Lin1;~Wenqing_wang5;~Yujiao_Shi1;~Akhil_Perincherry1;~Ankit_Vora1;~Hongdong_Li1",
        "aff": ";University of Surrey;ShanghaiTech University;;Ford Motor Company;Australian National University",
        "aff_domain": ";surrey.ac.uk;shanghaitech.edu.cn;;ford.com;anu.edu.au",
        "position": ";PhD student;Assistant Professor;;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nlin2025geometryguided,\ntitle={Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis},\nauthor={Tao Jun Lin and Wenqing wang and Yujiao Shi and Akhil Perincherry and Ankit Vora and Hongdong Li},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=FdIshoUrqU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FdIshoUrqU",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "GkXeLGWZNV",
        "title": "UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present UrbanIR (Urban Scene Inverse Rendering), a new inverse graphics model that enables realistic, free-viewpoint renderings of scenes under various lighting conditions with a single video. It accurately infers shape, albedo, visibility, and sun and sky illumination from wide-baseline videos, such as those from car-mounted cameras, differing from NeRF's dense view settings. In this context, standard methods often yield subpar geometry and material estimates, such as inaccurate roof representations and numerous `floaters.' UrbanIR addresses these issues with novel losses that reduce errors in inverse graphics inference and rendering artifacts. Its techniques allow for precise shadow volume estimation in the original scene. The model's outputs support controllable editing, enabling photorealistic free-viewpoint renderings of night simulations, relit scenes, and inserted objects, marking a significant improvement over existing state-of-the-art methods. Our code and data will be made publicly available upon acceptance.",
        "keywords": "Inverse rendering;Neural rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/09172fa5456591fb0b21af471c6c6a9f56b47ab1.pdf",
        "author": "Zhi-Hao Lin;Bohan Liu;Yi-Ting Chen;Kuan Sheng Chen;David Forsyth;Jia-Bin Huang;Anand Bhattad;Shenlong Wang",
        "authorids": "~Zhi-Hao_Lin2;~Bohan_Liu2;~Yi-Ting_Chen3;~Kuan_Sheng_Chen1;~David_Forsyth1;~Jia-Bin_Huang1;~Anand_Bhattad1;~Shenlong_Wang1",
        "gender": "M;M;F;;M;M;;M",
        "homepage": "https://zhihao-lin.github.io/;https://github.com/Liu-524;https://jamie725.github.io/website/;;https://cs.illinois.edu/directory/profile/daf;https://jbhuang0604.github.io/;https://anandbhattad.github.io/;https://shenlong.web.illinois.edu/",
        "dblp": "118/2127;https://dblp.org/rec/conf/embc/LiaoWWLOH22;;;f/DavidAForsyth;51/1815-1.html;215/4305;117/4842",
        "google_scholar": "QdqDnA0AAAAJ;;https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=5H0arvkAAAAJ;pp848fYAAAAJ;XUsauXIAAAAJ;QFpswmcAAAAJ",
        "orcid": ";;;;0000-0002-2278-0752;;;",
        "linkedin": ";;ytchen-umd/;;;jia-bin-huang-070a7418/;;shenlong-wang-3496023b",
        "or_profile": "~Zhi-Hao_Lin2;~Bohan_Liu2;~Yi-Ting_Chen3;~Kuan_Sheng_Chen1;~David_Forsyth1;~Jia-Bin_Huang1;~Anand_Bhattad1;~Shenlong_Wang1",
        "aff": "Department of Computer Science;University of Illinois, Urbana Champaign;University of Maryland, College Park;;University of Illinois, Urbana-Champaign;University of Maryland, College Park;Toyota Technological Institute at Chicago;University of Illinois, Urbana Champaign",
        "aff_domain": "cs.illinois.edu;uiuc.edu;umd.edu;;uiuc.edu;umd.edu;ttic.edu;illinois.edu",
        "position": "PhD student;MS student;PhD student;;Full Professor;Associate Professor;Research Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlin2025urbanir,\ntitle={Urban{IR}: Large-Scale Urban Scene Inverse Rendering from a Single Video},\nauthor={Zhi-Hao Lin and Bohan Liu and Yi-Ting Chen and Kuan Sheng Chen and David Forsyth and Jia-Bin Huang and Anand Bhattad and Shenlong Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=GkXeLGWZNV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GkXeLGWZNV",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "H2aizfRWXH",
        "title": "BiGS: Bidirectional Primitives for Relightable 3D Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present BiGS, an image-based novel view synthesis technique designed to model and render 3D objects with surface and volumetric materials under dynamic illumination, achieving real-time relighting of 3D objects with the rasterization algorithm of Gaussian Splatting. Our method represents light- and view-dependent scattering via bidirectional spherical harmonics that does not use a specific surface normal-related reflectance function, making it more compatible with volumetric representations like Gaussian splatting, where the normals are undefined. We demonstrate our method by reconstructing and rendering objects with complex materials on synthetic and captured One-Light-At-a-Time (OLAT) datasets, showcasing various photorealistic appearances our method captures and the real-time performance.",
        "keywords": "Computer Graphics;Image-Based Rendering;Relighting;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/9a35a2b3096bdb9d4190701e830d7b502299c2aa.pdf",
        "author": "Liu Zhenyuan;Xinyuan Li;Yu Guo;Bernd Bickel;Ran Zhang",
        "authorids": "~Liu_Zhenyuan1;~Xinyuan_Li1;~Yu_Guo10;~Bernd_Bickel1;~Ran_Zhang10",
        "gender": ";M;;M;M",
        "homepage": "https://desmondlzy.me;;https://tflsguoyu.github.io/;http://berndbickel.com/;https://www.ran-zhang.com/",
        "dblp": ";;53/382-7;;23/4835-7",
        "google_scholar": "C_n132YAAAAJ;;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=Bt-QKXYAAAAJ;V5QsRzMAAAAJ",
        "orcid": "0000-0001-9200-5690;;;0000-0001-6511-9385;0000-0002-3808-281X",
        "linkedin": ";xinyuan-li-543737217/;;;ran-zhang-coder/",
        "or_profile": "~Liu_Zhenyuan1;~Xinyuan_Li1;~Yu_Guo10;~Bernd_Bickel1;~Ran_Zhang10",
        "aff": "ETHZ - ETH Zurich;Tencent America;George Mason University;ETHZ - ETH Zurich+Google;Tencent America",
        "aff_domain": "ethz.ch;global.tencent.com;gmu.edu;ethz.ch+google.com;tencent.com",
        "position": "PhD student;Researcher;Researcher;Full Professor+Researcher;Researcher",
        "bibtex": "@inproceedings{\nzhenyuan2025bigs,\ntitle={Bi{GS}: Bidirectional Primitives for Relightable 3D Gaussian Splatting},\nauthor={Liu Zhenyuan and Xinyuan Li and Yu Guo and Bernd Bickel and Ran Zhang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=H2aizfRWXH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H2aizfRWXH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HEjv660F3j",
        "title": "CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone Feature Propagation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Depth completion using lightweight time-of-flight (ToF) depth sensors is attractive due to their low cost. However, lightweight ToF sensors usually have a limited field of view (FOV) compared with cameras. Thus, only pixels in the zone area of the image can be associated with depth signals. Previous methods fail to propagate depth features from the zone area to the outside-zone area effectively, thus suffering from degraded depth completion performance outside the zone. To this end, this paper proposes the CFPNet to achieve cross-zone feature propagation from the zone area to the outside-zone area with two novel modules. The first is a direct-attention-based propagation module (DAPM), which enforces direct cross-zone feature acquisition. The second is a large-kernel-based propagation module (LKPM), which realizes cross-zone feature propagation by utilizing convolution layers with kernel sizes up to 31. CFPNet achieves state-of-the-art (SOTA) depth completion performance by combining these two modules properly, as verified by extensive experimental results on the ZJU-L5 dataset. The code is available at https://github.com/denyingmxd/CFPNet.",
        "keywords": "Lightweight ToF Sensor;Depth Completion;Cross-zone Feature Propagation",
        "primary_area": "",
        "supplementary_material": "/attachment/e75c2cf679735f5719d12edb2cabc095698e8c0b.pdf",
        "author": "Laiyan Ding;Hualie Jiang;Rui Xu;Rui Huang",
        "authorids": "~Laiyan_Ding1;~Hualie_Jiang1;~Rui_Xu20;~Rui_Huang2",
        "gender": "M;M;M;M",
        "homepage": ";https://hualie.github.io/;;https://sse.cuhk.edu.cn/en/faculty/huangrui",
        "dblp": ";207/2313;;56/2875-1",
        "google_scholar": ";https://scholar.google.com.hk/citations?user=kKBsyGAAAAAJ;https://scholar.google.com/citations?view_op=list_works;t8UduWwAAAAJ",
        "orcid": "0009-0006-3093-5335;0000-0002-8241-0614;;",
        "linkedin": ";;;",
        "or_profile": "~Laiyan_Ding1;~Hualie_Jiang1;~Rui_Xu20;~Rui_Huang2",
        "aff": "CUHK-Shenzhen;Insta360;Insta360;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "link.cuhk.edu.cn;insta360.com;insta360.com;cuhk.edu.cn",
        "position": "PhD student;Researcher;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nding2025cfpnet,\ntitle={{CFPN}et: Improving Lightweight ToF Depth Completion via Cross-zone Feature Propagation},\nauthor={Laiyan Ding and Hualie Jiang and Rui Xu and Rui Huang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=HEjv660F3j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HEjv660F3j",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HJo4ckJG0Q",
        "title": "TEDRA: Text-based Editing of Dynamic and Photoreal Actors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Over the past years, significant progress was made in creating photorealistic and drivable 3D avatars solely from videos of real humans.\nHowever, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions.\nIn particular, text-based edits of full-body avatars should satisfy two properties: 1) Spatio-temporal consistency, i.e. the dynamics, and the photo-real quality of the original avatar, should remain intact;  2) The final result should respect the user-specified edit. To this end, we present TEDRA the first method allowing text-based edits of an avatar, that are photorealistic, space-time coherent, dynamic, and enable skeletal pose and view control.  We leverage a pre-trained avatar that is represented as a signed distance and radiance field, which is anchored to an explicit and deformable mesh template. After a pre-training stage, we obtain a drivable and photo-real digital counterpart of the real actor. Specifically, we employ an optimization strategy to integrate various frames capturing distinct camera perspectives and the dynamics of a video performance into a unified diffusion model. Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt, introducing the Normal Aligned Identity Preserving Score Distillation Sampling (NAIP-SDS) within a model-based guidance framework. Additionally, we implement a time-step annealing strategy to ensure the high quality of our edits. Our results demonstrate a clear improvement over prior work in terms of functionality and visual quality.\nThus, our method is a clear step towards intuitive and photorealistic editability of digital avatars, which explicitly accounts for dynamics and allows skeletal pose and view control at test time.",
        "keywords": "Neural Rendering;Neural (implicit) representations;3D human body shape modeling;Generative models",
        "primary_area": "",
        "supplementary_material": "/attachment/3c965f10d049fafb3752594531f4202abfd4cf84.zip",
        "author": "Basavaraj Sunagad;Heming Zhu;Mohit Mendiratta;Adam Kortylewski;Christian Theobalt;Marc Habermann",
        "authorids": "~Basavaraj_Sunagad1;~Heming_Zhu1;~Mohit_Mendiratta1;~Adam_Kortylewski1;~Christian_Theobalt2;~Marc_Habermann1",
        "gender": "M;M;M;;M;M",
        "homepage": ";https://scholar.google.com/citations?user=u5KCnv0AAAAJ&hl=en;https://people.mpi-inf.mpg.de/~mmendira/;https://gvrl.mpi-inf.mpg.de/;https://www.mpi-inf.mpg.de/~theobalt/;https://people.mpi-inf.mpg.de/~mhaberma/",
        "dblp": ";217/9475;280/1629.html;161/0772;55/3346;227/2744",
        "google_scholar": ";u5KCnv0AAAAJ;Y3Bc8isAAAAJ;https://scholar.google.ch/citations?user=tRLUOBIAAAAJ;https://scholar.google.com.tw/citations?user=eIWg8NMAAAAJ;oWstvNcAAAAJ",
        "orcid": ";0000-0003-3525-9349;;0000-0002-9146-4403;;",
        "linkedin": "basavaraj-sunagad/;;;;;",
        "or_profile": "~Basavaraj_Sunagad1;~Heming_Zhu1;~Mohit_Mendiratta1;~Adam_Kortylewski1;~Christian_Theobalt2;~Marc_Habermann1",
        "aff": "Albert-Ludwigs-Universit\u00e4t Freiburg;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute+Albert-Ludwigs-Universit\u00e4t Freiburg;Saarbruecken Research Center for Visual Computing, Interaction, and Artificial Intellligence+Max-Planck-Institute for Informatics, Saarland Informatics Campus;Saarland Informatics Campus, Max-Planck Institute",
        "aff_domain": "uni-freiburg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de+uni-freiburg.de;via-center.science+mpi-inf.mpg.de;mpi-inf.mpg.de",
        "position": "PhD student;PhD student;PhD student;Research Group Leader+Research Group Leader;Director+Director;Principal Researcher",
        "bibtex": "@inproceedings{\nsunagad2025tedra,\ntitle={{TEDRA}: Text-based Editing of Dynamic and Photoreal Actors},\nauthor={Basavaraj Sunagad and Heming Zhu and Mohit Mendiratta and Adam Kortylewski and Christian Theobalt and Marc Habermann},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=HJo4ckJG0Q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HJo4ckJG0Q",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "HOBlmFJS9b",
        "title": "ShadowSG: Spherical Gaussian Illumination from Shadows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work leverages shadow cues in a scene to infer the surrounding illumination of the shadow-casting object. \nUnlike prior works that optimize a discrete environment map, we model scene illumination using a mixture of spherical Gaussians (SGs). \nSG illumination provides more intuitive relations to shadow appearance and offers a more compact parameterization compared to discrete environment maps. \nTo estimate SG parameters, we employ an SG-based, differentiable, closed-form rendering equation to explain the shading of the shadow plane and minimize a photometric loss between the rendered and observed shadow plane shading. \nExperiments on synthetic and real-world images under various surrounding illumination demonstrate that our method estimates illumination more accurately than approaches based on discrete environment maps. \nWith our estimated lighting, consistent shadow effects are realized when blending virtual objects into real-world images.",
        "keywords": "illumination from shadow;spherical Gaussian;illumination estimation",
        "primary_area": "",
        "supplementary_material": "/attachment/e25c8b4ea12326d3e3ca8f62fa5d8b1db3ce8082.zip",
        "author": "Hanwei Zhang;Xu Cao;Hiroshi Kawasaki;Takafumi Taketomi",
        "authorids": "~Hanwei_Zhang3;~Xu_Cao2;~Hiroshi_Kawasaki1;~Takafumi_Taketomi1",
        "gender": "M;M;M;M",
        "homepage": "https://r0bertr.github.io/;https://xucao-42.github.io/homepage/;https://www.cvg.ait.kyushu-u.ac.jp;https://taketomitakafumi.sakura.ne.jp/web/en/",
        "dblp": "144/8890;;84/4575;95/1736",
        "google_scholar": ";oq19mdMAAAAJ;OkKk5rMAAAAJ;1SRp8P0AAAAJ",
        "orcid": ";0000-0003-4309-6922;0000-0001-5825-6066;",
        "linkedin": ";xu-cao-947605169/;;",
        "or_profile": "~Hanwei_Zhang3;~Xu_Cao2;~Hiroshi_Kawasaki1;~Takafumi_Taketomi1",
        "aff": "Kyushu University;CyberAgent, Inc.;Kyushu University;CyberAgent",
        "aff_domain": "kyushu-u.ac.jp;cyberagent.co.jp;kyushu-u.ac.jp;cyberagent.co.jp",
        "position": "PhD student;Researcher;Professor;Researcher",
        "bibtex": "@inproceedings{\nzhang2025shadowsg,\ntitle={Shadow{SG}: Spherical Gaussian Illumination from Shadows},\nauthor={Hanwei Zhang and Xu Cao and Hiroshi Kawasaki and Takafumi Taketomi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=HOBlmFJS9b}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HOBlmFJS9b",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Hr0qISJ3q0",
        "title": "Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "State-of-the-art novel view synthesis methods achieve impressive results for multi-view captures of static 3D scenes. However, the reconstructed scenes still lack \u201cliveliness,\u201d a key component for creating engaging 3D experiences. Recently, novel video diffusion models generate realistic videos with complex motion and enable animations of 2D images, however they cannot naively be used to animate 3D scenes as they lack multi-view consistency. To breathe life into the static world, we propose Gaussians2Life, a method for animating parts of high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is to leverage powerful video diffusion models as the generative component of our model and to combine these with a robust technique to lift 2D videos into meaningful 3D motion. We find that, in contrast to prior work, this enables realistic animations of complex, pre-existing 3D scenes in a robust manner and further enables the animation of a large variety of object classes, while related work is mostly focused on prior-based character animation, or single 3D objects due to biases of video diffusion models. Our model can readily be used to create immersive and engaging 3D experiences for arbitrary scenes in a consistent manner.",
        "keywords": "Gaussian Splatting;Diffusion Models;Generative Dynamics",
        "primary_area": "",
        "supplementary_material": "/attachment/cc05bf8125ab17accaed6b650ba50cffbe3268a1.pdf",
        "author": "Thomas Wimmer;Michael Oechsle;Michael Niemeyer;Federico Tombari",
        "authorids": "~Thomas_Wimmer1;~Michael_Oechsle1;~Michael_Niemeyer1;~Federico_Tombari1",
        "gender": "M;;M;M",
        "homepage": "https://wimmerth.github.io;https://moechsle.github.io/;https://m-niemeyer.github.io/;https://federicotombari.github.io/",
        "dblp": "142/3029-1;232/2468;232/1712;16/3539",
        "google_scholar": "LjdBF_IAAAAJ;vMcsUNAAAAAJ;https://scholar.google.de/citations?user=v1O7i_0AAAAJ;TFsE4BIAAAAJ",
        "orcid": "0009-0006-0895-3529;;;0000-0001-5598-5212",
        "linkedin": "wimmerth;;;fedet/",
        "or_profile": "~Thomas_Wimmer1;~Michael_Oechsle1;~Michael_Niemeyer1;~Federico_Tombari1",
        "aff": "Saarland Informatics Campus, Max-Planck Institute+ETHZ - ETH Zurich;Google;Google;Google+Technical University Munich (TUM)",
        "aff_domain": "mpi-inf.mpg.de+ethz.ch;google.com;google.com;google.com+in.tum.de",
        "position": "PhD student+PhD student;Researcher;Researcher;Research Director+Lecturer",
        "bibtex": "@inproceedings{\nwimmer2025gaussianstolife,\ntitle={Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes},\nauthor={Thomas Wimmer and Michael Oechsle and Michael Niemeyer and Federico Tombari},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Hr0qISJ3q0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Hr0qISJ3q0",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Hszx6norp3",
        "title": "InterTrack: Tracking Human Object Interaction without Object Templates",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Tracking human object interaction from videos is important to understand human behavior from the rapidly growing stream of video data. Previous video-based methods require predefined object templates while single-image-based methods are template-free but lack temporal consistency. In this paper, we present a method to track human object interaction without any object shape templates. We decompose the 4D tracking problem into per-frame pose tracking and canonical shape optimization. We first apply a single-view reconstruction method to obtain temporally-inconsistent per-frame interaction reconstructions. \nThen, for the human, we propose an efficient autoencoder to predict SMPL vertices directly from the per-frame reconstructions, introducing temporally consistent correspondence. \nFor the object, we introduce a pose estimator that leverages temporal information to predict smooth object rotations under occlusions. To train our model, we propose a method to generate synthetic interaction videos and synthesize in total 10 hour videos of 8.3k sequences with full 3D ground truth. Experiments on BEHAVE and InterCap show that our method significantly outperforms previous template-based video tracking and single-frame reconstruction methods. Our proposed synthetic video dataset also allows training video-based methods that generalize to real-world videos. Our code and dataset are publicly released.",
        "keywords": "Human object interaction;3D reconstruction;4D video tracking",
        "primary_area": "",
        "supplementary_material": "/attachment/b099a10ba3300a3159ebf4d25c81cf6ba3631420.pdf",
        "author": "Xianghui Xie;Jan Eric Lenssen;Gerard Pons-Moll",
        "authorids": "~Xianghui_Xie1;~Jan_Eric_Lenssen1;~Gerard_Pons-Moll2",
        "gender": ";M;",
        "homepage": "https://people.mpi-inf.mpg.de/~xxie/;https://janericlenssen.github.io/;",
        "dblp": "27/6971;195/9868;",
        "google_scholar": "J3TVNXEAAAAJ;https://scholar.google.de/citations?user=enXCzCgAAAAJ;",
        "orcid": ";0000-0003-4093-9840;",
        "linkedin": "xianghui-xie-3a8817198/;jan-eric-lenssen-08700b190/;",
        "or_profile": "~Xianghui_Xie1;~Jan_Eric_Lenssen1;~Gerard_Pons-Moll2",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen+Saarland Informatics Campus, Max-Planck Institute;Saarland Informatics Campus, Max-Planck Institute+Kumo;",
        "aff_domain": "uni-tuebingen.de+mpi-inf.mpg.de;mpi-inf.mpg.de+kumo.ai;",
        "position": "PhD student+PhD student;Principal Researcher+Researcher;",
        "bibtex": "@inproceedings{\nxie2025intertrack,\ntitle={InterTrack: Tracking Human Object Interaction without Object Templates},\nauthor={Xianghui Xie and Jan Eric Lenssen and Gerard Pons-Moll},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Hszx6norp3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Hszx6norp3",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "IviNrUpXHc",
        "title": "OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Dynamic neural radiance fields (dynamic NeRFs) have achieved remarkable successes in synthesizing novel views for 3D dynamic scenes. Traditional approaches typically necessitate full video sequences for the training phase prior to the synthesis of new views, akin to replaying a recording of a dynamic 3D event. In contrast, on-the-fly training allows for the immediate processing and rendering of dynamic scenes without the need for pre-training on full sequences, offering a more flexible and time-efficient solution for dynamic scene rendering tasks.In this paper, we propose a highly efficient on-the-fly training algorithm for dynamic NeRFs, named OD-NeRF. To accelerate the training process, our method minimizes the training required for the model at each frame by using: 1) a NeRF model conditioned on multi-view projected colors, which exhibits superior generalization across multiple frames with minimal training ,and 2) a transition and update algorithm that leverages the occupancy grid from the last frame to sample efficiently at the current frame. Our algorithm can achieve an interactive training speed of 10FPS on synthetic dynamic scenes on-the-fly, and a 3$\\times$-9$\\times$ training speed-up compared to the state-of-the-art on-the-fly NeRF on real-world dynamic scenes.",
        "keywords": "3D computer vision;novel view synthesis;dynamic scene reconstruction;model training acceleration;neural radiance field;NeRF",
        "primary_area": "",
        "supplementary_material": "/attachment/30765a1ce23e79173650fa608188e410020ec83d.pdf",
        "author": "Zhiwen Yan;Chen Li;Gim Hee Lee",
        "authorids": "~Zhiwen_Yan1;~Chen_Li13;~Gim_Hee_Lee1",
        "gender": "M;F;",
        "homepage": ";https://chaneyddtt.github.io/;https://www.comp.nus.edu.sg/~leegh/",
        "dblp": ";164/3294-38;49/9455",
        "google_scholar": "https://scholar.google.com/citations?hl=en;6_rJ2pcAAAAJ;https://scholar.google.com.sg/citations?user=7hNKrPsAAAAJ",
        "orcid": ";0009-0000-6807-3490;0000-0002-1583-0475",
        "linkedin": ";;",
        "or_profile": "~Zhiwen_Yan1;~Chen_Li13;~Gim_Hee_Lee1",
        "aff": "national university of singaore, National University of Singapore;A*STAR;National University of Singapore",
        "aff_domain": "u.nus.edu;astar.edu.sg;nus.edu.sg",
        "position": "PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nyan2025odnerf,\ntitle={{OD}-Ne{RF}: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields},\nauthor={Zhiwen Yan and Chen Li and Gim Hee Lee},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=IviNrUpXHc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IviNrUpXHc",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "J9DxFokoRL",
        "title": "Rigid Body Adversarial Attacks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Due to their performance and simplicity, rigid body simulators are often used in applications where the objects of interest can considered very stiff. However, no material has infinite stiffness, which means there are potentially cases where the non-zero compliance of the seemingly rigid object can cause a significant difference between its trajectories when simulated in a rigid body or deformable simulator. \n\nSimilarly to how adversarial attacks are developed against image classifiers, we propose an adversarial attack against rigid body simulators. In this adversarial attack, we solve an optimization problem to construct perceptually rigid adversarial objects that have the same collision geometry and moments of mass to a reference object, so that they behave identically in rigid body simulations but maximally different in more accurate deformable simulations. We demonstrate the validity of our method by comparing simulations of several examples in commercially available simulators.",
        "keywords": "adversarial attack;robotics;simulation;rigid body simulation;deformable simulation;differentiable simulation",
        "primary_area": "",
        "supplementary_material": "/attachment/e7bfd43a580db279485323a73ae08e5f6e08f879.zip",
        "author": "Aravind Ramakrishnan;David Levin I.W.;Alec Jacobson",
        "authorids": "~Aravind_Ramakrishnan1;~David_Levin_I.W.1;~Alec_Jacobson1",
        "gender": ";M;M",
        "homepage": ";http://cs.toronto.edu/~diwlevin;http://www.cs.toronto.edu/~jacobson/",
        "dblp": ";97/8315.html;33/8698.html",
        "google_scholar": ";https://scholar.google.ca/citations?user=UokN-dcAAAAJ;https://scholar.google.ca/citations?user=lSJavJUAAAAJ",
        "orcid": ";;0000-0003-4603-7143",
        "linkedin": ";;",
        "or_profile": "~Aravind_Ramakrishnan1;~David_Levin_I.W.1;~Alec_Jacobson1",
        "aff": ";Department of Computer Science, University of Toronto;Adobe Systems+Department of Computer Science, University of Toronto",
        "aff_domain": ";cs.toronto.edu;adobe.com+cs.toronto.edu",
        "position": ";Assistant Professor;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nramakrishnan2025rigid,\ntitle={Rigid Body Adversarial Attacks},\nauthor={Aravind Ramakrishnan and David Levin I.W. and Alec Jacobson},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=J9DxFokoRL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=J9DxFokoRL",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "JmQ28zMTcU",
        "title": "Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Text-driven 3D indoor scene generation is useful for gaming, film industry, and AR/VR applications. However, existing methods cannot faithfully capture the scene layout based on text descriptions, nor do they allow flexible editing of individual objects in the room.\nTo address these problems, we present Ctrl-Room, which can generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Our key insight is to separate the modeling of layouts and appearance.\nOur proposed method consists of two stages: a Layout Generation Stage and an Appearance Generation Stage. The Layout Generation Stage trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the Appearance Generation Stage employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout, then further upgrades to a panoramic NeRF model. \nBenefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive edit-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from text prompts.",
        "keywords": "3D Room genenration; Layout geenration; Panorama genneration;",
        "primary_area": "",
        "supplementary_material": "/attachment/3533dd07985f01b33cb15812bab751a3e6cd1d00.zip",
        "author": "Chuan Fang;Yuan Dong;Kunming Luo;Xiaotao Hu;Rakesh Shrestha;Ping Tan",
        "authorids": "~Chuan_Fang1;~Yuan_Dong3;~Kunming_Luo1;~Xiaotao_Hu1;~Rakesh_Shrestha1;~Ping_Tan2",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://github.com/fangchuan/fangchuan.github.io;https://fdyuandong.github.io/;https://coolbeam.github.io/index.html;;http://www.sfu.ca/~rakeshs/;http://www.cs.sfu.ca/~pingtan/",
        "dblp": "289/0014;66/8751;213/4872;174/1500;36/7723;",
        "google_scholar": "cBUlGS8AAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.hk/citations?hl=zh-CN;PRcnqk4AAAAJ;R_oyktoAAAAJ;XhyKVFMAAAAJ",
        "orcid": "0000-0003-2626-6029;0000-0002-8856-995X;0000-0002-5070-7392;;0000-0001-6386-7072;0000-0002-4506-6973",
        "linkedin": ";;;;rakesh-shrestha/;",
        "or_profile": "~Chuan_Fang1;~Yuan_Dong3;~Kunming_Luo1;~Xiaotao_Hu1;~Rakesh_Shrestha1;~Ping_Tan2",
        "aff": "Hong Kong University of Science and Technology;Alibaba Group;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;;Hong Kong University of Science and Technology",
        "aff_domain": "ust.hk;alibaba-inc.com;ust.hk;connect.ust.hk;;ust.hk",
        "position": "PhD student;Researcher;PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nfang2025ctrlroom,\ntitle={Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints},\nauthor={Chuan Fang and Yuan Dong and Kunming Luo and Xiaotao Hu and Rakesh Shrestha and Ping Tan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=JmQ28zMTcU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JmQ28zMTcU",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "K7UkM0XXBQ",
        "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce GaussianAvatar-Editor, an innovative framework for text-driven editing of animatable Gaussian head avatars that can be fully controlled in expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing animatable 4D Gaussian avatars presents challenges related to motion occlusion and spatial-temporal inconsistency. To address these issues, we propose the Weighted Alpha Blending Equation (WABE). This function enhances the blending weight of visible Gaussians while suppressing the influence on non-visible Gaussians, effectively handling motion occlusion during editing. Furthermore, to improve editing quality and ensure 4D consistency, we incorporate conditional adversarial learning into the editing process. This strategy helps to refine the edited results and maintain consistency throughout the animation. By integrating these methods, our GaussianAvatar-Editor achieves photorealistic and consistent results in animatable 4D Gaussian editing. We conduct comprehensive experiments across various subjects to validate the effectiveness of our proposed techniques, which demonstrates the superiority of our approach over existing methods. More results and code are available at: \\url{https://xiangyueliu.github.io/GaussianAvatar-Editor/}.",
        "keywords": "Gaussian Head Avatar;Text-driven Editing;Animatable",
        "primary_area": "",
        "supplementary_material": "/attachment/4479120967714be7c9507618e780fcfee68ee2b5.zip",
        "author": "Xiangyue Liu;Kunming Luo;Heng Li;Qi Zhang;Yuan Liu;Li Yi;Ping Tan",
        "authorids": "~Xiangyue_Liu1;~Kunming_Luo1;~Heng_Li6;~Qi_Zhang10;~Yuan_Liu3;~Li_Yi2;~Ping_Tan2",
        "gender": "F;M;M;M;M;M;M",
        "homepage": "https://xiangyueliu.github.io/;https://coolbeam.github.io/index.html;http://hengli.me;https://qzhang-cv.github.io/;https://liuyuan-pal.github.io/;https://ericyi.github.io/;http://www.cs.sfu.ca/~pingtan/",
        "dblp": ";213/4872;02/3672-9;52/323-29;87/2948-25;26/4239-1;",
        "google_scholar": "m3c3jnYAAAAJ;https://scholar.google.com.hk/citations?hl=zh-CN;tjbbehcAAAAJ;2vFjhHMAAAAJ;yRAHVcgAAAAJ;UyZL660AAAAJ;XhyKVFMAAAAJ",
        "orcid": ";0000-0002-5070-7392;0000-0001-5143-5061;;;;0000-0002-4506-6973",
        "linkedin": ";;;;;;",
        "or_profile": "~Xiangyue_Liu1;~Kunming_Luo1;~Heng_Li6;~Qi_Zhang10;~Yuan_Liu3;~Li_Yi2;~Ping_Tan2",
        "aff": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Vivo;Hong Kong University of Science and Technology+Nanyang Technological University;Tsinghua University;Hong Kong University of Science and Technology",
        "aff_domain": "hkust.edu;ust.hk;ust.hk;vivo.com;ust.hk+ntu.edu.sg;tsinghua.edu.cn;ust.hk",
        "position": "PhD student;PhD student;PhD student;Principal Researcher;Assistant Professor+Postdoc;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nliu2025gaussianavatareditor,\ntitle={GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor},\nauthor={Xiangyue Liu and Kunming Luo and Heng Li and Qi Zhang and Yuan Liu and Li Yi and Ping Tan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=K7UkM0XXBQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=K7UkM0XXBQ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "KhjlXNbYea",
        "title": "LangOcc: Open Vocabulary Occupancy Estimation via Volume Rendering",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "The 3D occupancy estimation task has become an important challenge in the area of vision-based autonomous driving recently.\nHowever, most existing camera-based methods rely on costly 3D voxel labels or LiDAR scans for training, limiting their practicality and scalability.\nMoreover, most methods are tied to a predefined set of classes which they can detect.\nIn this work we present a novel approach for open vocabulary occupancy estimation called LangOcc, that is trained only via camera images, and can detect arbitrary semantics via vision-language alignment.\nIn particular, we distill the knowledge of the strong vision-language aligned encoder CLIP into a 3D occupancy model via differentiable volume rendering. \nOur model estimates vision-language aligned features in a 3D voxel grid using only images.\nIt is trained in a weakly-supervised manner by rendering our estimations back to 2D space, where features can easily be aligned with CLIP.\nThis training mechanism automatically supervises the scene geometry, allowing for a straight-forward and powerful training method without any explicit geometry supervision.\nLangOcc outperforms LiDAR-supervised competitors in open vocabulary occupancy with a mAP of $22.7$ by a large margin ($+4.3 \\%$), solely relying on vision-based training.\nWe also achieve a mIoU score of $11.84$ on the Occ3D-nuScenes dataset, surpassing previous vision-only semantic occupancy estimation methods ($+1.71\\%$), despite not being limited to a specific set of categories.",
        "keywords": "Occupancy Estimation;Open Vocabulary;Volume Rendering;CLIP",
        "primary_area": "",
        "supplementary_material": "/attachment/9a6225b63771a71f0e35597f12fd4757d103babb.zip",
        "author": "Simon Boeder;Fabian Gigengack;Benjamin Risse",
        "authorids": "~Simon_Boeder1;~Fabian_Gigengack2;~Benjamin_Risse1",
        "gender": "M;;M",
        "homepage": ";;https://cvmls.uni-muenster.de",
        "dblp": "291/2671;51/8066.html;124/2684",
        "google_scholar": "https://scholar.google.de/citations?user=eEsebSgAAAAJ;_Zk9J1MAAAAJ;rWx-1t0AAAAJ",
        "orcid": ";;0000-0001-5691-4029",
        "linkedin": ";;",
        "or_profile": "~Simon_Boeder1;~Fabian_Gigengack2;~Benjamin_Risse1",
        "aff": "Robert Bosch GmbH, Bosch+Westf\u00e4lische Wilhelms-Universit\u00e4t M\u00fcnster;Robert Bosch GmbH, Bosch;University of M\u00fcnster",
        "aff_domain": "de.bosch.com+uni-muenster.de;de.bosch.com;uni-muenster.de",
        "position": "PhD student+PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nboeder2025langocc,\ntitle={LangOcc: Open Vocabulary Occupancy Estimation via Volume Rendering},\nauthor={Simon Boeder and Fabian Gigengack and Benjamin Risse},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=KhjlXNbYea}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KhjlXNbYea",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LTOXGzVA0R",
        "title": "3D Whole-body Grasp Synthesis with Directional Controllability",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Synthesizing 3D whole bodies that realistically grasp objects is useful for animation, mixed reality, and robotics. This is challenging, because the hands and body need to look natural w.r.t. each other, the grasped object, as well as the local scene (i.e., a receptacle supporting the object). Moreover, training data for this task is really scarce, while capturing new data is expensive. Recent work goes beyond finite datasets via a divide-and-conquer approach; it first generates a \u201cguiding\u201d right-hand grasp, and then searches for bodies that match this. However, the guiding-hand synthesis lacks controllability and receptacle awareness, so it likely has an implausible direction (i.e., a body can\u2019t match this without penetrating the receptacle) and needs corrections through major post-processing. Moreover, the body search needs exhaustive sampling and is expensive. These are strong limitations. We tackle these with a novel method called CWGrasp. Our key idea is that performing geometry-based reasoning \u201cearly on,\u201d instead of \u201ctoo late,\u201d provides rich \u201ccontrol\u201d signals for inference. To this end, CWGrasp first samples a plausible reaching-direction vector (used later for both the arm and hand) from a probabilistic model built via ray-casting from the object and collision checking. Then, it generates a reaching body with a desired arm direction, as well as a \u201cguiding\u201d grasping hand with a desired palm direction that complies with the arm\u2019s one. Eventually, CWGrasp refines the body to match the \u201cguiding\u201d hand, while plausibly contacting the scene. Notably, generating already-compatible \u201cparts\u201d greatly simplifies the \u201cwhole\u201d. Moreover, CWGrasp uniquely tackles both right and left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms baselines, at lower runtime and budget, while all components help performance. Code and models are available at https://gpaschalidis.github.io/cwgrasp.",
        "keywords": "3D pose synthesis;3D grasp synthesis;3D human-object interaction;3D hand-object interaction;3D whole-body grasps",
        "primary_area": "",
        "supplementary_material": "/attachment/55c38280b7047b75085b09eb66c0c25c2744994d.pdf",
        "author": "Georgios Paschalidis;Romana Wilschut;Dimitrije Anti\u0107;Omid Taheri;Dimitrios Tzionas",
        "authorids": "~Georgios_Paschalidis1;~Romana_Wilschut1;~Dimitrije_Anti\u01071;~Omid_Taheri1;~Dimitrios_Tzionas1",
        "gender": ";;;M;M",
        "homepage": ";;;https://ps.is.tuebingen.mpg.de/person/otaheri;https://dtzionas.com",
        "dblp": ";;;34/4091;134/3198",
        "google_scholar": ";;;BahpFDAAAAAJ;vPVX4TIAAAAJ",
        "orcid": ";;;;0000-0002-7963-2582",
        "linkedin": ";;;omidtaherii/;dimitris-tzionas",
        "or_profile": "~Georgios_Paschalidis1;~Romana_Wilschut1;~Dimitrije_Anti\u01071;~Omid_Taheri1;~Dimitrios_Tzionas1",
        "aff": ";;;Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Amsterdam",
        "aff_domain": ";;;tue.mpg.de;uva.nl",
        "position": ";;;Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\npaschalidis2025d,\ntitle={3D Whole-body Grasp Synthesis with Directional Controllability},\nauthor={Georgios Paschalidis and Romana Wilschut and Dimitrije Anti{\\'c} and Omid Taheri and Dimitrios Tzionas},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=LTOXGzVA0R}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LTOXGzVA0R",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LfYcNWCEsN",
        "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We present a method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight-fitting clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for high-quality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated \"bag-of-bones\" deformation (fit to a single video via test-time optimization). We accomplish this by learning a neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage image-based priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or further refined as explicit 3D gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art.",
        "keywords": "Neural Rendering;4D Reconstruction;Dynamic Avatars",
        "primary_area": "",
        "supplementary_material": "/attachment/7162fa097b3189a24247b8cd89779370db14f212.zip",
        "author": "Jeff Tan;Donglai Xiang;Shubham Tulsiani;Deva Ramanan;Gengshan Yang",
        "authorids": "~Jeff_Tan1;~Donglai_Xiang1;~Shubham_Tulsiani1;~Deva_Ramanan1;~Gengshan_Yang1",
        "gender": ";M;M;M;",
        "homepage": "https://jefftan969.github.io/;https://xiangdonglai.github.io/;https://shubhtuls.github.io/;https://www.cs.cmu.edu/~deva/;http://gengshan-y.github.io/",
        "dblp": ";172/1223;135/6623;49/488;180/7347",
        "google_scholar": "HKtnoYkAAAAJ;tjT-DfsAAAAJ;06rffEkAAAAJ;9B8PoXUAAAAJ;yRaFnrcAAAAJ",
        "orcid": ";0000-0002-6487-1935;;;",
        "linkedin": "jeff-tan-2344651b6;;;;",
        "or_profile": "~Jeff_Tan1;~Donglai_Xiang1;~Shubham_Tulsiani1;~Deva_Ramanan1;~Gengshan_Yang1",
        "aff": "Carnegie Mellon University;NVIDIA;Carnegie Mellon University;School of Computer Science, Carnegie Mellon University;World Labs",
        "aff_domain": "cs.cmu.edu;nvidia.com;cmu.edu;cs.cmu.edu;worldlabs.ai",
        "position": "MS student;Researcher;Assistant Professor;Full Professor;Researcher",
        "bibtex": "@inproceedings{\ntan2025dressrecon,\ntitle={DressRecon: Freeform 4D Human Reconstruction from Monocular Video},\nauthor={Jeff Tan and Donglai Xiang and Shubham Tulsiani and Deva Ramanan and Gengshan Yang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=LfYcNWCEsN}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LfYcNWCEsN",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "LuufG3EA6l",
        "title": "Online 3D Scene Reconstruction Using Neural Object Priors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper addresses the problem of reconstructing a scene online at the level of objects given an RGB-D video sequence.  While current object-aware neural implicit representations hold promise, they are limited in online reconstruction efficiency and shape completion. Our main contributions to alleviate the above limitations are twofold. First, we propose a feature grid interpolation mechanism to continuously update grid-based object-centric neural implicit representations as new object parts are revealed. Second, we construct an object library with previously mapped objects in advance and leverage the corresponding shape priors to initialize geometric object models in new videos, subsequently completing them with novel views as well as synthesized past views to avoid losing original object details. Extensive experiments on synthetic environments from the Replica dataset, real-world ScanNet sequences and videos captured in our laboratory demonstrate that our approach outperforms state-of-the-art neural implicit models for this task in terms of reconstruction accuracy and completeness.",
        "keywords": "Online 3D reconstruction;neural implicit models;object-centric models;object priors",
        "primary_area": "",
        "supplementary_material": "/attachment/083135a003c93db1657857dfa11b1189b42b4539.pdf",
        "author": "Thomas Chabal;Shizhe Chen;Jean Ponce;Cordelia Schmid",
        "authorids": "~Thomas_Chabal1;~Shizhe_Chen1;~Jean_Ponce1;~Cordelia_Schmid1",
        "gender": ";F;M;F",
        "homepage": "https://thomaschabal.github.io/;https://cshizhe.github.io/;http://www.di.ens.fr/~ponce/;https://cordeliaschmid.github.io/",
        "dblp": "318/9353.html;153/0734;p/JeanPonce;s/CordeliaSchmid",
        "google_scholar": "5z3sNRwAAAAJ;wZhRRy0AAAAJ;https://scholar.google.com.tw/citations?user=vC2vywcAAAAJ;IvqCXP4AAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;cordelia-schmid-47985a9",
        "or_profile": "~Thomas_Chabal1;~Shizhe_Chen1;~Jean_Ponce1;~Cordelia_Schmid1",
        "aff": "INRIA;INRIA;Ecole Normale Sup\u00e9rieure de Paris;Google+INRIA+Inria",
        "aff_domain": "inria.fr;inria.fr;ens.fr;google.com+inria.fr+inria.fr",
        "position": "PhD student;Researcher;Full Professor;Researcher+Full Professor+Researcher",
        "bibtex": "@inproceedings{\nchabal2025online,\ntitle={Online 3D Scene Reconstruction Using Neural Object Priors},\nauthor={Thomas Chabal and Shizhe Chen and Jean Ponce and Cordelia Schmid},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=LuufG3EA6l}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LuufG3EA6l",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MBF4wjyIzi",
        "title": "AutoVFX: Physically Realistic Video Editing from Natural Language Instructions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.",
        "keywords": "Visual Effects;Text-guided Video Editing;Scene Simulation;Physical Simulation;LLM Agent;Object Insertion;Material Editing",
        "primary_area": "",
        "supplementary_material": "/attachment/34087808a5935c5740ed447f433638dfd951759a.zip",
        "author": "Hao-Yu Hsu;Zhi-Hao Lin;Albert J. Zhai;Hongchi Xia;Shenlong Wang",
        "authorids": "~Hao-Yu_Hsu1;~Zhi-Hao_Lin2;~Albert_J._Zhai1;~Hongchi_Xia1;~Shenlong_Wang1",
        "gender": "M;M;;M;M",
        "homepage": "https://haoyuhsu.github.io;https://zhihao-lin.github.io/;;https://xiahongchi.github.io/;https://shenlong.web.illinois.edu/",
        "dblp": "319/4481;118/2127;;355/1029;117/4842",
        "google_scholar": "fO4FU-IAAAAJ;QdqDnA0AAAAJ;;9iXQ-wsAAAAJ;QFpswmcAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;hongchi-xia-74b19a294/;shenlong-wang-3496023b",
        "or_profile": "~Hao-Yu_Hsu1;~Zhi-Hao_Lin2;~Albert_J._Zhai1;~Hongchi_Xia1;~Shenlong_Wang1",
        "aff": "University of Illinois, Urbana Champaign+University of Illinois, Urbana Champaign;Department of Computer Science;;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",
        "aff_domain": "illinois.edu+illinois.edu;cs.illinois.edu;;illinois.edu;illinois.edu",
        "position": "PhD student+MS student;PhD student;;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nhsu2025autovfx,\ntitle={Auto{VFX}: Physically Realistic Video Editing from Natural Language Instructions},\nauthor={Hao-Yu Hsu and Zhi-Hao Lin and Albert J. Zhai and Hongchi Xia and Shenlong Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=MBF4wjyIzi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MBF4wjyIzi",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MVWumPOpAO",
        "title": "Mesh Extraction for Unbounded Scenes Using Camera-Aware Octrees",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mesh extraction from occupancy functions is a useful tool in creating synthetic datasets for computer vision. However, existing mesh extraction methods have artifacts or performance profiles that limit their use. We propose OcMesher, a mesh extractor that efficiently handles high-detail unbounded scenes with perfect view consistency, with easy export to downstream real-time engines. The main novelty of our solution is an algorithm to construct an octree based on a given occupancy function and multiple camera views. We performed extensive experiments, and demonstrate OcMesher's usefulness for synthetic training & benchmark datasets, generating real-time environments for embodied AI and mesh extraction from depthmaps or novel view synthesis methods.",
        "keywords": "Mesh extraction;octree;synthetic data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zeyu Ma;Alexander Raistrick;Lahav Lipson;Jia Deng",
        "authorids": "~Zeyu_Ma1;~Alexander_Raistrick1;~Lahav_Lipson1;~Jia_Deng1",
        "gender": "M;M;M;M",
        "homepage": "https://github.com/mazeyu;https://araistrick.com;https://www.lahavlipson.com;",
        "dblp": "274/9586;292/3236;302/0769;07/6526-1.html",
        "google_scholar": ";hw7BEc0AAAAJ;;U3Eub-EAAAAJ",
        "orcid": ";;;",
        "linkedin": ";araistrick/;lahav-lipson-076493113/;",
        "or_profile": "~Zeyu_Ma1;~Alexander_Raistrick1;~Lahav_Lipson1;~Jia_Deng1",
        "aff": "Princeton University;Princeton University;Princeton University;Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "position": "PhD student;PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nma2025mesh,\ntitle={Mesh Extraction for Unbounded Scenes Using Camera-Aware Octrees},\nauthor={Zeyu Ma and Alexander Raistrick and Lahav Lipson and Jia Deng},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=MVWumPOpAO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MVWumPOpAO",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "MaN2x3O2Rk",
        "title": "LSSInst: Improving Geometric Modeling in LSS-Based BEV Perception with Instance Representation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "With the attention gained by camera-only 3D object detection in autonomous driving, methods based on Bird-Eye-View (BEV) representation especially derived from the forward view transformation paradigm, i.e., lift-splat-shoot (LSS), have recently seen significant progress. The BEV representation formulated by the frustum based on depth distribution prediction is ideal for learning the road structure and scene layout from multi-view images. However, to retain computational efficiency, the compressed BEV representation such as in resolution and axis is inevitably weak in retaining the individual geometric details, undermining the methodological generality and applicability. With this in mind, to compensate for the missing details and utilize multi-view geometry constraints, we propose LSSInst, a two-stage object detector incorporating BEV and instance representations in tandem. The proposed detector exploits fine-grained pixel-level features that can be flexibly integrated into existing LSS-based BEV networks. Having said that, due to the inherent gap between two representation spaces, we design the instance adaptor for the BEV-to-instance semantic coherence rather than pass the proposal naively. Extensive experiments demonstrated that our proposed framework is of excellent generalization ability and performance, which boosts the performances of modern LSS-based BEV perception methods without bells and whistles and outperforms current LSS-based state-of-the-art works on the large-scale nuScenes benchmark.",
        "keywords": "Multi-view 3D Object Detection; Geometric Modeling; LSS-based detection; Two-stage 3D Perception; Instance Representation;",
        "primary_area": "",
        "supplementary_material": "/attachment/c159399d144f48d431a3afa368e421590494e97f.pdf",
        "author": "Weijie Ma;Jingwei Jiang;Yang Yang;Zehui Chen;Hao Chen",
        "authorids": "~Weijie_Ma1;~Jingwei_Jiang1;~Yang_Yang41;~Zehui_Chen1;~Hao_Chen17",
        "gender": "M;;M;M;",
        "homepage": "https://weijiemax.github.io;;https://github.com/Young98CN;https://lovesnowbest.site;",
        "dblp": "258/3324;;;;",
        "google_scholar": "https://scholar.google.com.hk/citations?user=LGhmrxMAAAAJ;;Fq7jXZ4AAAAJ;NfSsLncAAAAJ;",
        "orcid": ";;;0000-0002-1843-4478;",
        "linkedin": ";;;;",
        "or_profile": "~Weijie_Ma1;~Jingwei_Jiang1;~Yang_Yang41;~Zehui_Chen1;~Hao_Chen17",
        "aff": "Fudan University+Shanghai AI Laboratory;;Zhejiang University;University of Science and Technology of China;",
        "aff_domain": "fudan.edu.cn+pjlab.org.cn;;zju.edu.cn;ustc.edu.cn;",
        "position": "PhD student+Intern;;PhD student;PhD student;",
        "bibtex": "@inproceedings{\nma2025lssinst,\ntitle={{LSSI}nst: Improving Geometric Modeling in {LSS}-Based {BEV} Perception with Instance Representation},\nauthor={Weijie Ma and Jingwei Jiang and Yang Yang and Zehui Chen and Hao Chen},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=MaN2x3O2Rk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MaN2x3O2Rk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NB0vudVCZH",
        "title": "Towards Foundation Models for 3D Vision: How Close Are We?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Building a foundation model for 3D vision is a complex challenge that remains unsolved. Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans. Therefore, we construct a new 3D visual understanding benchmark named UniQA-3D. UniQA-3D covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format. We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it. Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations. In contrast, human vision continues to be the most reliable 3D visual system. We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs. We hope our study will benefit the future development of foundation models for 3D vision. Code is available at https://github.com/princeton-vl/UniQA-3D.",
        "keywords": "foundation model;benchmark;dataset;human subject research",
        "primary_area": "",
        "supplementary_material": "/attachment/1aa9be57e987fbe38b5a08d515a4af970e33acf3.pdf",
        "author": "Yiming Zuo;Karhan Kayan;Maggie Wang;Kevin Jeon;Jia Deng;Thomas L. Griffiths",
        "authorids": "~Yiming_Zuo2;~Karhan_Kayan1;~Maggie_Wang2;~Kevin_Jeon1;~Jia_Deng1;~Thomas_L._Griffiths1",
        "gender": "M;;F;;M;",
        "homepage": "https://zuoym15.github.io/;;;;;http://cocosci.princeton.edu/tom/",
        "dblp": "146/3853-1;;;;07/6526-1.html;34/4472",
        "google_scholar": ";;https://scholar.google.com/citations?view_op=list_works;https://scholar.google.com/citations?hl=en;U3Eub-EAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;",
        "linkedin": ";;maggiewang0785?trk=contact-info;kevin-jeon/;;",
        "or_profile": "~Yiming_Zuo2;~Karhan_Kayan1;~Maggie_Wang2;~Kevin_Jeon1;~Jia_Deng1;~Thomas_L._Griffiths1",
        "aff": "Princeton University;;Department of Computer Science, Princeton University;Princeton University;Princeton University;Princeton University",
        "aff_domain": "princeton.edu;;cs.princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "position": "PhD student;;Undergrad student;Undergrad student;Full Professor;Professor",
        "bibtex": "@inproceedings{\nzuo2025towards,\ntitle={Towards Foundation Models for 3D Vision: How Close Are We?},\nauthor={Yiming Zuo and Karhan Kayan and Maggie Wang and Kevin Jeon and Jia Deng and Thomas L. Griffiths},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NB0vudVCZH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NB0vudVCZH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NDIwtzzXAH",
        "title": "Dream-in-Style: Text-to-3D Generation using Stylized Score Distillation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a method to generate 3D objects in styles. Our method takes a text prompt and a style reference image as input and reconstructs a neural radiance field to synthesize a 3D model with the content aligning with the text prompt and the style following the reference image. \nTo simultaneously generate the 3D object and perform style transfer in one go, we propose a stylized score distillation loss to guide a text-to-3D optimization process to output visually plausible geometry and appearance. \nOur stylized score distillation is based on a combination of an original pretrained text-to-image model and its modified sibling with the key and value features of self-attention layers manipulated to inject styles from the reference image. \nComparisons with state-of-the-art methods demonstrated the strong visual performance of our method, further supported by the quantitative results from our user study.",
        "keywords": "3D generation;text-to-3D",
        "primary_area": "",
        "supplementary_material": "/attachment/87e8b73b2fe5475a5b60ffc14c79dcc6bd6b3bfe.zip",
        "author": "Hubert Kompanowski;Binh-Son Hua",
        "authorids": "~Hubert_Kompanowski1;~Binh-Son_Hua1",
        "gender": "M;M",
        "homepage": "https://hubert-kompanowski.github.io/;https://sonhua.github.io",
        "dblp": ";44/8499",
        "google_scholar": ";sV_VjsAAAAAJ",
        "orcid": "0000-0002-4614-5622;0000-0002-5706-8634",
        "linkedin": "hubert-kompanowski/;binh-son-hua-40895b14/",
        "or_profile": "~Hubert_Kompanowski1;~Binh-Son_Hua1",
        "aff": "University of Dublin, Trinity College;University of Dublin, Trinity College",
        "aff_domain": "tcd.ie;tcd.ie",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nkompanowski2025dreaminstyle,\ntitle={Dream-in-Style: Text-to-3D Generation using Stylized Score Distillation},\nauthor={Hubert Kompanowski and Binh-Son Hua},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NDIwtzzXAH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NDIwtzzXAH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NMZSeQI5dd",
        "title": "INRet: A General Framework for Accurate Retrieval of INRs for Shapes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Implicit neural representations (INRs) have become an important method for encoding various data types, such as 3D objects or scenes, images, and videos. They have proven to be particularly effective at representing 3D content, e.g., 3D scene reconstruction from 2D images, novel 3D content creation, as well as the representation, interpolation, and completion of 3D shapes. With the widespread generation of 3D data in an INR format, there is a need to support effective organization and retrieval of INRs saved in a data store. A key aspect of retrieval and clustering of INRs in a data store is the formulation of similarity between INRs that would, for example, enable retrieval of similar INRs using a query INR. In this work, we propose INRet, a method for determining similarity between INRs that represent shapes, thus enabling accurate retrieval of similar shape INRs from an INR data store. INRet flexibly supports different INR architectures such as INRs with octree grids, triplanes, and hash grids, as well as different implicit functions including signed/unsigned distance function and occupancy field. We demonstrate that our method is more general and accurate than the existing INR retrieval method, which only supports simple MLP INRs and requires the same architecture between the query and stored INRs. Furthermore, compared to converting INRs to other representations (e.g., point clouds or multi-view images) for 3D shape retrieval, INRet achieves higher accuracy while avoiding the conversion overhead.",
        "keywords": "Implicit Neural Representation; Shape Retrieval; Shape Database",
        "primary_area": "",
        "supplementary_material": "/attachment/279ffe086c36955cfc2443af16cefe42b7cfc2b6.pdf",
        "author": "Yushi Guan;Daniel Kwan;Ruofan Liang;Selvakumar Panneer;Nilesh Jain;Nilesh Ahuja;Nandita Vijaykumar",
        "authorids": "~Yushi_Guan1;~Daniel_Kwan1;~Ruofan_Liang1;~Selvakumar_Panneer1;~Nilesh_Jain1;~Nilesh_Ahuja1;~Nandita_Vijaykumar1",
        "gender": "M;M;M;M;;;F",
        "homepage": "https://gavinguan95.github.io/;;https://nexuslrf.github.io/;;;;http://www.cs.toronto.edu/~nandita/",
        "dblp": "280/0249.html;398/4473;246/4635;211/9289;134/6343.html;66/1132;163/0027",
        "google_scholar": "https://scholar.google.ca/citations?user=SjjJ94kAAAAJ;;;rcZzmS0AAAAJ;sWUGELEAAAAJ;y9njJHAAAAAJ;",
        "orcid": "0009-0009-8701-0992;;;;;;",
        "linkedin": ";daniel-kwan-525ab323a;;selvakumarpanneer/;nilesh-jain-4693532/;;",
        "or_profile": "~Yushi_Guan1;~Daniel_Kwan1;~Ruofan_Liang1;~Selvakumar_Panneer1;~Nilesh_Jain1;~Nilesh_Ahuja1;~Nandita_Vijaykumar1",
        "aff": "University of Toronto;University of Toronto;NVIDIA+University of Toronto;Intel;Intel Corp;Intel;University of Toronto",
        "aff_domain": "cs.toronto.edu;utoronto.ca;nvidia.com+toronto.edu;intel.com;intel.com;intel.com;cs.toronto.edu",
        "position": "PhD student;Intern;Intern+PhD student;Principal Researcher;Principal Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nguan2025inret,\ntitle={{INR}et: A General Framework for Accurate Retrieval of {INR}s for Shapes},\nauthor={Yushi Guan and Daniel Kwan and Ruofan Liang and Selvakumar Panneer and Nilesh Jain and Nilesh Ahuja and Nandita Vijaykumar},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NMZSeQI5dd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NMZSeQI5dd",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NXEwjCZKdf",
        "title": "TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Test-Time Training (TTT) proposes to adapt a pre-trained network to changing data distributions on-the-fly. In this work, we propose the first TTT method for 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD) from foundation models (e.g. DINOv2) as a self-supervised objective for adaptation to distribution shifts at test-time. Given access to paired image-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for the main task of semantic segmentation using the pointclouds and the task of 2D \u2192 3D KD by using an off-the-shelf 2D pre-trained foundation model. At test-time, our TTT-KD updates the 3D segmentation backbone for each test sample, by using the self-supervised task of knowledge distillation, before performing the final prediction. Extensive evaluations on multiple indoor and outdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves performance for both in-distribution (ID) and out-of-distribution (OOD) test datasets. We achieve a gain of up to 13 % mIoU (7 % on average) when the train and test distributions are similar and up to 45 % (20 % on average) when adapting to OOD test samples",
        "keywords": "Point clouds;semantic segmentation;test-time training;domain adaptation",
        "primary_area": "",
        "supplementary_material": "/attachment/89433fe3cb61186b7b0d989cb0fd37e5379b6e33.pdf",
        "author": "Lisa Weijler;Muhammad Jehanzeb Mirza;Leon Sick;Can Ekkazan;Pedro Hermosilla",
        "authorids": "~Lisa_Weijler1;~Muhammad_Jehanzeb_Mirza1;~Leon_Sick1;~Can_Ekkazan1;~Pedro_Hermosilla1",
        "gender": "F;M;M;M;M",
        "homepage": ";;https://leonsick.github.io;;https://phermosilla.github.io/",
        "dblp": "291/8727;295/9034;356/2434;;170/7065",
        "google_scholar": ";cES2rkAAAAAJ;https://scholar.google.de/citations?user=KL3_OLwAAAAJ;;C7F4B6MAAAAJ",
        "orcid": "0000-0003-1660-0329;;;0009-0002-3853-0480;",
        "linkedin": "lweijler/;;leon-sick-632293108/;canekkazan/;",
        "or_profile": "~Lisa_Weijler1;~Muhammad_Jehanzeb_Mirza1;~Leon_Sick1;~Can_Ekkazan1;~Pedro_Hermosilla1",
        "aff": "Technische Universit\u00e4t Wien;Massachusetts Institute of Technology;Universit\u00e4t Ulm;University of Cambridge+Technische Universit\u00e4t Wien+Yildiz Technical University;Technische Universit\u00e4t Wien",
        "aff_domain": "tuwien.ac.at;mit.edu;uni-ulm.de;cam.ac.uk+tuwien.ac.at+yildiz.edu.tr;tuwien.ac.at",
        "position": "PhD student;Postdoc;PhD student;Intern+Intern+MS student;Assistant Professor",
        "bibtex": "@inproceedings{\nweijler2025tttkd,\ntitle={{TTT}-{KD}: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models},\nauthor={Lisa Weijler and Muhammad Jehanzeb Mirza and Leon Sick and Can Ekkazan and Pedro Hermosilla},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NXEwjCZKdf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NXEwjCZKdf",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NXS1xZLUFr",
        "title": "Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To evaluate end-to-end autonomous driving systems, a simulation environment based on Novel View Synthesis (NVS) techniques is essential, which synthesizes photo-realistic images and point clouds from previously recorded sequences under new vehicle poses, particularly in cross-lane scenarios. Therefore, the development of a multi-lane dataset and benchmark is necessary. While recent synthetic scene-based NVS datasets have been prepared for cross-lane benchmarking, they still lack the realism of captured images and point clouds. To further assess the performance of existing methods based on NeRF and 3DGS, we present the first multi-lane dataset registering parallel scans specifically for novel driving view synthesis dataset derived from real-world scans, comprising 25 groups of associated sequences, including 16,000 front-view images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are labeled to differentiate moving objects from static elements. Using this dataset, we evaluate the performance of existing approaches in various testing scenarios at different lanes and distances. Additionally, our method provides the solution for solving and assessing the quality of multi-sensor poses for multi-modal data alignment for curating such a dataset in real-world. We plan to continually add new sequences to test the generalization of existing methods across different scenarios. The dataset is released publicly at the project page: https://nizqleo.github.io/paralane-dataset/.",
        "keywords": "Dataset;Novel View Synthesis;SLAM;Structure from Motion;3D Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/604ef8728c71a40c1c4fabfc4536c9464ac89057.zip",
        "author": "Ziqian Ni;Sicong Du;Zhenghua Hou;Chenming Wu;Sheng Yang",
        "authorids": "~Ziqian_Ni1;~Sicong_Du1;~Zhenghua_Hou1;~Chenming_Wu1;~Sheng_Yang4",
        "gender": "M;M;;M;M",
        "homepage": "https://github.com/nizqleo;;;https://chenming-wu.github.io/;https://shengyang93.github.io/",
        "dblp": ";;;190/5879;69/4104-7",
        "google_scholar": ";;;https://scholar.google.com.hk/citations?user=eOkkQWUAAAAJ;G6IztksAAAAJ",
        "orcid": "0009-0004-2233-8106;0000-0001-7942-5068;;0000-0001-8012-1547;",
        "linkedin": ";;;;shengyang93/",
        "or_profile": "~Ziqian_Ni1;~Sicong_Du1;~Zhenghua_Hou1;~Chenming_Wu1;~Sheng_Yang4",
        "aff": "Alibaba Group;Alibaba Group;;Baidu;CaiNiao Inc., Alibaba Group",
        "aff_domain": "alibaba-inc.com;cainiao.com;;baidu.com;cainiao.com",
        "position": "Researcher;Researcher;;Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nni2025paralane,\ntitle={Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis},\nauthor={Ziqian Ni and Sicong Du and Zhenghua Hou and Chenming Wu and Sheng Yang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NXS1xZLUFr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NXS1xZLUFr",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "NaTnWehi7O",
        "title": "3D Reconstruction with Spatial Memory",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress a global pointmap from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict a per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and is able to process ordered image collections in real-time.",
        "keywords": "3D reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/45c62c625250892ad6ee3a9fa8efab2ab6e08e1b.pdf",
        "author": "Hengyi Wang;Lourdes Agapito",
        "authorids": "~Hengyi_Wang2;~Lourdes_Agapito1",
        "gender": "M;",
        "homepage": "https://hengyiwang.github.io/;",
        "dblp": ";",
        "google_scholar": "2d9j2_wAAAAJ;",
        "orcid": ";",
        "linkedin": "hengyi-wang-a335b11bb/;",
        "or_profile": "~Hengyi_Wang2;~Lourdes_Agapito1",
        "aff": "University College London;",
        "aff_domain": "ucl.ac.uk;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nwang2025d,\ntitle={3D Reconstruction with Spatial Memory},\nauthor={Hengyi Wang and Lourdes Agapito},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=NaTnWehi7O}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NaTnWehi7O",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "O9GMl5UJbe",
        "title": "SparseGS: Sparse View Synthesis using 3D Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "3D Gaussian Splatting (3DGS) has recently enabled real-time rendering of unbounded 3D scenes for novel view synthesis. However, this technique requires dense training views to accurately reconstruct 3D geometry. A limited number of input views will significantly degrade reconstruction quality, resulting in artifacts such as \"floaters'' and \"background collapse'' at unseen viewpoints.  In this work, we introduce SparseGS, an efficient training pipeline designed to address the limitations of 3DGS in scenarios with sparse training views. SparseGS incorporates depth priors, novel depth rendering techniques, and a pruning heuristic to mitigate floater artifacts, alongside an Unseen Viewpoint Regularization module to alleviate background collapses. Our extensive evaluations on the Mip-NeRF360, LLFF, and DTU datasets demonstrate that SparseGS achieves high-quality reconstruction in both unbounded and forward-facing scenarios, with as few as 12 and 3 input images, respectively, while maintaining fast training and real-time rendering capabilities.",
        "keywords": "3DGS;NeRF;Scene Reconstruction;3D Computer Vision;Sparse-view Reconstruction;Generative AI",
        "primary_area": "",
        "supplementary_material": "/attachment/43547ce915e6a70c3b6fda05b7be2a4abcc1f082.pdf",
        "author": "Haolin Xiong;Sairisheek Muttukuru;Hanyuan Xiao;Rishi Upadhyay;Pradyumna Chari;Yajie Zhao;Achuta Kadambi",
        "authorids": "~Haolin_Xiong1;~Sairisheek_Muttukuru1;~Hanyuan_Xiao2;~Rishi_Upadhyay1;~Pradyumna_Chari1;~Yajie_Zhao1;~Achuta_Kadambi2",
        "gender": "M;M;M;;M;F;M",
        "homepage": ";;https://corneliushsiao.github.io;;https://pradyumnachari.github.io/;https://www.yajie-zhao.com/;http://visual.ee.ucla.edu",
        "dblp": ";;344/5968;362/6039;254/2711;54/7467;",
        "google_scholar": ";;bKdDoYMAAAAJ;j5tGBdAAAAAJ;GE0ylLMAAAAJ;;",
        "orcid": ";;;0009-0004-7472-3508;;;",
        "linkedin": "haolin-xiong-927221176;sairisheek;;;;;achuta-kadambi/",
        "or_profile": "~Haolin_Xiong1;~Sairisheek_Muttukuru1;~Hanyuan_Xiao2;~Rishi_Upadhyay1;~Pradyumna_Chari1;~Yajie_Zhao1;~Achuta_Kadambi2",
        "aff": ";;University of Southern California+USC Institute for Creative Technologies, University of Southern California;University of California, Los Angeles;Massachusetts Institute of Technology;University of Southern California+USC Institute for Creative Technologies, University of Southern California;University of California, Los Angeles",
        "aff_domain": ";;usc.edu+ict.usc.edu;ucla.edu;mit.edu;usc.edu+ict.usc.edu;ucla.edu",
        "position": ";;PhD student+Researcher;PhD student;Postdoc;Assistant Professor+Director;Assistant Professor",
        "bibtex": "@inproceedings{\nxiong2025sparsegs,\ntitle={Sparse{GS}: Sparse View Synthesis using 3D Gaussian Splatting},\nauthor={Haolin Xiong and Sairisheek Muttukuru and Hanyuan Xiao and Rishi Upadhyay and Pradyumna Chari and Yajie Zhao and Achuta Kadambi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=O9GMl5UJbe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=O9GMl5UJbe",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ODBsDFjMr8",
        "title": "Spurfies: Sparse-View Surface Reconstruction using Local Geometry Priors",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We introduce Spurfies, a novel method for sparse-view surface reconstruction that disentangles appearance and geometry information to utilize local geometry priors trained on synthetic data. Recent research heavily focuses on 3D reconstruction using dense multi-view setups, typically requiring hundreds of images. However, these methods often struggle with few-view scenarios. Existing sparse-view reconstruction techniques often rely on multi-view stereo networks that need to learn joint priors for geometry and appearance from a large amount of data. In contrast, we introduce a neural point representation that disentangles geometry and appearance to train a local geometry prior using a subset of the synthetic ShapeNet dataset only. During inference, we utilize this surface prior as additional constraint for surface and appearance reconstruction from sparse input views via differentiable volume rendering, restricting the space of possible solutions. We validate the effectiveness of our method on the DTU dataset and demonstrate that it outperforms previous work by 35\\% in surface quality while achieving competitive novel view synthesis quality. Moreover, in contrast to previous works, our method can be applied to larger, unbounded scenes, such as Mip-NeRF360.",
        "keywords": "sparse view reconstruction;volume rendering;implicit surfaces;point clouds",
        "primary_area": "",
        "supplementary_material": "/attachment/ae89b86d3c15db5e5fa9449212eb13aad5e8bba3.pdf",
        "author": "Kevin Raj;Christopher Wewer;Raza Yunus;Eddy Ilg;Jan Eric Lenssen",
        "authorids": "~Kevin_Raj1;~Christopher_Wewer1;~Raza_Yunus1;~Eddy_Ilg3;~Jan_Eric_Lenssen1",
        "gender": "M;M;M;M;M",
        "homepage": "https://kevinyitshak.github.io/;;;https://www.utn.de/departments/department-engineering/cvmp-lab/;https://janericlenssen.github.io/",
        "dblp": "243/6633;302/3666;233/2806;151/9307;195/9868",
        "google_scholar": "GBEtldQAAAAJ;y6VGjBIAAAAJ;;MYvSvGsAAAAJ;https://scholar.google.de/citations?user=enXCzCgAAAAJ",
        "orcid": "0009-0007-3271-5990;;0000-0002-3979-9038;;0000-0003-4093-9840",
        "linkedin": ";;;eddy-ilg/;jan-eric-lenssen-08700b190/",
        "or_profile": "~Kevin_Raj1;~Christopher_Wewer1;~Raza_Yunus1;~Eddy_Ilg3;~Jan_Eric_Lenssen1",
        "aff": ";Saarland Informatics Campus, Max-Planck Institute;University of Technology Nuremberg+Saarland Informatics Campus, Max-Planck Institute;University of Technology Nuremberg;Saarland Informatics Campus, Max-Planck Institute+Kumo",
        "aff_domain": ";mpi-inf.mpg.de;utn.de+mpi-inf.mpg.de;utn.de;mpi-inf.mpg.de+kumo.ai",
        "position": ";PhD student;PhD student+PhD student;Full Professor;Principal Researcher+Researcher",
        "bibtex": "@inproceedings{\nraj2025spurfies,\ntitle={Spurfies: Sparse-View Surface Reconstruction using Local Geometry Priors},\nauthor={Kevin Raj and Christopher Wewer and Raza Yunus and Eddy Ilg and Jan Eric Lenssen},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ODBsDFjMr8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ODBsDFjMr8",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OE1lOjnliK",
        "title": "Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "3D Gaussian Splatting (3DGS) has attracted great attention in novel view synthesis because of its superior rendering efficiency and high fidelity. However, the trained Gaussians suffer from severe zooming degradation due to non-adjustable representation derived from single-scale training. Though some methods attempt to tackle this problem via post-processing techniques such as selective rendering or filtering techniques towards primitives, the scale-specific information is not involved in Gaussians. In this paper, we propose a unified optimization method to make Gaussians adaptive for arbitrary scales by self-adjusting the primitive properties (e.g., color, shape and size) and distribution (e.g., position). Inspired by the mipmap technique, we design pseudo ground-truth for the target scale and propose a scale-consistency guidance loss to inject scale information into 3D Gaussians. Our method is a plug-in module, applicable for any 3DGS models to solve the zoom-in and zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.",
        "keywords": "3D Gaussian Splatting;Self-supervised Optimization;Anti-aliasing Rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/c1a0d15b53a5478516e37b8d5689d273875f46bd.zip",
        "author": "Jiameng Li;Yue Shi;Jiezhang Cao;Bingbing Ni;Wenjun Zhang;Kai Zhang;Luc Van Gool",
        "authorids": "~Jiameng_Li2;~Yue_Shi4;~Jiezhang_Cao2;~Bingbing_Ni3;~Wenjun_Zhang3;~Kai_Zhang8;~Luc_Van_Gool1",
        "gender": "F;;;M;M;M;",
        "homepage": "https://renaissanceee.github.io/;;;;https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66;https://github.com/cszn;",
        "dblp": ";;;64/831.html;;55/957-8;61/5017",
        "google_scholar": ";;;V9W87PYAAAAJ;;0RycFIIAAAAJ;https://scholar.google.be/citations?user=TwMib_QAAAAJ",
        "orcid": ";;;;;0000-0002-6319-3722;",
        "linkedin": ";;;;;;",
        "or_profile": "~Jiameng_Li2;~Yue_Shi4;~Jiezhang_Cao2;~Bingbing_Ni3;~Wenjun_Zhang3;~Kai_Zhang8;~Luc_Van_Gool1",
        "aff": "KU Leuven;;;Shanghai Jiaotong University;Shanghai Jiaotong University;Nanjing University;Sofia Un. St. Kliment Ohridski",
        "aff_domain": "kuleuven.be;;;sjtu.edu.cn;sjtu.edu.cn;nju.edu.cn;insait.ai",
        "position": "PhD student;;;Full Professor;Full Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nli2025mipmapgs,\ntitle={Mipmap-{GS}: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering},\nauthor={Jiameng Li and Yue Shi and Jiezhang Cao and Bingbing Ni and Wenjun Zhang and Kai Zhang and Luc Van Gool},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=OE1lOjnliK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OE1lOjnliK",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OS63nLsQ47",
        "title": "VXP: Voxel-Cross-Pixel Large-Scale Camera-LiDAR Place Recognition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Cross-modal place recognition methods are flexible GPS-alternatives under varying environment conditions and sensor setups. However, this task is non-trivial since extracting consistent and robust global descriptors from different modalities is challenging. To tackle this issue, we propose \\textit{Voxel-Cross-Pixel (VXP)}, a novel camera-to-LiDAR place recognition framework that enforces local similarities in a self-supervised manner and effectively brings global context from images and LiDAR scans into a shared feature space. Specifically, VXP is trained in three stages: first, we deploy a visual transformer to compactly represent input images. Secondly, we establish local correspondences between image-based and point cloud-based feature spaces using our novel geometric alignment module. We then aggregate local similarities into an expressive shared latent space. Extensive experiments on the three benchmarks (Oxford RobotCar, ViViD++ and KITTI) demonstrate that our method surpasses the state-of-the-art cross-modal retrieval by a large margin. Our evaluations show that the proposed method is accurate, efficient and light-weight. Our project page is available at: \\href{https://yunjinli.github.io/projects-vxp/}{https://yunjinli.github.io/projects-vxp/}.",
        "keywords": "place recognition;LiDAR;cross-modality;vision",
        "primary_area": "",
        "supplementary_material": "/attachment/19fe1652a51c6af4ce32eedaf1b74e171b8e3684.pdf",
        "author": "Yun-Jin Li;Mariia Gladkova;Yan Xia;Rui Wang;Daniel Cremers",
        "authorids": "~Yun-Jin_Li1;~Mariia_Gladkova1;~Yan_Xia5;~Rui_Wang12;~Daniel_Cremers1",
        "gender": "M;F;M;M;M",
        "homepage": "https://yunjinli.github.io/;;https://yan-xia.github.io/;https://rui2016.github.io/;https://vision.in.tum.de/members/cremers",
        "dblp": "372/5539;284/8530;17/6518-3;06/2293-37;c/DanielCremers",
        "google_scholar": "JFgQcQcAAAAJ;;xkBn4mMAAAAJ;https://scholar.google.de/citations?user=buN3yw8AAAAJ;cXQciMEAAAAJ",
        "orcid": ";;;0000-0002-2252-9955;",
        "linkedin": "jim-li-9072ba212/;mgladkova;;rui-wang-5367398a/;",
        "or_profile": "~Yun-Jin_Li1;~Mariia_Gladkova1;~Yan_Xia5;~Rui_Wang12;~Daniel_Cremers1",
        "aff": "Technische Universit\u00e4t M\u00fcnchen;Department of Informatics, Technische Universit\u00e4t M\u00fcnchen;University of Science and Technology of China+Technische Universit\u00e4t M\u00fcnchen;Microsoft;Technical University Munich",
        "aff_domain": "tum.de;in.tum.de;ustc.edu.cn+tum.de;microsoft.com;tum.de",
        "position": "MS student;PhD student;Associate Professor+Researcher;Senior Research Scientist;Full Professor",
        "bibtex": "@inproceedings{\nli2025vxp,\ntitle={{VXP}: Voxel-Cross-Pixel Large-Scale Camera-Li{DAR} Place Recognition},\nauthor={Yun-Jin Li and Mariia Gladkova and Yan Xia and Rui Wang and Daniel Cremers},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=OS63nLsQ47}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OS63nLsQ47",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "OdgsXF2hpe",
        "title": "Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present Rig3DGS, a novel technique for creating reanimatable 3D portraits from short monocular smartphone videos. Rig3DGS learns to reconstruct a set of controllable 3D gaussians from a monocular video of a dynamic subject captured with varying head poses and facial expressions in an in-the-wild scene. In contrast to synchronized multi-view studio captures, this in-the-wild, single camera setup brings fresh challenges to learning high quality 3D gaussians. We address these challenges by learning to deform 3D gaussians from a fixed canonical space to the deformed space that is consistent with the target facial expression and head-pose. Our key contribution is a carefully designed deformation model that is guided by a 3D face morphable model.  This deformation not only enables control over facial expression and head-poses but also allows our method to generates high quality photorealistic renders of the whole scene. Once trained, Rig3DGS is able to generate photorealistic renders of a subject and their scene for novel facial expression, head-poses, and viewing directions.  Through extensive experiments we demonstrate that Rig3DGS significantly outperforms prior art while being orders of magnitude faster.",
        "keywords": "Neural Rendering; Neural Portraits;",
        "primary_area": "",
        "supplementary_material": "/attachment/8b5fdd4711756b858f8ed2c52fb8a063f7fe1516.zip",
        "author": "Alfredo Rivero;ShahRukh Athar;Zhixin Shu;Dimitris Samaras",
        "authorids": "~Alfredo_Rivero1;~ShahRukh_Athar1;~Zhixin_Shu1;~Dimitris_Samaras3",
        "gender": "M;;M;M",
        "homepage": "https://github.com/alrivero;http://shahrukhathar.github.io/;https://zhixinshu.github.io/;https://www.cs.stonybrook.edu/~samaras/",
        "dblp": ";79/9032;129/3987;s/DimitrisSamaras",
        "google_scholar": ";mdUv8wcAAAAJ;gp6HUP0AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0002-1373-0294",
        "linkedin": ";;;",
        "or_profile": "~Alfredo_Rivero1;~ShahRukh_Athar1;~Zhixin_Shu1;~Dimitris_Samaras3",
        "aff": "State University of New York at Stony Brook;State University of New York, Stony Brook;Adobe Systems;Stony Brook University",
        "aff_domain": "stonybrook.edu;stonybrook.edu;adobe.com;cs.stonybrook.edu",
        "position": "PhD student;PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nrivero2025rigdgs,\ntitle={Rig3{DGS}: Creating Controllable Portraits from Casual Monocular Videos},\nauthor={Alfredo Rivero and ShahRukh Athar and Zhixin Shu and Dimitris Samaras},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=OdgsXF2hpe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OdgsXF2hpe",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PWWSGXhuov",
        "title": "GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Variational Score Distillation (VSD) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives.",
        "keywords": "3D generation;garment synthesis",
        "primary_area": "",
        "supplementary_material": "/attachment/f9352660abf53e4c348772b25101fbffb6268b01.zip",
        "author": "Boqian Li;Xuan Li;Ying Jiang;Tianyi Xie;Feng Gao;Huamin Wang;Yin Yang;Chenfanfu Jiang",
        "authorids": "~Boqian_Li1;~Xuan_Li8;~Ying_Jiang2;~Tianyi_Xie1;~Feng_Gao2;~Huamin_Wang2;~Yin_Yang4;~Chenfanfu_Jiang3",
        "gender": "M;M;F;M;M;;M;",
        "homepage": "https://boqian-li.github.io/;https://xuan-li.github.io/;https://yingjiang96.github.io/;https://xpandora.github.io/;https://fen9.github.io/;;https://yangzzzy.github.io/;",
        "dblp": "288/9833;;;161/7169;10/2674-13;;56/2998-2;132/7630",
        "google_scholar": "7I8FkuoAAAAJ;;;oRt4qcgAAAAJ;amaLnocAAAAJ;;-z2_nggAAAAJ;",
        "orcid": "0009-0002-3047-4357;0000-0003-0677-8369;;0009-0006-3101-7659;0000-0003-1515-1357;;0000-0001-7645-5931;",
        "linkedin": "boqian-li-3b8528282/;;;;;;;",
        "or_profile": "~Boqian_Li1;~Xuan_Li8;~Ying_Jiang2;~Tianyi_Xie1;~Feng_Gao2;~Huamin_Wang2;~Yin_Yang4;~Chenfanfu_Jiang3",
        "aff": "Huazhong University of Science and Technology;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;Amazon;;University of Utah;University of California, Los Angeles",
        "aff_domain": "hust.edu.cn;ucla.edu;ucla.edu;ucla.edu;amazon.com;;utah.edu;ucla.edu",
        "position": "Undergrad student;PhD student;Postdoc;PhD student;Researcher;;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nli2025garmentdreamer,\ntitle={GarmentDreamer: 3{DGS} Guided Garment Synthesis with Diverse Geometry and Texture Details},\nauthor={Boqian Li and Xuan Li and Ying Jiang and Tianyi Xie and Feng Gao and Huamin Wang and Yin Yang and Chenfanfu Jiang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=PWWSGXhuov}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PWWSGXhuov",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "PyCoMnd5P4",
        "title": "360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel layout-guided $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 3D Gaussians by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.",
        "keywords": "novel view synthesis;radiance fields;3D gaussians;real-time rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/af9264ee3e1486cbc4921fb53ac00aa81bb76dea.zip",
        "author": "Jiayang Bai;Letian Huang;Jie Guo;Wen Gong;Yuanqi Li;Yanwen Guo",
        "authorids": "~Jiayang_Bai1;~Letian_Huang1;~Jie_Guo2;~Wen_Gong2;~Yuanqi_Li1;~Yanwen_Guo1",
        "gender": "M;M;M;;M;M",
        "homepage": "https://github.com/LeoDarcy;https://letianhuang.github.io/;;https://github.com/Lesliewinnie;http://www.njumeta.com/liyq/;https://cs.nju.edu.cn/ywguo/",
        "dblp": "249/0968;147/7672;77/2751-1;;228/9753;44/185-1",
        "google_scholar": ";ZFrewMgAAAAJ;https://scholar.google.com.hk/citations?user=Sx4PQpQAAAAJ;;;hVlfEkwAAAAJ",
        "orcid": ";0009-0003-1454-7824;0000-0002-4176-7617;;0000-0003-4100-7471;",
        "linkedin": ";;;;;",
        "or_profile": "~Jiayang_Bai1;~Letian_Huang1;~Jie_Guo2;~Wen_Gong2;~Yuanqi_Li1;~Yanwen_Guo1",
        "aff": "Nanjing University;Nanjing University;Nanjing university;;Nanjing University;Nanjing University",
        "aff_domain": "nju.edu.cn;nju.edu.cn;nju.edu.cn;;nju.edu.cn;nju.edu.cn",
        "position": "PhD student;MS student;Associate Professor;;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nbai2025gs,\ntitle={360-{GS}: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming},\nauthor={Jiayang Bai and Letian Huang and Jie Guo and Wen Gong and Yuanqi Li and Yanwen Guo},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=PyCoMnd5P4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PyCoMnd5P4",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "QI6HrBseVF",
        "title": "FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce a differentiable re-parameterization of depth, intrinsics, and pose that is amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360\u00b0 trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360\u00b0 novel view synthesis - even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM. Our result opens the door to the self-supervised training of neural networks that perform camera parameter estimation, 3D reconstruction, and novel view synthesis.",
        "keywords": "structure from motion;self-supervised 3D representation learning;gaussian splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/ba03bb597a72db8ee36844fbf0817fa3a8e53ded.zip",
        "author": "Cameron Omid Smith;David Charatan;Ayush Tewari;Vincent Sitzmann",
        "authorids": "~Cameron_Omid_Smith1;~David_Charatan1;~Ayush_Tewari2;~Vincent_Sitzmann1",
        "gender": ";M;;M",
        "homepage": ";https://davidcharatan.com/;https://ayushtewari.com;https://vsitzmann.github.io",
        "dblp": ";;198/1021;192/1958",
        "google_scholar": ";;pDnzpeoAAAAJ;X44QVV4AAAAJ",
        "orcid": ";;;0000-0002-0107-5704",
        "linkedin": ";;;vincentsitzmann/",
        "or_profile": "~Cameron_Omid_Smith1;~David_Charatan1;~Ayush_Tewari2;~Vincent_Sitzmann1",
        "aff": ";Massachusetts Institute of Technology;University of Cambridge;Yellow Technologies+Massachusetts Institute of Technology",
        "aff_domain": ";mit.edu;cam.ac.uk;yellow3d.com+mit.edu",
        "position": ";PhD student;Assistant Professor;Chief Scientist+Assistant Professor",
        "bibtex": "@inproceedings{\nsmith2025flowmap,\ntitle={FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent},\nauthor={Cameron Omid Smith and David Charatan and Ayush Tewari and Vincent Sitzmann},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=QI6HrBseVF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QI6HrBseVF",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RHZkF3DtDu",
        "title": "Object Agnostic 3D Lifting in Space and Time",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a spatio-temporal perspective on category-agnostic 3D lifting of 2D keypoints over a temporal sequence. Our approach differs from existing state-of-the-art methods that are either: (i) object-agnostic, but can only operate on individual frames, or (ii) can model space-time dependencies, but are only designed to work with a single object category. Our approach is grounded in two core principles. First, general information about similar objects can be leveraged to achieve better performance when there is little object-specific training data. Second, a temporally-proximate context window is advantageous for achieving consistency throughout a sequence. These two principles allow us to outperform current state-of-the-art methods on per-frame and per-sequence metrics for a variety of animal categories. Lastly, we release a new synthetic dataset containing 3D skeletons and motion sequences for a variety of animal categories.",
        "keywords": "lifting;2d;3d;transformer;geometry;dataset;time;space;animal;synthetic",
        "primary_area": "",
        "supplementary_material": "/attachment/83b09eef684c1996d489e2cf47704d2eba38457a.zip",
        "author": "Christopher Fusco;Simon Lucey;Shin-Fang Chng;Mosam Dabhi",
        "authorids": "~Christopher_Fusco1;~Simon_Lucey2;~Shin-Fang_Chng1;~Mosam_Dabhi1",
        "gender": "M;M;F;M",
        "homepage": "https://www.christopherfusco.com;https://www.adelaide.edu.au/directory/simon.lucey;https://sfchng.github.io;https://mosamdabhi.github.io/",
        "dblp": ";01/3542;249/5593;241/9794",
        "google_scholar": ";vmAe35UAAAAJ;;b14VbHEAAAAJ",
        "orcid": ";;;0000-0001-5822-3838",
        "linkedin": ";;;mosam-dabhi-9395b09a/",
        "or_profile": "~Christopher_Fusco1;~Simon_Lucey2;~Shin-Fang_Chng1;~Mosam_Dabhi1",
        "aff": ";University of Adelaide;University of Adelaide;Carnegie Mellon University",
        "aff_domain": ";adelaide.edu.au;adelaide.edu.au;cmu.edu",
        "position": ";Full Professor;Postdoc;PhD student",
        "bibtex": "@inproceedings{\nfusco2025object,\ntitle={Object Agnostic 3D Lifting in Space and Time},\nauthor={Christopher Fusco and Simon Lucey and Shin-Fang Chng and Mosam Dabhi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=RHZkF3DtDu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RHZkF3DtDu",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RWJkGYfdNn",
        "title": "MorphoSkel3D: Morphological Skeletonization of 3D Point Clouds for Informed Sampling in Object Classification and Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Point clouds are a set of data points in space to represent the 3D geometry of objects. A fundamental step in the processing is to identify a subset of points to represent the shape. While traditional sampling methods often ignore to incorporate geometrical information, recent developments in learning-based sampling models have achieved significant levels of performance. With the integration of geometrical priors, the ability to learn and preserve the underlying structure can be enhanced when sampling. To shed light into the shape, a qualitative skeleton serves as an effective descriptor to guide sampling for both local and global geometries. In this paper, we introduce MorphoSkel3D as a new technique based on morphology to facilitate an efficient skeletonization of shapes. With its low computational cost, MorphoSkel3D is a unique, rule-based algorithm to benchmark its quality and performance on two large datasets, ModelNet and ShapeNet, under different sampling ratios. The results show that training with MorphoSkel3D leads to an informed and more accurate sampling in the practical application of object classification and point cloud retrieval.",
        "keywords": "point clouds;morphology;distance function;skeleton;sampling",
        "primary_area": "",
        "supplementary_material": "/attachment/552821c1a02a9be5dbe6fcf0e5a9b13038884e75.pdf",
        "author": "Pierre Onghena;Santiago Velasco-Forero;Beatriz Marcotegui",
        "authorids": "~Pierre_Onghena1;~Santiago_Velasco-Forero1;~Beatriz_Marcotegui1",
        "gender": ";;F",
        "homepage": ";;https://people.cmm.minesparis.psl.eu/users/marcoteg/",
        "dblp": ";;",
        "google_scholar": ";;cPUNabUAAAAJ",
        "orcid": ";;0000-0002-2825-7292",
        "linkedin": ";;beatrizmarcotegui/",
        "or_profile": "~Pierre_Onghena1;~Santiago_Velasco-Forero1;~Beatriz_Marcotegui1",
        "aff": ";;Mines ParisTech",
        "aff_domain": ";;mines-paristech.fr",
        "position": ";;Full Professor",
        "bibtex": "@inproceedings{\nonghena2025morphoskeld,\ntitle={MorphoSkel3D: Morphological Skeletonization of 3D Point Clouds for Informed Sampling in Object Classification and Retrieval},\nauthor={Pierre Onghena and Santiago Velasco-Forero and Beatriz Marcotegui},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=RWJkGYfdNn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RWJkGYfdNn",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "RaKmcdvHnG",
        "title": "MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The latest regularized Neural Radiance Field (NeRF) approaches produce poor geometry and view extrapolation for large scale sparse view scenes, such as ETH3D. Density-based approaches tend to be under-constrained, while surface-based approaches tend to miss details. In this paper, we take a density-based approach, sampling patches instead of individual rays to better incorporate monocular depth and normal estimates and patch-based photometric consistency constraints between training views and sampled virtual views. Loosely constraining densities based on estimated depth aligned to sparse points further improves geometric accuracy.  While maintaining similar view synthesis quality, our approach significantly improves geometric accuracy on the ETH3D benchmark, e.g. increasing the F1@2cm score by 4x-8x compared to other regularized density-based approaches, with much lower training and inference time than other approaches.",
        "keywords": "Neural Radiance Fields;Sparse-view inputs;Large-scale scenes",
        "primary_area": "",
        "supplementary_material": "/attachment/8764d0a929b918cbf2ca099e020b2c06cb2a2f38.zip",
        "author": "Yuqun Wu;Jae Yong Lee;Chuhang Zou;Shenlong Wang;Derek Hoiem",
        "authorids": "~Yuqun_Wu1;~Jae_Yong_Lee1;~Chuhang_Zou3;~Shenlong_Wang1;~Derek_Hoiem1",
        "gender": "M;M;F;M;M",
        "homepage": "http://yuqunw.github.io;https://jyl.kr;https://zouchuhang.github.io;https://shenlong.web.illinois.edu/;http://dhoiem.cs.illinois.edu/",
        "dblp": ";;138/1927;117/4842;08/6948",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;jBcKes0AAAAJ;QFpswmcAAAAJ;8Sfj7q8AAAAJ",
        "orcid": ";0000-0001-9256-7714;0000-0003-2537-284X;;",
        "linkedin": ";jae-yong-lee-72a8b18a/;chuhang-zou/;shenlong-wang-3496023b;",
        "or_profile": "~Yuqun_Wu1;~Jae_Yong_Lee1;~Chuhang_Zou3;~Shenlong_Wang1;~Derek_Hoiem1",
        "aff": "Department of Computer Science;Meta Facebook;Meta;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign+Reconstruct",
        "aff_domain": "cs.illinois.edu;meta.com;meta.com;illinois.edu;illinois.edu+reconstructinc.com",
        "position": "PhD student;Computer Vision Engineer;Researcher;Assistant Professor;Full Professor+Chief Scientist",
        "bibtex": "@inproceedings{\nwu2025monopatchnerf,\ntitle={MonoPatchNe{RF}: Improving Neural Radiance Fields with Patch-based Monocular Guidance},\nauthor={Yuqun Wu and Jae Yong Lee and Chuhang Zou and Shenlong Wang and Derek Hoiem},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=RaKmcdvHnG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RaKmcdvHnG",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "T3QEabmziq",
        "title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We introduce a new approach for generating realistic 3D models with UV maps through a representation termed \"Object Images.\" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.",
        "keywords": "Generative Models;3D Generation;Mesh Generation;Geometry Images;3D Structure Generation",
        "primary_area": "",
        "supplementary_material": "/attachment/7c0343dfc1fc2f29ec87a51600f1ed24ed0ce3bc.zip",
        "author": "Xingguang Yan;Han-Hung Lee;Ziyu Wan;Angel X Chang",
        "authorids": "~Xingguang_Yan1;~Han-Hung_Lee1;~Ziyu_Wan1;~Angel_X_Chang1",
        "gender": "M;M;M;F",
        "homepage": "http://yanxg.art;https://hanhung.github.io/;http://raywzy.com;https://angelxuanchang.github.io",
        "dblp": "234/6114;294/0092;234/6098;46/10489",
        "google_scholar": "GwsGU8IAAAAJ;32ebx0UAAAAJ;RfIHcSoAAAAJ;8gfs8XIAAAAJ",
        "orcid": "0000-0002-1587-5366;;;0009-0003-5055-6437",
        "linkedin": ";;;",
        "or_profile": "~Xingguang_Yan1;~Han-Hung_Lee1;~Ziyu_Wan1;~Angel_X_Chang1",
        "aff": "Simon Fraser University;Simon Fraser University;Microsoft;Simon Fraser University",
        "aff_domain": "sfu.ca;sfu.ca;microsoft.com;sfu.ca",
        "position": "PhD student;PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nyan2025an,\ntitle={An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion},\nauthor={Xingguang Yan and Han-Hung Lee and Ziyu Wan and Angel X Chang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=T3QEabmziq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=T3QEabmziq",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "TicP3jTxeC",
        "title": "Robust Spectral Translation Synchronization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper introduces a robust translation synchronization approach which takes relative directions between pairs of images as inputs and outputs absolute image translations. Our approach is based on a generalized eigenvalue problem, where the formulation contains edge weights in relative directions and vertex weights in absolute image translations. We present a rigorous stability analysis to determine how to set these weights optimally. Specifically, optimal vertex weights are always $1$, while optimal edge weights depend on magnitudes of relative transformations and variances of relative directions. These results lead to an iterative translation synchronization formulation, which progressively removes outliers in the inputs by adaptively adjusting the edge weights. We present exact and robust recovery conditions for our approach under a standard noise model. Experimental results justify our theoretical results and show that our approach outperforms state-of-the-art baseline approaches on both synthetic and real datasets.",
        "keywords": "Spectral techniques; Structure-from-motion",
        "primary_area": "",
        "supplementary_material": "/attachment/33167c576152f13b8592f1c6cafcc72c7c1b19af.pdf",
        "author": "Zihang He;Hang Ruan;Qixing Huang",
        "authorids": "~Zihang_He4;~Hang_Ruan1;~Qixing_Huang1",
        "gender": "M;F;M",
        "homepage": "https://hzh16.github.io/;;https://www.cs.utexas.edu/~huangqx/",
        "dblp": ";;82/241",
        "google_scholar": ";;https://scholar.google.com.tw/citations?user=pamL_rIAAAAJ",
        "orcid": ";0009-0007-7860-0670;",
        "linkedin": ";;",
        "or_profile": "~Zihang_He4;~Hang_Ruan1;~Qixing_Huang1",
        "aff": "University of Texas at Austin;Shanghai Jiaotong University;University of Texas at Austin",
        "aff_domain": "utexas.edu;sjtu.edu;utexas.edu",
        "position": "PhD student;Undergrad student;Associate Professor",
        "bibtex": "@inproceedings{\nhe2025robust,\ntitle={Robust Spectral Translation Synchronization},\nauthor={Zihang He and Hang Ruan and Qixing Huang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=TicP3jTxeC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TicP3jTxeC",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "UH0PU5scKi",
        "title": "SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Instance Segmentation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce SAMPro3D for zero-shot instance segmentation of 3D scenes. Given the 3D point cloud and multiple posed RGB-D frames of 3D scenes, our approach segments 3D instances by applying the pretrained Segment Anything Model (SAM) to 2D frames. Our key idea involves locating SAM prompts in 3D to align their projected pixel prompts across frames, ensuring the view consistency of SAM-predicted masks. Moreover, we suggest selecting prompts from the initial set guided by the information of SAM-predicted masks across all views, which enhances the overall performance. We further propose to consolidate different prompts if they are segmenting different surface parts of the same 3D instance, bringing a more comprehensive segmentation. Notably, our method does **not** require any additional training. Extensive experiments on diverse benchmarks show that our method achieves comparable or better performance compared to previous zero-shot or fully supervised approaches, and in many cases surpasses human annotations. Furthermore, since our fine-grained predictions often lack annotations in available datasets, we present ScanNet200-Fine50 test data which provides more fine-grained annotations on 50 scenes from ScanNet200 dataset.",
        "keywords": "Zero-shot 3D instance segmentation;3D SAM prompts;Prompt selection and consolidation;No training;Fine-grained test data",
        "primary_area": "",
        "supplementary_material": "/attachment/1dc1f6b02ae1cf09c6e9f9c561efeaf2dfaa4f12.zip",
        "author": "Mutian Xu;Xingyilang Yin;Lingteng Qiu;Yang Liu;Xin Tong;Xiaoguang Han",
        "authorids": "~Mutian_Xu1;~Xingyilang_Yin1;~Lingteng_Qiu1;~Yang_Liu49;~Xin_Tong1;~Xiaoguang_Han2",
        "gender": "M;M;M;;M;M",
        "homepage": "https://mutianxu.github.io/;https://flow0314.github.io/;https://lingtengqiu.github.io/;;https://www.microsoft.com/en-us/research/people/xtong/;https://gaplab.cuhk.edu.cn/",
        "dblp": "281/7626;;235/8437.html;;86/2176-1;60/8294",
        "google_scholar": "OpWRJOoAAAAJ;bo0YGokAAAAJ;YJFpZ2kAAAAJ;;P91a-UQAAAAJ;",
        "orcid": ";;;;0000-0001-8788-2453;",
        "linkedin": "mutian-xu-684811172/;yxyl0314/;;;xin-tong-8892039/;",
        "or_profile": "~Mutian_Xu1;~Xingyilang_Yin1;~Lingteng_Qiu1;~Yang_Liu49;~Xin_Tong1;~Xiaoguang_Han2",
        "aff": "Nanyang Technological University+The Chinese University of Hong Kong (Shenzhen);University of Macau+Xidian University;Alibaba Group;;Anuttacon;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "ntu.edu.sg+link.cuhk.edu.cn;um.edu.mo+xidian.edu;alibaba-inc.com;;anuttacon.com;cuhk.edu.cn",
        "position": "Postdoc+PhD student;PhD student+MS student;Researcher;;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025samprod,\ntitle={{SAMP}ro3D: Locating {SAM} Prompts in 3D for Zero-Shot Instance Segmentation},\nauthor={Mutian Xu and Xingyilang Yin and Lingteng Qiu and Yang Liu and Xin Tong and Xiaoguang Han},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=UH0PU5scKi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UH0PU5scKi",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VLz4FjIBim",
        "title": "LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a method for reconstructing a clear Neural Radiance Field (NeRF) even with fast camera motions. To address blur artifacts, we leverage both (blurry) RGB images and event camera data captured in a binocular configuration. Importantly, when reconstructing our clear NeRF, we consider the camera modeling imperfections that arise from the simple pinhole camera model as learned embeddings for each camera measurement, and further learn a mapper that connects event camera measurements with RGB data. As no previous dataset exists for our binocular setting, we introduce an event camera dataset with captures from a 3D-printed stereo configuration between RGB and event cameras. Empirically, we evaluate on our introduced dataset and EVIMOv2 and show that our method leads to improved reconstructions. We are committed to making our code and dataset public.",
        "keywords": "Event cameras;NeRF;3d reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/bf42c10109e4fdda6e84c992c195fd26801ffe2c.pdf",
        "author": "Wei Zhi Tang;Daniel Rebain;Konstantinos G. Derpanis;Kwang Moo Yi",
        "authorids": "~Wei_Zhi_Tang2;~Daniel_Rebain1;~Konstantinos_G._Derpanis1;~Kwang_Moo_Yi1",
        "gender": "M;;;",
        "homepage": "https://github.com/Goulustis;;;",
        "dblp": ";242/9301;;",
        "google_scholar": ";https://scholar.google.ca/citations?user=h-qFKrQAAAAJ;;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Wei_Zhi_Tang2;~Daniel_Rebain1;~Konstantinos_G._Derpanis1;~Kwang_Moo_Yi1",
        "aff": "University of British Columbia;University of British Columbia;;",
        "aff_domain": "ubc.ca;cs.ubc.ca;;",
        "position": "MS student;PhD student;;",
        "bibtex": "@inproceedings{\ntang2025lsenerf,\ntitle={{LSE}-Ne{RF}: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with {RGB}-Event Stereo},\nauthor={Wei Zhi Tang and Daniel Rebain and Konstantinos G. Derpanis and Kwang Moo Yi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=VLz4FjIBim}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VLz4FjIBim",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VSG65wVNuL",
        "title": "GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Learning-based methods address its inherent scale ambiguity by leveraging increasingly large labeled and unlabeled datasets, to produce geometric priors capable of generating accurate predictions across domains. As a result, state of the art approaches show impressive performance in zero-shot relative and metric depth estimation. Recently, diffusion models have exhibited remarkable scalability and generalizable properties in their learned representations. However, because these models repurpose tools originally designed for image generation, they can only operate on dense ground-truth, which is not available for most depth labels, especially in real-world settings. In this paper we present GRIN, an efficient diffusion model designed to ingest sparse unstructured training data. We use image features with 3D geometric positional encodings to condition the diffusion process both globally and locally, generating depth predictions at a pixel-level. With comprehensive experiments across eight indoor and outdoor datasets, we show that GRIN establishes a new state of the art in zero-shot metric monocular depth estimation even when trained from scratch.",
        "keywords": "monocular depth estimation;zero-shot metric depth;diffusion models",
        "primary_area": "",
        "supplementary_material": "/attachment/641f0785d27cf41110972101d14d423d8f06737d.zip",
        "author": "Vitor Campagnolo Guizilini;Pavel Tokmakov;Achal Dave;Rares Andrei Ambrus",
        "authorids": "~Vitor_Campagnolo_Guizilini2;~Pavel_Tokmakov2;~Achal_Dave1;~Rares_Andrei_Ambrus1",
        "gender": "M;M;M;M",
        "homepage": ";https://pvtokmakov.github.io/home/;http://www.achaldave.com/;http://www.csc.kth.se/~raambrus/",
        "dblp": ";153/2264;156/1161;25/76",
        "google_scholar": "UH9tP6QAAAAJ;https://scholar.google.fr/citations?user=b15vJuEAAAAJ;oQyYH9kAAAAJ;2xjjS3oAAAAJ",
        "orcid": ";;;0000-0002-3111-3812",
        "linkedin": "vitorguizilini/;;;rare%C8%99-ambru%C8%99-b04812125/",
        "or_profile": "~Vitor_Campagnolo_Guizilini2;~Pavel_Tokmakov2;~Achal_Dave1;~Rares_Andrei_Ambrus1",
        "aff": "Toyota Research Institute;Toyota Research Institute;Anthropic;Toyota Research Institute",
        "aff_domain": "tri.global;tri.global;anthropic.com;tri.global",
        "position": "Staff Research Scientist;Research Scientist;Researcher;Researcher",
        "bibtex": "@inproceedings{\nguizilini2025grin,\ntitle={{GRIN}: Zero-Shot Metric Depth with Pixel-Level Diffusion},\nauthor={Vitor Campagnolo Guizilini and Pavel Tokmakov and Achal Dave and Rares Andrei Ambrus},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=VSG65wVNuL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VSG65wVNuL",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "VsSmQSmM7k",
        "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents a novel framework for compactly representing a 3D indoor scene using a set of polycuboids through a deep learning-based fitting method. Indoor scenes mainly consist of man-made objects, such as furniture, which often exhibit rectilinear geometry. This property allows indoor scenes to be represented using combinations of polycuboids, providing a compact representation that benefits downstream applications like furniture rearrangement. Our framework takes a noisy point cloud as input and first detects six types of cuboid faces using a transformer network. Then, a graph neural network is used to validate the spatial relationships of the detected faces to form potential polycuboids. Finally, each polycuboid instance is reconstructed by forming a set of boxes based on the aggregated face labels. To train our networks, we introduce a synthetic dataset encompassing a diverse range of cuboid and polycuboid shapes that reflect the characteristics of indoor scenes. Our framework generalizes well to real-world indoor scene datasets, including Replica, ScanNet, and scenes captured with an iPhone. The versatility of our method is demonstrated through practical applications, such as virtual room tours and scene editing.",
        "keywords": "shape abstraction; polycuboid; cuboid; point cloud; indoor scene; shape representation and features; shape recognition and analysis; space",
        "primary_area": "",
        "supplementary_material": "/attachment/08b8a4697041abc96a9c25fd19cbbceccf249c15.zip",
        "author": "Gahye Lee;Hyejeong Yoon;Jungeon Kim;Seungyong Lee",
        "authorids": "~Gahye_Lee2;~Hyejeong_Yoon1;~Jungeon_Kim1;~Seungyong_Lee1",
        "gender": "F;;M;M",
        "homepage": "https://cvlab.khu.ac.kr;http://cg.postech.ac.kr/;http://jgkim.info/;http://cg.postech.ac.kr/leesy",
        "dblp": "230/8474;;228/5100;60/2559-1",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;;https://scholar.google.co.kr/citations?user=VL0fjaIAAAAJ;yGPH-nAAAAAJ",
        "orcid": ";;0000-0003-4212-1970;0000-0002-8159-4271",
        "linkedin": "gahye-lee-7ba69b163/;;jungeon-kim-383155108/;",
        "or_profile": "~Gahye_Lee2;~Hyejeong_Yoon1;~Jungeon_Kim1;~Seungyong_Lee1",
        "aff": "Pohang University of Science and Technology;POSTECH;Pohang University of Science and Technology;POSTECH",
        "aff_domain": "postech.edu;postech.ac.kr;postech.edu;postech.ac.kr",
        "position": "PhD student;PhD student;PhD student;Professor",
        "bibtex": "@inproceedings{\nlee2025deep,\ntitle={Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes},\nauthor={Gahye Lee and Hyejeong Yoon and Jungeon Kim and Seungyong Lee},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=VsSmQSmM7k}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VsSmQSmM7k",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "W7vOFBCGPm",
        "title": "iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present iFusion, a novel 3D object reconstruction framework that requires only two views with unknown camera poses.\nWhile single-view reconstruction yields visually appealing results, it can deviate significantly from the actual object, especially on unseen sides. Additional views improve reconstruction fidelity but necessitate known camera poses. However, assuming the availability of pose may be unrealistic, and existing pose estimators fail in sparse-view scenarios. To address this, we harness a pre-trained novel view synthesis diffusion model, which embeds implicit knowledge about the geometry and appearance of diverse objects. Our strategy unfolds in three steps: (1) We invert the diffusion model for camera pose estimation instead of synthesizing novel views. (2) The diffusion model is fine-tuned using provided views and estimated poses, turned into a novel view synthesizer tailored for the target object. (3) Leveraging registered views and the fine-tuned diffusion model, we reconstruct the 3D object. Experiments demonstrate strong performance in both pose estimation and novel view synthesis. Moreover, iFusion seamlessly integrates with various reconstruction methods and enhances them.",
        "keywords": "pose-estimation;diffusion models;pose-free reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/3be92fd8835a31e85d715e6133935731bdb8c7c1.zip",
        "author": "Chin-Hsuan Wu;Yen-Chun Chen;Bolivar Enrique Solarte;Lu Yuan;Min Sun",
        "authorids": "~Chin-Hsuan_Wu1;~Yen-Chun_Chen1;~Bolivar_Enrique_Solarte1;~Lu_Yuan1;~Min_Sun1",
        "gender": "Not Specified;M;M;M;M",
        "homepage": "https://chinhsuanwu.github.io;;https://enriquesolarte.github.io/;https://www.microsoft.com/en-us/research/people/luyuan/;http://aliensunmin.github.io",
        "dblp": ";160/0623-1;;;62/2750-1",
        "google_scholar": ";Gptgy4YAAAAJ;https://scholar.google.com/citations?hl=en;k9TsUVsAAAAJ;1Rf6sGcAAAAJ",
        "orcid": ";;0000-0003-3518-755X;;",
        "linkedin": ";;enriquesolartepardo1988/;;",
        "or_profile": "~Chin-Hsuan_Wu1;~Yen-Chun_Chen1;~Bolivar_Enrique_Solarte1;~Lu_Yuan1;~Min_Sun1",
        "aff": "University of Toronto;Microsoft;Industrial Technology Research Institute;Meta Facebook;National Tsing Hua University",
        "aff_domain": "utoronto.ca;microsoft.com;itri.org.tw;meta.com;nthu.edu.tw",
        "position": "PhD student;Researcher;Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwu2025ifusion,\ntitle={iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views},\nauthor={Chin-Hsuan Wu and Yen-Chun Chen and Bolivar Enrique Solarte and Lu Yuan and Min Sun},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=W7vOFBCGPm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=W7vOFBCGPm",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Wd8ZPdnYdZ",
        "title": "Incorporating dense metric depth into neural 3D representations for view synthesis and relighting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Synthesizing accurate geometry and photo-realistic appearance of small scenes is an active area of research with compelling use cases in gaming, virtual reality, robotic-manipulation, autonomous driving, convenient product capture, and consumer-level photography. When applying scene geometry and appearance estimation techniques to robotics, we found that the narrow cone of possible viewpoints due to the limited range of robot motion and scene clutter caused current estimation techniques to produce poor quality estimates or even fail. On the other hand, in robotic applications, dense metric depth can often be measured directly using stereo and illumination can be controlled. Depth can provide a good initial estimate of the object geometry to improve reconstruction, while multi-illumination images can facilitate relighting. In this work we demonstrate a method to incorporate dense metric depth into the training of neural 3D representations and address an artifact observed while jointly refining geometry and appearance by disambiguating between texture and geometry edges. We also discuss a multi-flash stereo camera system developed in-house to capture the necessary data for our pipeline and show results on relighting and view synthesis with a few training views.",
        "keywords": "neural 3D view synthesis;relighting;multi-flash cameras",
        "primary_area": "",
        "supplementary_material": "/attachment/73ec1e57411b87e29fed01a50a97a919f8a94399.pdf",
        "author": "Arkadeep Narayan Chaudhury;Igor Vasiljevic;Sergey Zakharov;Vitor Campagnolo Guizilini;Rares Andrei Ambrus;Srinivasa Narasimhan;Christopher G Atkeson",
        "authorids": "~Arkadeep_Narayan_Chaudhury1;~Igor_Vasiljevic1;~Sergey_Zakharov1;~Vitor_Campagnolo_Guizilini2;~Rares_Andrei_Ambrus1;~Srinivasa_Narasimhan1;~Christopher_G_Atkeson1",
        "gender": "M;M;M;M;M;M;",
        "homepage": "https://arkadeepnc.github.io/;https://scholar.google.com/citations?user=Sl_2kHcAAAAJ&hl=en;https://zakharos.github.io/;;http://www.csc.kth.se/~raambrus/;http://www.cs.cmu.edu/~srinivas/;",
        "dblp": "203/4224;;195/5832;;25/76;57/2011;",
        "google_scholar": "rZ58Bf8AAAAJ;;https://scholar.google.de/citations?user=3DK3I-8AAAAJ;UH9tP6QAAAAJ;2xjjS3oAAAAJ;MhYrLJAAAAAJ;",
        "orcid": ";;;;0000-0002-3111-3812;;",
        "linkedin": "arkadeepnc/;;;vitorguizilini/;rare%C8%99-ambru%C8%99-b04812125/;;",
        "or_profile": "~Arkadeep_Narayan_Chaudhury1;~Igor_Vasiljevic1;~Sergey_Zakharov1;~Vitor_Campagnolo_Guizilini2;~Rares_Andrei_Ambrus1;~Srinivasa_Narasimhan1;~Christopher_G_Atkeson1",
        "aff": "Epic Games;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute;Carnegie Mellon University;",
        "aff_domain": "epicgames.com;tri.global;tri.global;tri.global;tri.global;cmu.edu;",
        "position": "Researcher;Research Scientist;Researcher;Staff Research Scientist;Researcher;Full Professor;",
        "bibtex": "@inproceedings{\nchaudhury2025incorporating,\ntitle={Incorporating dense metric depth into neural 3D representations for view synthesis and relighting},\nauthor={Arkadeep Narayan Chaudhury and Igor Vasiljevic and Sergey Zakharov and Vitor Campagnolo Guizilini and Rares Andrei Ambrus and Srinivasa Narasimhan and Christopher G Atkeson},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Wd8ZPdnYdZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Wd8ZPdnYdZ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "WgzBQK0kFh",
        "title": "U\u2013ARE\u2013ME: Uncertainty-Aware Rotation Estimation in Manhattan Environments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Camera rotation estimation from a single image is a challenging task, often requiring depth data and/or camera intrinsics, which are generally not available for in-the-wild videos. Although external sensors such as inertial measurement units (IMUs) can help, they often suffer from drift and are not applicable in non-inertial reference frames.\nWe present U-ARE-ME, an algorithm that estimates camera rotation along with uncertainty from uncalibrated RGB images. Using a Manhattan World assumption, our method leverages the per-pixel geometric priors encoded in single-image surface normal predictions and performs optimisation over the SO(3) manifold.\nGiven a sequence of images, we can use the per-frame rotation estimates and their uncertainty to perform multi-frame optimisation, achieving robustness and temporal consistency.\nOur experiments demonstrate that U-ARE-ME performs comparably to RGB-D methods and is more robust than feature-based vanishing point and SLAM methods.",
        "keywords": "Rotation Estimation;Manhattan World",
        "primary_area": "",
        "supplementary_material": "/attachment/c979de52b3d91a13963ba222255aed6cfa15f58e.zip",
        "author": "Aalok Patwardhan;Callum Rhodes;Gwangbin Bae;Andrew Davison",
        "authorids": "~Aalok_Patwardhan1;~Callum_Rhodes1;~Gwangbin_Bae1;~Andrew_Davison1",
        "gender": "M;;M;M",
        "homepage": ";;https://www.baegwangbin.com;http://www.doc.ic.ac.uk/~ajd/",
        "dblp": ";;275/6835;d/AndrewJDavison",
        "google_scholar": "FWcCBhsAAAAJ;https://scholar.google.co.uk/citations?user=aQSQwcUAAAAJ;zbXgufQAAAAJ;https://scholar.google.co.uk/citations?user=A0ae1agAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;gwangbin-bae-118b18183/;",
        "or_profile": "~Aalok_Patwardhan1;~Callum_Rhodes1;~Gwangbin_Bae1;~Andrew_Davison1",
        "aff": "Imperial College London;Imperial College London;Imperial College London;Imperial College London",
        "aff_domain": "ic.ac.uk;ic.ac.uk;ic.ac.uk;imperial.ac.uk",
        "position": "PhD student;Postdoc;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\npatwardhan2025uareme,\ntitle={U{\\textendash}{ARE}{\\textendash}{ME}: Uncertainty-Aware Rotation Estimation in Manhattan Environments},\nauthor={Aalok Patwardhan and Callum Rhodes and Gwangbin Bae and Andrew Davison},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=WgzBQK0kFh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=WgzBQK0kFh",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Wy9v7wJaLu",
        "title": "FastGrasp: Efficient Grasp Synthesis with Diffusion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Effectively modeling the interaction between human hands and objects is challenging due to the complex physical constraints and the equirement for high generation efficiency in applications. Prior approaches often employ computationally intensive two-stage approaches, which first generate an intermediate representation, such as contact maps, followed by an iterative optimization procedure that updates hand meshes to capture the hand-object relation. However, due to the high computation complexity during the optimization stage, such strategies often suffer from low efficiency in inference. To address this limitation, this work introduces a novel diffusion-model-based approach that generates the grasping pose in a one-stage manner. This allows us to significantly improve generation speed and the diversity of generated hand poses. In particular, we develop a Latent Diffusion Model with an Adaptation Module for object-conditioned hand pose generation and a contact-aware loss to enforce the physical constraints between hands and objects. Extensive experiments demonstrate that our method achieves faster inference, higher diversity, and superior pose quality than state-of-the-art approaches. Code is available at https://github.com/wuxiaofei01/FastGrasp .",
        "keywords": "hand-object interaction;grasp generation;diffusion model",
        "primary_area": "",
        "supplementary_material": "/attachment/39002eeb133c9c7e7105fa0aa04a6c64c8157805.pdf",
        "author": "xiaofei wu;Tao Liu;caoji li;Yuexin Ma;Yujiao Shi;Xuming He",
        "authorids": "~xiaofei_wu3;~Tao_Liu27;~caoji_li1;~Yuexin_Ma2;~Yujiao_Shi1;~Xuming_He3",
        "gender": "M;;;F;F;M",
        "homepage": "https://github.com/wxf-0415;;;http://yuexinma.me/aboutme.html;https://shiyujiao.github.io/;https://faculty.sist.shanghaitech.edu.cn/faculty/hexm/index.html",
        "dblp": ";;;209/5925;159/2546;03/4230",
        "google_scholar": ";https://scholar.google.cz/citations?user=x44vYKYAAAAJ;;;rVsRpZEAAAAJ;0KyeZ2QAAAAJ",
        "orcid": ";;;;0000-0001-6028-9051;",
        "linkedin": ";;;;yujiao-shi-053a12198/;",
        "or_profile": "~xiaofei_wu3;~Tao_Liu27;~caoji_li1;~Yuexin_Ma2;~Yujiao_Shi1;~Xuming_He3",
        "aff": "ShanghaiTech University;ShanghaiTech University;;ShanghaiTech University;ShanghaiTech University;ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn;;shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "position": "MS student;PhD student;;Assistant Professor;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nwu2025fastgrasp,\ntitle={FastGrasp: Efficient Grasp Synthesis with Diffusion},\nauthor={xiaofei wu and Tao Liu and caoji li and Yuexin Ma and Yujiao Shi and Xuming He},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Wy9v7wJaLu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Wy9v7wJaLu",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XGk75JASDh",
        "title": "mmDiffusion: mmWave Diffusion for Sequential 3D Human Dense Point Cloud Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Owing to the distinctive attributes inherent to mmWave radar, the utilization of millimeter-wave (mmWave) point cloud data in contexts involving human-related scenarios is poised to yield significant promise. However, the generation of dense and temporally consistent 3D human point clouds from sequential mmWave signals is a challenging task, yet a critical endeavor with far-reaching implications. In this work, we present a groundbreaking approach to address the challenge of generating dense and temporally consistent 3D human point clouds from sequential mmWave signals. We redefine the problem as a 3D point cloud denoising task, leveraging reverse diffusion processes to transform sparse mmWave data into detailed human representations. Our proposed method, mmDiffusion, effectively exploits diffusion models and temporal context within mmWave sequences to learn the denoising process, resulting in denser and temporally coherent human point clouds. We also introduce a novel evaluation metric tailored to measure temporal consistency. Experimental results demonstrate that mmDiffusion outperforms existing methods. Code and dataset will be public upon acceptance.",
        "keywords": "mmWave;point cloud;3D human;diffuion",
        "primary_area": "",
        "supplementary_material": "/attachment/cff6fe7a12c65f9c2ef42be502779e912f1354c4.pdf",
        "author": "Qian Xie;Xinyu Hou;Qianyi Deng;Amir Patel;Niki Trigoni;Andrew Markham",
        "authorids": "~Qian_Xie3;~Xinyu_Hou2;~Qianyi_Deng1;~Amir_Patel1;~Niki_Trigoni2;~Andrew_Markham2",
        "gender": "M;F;F;M;F;M",
        "homepage": "https://nuaaxq.github.io/personal_website/;https://www.cs.ox.ac.uk/people/xinyu.hou/;https://www.cs.ox.ac.uk/people/qianyi.deng/;https://scholar.google.com/citations?user=RxMigV4AAAAJ&hl=en;https://www.cs.ox.ac.uk/niki.trigoni;",
        "dblp": "36/789-1;;;;;83/7169",
        "google_scholar": "we9tUrgAAAAJ;;;RxMigV4AAAAJ;https://scholar.google.co.uk/citations?user=185g9ckAAAAJ;https://scholar.google.co.uk/citations?user=g3JTO9EAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Qian_Xie3;~Xinyu_Hou2;~Qianyi_Deng1;~Amir_Patel1;~Niki_Trigoni2;~Andrew_Markham2",
        "aff": "University of Leeds;University of Oxford;Department of Computer Science;University College London, University of London;Birkbeck College, University of London+University of Oxford;University of Oxford",
        "aff_domain": "leeds.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;ucl.ac.uk;bbk.ac.uk+cs.ox.ac.uk;ox.ac.uk",
        "position": "Lecturer;Postdoc;PhD student;Assistant Professor;Lecturer+Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nxie2025mmdiffusion,\ntitle={mmDiffusion: mmWave Diffusion for Sequential 3D Human Dense Point Cloud Generation},\nauthor={Qian Xie and Xinyu Hou and Qianyi Deng and Amir Patel and Niki Trigoni and Andrew Markham},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=XGk75JASDh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XGk75JASDh",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "XNENOxAFbl",
        "title": "A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Visual localization involves estimating the 6-degree-of-freedom (6-DoF) camera pose within a known scene. A critical step in this process is identifying pixel-to-point correspondences between 2D query images and 3D models. Most advanced approaches currently rely on extensive visual descriptors to establish these correspondences, facing challenges in storage, privacy issues and model maintenance. Direct 2D-3D keypoint matching without visual descriptors is becoming popular as it can overcome those challenges. However, existing descriptor-free methods suffer from low accuracy or heavy computation. Addressing this gap, this paper introduces the Angle-Annular Graph Neural Network (A2-GNN), a simple approach that efficiently learns robust geometric structural representations with annular feature extraction. Specifically, this approach clusters neighbors and embeds each group\u2019s distance information and angle as supplementary information to capture local structures. Evaluation on matching and visual localization datasets demonstrates that our approach achieves state-of-the-art accuracy with low computational overhead among visual description-free methods.",
        "keywords": "Visual Localization;Graph Neural Networks;Deep Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/bc25b66a85a2de59ce3cab0187b735204e76a903.pdf",
        "author": "Yejun Zhang;Shuzhe Wang;Juho Kannala",
        "authorids": "~Yejun_Zhang1;~Shuzhe_Wang1;~Juho_Kannala5",
        "gender": ";;M",
        "homepage": ";;https://users.aalto.fi/~kannalj1/",
        "dblp": ";;47/4656.html",
        "google_scholar": ";;c4mWQPQAAAAJ",
        "orcid": ";;0000-0001-5088-4041",
        "linkedin": "yejun-zhang-5b216a229/;;",
        "or_profile": "~Yejun_Zhang1;~Shuzhe_Wang1;~Juho_Kannala5",
        "aff": "Aalto University;;Aalto University",
        "aff_domain": "aalto.fi;;aalto.fi",
        "position": "PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025agnn,\ntitle={A2-{GNN}: Angle-Annular {GNN} for Visual Descriptor-free Camera Relocalization},\nauthor={Yejun Zhang and Shuzhe Wang and Juho Kannala},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=XNENOxAFbl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XNENOxAFbl",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Y2SblTyTxd",
        "title": "FORCE: Physics-aware Human-object Interaction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Interactions between human and objects are influenced not only by the object's pose and shape, but also by physical attributes such as object mass and surface friction. They introduce important motion nuances that are essential for diversity and realism. Despite advancements in recent human-object interaction methods, this aspect has been overlooked.\nGenerating nuanced human motion presents two challenges. First, it is non-trivial to learn from multi-modal human and object information derived from both the physical and non-physical attributes. Second, there exists no dataset capturing nuanced human interactions with objects of varying physical properties, hampering model development.\nThis work addresses the gap by introducing the FORCE model, an approach for synthesizing diverse, nuanced human-object interactions by modeling physical attributes. Our key insight is that human motion is dictated by the interrelation between the force exerted by the human and the perceived resistance. Guided by a novel intuitive physics encoding, the model captures the interplay between human force and resistance. Experiments also demonstrate incorporating human force facilitates learning multi-class motion.\nAccompanying our model, we contribute the a dataset, which features diverse, different-styled motion through interactions with varying resistances. Our code, dataset, and models will be released to foster future research.",
        "keywords": "Human Motion Synthesis;Human-Scene Interaction;Character Animation",
        "primary_area": "",
        "supplementary_material": "/attachment/2439a4466817fe48923ea39b68e05a905b89c462.zip",
        "author": "Xiaohan Zhang;Bharat Lal Bhatnagar;Sebastian Starke;Ilya A. Petrov;Vladimir Guzov;Helisa Dhamo;Eduardo P\u00e9rez-Pellitero;Gerard Pons-Moll",
        "authorids": "~Xiaohan_Zhang2;~Bharat_Lal_Bhatnagar1;~Sebastian_Starke2;~Ilya_A._Petrov2;~Vladimir_Guzov1;~Helisa_Dhamo1;~Eduardo_P\u00e9rez-Pellitero1;~Gerard_Pons-Moll2",
        "gender": ";M;M;M;;F;M;",
        "homepage": ";http://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html;;https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html;https://virtualhumans.mpi-inf.mpg.de/people/Guzov.html;http://campar.in.tum.de/Main/HelisaDhamo;https://perezpellitero.github.io;",
        "dblp": ";204/2998;;131/3876;289/0612;223/9995;141/9842;",
        "google_scholar": "neKcoMgAAAAJ;https://scholar.google.co.in/citations?user=M_NWm1wAAAAJ;ScpOkvAAAAAJ;7rpD7dAAAAAJ;vFSOgzEAAAAJ;oggfqsYAAAAJ;oLWr6EwAAAAJ;",
        "orcid": ";;;0000-0002-8900-1071;0000-0003-1304-5577;;;",
        "linkedin": "xiaohan-zhang-1a7426a2/;;;ptrvilya/;;;;",
        "or_profile": "~Xiaohan_Zhang2;~Bharat_Lal_Bhatnagar1;~Sebastian_Starke2;~Ilya_A._Petrov2;~Vladimir_Guzov1;~Helisa_Dhamo1;~Eduardo_P\u00e9rez-Pellitero1;~Gerard_Pons-Moll2",
        "aff": "University of Tuebingen and MPI Informatics;Meta;Meta Facebook;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Eberhard-Karls-Universit\u00e4t T\u00fcbingen+Saarland Informatics Campus, Max-Planck Institute;Apple;Huawei Technologies R&D (UK) Ltd.;",
        "aff_domain": "mpi-inf.mpg.de;meta.com;facebook.com;uni-tuebingen.de;uni-tuebingen.de+mpi-inf.mpg.de;apple.com;huawei.com;",
        "position": "PhD student;Researcher;Researcher;PhD student;PhD student+PhD student;Researcher;Principal Researcher;",
        "bibtex": "@inproceedings{\nzhang2025force,\ntitle={{FORCE}: Physics-aware Human-object Interaction},\nauthor={Xiaohan Zhang and Bharat Lal Bhatnagar and Sebastian Starke and Ilya A. Petrov and Vladimir Guzov and Helisa Dhamo and Eduardo P{\\'e}rez-Pellitero and Gerard Pons-Moll},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Y2SblTyTxd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Y2SblTyTxd",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "YKvWJBKEbH",
        "title": "Obfuscation Based Privacy Preserving Representations are Recoverable Using Neighborhood Information",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "The rapid growth of AR/VR/MR applications and cloud-based visual localization has heightened concerns over user privacy. \nThis privacy concern has been further escalated by the ability of deep neural networks to recover detailed images of a scene from a sparse set of 3D or 2D points and their descriptors -  the so-called inversion attacks.\nResearch on privacy-preserving localization has therefore focused on preventing such attacks through geometry obfuscation techniques like lifting points to higher dimensions or swapping coordinates. \nIn this paper, we reveal a common vulnerability in these methods that allows approximate point recovery using known neighborhoods.\nWe further show that these neighborhoods can be computed by learning to identify descriptors that co-occur in neighborhoods.\nExtensive experiments demonstrate that all existing geometric obfuscation schemes remain susceptible to such recovery, challenging their claims of being privacy-preserving.",
        "keywords": "Visual Localization;Privacy presrving representations;Privacy preserving localization",
        "primary_area": "",
        "supplementary_material": "/attachment/4852188fc715669a370764bd3b6f01ae19bc0b32.zip",
        "author": "Kunal Chelani;Assia Benbihi;Fredrik Kahl;Torsten Sattler;Zuzana Kukelova",
        "authorids": "~Kunal_Chelani1;~Assia_Benbihi1;~Fredrik_Kahl3;~Torsten_Sattler1;~Zuzana_Kukelova3",
        "gender": "M;;M;M;F",
        "homepage": "http://www.chalmers.se/en/Staff/Pages/chelani.aspx;https://abenbihi.github.io/;https://fredkahl.github.io/;https://tsattler.github.io/;http://cmp.felk.cvut.cz/~kukelova",
        "dblp": "200/8849;220/3391;01/7013;51/9054;17/4583",
        "google_scholar": "https://scholar.google.co.in/citations?user=VFduVf4AAAAJ;https://scholar.google.fr/citations?user=0XFEGeoAAAAJ;P_w6UgMAAAAJ;jzx6_ZIAAAAJ;https://scholar.google.ca/citations?user=M4a3VyYAAAAJ",
        "orcid": ";;;0000-0001-9760-4553;",
        "linkedin": ";assia-benbihi-56ab7894/;;torsten-sattler-ba2ab0145;",
        "or_profile": "~Kunal_Chelani1;~Assia_Benbihi1;~Fredrik_Kahl3;~Torsten_Sattler1;~Zuzana_Kukelova3",
        "aff": "Chalmers University;Snap Inc.+CIIRC, Czech Technical University, Czech Technical University of Prague;Chalmers University;CIIRC, Czech Technical University in Prague;Czech Technical University in Prague+Czech Technical University in Prague",
        "aff_domain": "chalmers.se;snapchat.com+ciirc.cvut.cz;chalmers.se;cvut.cz;cvut.cz+cvut.cz",
        "position": "PhD student;Engineer+Postdoc;Full Professor;Senior Researcher;Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\nchelani2025obfuscation,\ntitle={Obfuscation Based Privacy Preserving Representations are Recoverable Using Neighborhood Information},\nauthor={Kunal Chelani and Assia Benbihi and Fredrik Kahl and Torsten Sattler and Zuzana Kukelova},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=YKvWJBKEbH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=YKvWJBKEbH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Z8bwCg6tJH",
        "title": "NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a novel approach to large-scale point cloud surface reconstruction by developing an efficient framework that converts an irregular point cloud into a signed distance field(SDF).\nOur backbone builds upon recent transformer-based architectures(i.e.PointTransformerV3), that serializes the point cloud into a locality-preserving sequence of tokens.\nWe efficiently predict the SDF value at a point by aggregating nearby tokens, where fast approximate neighbors can be retrieved thanks to the serialization.\nWe serialize the point cloud at different levels/scales, and non-linearly aggregate a feature to predict the SDF value.\nWe show that aggregating across multiple scales is critical to overcome the approximations introduced by the serialization (i.e. false negatives in the neighborhood).\nOur frameworks sets the new state-of-the-art in terms of accuracy and efficiency(better or similar performance with half the latency of the best prior method, coupled with a simpler implementation), particularly on outdoor datasets where sparse-grid methods have shown limited performance.\nTo foster the continuation of research in this topic, we will release our complete source code, as well as our pre-trained models.",
        "keywords": "point-cloud;surface reconstruction;reconstruction;neural field",
        "primary_area": "",
        "supplementary_material": "/attachment/410a422a5f8120d627854055146eb5dd818620e1.pdf",
        "author": "Zhen Li;Weiwei Sun;Shrisudhan Govindarajan;Shaobo Xia;Daniel Rebain;Kwang Moo Yi;Andrea Tagliasacchi",
        "authorids": "~Zhen_Li18;~Weiwei_Sun4;~Shrisudhan_Govindarajan1;~Shaobo_Xia2;~Daniel_Rebain1;~Kwang_Moo_Yi1;~Andrea_Tagliasacchi2",
        "gender": "M;M;M;M;;;M",
        "homepage": "http://colinzhenli.github.io;http://wsunid.github.io/;https://shrisudhan.github.io/;;;;http://taiya.github.io",
        "dblp": ";63/6566-6;333/2820;178/8198.html;242/9301;;46/5514",
        "google_scholar": "https://scholar.google.ca/citations?view_op=list_works;https://scholar.google.ca/citations?user=XXC5tSEAAAAJ;Y9l_dogAAAAJ;eOPO9E0AAAAJ;https://scholar.google.ca/citations?user=h-qFKrQAAAAJ;;1RmD-YsAAAAJ",
        "orcid": ";;0000-0002-3546-8223;;;;",
        "linkedin": ";weiwei-sun-5705b013b/;shrisudhan-govindarajan-b6a914157/;;;;",
        "or_profile": "~Zhen_Li18;~Weiwei_Sun4;~Shrisudhan_Govindarajan1;~Shaobo_Xia2;~Daniel_Rebain1;~Kwang_Moo_Yi1;~Andrea_Tagliasacchi2",
        "aff": "Simon Fraser University;Amazon;Simon Fraser University;Changsha University of Science and Technology;University of British Columbia;;Simon Fraser University+University of Toronto+Google DeepMind",
        "aff_domain": "sfu.ca;amazon.com;sfu.ca;csust.edu.cn;cs.ubc.ca;;sfu.ca+utoronto.ca+google.com",
        "position": "MS student;Researcher;PhD student;Assistant Professor;PhD student;;Associate Professor+Associate Professor+Researcher",
        "bibtex": "@inproceedings{\nli2025noksr,\ntitle={No{KSR}: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization},\nauthor={Zhen Li and Weiwei Sun and Shrisudhan Govindarajan and Shaobo Xia and Daniel Rebain and Kwang Moo Yi and Andrea Tagliasacchi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Z8bwCg6tJH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Z8bwCg6tJH",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "Z9yn9YgNIz",
        "title": "WaterSplatting: Fast Underwater 3D Scene Reconstruction using Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The underwater 3D scene reconstruction is a challenging, yet interesting problem with applications ranging from naval robots to VR experiences. The problem was successfully tackled by fully volumetric NeRF-based methods which can model both the geometry and the medium (water). Unfortunately, these methods are slow to train and do not offer real-time rendering. More recently, 3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs. However, because it is an explicit method that renders only the geometry, it cannot render the medium and is therefore unsuited for underwater reconstruction. Therefore, we propose a novel approach that fuses volumetric rendering with 3DGS to handle underwater data effectively. Our method employs 3DGS for explicit geometry representation and a separate volumetric field (queried once per pixel) for capturing the scattering medium. This dual representation further allows the restoration of the scenes by removing the scattering medium. Our method outperforms state-of-the-art NeRF-based methods in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it does so while offering real-time rendering performance, addressing the efficiency limitations of existing methods.",
        "keywords": "Rendering; Point-based models; Volumetric models",
        "primary_area": "",
        "supplementary_material": "/attachment/7c51cce76ac4df12a3df410af73eb5b969bb5c8a.zip",
        "author": "Huapeng Li;Wenxuan Song;Tianao Xu;Alexandre Elsig;Jonas Kulhanek",
        "authorids": "~Huapeng_Li1;~Wenxuan_Song2;~Tianao_Xu1;~Alexandre_Elsig1;~Jonas_Kulhanek1",
        "gender": "M;M;M;M;M",
        "homepage": ";;;https://elsiga.ch;https://jkulhanek.com",
        "dblp": ";;;;247/1194",
        "google_scholar": ";;;;YDNzfN4AAAAJ",
        "orcid": "0009-0001-6790-4974;;0009-0002-0290-5148;;0000-0002-8437-3626",
        "linkedin": "huapeng-li-1ba269293/;wenxuan-song-901123188/;;;",
        "or_profile": "~Huapeng_Li1;~Wenxuan_Song2;~Tianao_Xu1;~Alexandre_Elsig1;~Jonas_Kulhanek1",
        "aff": "University of Zurich;;ETHZ - ETH Zurich;Department of Computer Science, ETHZ - ETH Zurich;Czech Technical University of Prague+Google",
        "aff_domain": "uzh.ch;;ethz.ch;inf.ethz.ch;cvut.cz+google.com",
        "position": "MS student;;MS student;MS student;PhD student+Intern",
        "bibtex": "@inproceedings{\nli2025watersplatting,\ntitle={WaterSplatting: Fast Underwater 3D Scene Reconstruction using Gaussian Splatting},\nauthor={Huapeng Li and Wenxuan Song and Tianao Xu and Alexandre Elsig and Jonas Kulhanek},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=Z9yn9YgNIz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Z9yn9YgNIz",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZOVgzgERfS",
        "title": "HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.",
        "keywords": "head;avatar;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/b702a032d5b0b3627c96b30d7c674b577068bbc1.zip",
        "author": "Xiaozheng Zheng;Chao Wen;Zhaohu Li;Weiyi Zhang;Zhuo Su;Xu Chang;Yang Zhao;Zheng Lv;Xiaoyuan Zhang;Yongjie Zhang;Guidong Wang;Lan Xu",
        "authorids": "~Xiaozheng_Zheng1;~Chao_Wen1;~Zhaohu_Li2;~Weiyi_Zhang4;~Zhuo_Su3;~Xu_Chang1;~Yang_Zhao29;~Zheng_Lv1;~Xiaoyuan_Zhang5;~Yongjie_Zhang3;~Guidong_Wang1;~Lan_Xu2",
        "gender": "M;M;;M;M;M;M;;F;;;M",
        "homepage": ";https://walsvid.github.io/;;https://github.com/Octave1990;https://suzhuo.github.io/;https://github.com/CHANGXU12;https://github.com/uzhaoyang;;https://github.com/zhangxy-xyz;;;http://xu-lan.com/",
        "dblp": "305/5577;11/5790-1;;;274/0946;;;;;;;",
        "google_scholar": "3hSD41oAAAAJ;v8TFZI4AAAAJ;;;iaqDkqMAAAAJ;;;;;;;aPS5pJkAAAAJ",
        "orcid": ";;;;0000-0002-7728-0835;;;;;;;0000-0002-8807-7787",
        "linkedin": ";;;;;;;;;;;",
        "or_profile": "~Xiaozheng_Zheng1;~Chao_Wen1;~Zhaohu_Li2;~Weiyi_Zhang4;~Zhuo_Su3;~Xu_Chang1;~Yang_Zhao29;~Zheng_Lv1;~Xiaoyuan_Zhang5;~Yongjie_Zhang3;~Guidong_Wang1;~Lan_Xu2",
        "aff": "ByteDance;ByteDance;;;ByteDance;Bytedance Inc;bytedance;;ByteDance Inc.;;;ShanghaiTech University",
        "aff_domain": "bytedance.com;bytedance.com;;;bytedance.com;bytedance.com;bytedance.com;;bytedance.com;;;shanghaitech.edu.cn",
        "position": "Researcher;Researcher;;;Researcher;Researcher;Researcher;;Engineer;;;Assistant Professor",
        "bibtex": "@inproceedings{\nzheng2025headgap,\ntitle={Head{GAP}: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors},\nauthor={Xiaozheng Zheng and Chao Wen and Zhaohu Li and Weiyi Zhang and Zhuo Su and Xu Chang and Yang Zhao and Zheng Lv and Xiaoyuan Zhang and Yongjie Zhang and Guidong Wang and Lan Xu},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ZOVgzgERfS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZOVgzgERfS",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZQntT7oRK9",
        "title": "RISE-SDF: A Relightable Information-Shared Signed Distance Field for Glossy Object Inverse Rendering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Inverse rendering aims to reconstruct the 3D geometry, bidirectional reflectance distribution function (BRDF) parameters, and lighting conditions in a 3D scene from multi-view input images. To address this problem, some recent methods utilize a neural field combined with a physically based rendering model to reconstruct the scene parameters. Although these methods achieve impressive geometry reconstruction for glossy objects, the performance of material estimation and relighting remains limited. In this paper, we propose a novel end-to-end relightable neural inverse rendering system that achieves high-quality reconstruction of geometry and material properties, thus enabling high-quality relighting. The cornerstone of our method is a two-stage approach for learning a better factorization of scene parameters. In the first stage, we develop a reflection-aware radiance field using a neural signed distance field (SDF) as the geometry representation and deploy an MLP (multilayer perceptron) to estimate indirect illumination. In the second stage, we introduce a novel information-sharing network structure to jointly learn the radiance field and the physically based factorization of the scene. For the physically based factorization, to reduce the noise caused by Monte Carlo sampling, we apply a split-sum approximation with a simplified Disney BRDF and cube mipmap as the environment light representation. In the relighting phase, to enhance the quality of indirect illumination, we propose a second split-sum algorithm to trace secondary rays under the split-sum rendering framework. Furthermore, there is no dataset or protocol available to quantitatively evaluate the inverse rendering performance for glossy objects. To assess the quality of material reconstruction and relighting, we have created a new dataset with ground truth BRDF parameters and relighting results. Our experiments demonstrate that our algorithm achieves state-of-the-art performance in inverse rendering and relighting, with particularly strong results in the reconstruction of highly reflective objects.",
        "keywords": "Neural rendering;Reflectance modeling;Mesh geometry models;Neural radiance field;Inverse rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/914c0197e1816c455b3d7a1c5f4bec045c21e662.zip",
        "author": "Deheng Zhang;Jingyu Wang;Shaofei Wang;Marko Mihajlovic;Sergey Prokudin;Hendrik Lensch;Siyu Tang",
        "authorids": "~Deheng_Zhang1;~Jingyu_Wang10;~Shaofei_Wang3;~Marko_Mihajlovic1;~Sergey_Prokudin1;~Hendrik_Lensch2;~Siyu_Tang1",
        "gender": "M;;M;M;M;M;F",
        "homepage": "https://dehezhang2.github.io/;;https://taconite.github.io/;https://markomih.github.io/;https://github.com/sergeyprokudin;https://www.graphics.uni-tuebingen.de;https://vlg.inf.ethz.ch",
        "dblp": "341/0997;;;252/5156;205/4367;99/6552.html;22/845-1",
        "google_scholar": "7pEFc9wAAAAJ;;iz6gEKcAAAAJ;RYicr-QAAAAJ;xSywCzAAAAAJ;https://scholar.google.de/citations?hl=de;BUDh_4wAAAAJ",
        "orcid": "0009-0004-3667-3247;;;0000-0001-6305-3896;;;0000-0002-1015-4770",
        "linkedin": "deheng-zhang-355423178;jingyu-wang-8a3957225/;;marko-mihajlovic/;sergey-prokudin-9bb045a8/;;",
        "or_profile": "~Deheng_Zhang1;~Jingyu_Wang10;~Shaofei_Wang3;~Marko_Mihajlovic1;~Sergey_Prokudin1;~Hendrik_Lensch2;~Siyu_Tang1",
        "aff": "Institute for Computer Science, Artificial Intelligence and Technology;;Department of Computer Science, Swiss Federal Institute of Technology;Swiss Federal Institute of Technology;ETHZ - ETH Zurich;University of T\u00fcbingen;Department of Computer Science, Swiss Federal Institute of Technology",
        "aff_domain": "insait.ai;;inf.ethz.ch;ethz.ch;ethz.ch;uni-tuebingen.de;inf.ethz.ch",
        "position": "PhD student;;PhD student;PhD student;Senior Scientist;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025risesdf,\ntitle={{RISE}-{SDF}: A Relightable Information-Shared Signed Distance Field for Glossy Object Inverse Rendering},\nauthor={Deheng Zhang and Jingyu Wang and Shaofei Wang and Marko Mihajlovic and Sergey Prokudin and Hendrik Lensch and Siyu Tang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ZQntT7oRK9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZQntT7oRK9",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZmZXUtyN1I",
        "title": "MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent works in inverse rendering have shown promise in using multi-view images of an object to recover shape, albedo, and materials. However, the recovered components often fail to render accurately under new lighting conditions due to the intrinsic challenge of disentangling albedo and material properties from input images. To address this challenge, we introduce MaterialFusion, an enhanced conventional 3D inverse rendering pipeline that incorporates a 2D prior on texture and material properties. We present StableMaterial, a 2D diffusion model prior that refines multi-lit data to estimate the most likely albedo and material from given input appearances. This model is trained on albedo, material, and relit image data derived from a curated dataset of approximately ~12K artist-designed synthetic Blender objects called BlenderVault. We incorporate this diffusion prior with an inverse rendering framework where we use score distillation sampling (SDS) to guide the optimization of the albedo and materials, improving relighting performance in comparison with previous work. We validate MaterialFusion's relighting performance on 4 datasets of synthetic and real objects under diverse illumination conditions, showing our diffusion-aided approach significantly improves the appearance of reconstructed objects under novel lighting conditions. We intend to publicly release our BlenderVault dataset to support further research in this field.",
        "keywords": "Differentiable Rendering;Relighting;Texture Synthesis;Neural Rendering;Multi-View & 3D",
        "primary_area": "",
        "supplementary_material": "/attachment/2a9f9d5b561548f5bffec7159a7dcbe93b2251a9.zip",
        "author": "Yehonathan Litman;Or Patashnik;Kangle Deng;Aviral Agrawal;Rushikesh Zawar;Fernando De la Torre;Shubham Tulsiani",
        "authorids": "~Yehonathan_Litman1;~Or_Patashnik1;~Kangle_Deng1;~Aviral_Agrawal1;~Rushikesh_Zawar1;~Fernando_De_la_Torre2;~Shubham_Tulsiani1",
        "gender": ";F;M;M;M;;M",
        "homepage": "https://yehonathanlitman.github.io;https://orpatashnik.github.io/;https://dunbar12138.github.io;https://aviral-agrawal.github.io;;;https://shubhtuls.github.io/",
        "dblp": "257/3809.html;271/8264;246/3131;;;;135/6623",
        "google_scholar": ";-SlS0mgAAAAJ;;NiW49k4AAAAJ;Qaol8LoAAAAJ;;06rffEkAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": "yehonathan-litman-4121a6157/;;;aviral-agrawal-783a01162/;rushikesh-zawar-a67063153/;;",
        "or_profile": "~Yehonathan_Litman1;~Or_Patashnik1;~Kangle_Deng1;~Aviral_Agrawal1;~Rushikesh_Zawar1;~Fernando_De_la_Torre2;~Shubham_Tulsiani1",
        "aff": "Carnegie Mellon University;Tel Aviv University;Carnegie Mellon University;Carnegie Mellon University;Adobe Systems;;Carnegie Mellon University",
        "aff_domain": "cmu.edu;tau.post.ac.il;cmu.edu;andrew.cmu.edu;adobe.com;;cmu.edu",
        "position": "PhD student;PhD student;PhD student;MS student;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\nlitman2025materialfusion,\ntitle={MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors},\nauthor={Yehonathan Litman and Or Patashnik and Kangle Deng and Aviral Agrawal and Rushikesh Zawar and Fernando De la Torre and Shubham Tulsiani},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ZmZXUtyN1I}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZmZXUtyN1I",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ZnzfLRb3TD",
        "title": "HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current advances in human head modeling allow the generation of plausible-looking 3D head models via neural representations, such as NeRFs and SDFs. Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue. Furthermore, completing the head geometry based on a partial observation, e.g., coming from a depth sensor, while preserving a high level of detail is often problematic for the existing methods. We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM, simultaneously allowing explicit animation and high-detail preservation. Our method is trained in two stages. First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans. The estimated displacements are baked into a hand-crafted UV layout. Second, we train a StyleGAN model to generalize over the UV maps of displacements, which we later refer to as HeadCraft. The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify the regions semantically. We demonstrate the results of unconditional sampling, fitting to a scan and editing.",
        "keywords": "meshes;3D representations;human heads;generative modeling;deformation modeling",
        "primary_area": "",
        "supplementary_material": "/attachment/908c52b14d683491541aa017c7e753e2873698c7.zip",
        "author": "Artem Sevastopolsky;Philip-William Grassal;Simon Giebenhain;ShahRukh Athar;Luisa Verdoliva;Matthias Nie\u00dfner",
        "authorids": "~Artem_Sevastopolsky2;~Philip-William_Grassal2;~Simon_Giebenhain1;~ShahRukh_Athar1;~Luisa_Verdoliva1;~Matthias_Nie\u00dfner2",
        "gender": "M;;M;;F;",
        "homepage": "https://seva100.github.io;;https://simongiebenhain.github.io;http://shahrukhathar.github.io/;https://www.grip.unina.it/members/verdoliva;",
        "dblp": "199/2351;;304/8343.html;79/9032;62/92;",
        "google_scholar": "fTSCTYQAAAAJ;;https://scholar.google.com/citations?;mdUv8wcAAAAJ;HItjEd0AAAAJ;",
        "orcid": ";;0000-0002-7588-8767;;0000-0001-7286-7963;",
        "linkedin": ";;;;;",
        "or_profile": "~Artem_Sevastopolsky2;~Philip-William_Grassal2;~Simon_Giebenhain1;~ShahRukh_Athar1;~Luisa_Verdoliva1;~Matthias_Nie\u00dfner2",
        "aff": "Apple+Technische Universit\u00e4t M\u00fcnchen;;Technische Universit\u00e4t M\u00fcnchen;State University of New York, Stony Brook;University of Naples Federico II;",
        "aff_domain": "apple.com+tum.de;;tum.de;stonybrook.edu;unina.it;",
        "position": "Researcher+PhD student;;PhD student;PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nsevastopolsky2025headcraft,\ntitle={HeadCraft: Modeling High-Detail Shape Variations for Animated 3{DMM}s},\nauthor={Artem Sevastopolsky and Philip-William Grassal and Simon Giebenhain and ShahRukh Athar and Luisa Verdoliva and Matthias Nie{\\ss}ner},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ZnzfLRb3TD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZnzfLRb3TD",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "aS7a89jH3w",
        "title": "ViSkin: Physics-based Simulation of Virtual Skin on Personalized Avatars",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce ViSkin, a biomechanically principled approach to simulate skin mechanics on personalized avatars. Our model captures the salient characteristics of human skin, i.e., nonlinear stretching properties, anisotropic stiffness, direction-dependent pre-stretch, and heterogeneous sliding behavior. In particular, we introduce a novel representation of Langer lines, which describe the distribution of principal material directions across the human body. We further propose an optimization-based approach for inferring spatially-varying pre-stretch from motion capture data. We implement our new model using a computationally efficient intrinsic representation that simulates skin as a two-dimensional Lagrangian mesh embedded in the three-dimensional body surface.\nWe demonstrate our method on a diverse set of body models, shapes, and poses and compare to experimentally-obtained skin motion data. Our results indicate that our method produces smoother and more plausible skin deformations than a baseline method and shows good accuracy compared to real-world data.",
        "keywords": "Skin mechanics;Simulation;Personalized Avatars",
        "primary_area": "",
        "supplementary_material": "/attachment/68624662d8694501ccc690e84b83e73e14fa634e.zip",
        "author": "Davide Corigliano;Juan Montes;Ronan Hinchet;Stelian Coros;Bernhard Thomaszewski",
        "authorids": "~Davide_Corigliano1;~Juan_Montes2;~Ronan_Hinchet1;~Stelian_Coros1;~Bernhard_Thomaszewski1",
        "gender": "M;M;Not Specified;M;",
        "homepage": "https://github.com/daviC1999;https://crl.ethz.ch/people/index.html;;http://crl.ethz.ch/index.html;https://n.ethz.ch/~bthomasz/",
        "dblp": ";;227/7987;;",
        "google_scholar": ";;L-mz4LIAAAAJ;sX31JjwAAAAJ;IDw2HJAAAAAJ",
        "orcid": ";;0000-0003-3356-8930;;",
        "linkedin": "davide-cori/;;;;",
        "or_profile": "~Davide_Corigliano1;~Juan_Montes2;~Ronan_Hinchet1;~Stelian_Coros1;~Bernhard_Thomaszewski1",
        "aff": ";ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;Swiss Federal Institute of Technology",
        "aff_domain": ";ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "position": ";Postdoc;Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\ncorigliano2025viskin,\ntitle={ViSkin: Physics-based Simulation of Virtual Skin on Personalized Avatars},\nauthor={Davide Corigliano and Juan Montes and Ronan Hinchet and Stelian Coros and Bernhard Thomaszewski},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=aS7a89jH3w}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aS7a89jH3w",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bVdL6BXRvJ",
        "title": "\u03b1Surf: Implicit Surface Reconstruction for Semi-Transparent and Thin Objects with Decoupled Geometry and Opacity",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Implicit surface representations such as the signed distance function (SDF) have emerged as a promising approach for image-based surface reconstruction. However, existing optimization methods assume opaque surfaces and therefore cannot properly reconstruct translucent surfaces and sub-pixel thin structures, which also exhibit low opacity due to the blending effect. While neural radiance field (NeRF) based methods can model semi-transparency and synthesize novel views with photo-realistic quality, their volumetric representation tightly couples geometry (surface occupancy) and material property (surface opacity), and therefore cannot be easily converted into surfaces without introducing artifacts. We present \u03b1Surf, a novel scene representation with decoupled geometry and opacity for the reconstruction of surfaces with translucent or blending effects. Ray-surface intersections on our representation can be found in closed-form via analytical solutions of cubic polynomials, avoiding Monte-Carlo sampling, and are fully differentiable by construction. Our qualitative and quantitative evaluations show that our approach can accurately reconstruct translucent and extremely thin surfaces, achieving better reconstruction quality than state-of-the-art SDF and NeRF methods.",
        "keywords": "Neural Radiance Field;Surface Reconstruction from RGB",
        "primary_area": "",
        "supplementary_material": "/attachment/0351a08c31828e4ca41535e62a4b1fef3af8f31d.zip",
        "author": "Tianhao Walter Wu;hanxue liang;Fangcheng Zhong;Gernot Riegler;Shimon Vainer;Jiankang Deng;Cengiz Oztireli",
        "authorids": "~Tianhao_Walter_Wu1;~hanxue_liang1;~Fangcheng_Zhong1;~Gernot_Riegler3;~Shimon_Vainer1;~Jiankang_Deng1;~Cengiz_Oztireli1",
        "gender": "M;M;;;M;M;",
        "homepage": "https://chikayan.github.io/;https://hanxuel.github.io/;https://www.cl.cam.ac.uk/~fz261/;;https://www.linkedin.com/in/shimonvainer/;https://jiankangdeng.github.io/;",
        "dblp": "17/1976-3;295/9018;253/0188;;;156/7808;",
        "google_scholar": "HwE5K78AAAAJ;https://scholar.google.com/citations?view_op=list_works;;;;Z_UoQFsAAAAJ;",
        "orcid": "0000-0002-3807-5839;;;;;0000-0002-3709-6216;",
        "linkedin": ";hanxue-charles-liang-78b581177/;fangcheng-zhong-125b9a85/;;shimonvainer?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;jiankang-deng-b45b21b4/?originalSubdomain=uk;",
        "or_profile": "~Tianhao_Walter_Wu1;~hanxue_liang1;~Fangcheng_Zhong1;~Gernot_Riegler3;~Shimon_Vainer1;~Jiankang_Deng1;~Cengiz_Oztireli1",
        "aff": "University of Cambridge;University of Cambridge;University of Cambridge+University of Cambridge;;;Imperial College London;",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk+cam.ac.uk;;;imperial.ac.uk;",
        "position": "PhD student;PhD student;Lecturer+Researcher;;;Assistant Professor;",
        "bibtex": "@inproceedings{\nwu2025surf,\ntitle={\\ensuremath{\\alpha}Surf: Implicit Surface Reconstruction for Semi-Transparent and Thin Objects with Decoupled Geometry and Opacity},\nauthor={Tianhao Walter Wu and hanxue liang and Fangcheng Zhong and Gernot Riegler and Shimon Vainer and Jiankang Deng and Cengiz Oztireli},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=bVdL6BXRvJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bVdL6BXRvJ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "bn3F5vMD9t",
        "title": "GaussianStyle: Gaussian Head Avatar via StyleGAN",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.",
        "keywords": "Gaussian Splatting;Human Head;StyleGAN",
        "primary_area": "",
        "supplementary_material": "/attachment/259815d9a05efa54651df6e36411a3b91d973f07.zip",
        "author": "Pinxin Liu;Luchuan Song;Daoan Zhang;Hang Hua;Yunlong Tang;Huaijin Tu;Jiebo Luo;Chenliang Xu",
        "authorids": "~Pinxin_Liu1;~Luchuan_Song1;~Daoan_Zhang2;~Hang_Hua1;~Yunlong_Tang2;~Huaijin_Tu1;~Jiebo_Luo1;~Chenliang_Xu1",
        "gender": "M;;;M;;M;;M",
        "homepage": "https://andypinxinliu.github.io/;;;https://hanghuacs.owlstown.net/;https://yunlong10.github.io/;;;https://www.cs.rochester.edu/~cxu22/",
        "dblp": ";;;226/9632;174/7086-2;;;117/4770",
        "google_scholar": "ZJQldrQAAAAJ;;;K9aLTwUAAAAJ;xf1rCgoAAAAJ;;;https://scholar.google.com.tw/citations?user=54HfyDIAAAAJ",
        "orcid": "0009-0009-6538-7174;;;;0000-0003-2796-1787;0009-0001-4272-5216;;",
        "linkedin": ";;;;;https://linkedin.com/in/huaijin-tu-912627224/;;",
        "or_profile": "~Pinxin_Liu1;~Luchuan_Song1;~Daoan_Zhang2;~Hang_Hua1;~Yunlong_Tang2;~Huaijin_Tu1;~Jiebo_Luo1;~Chenliang_Xu1",
        "aff": "University of Rochester;;;University of Rochester;University of Rochester;Georgia Institute of Technology;;University of Rochester",
        "aff_domain": "rochester.edu;;;rochester.edu;rochester.edu;gatech.edu;;rochester.edu",
        "position": "PhD student;;;PhD student;PhD student;Undergrad student;;Associate Professor",
        "bibtex": "@inproceedings{\nliu2025gaussianstyle,\ntitle={GaussianStyle: Gaussian Head Avatar via Style{GAN}},\nauthor={Pinxin Liu and Luchuan Song and Daoan Zhang and Hang Hua and Yunlong Tang and Huaijin Tu and Jiebo Luo and Chenliang Xu},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=bn3F5vMD9t}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bn3F5vMD9t",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "c6RR0bqNVI",
        "title": "Efficient Continuous Group Convolutions for Local SE(3) Equivariance in 3D Point Clouds",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Extending the translation equivariance property of convolutional neural networks to larger symmetry groups has been shown to reduce sample complexity and enable more discriminative feature learning. Further, exploiting additional symmetries facilitates greater weight sharing than standard convolutions, leading to an enhanced network expressivity without an increase in parameter count. However, extending the equivariant properties of a convolution layer comes at a computational cost. In particular, for 3D data, expanding equivariance to the SE(3) group (rotation and translation) results in a 6D convolution operation, which is not tractable for larger data samples such as 3D scene scans. While efforts have been made to develop efficient SE(3) equivariant networks, existing approaches rely on discretization or only introduce global rotation equivariance. This limits their applicability to point clouds representing a scene composed of multiple objects. This work presents an efficient, continuous, and local SE(3) equivariant convolution layer for point cloud processing based on general group convolution and local reference frames. Our experiments show that our approach achieves competitive or superior performance across a range of datasets and tasks, including object classification and semantic segmentation, with negligible computational overhead.",
        "keywords": "point clouds;SE(3) equivariance;efficient group convolution",
        "primary_area": "",
        "supplementary_material": "/attachment/03d93d9b39817b29f6fc70c6c377e905bf0239b0.pdf",
        "author": "Lisa Weijler;Pedro Hermosilla",
        "authorids": "~Lisa_Weijler1;~Pedro_Hermosilla1",
        "gender": "F;M",
        "homepage": ";https://phermosilla.github.io/",
        "dblp": "291/8727;170/7065",
        "google_scholar": ";C7F4B6MAAAAJ",
        "orcid": "0000-0003-1660-0329;",
        "linkedin": "lweijler/;",
        "or_profile": "~Lisa_Weijler1;~Pedro_Hermosilla1",
        "aff": "Technische Universit\u00e4t Wien;Technische Universit\u00e4t Wien",
        "aff_domain": "tuwien.ac.at;tuwien.ac.at",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nweijler2025efficient,\ntitle={Efficient Continuous Group Convolutions for Local {SE}(3) Equivariance in 3D Point Clouds},\nauthor={Lisa Weijler and Pedro Hermosilla},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=c6RR0bqNVI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=c6RR0bqNVI",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "cK8Vg7KYOo",
        "title": "Learning Naturally Aggregated Appearance for Efficient 3D Editing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Neural radiance fields, which represent a 3D scene as a color field and a density field, have demonstrated great progress in novel view synthesis yet are unfavorable for editing due to the implicitness. This work studies the task of efficient 3D editing, where we focus on **editing speed** and **user interactivity**. To this end, we propose to learn the color field as an explicit 2D appearance aggregation, also called canonical image, with which users can easily customize their 3D editing via 2D image processing. We complement the canonical image with a projection field that maps 3D points onto 2D pixels for texture query. This field is initialized with a pseudo canonical camera model and optimized with offset regularity to ensure the **naturalness** of the canonical image. Extensive experiments on different datasets suggest that our representation, dubbed ***AGAP***, well supports various ways of 3D editing (*e.g.*, stylization, instance segmentation, and interactive drawing). Our approach demonstrates remarkable efficiency by being at least 20$\\times$ faster per edit compared to existing NeRF-based editing methods. Project page is available at https://felixcheng97.github.io/AGAP/.",
        "keywords": "Appearance Modelling;Vision Application",
        "primary_area": "",
        "supplementary_material": "/attachment/7e03f4db4c3b9a399e74e91be9228b8819c3c221.zip",
        "author": "Ka Leong Cheng;Qiuyu Wang;Zifan Shi;Kecheng Zheng;Yinghao Xu;Hao OUYANG;Qifeng Chen;Yujun Shen",
        "authorids": "~Ka_Leong_Cheng2;~Qiuyu_Wang1;~Zifan_Shi2;~Kecheng_Zheng2;~Yinghao_Xu1;~Hao_OUYANG1;~Qifeng_Chen1;~Yujun_Shen1",
        "gender": ";M;;M;M;M;M;",
        "homepage": ";https://github.com/qiuyu96;;https://zkcys001.github.io/;https://justimyhxu.github.io/;https://ken-ouyang.github.io/;http://cqf.io/;",
        "dblp": ";37/9650;;228/1362;232/2482;234/6249;117/4819;",
        "google_scholar": ";VRsy9v8AAAAJ;;hMDQifQAAAAJ;https://scholar.google.com/citations?hl=en;HupEGRYAAAAJ;lLMX9hcAAAAJ;",
        "orcid": ";;;;;;;",
        "linkedin": ";;;;;ouyang-hao-9a532bb6/;;",
        "or_profile": "~Ka_Leong_Cheng2;~Qiuyu_Wang1;~Zifan_Shi2;~Kecheng_Zheng2;~Yinghao_Xu1;~Hao_OUYANG1;~Qifeng_Chen1;~Yujun_Shen1",
        "aff": ";Ant Group;;Ant Research;Stanford University;Department of Computer Science and Engineering, Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;",
        "aff_domain": ";antgroup.com;;antgroup.com;stanford.edu;cse.ust.hk;hkust.edu;",
        "position": ";Researcher;;Researcher;Postdoc;PhD student;Associate Professor;",
        "bibtex": "@inproceedings{\ncheng2025learning,\ntitle={Learning Naturally Aggregated Appearance for Efficient 3D Editing},\nauthor={Ka Leong Cheng and Qiuyu Wang and Zifan Shi and Kecheng Zheng and Yinghao Xu and Hao OUYANG and Qifeng Chen and Yujun Shen},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=cK8Vg7KYOo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cK8Vg7KYOo",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "d9NjNfoUH5",
        "title": "Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large diffusion models demonstrate remarkable zero-shot capabilities in novel view synthesis from a single image. However, these models often face challenges in maintaining consistency across novel and reference views. A crucial factor leading to this issue is the limited utilization of contextual information from reference views. Specifically, when there is an overlap in the viewing frustum between two views, it is essential to ensure that the corresponding regions maintain consistency in both geometry and appearance. This observation leads to a simple yet effective approach, where we propose to use epipolar geometry to locate and retrieve overlapping information from the input view. This information is then incorporated into the generation of target views, eliminating the need for training or fine-tuning, as the process requires no learnable parameters. Furthermore, to enhance the overall consistency of generated views, we extend the utilization of epipolar attention to a multi-view setting, allowing retrieval of overlapping information from the input view and other target views. Qualitative and quantitative experimental results demonstrate the effectiveness of our method in significantly improving the consistency of synthesized views without the need for any fine-tuning. Moreover, This enhancement also boosts the performance of downstream applications such as 3D reconstruction. The code is available at https://github.com/botaoye/ConsisSyn.",
        "keywords": "Novel View Synthesis;Diffusion;Consistent Generation",
        "primary_area": "",
        "supplementary_material": "/attachment/9a0c8d8bf70022cb8677f41154c92d3bcd23825d.pdf",
        "author": "Botao Ye;Sifei Liu;Xueting Li;Marc Pollefeys;Ming-Hsuan Yang",
        "authorids": "~Botao_Ye1;~Sifei_Liu2;~Xueting_Li2;~Marc_Pollefeys2;~Ming-Hsuan_Yang1",
        "gender": "M;F;;M;M",
        "homepage": ";https://www.sifeiliu.net;;;https://faculty.ucmerced.edu/mhyang/",
        "dblp": "227/4610;118/1301;;p/MarcPollefeys;79/3711.html",
        "google_scholar": "BdIyfRgAAAAJ;j4pcHV4AAAAJ;;YYH0BjEAAAAJ;p9-ohHsAAAAJ",
        "orcid": ";;;;0000-0003-4848-2304",
        "linkedin": ";;;marc-pollefeys-30a7075/;minghsuanyang/",
        "or_profile": "~Botao_Ye1;~Sifei_Liu2;~Xueting_Li2;~Marc_Pollefeys2;~Ming-Hsuan_Yang1",
        "aff": "ETHZ - ETH Zurich;NVIDIA;;Microsoft+ETHZ - ETH Zurich+Swiss Federal Institute of Technology;Google DeepMind+University of California at Merced",
        "aff_domain": "ethz.ch;nvidia.com;;microsoft.com+ethz.ch+ethz.ch;google.com+umcerced.edu",
        "position": "PhD student;Researcher;;Director+Professor+Full Professor;Senior Staff Research Scientist+Professor",
        "bibtex": "@inproceedings{\nye2025synthesizing,\ntitle={Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training},\nauthor={Botao Ye and Sifei Liu and Xueting Li and Marc Pollefeys and Ming-Hsuan Yang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=d9NjNfoUH5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=d9NjNfoUH5",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dLBQkFcDtn",
        "title": "VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Audio-driven talking head generation has drawn much attention in recent years, and many efforts have been made in lip-sync, facial motion, head pose generation, and video quality. However, no model has yet led or tied on all these metrics due to the one-to-many mapping between audio and motion. In this paper, we propose VividTalk, a two-stage generic framework that supports generating high-visual quality talking head videos with all the above properties. Specifically, in the first stage, we map the audio to mesh by learning two motions, including non-rigid facial motion and rigid head motion. For facial motion, both blendshape and vertex are adopted as the intermediate representation to maximize the representation ability of the model. For head motion, a novel learnable head pose codebook with a two-phase training mechanism is proposed. In the second stage, we proposed a dual branch motion-vae and a generator to transform the meshes into dense motion and synthesize high-quality video frame-by-frame. Extensive experiments show that the proposed VividTalk can generate high-visual quality talking head videos with lip-sync and realistic enhanced by a large margin, and outperforms previous state-of-the-art works in objective and subjective comparisons. The code will be publicly released upon publication.",
        "keywords": "Talking Head;Image Animation;Image Synthesis",
        "primary_area": "",
        "supplementary_material": "/attachment/de8763f158d26d5b64386209a63d9cca12f67adf.zip",
        "author": "Xusen Sun;Longhao Zhang;Hao Zhu;Peng Zhang;Bang Zhang;Xinya Ji;Kangneng Zhou;Daiheng Gao;Liefeng Bo;Xun Cao",
        "authorids": "~Xusen_Sun1;~Longhao_Zhang2;~Hao_Zhu5;~Peng_Zhang32;~Bang_Zhang1;~Xinya_Ji1;~Kangneng_Zhou1;~Daiheng_Gao3;~Liefeng_Bo1;~Xun_Cao1",
        "gender": "M;M;M;;M;;M;M;M;M",
        "homepage": ";;http://zhuhao.cc/home/;;https://sites.google.com/view/mattzhang/home;;https://montaellis.github.io;https://tomguluson92.github.io;https://research.cs.washington.edu/istc/lfb/;http://cite.nju.edu.cn",
        "dblp": "308/0824;;10/3520-4;;11/4046;290/1747;257/7652;254/8137;17/6808;78/7658",
        "google_scholar": ";qkJD6c0AAAAJ;YRxe0FkAAAAJ;;;;https://scholar.google.com.hk/citations?user=y1vvxWYAAAAJ;Y-ql3zMAAAAJ;FJwtMf0AAAAJ;8hZIngIAAAAJ",
        "orcid": ";;;;;;;;;",
        "linkedin": ";;;;;theajxy/;;;;",
        "or_profile": "~Xusen_Sun1;~Longhao_Zhang2;~Hao_Zhu5;~Peng_Zhang32;~Bang_Zhang1;~Xinya_Ji1;~Kangneng_Zhou1;~Daiheng_Gao3;~Liefeng_Bo1;~Xun_Cao1",
        "aff": ";ByteDance;Nanjing University;;Alibaba Group;Nanjing University;Nankai University;University of Science and Technology of China;Alibaba Group;Nanjing University",
        "aff_domain": ";bytedance.com;nju.edu.cn;;alibaba-inc.com;nju.edu.cn;nankai.edu.cn;ustc.edu.cn;alibaba-inc.com;nju.edu.cn",
        "position": ";Researcher;Assistant Professor;;Researcher;PhD student;PhD student;Researcher;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nsun2025vividtalk,\ntitle={VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior},\nauthor={Xusen Sun and Longhao Zhang and Hao Zhu and Peng Zhang and Bang Zhang and Xinya Ji and Kangneng Zhou and Daiheng Gao and Liefeng Bo and Xun Cao},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=dLBQkFcDtn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dLBQkFcDtn",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dOpxroaprM",
        "title": "MAC++: Going Further with Maximal Cliques for 3D Registration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Maximal cliques (MAC) represent a novel state-of-the-art approach for 3D registration from correspondences, however, it still suffers from extremely severe outliers. In this paper, we introduce a robust learning-free estimator called MAC++, exploring maximal cliques for 3D registration from the following two perspectives:  1) A novel hypothesis generation method utilizing putative seeds through voting to guide the construction of maximal clique pools, effectively preserving more potential correct hypotheses. 2) A progressive hypothesis evaluation method that continuously reduces the solution space in a ``global-clusters-cluster-individual'' manner rather than traditional one-shot techniques, greatly alleviating the issue of missing good hypotheses. Experiments conducted on U3M, 3DMatch/3DLoMatch, and KITTI-LC datasets show the new state-of-the-art performance of MAC++. MAC++ demonstrates the capability to handle extremely low inlier ratio data where MAC fails (e.g., showing 27.1\\%/30.6\\% registration recall improvements on 3DMatch/3DLoMatch with < 1\\% inliers). The code will be released.",
        "keywords": "Hypothesis generation;Hypothesis evaluation;Maximal clique;3D registration",
        "primary_area": "",
        "supplementary_material": "/attachment/ba892666d72ccf8dfb6853a6d0ea3ea0a3480027.pdf",
        "author": "Xiyu Zhang;Yanning Zhang;Jiaqi Yang",
        "authorids": "~Xiyu_Zhang1;~Yanning_Zhang1;~Jiaqi_Yang5",
        "gender": "M;F;M",
        "homepage": "https://zhangxy0517.github.io;http://teacher.nwpu.edu.cn/ynzhang;https://sites.google.com/view/jiaqiyang",
        "dblp": "236/4688-1;14/6655;131/7234-2",
        "google_scholar": "fbJLEX8AAAAJ;;d6l7980AAAAJ",
        "orcid": "0000-0003-0775-1192;0000-0002-2977-8057;",
        "linkedin": ";;",
        "or_profile": "~Xiyu_Zhang1;~Yanning_Zhang1;~Jiaqi_Yang5",
        "aff": "Northwest Polytechnical University ;Northwestern Polytechnical University;Northwestern Polytechnical University",
        "aff_domain": "nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn",
        "position": "MS student;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025mac,\ntitle={{MAC}++: Going Further with Maximal Cliques for 3D Registration},\nauthor={Xiyu Zhang and Yanning Zhang and Jiaqi Yang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=dOpxroaprM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dOpxroaprM",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "dzeSlsIc8Z",
        "title": "CatFree3D: Category-Agnostic 3D Object Detection With Diffusion",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Image-based 3D object detection is widely employed in applications such as autonomous vehicles and robotics, yet current systems struggle with generalisation due to the complex problem setup and limited training data. We introduce a novel pipeline that decouples 3D detection from 2D detection and depth prediction, using a diffusion-based approach to improve accuracy and support category-agnostic detection. Additionally, we introduce the Normalised Hungarian Distance (NHD) metric for an accurate evaluation of 3D detection results, addressing the limitations of traditional IoU and GIoU metrics. Experimental results demonstrate that our method achieves state-of-the-art accuracy and strong generalisation across various object categories and datasets.",
        "keywords": "Diffusion;3D Object Detection;3D Detection Metric",
        "primary_area": "",
        "supplementary_material": "/attachment/3517476479f6afe30cd43c90ce9cb2c7e62a83c2.pdf",
        "author": "Wenjing Bian;Zirui Wang;Andrea Vedaldi",
        "authorids": "~Wenjing_Bian1;~Zirui_Wang3;~Andrea_Vedaldi1",
        "gender": "F;M;M",
        "homepage": ";;https://www.robots.ox.ac.uk/~vedaldi/",
        "dblp": "296/4123;;99/2825",
        "google_scholar": "IVfbqkgAAAAJ;zCBKqa8AAAAJ;bRT7t28AAAAJ",
        "orcid": ";;0000-0003-1374-2858",
        "linkedin": ";;",
        "or_profile": "~Wenjing_Bian1;~Zirui_Wang3;~Andrea_Vedaldi1",
        "aff": "University of Oxford;University of Oxford;University of Oxford+Meta",
        "aff_domain": "ox.ac.uk;ox.ac.uk;ox.ac.uk+meta.com",
        "position": "PhD student;Postdoc;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nbian2025catfreed,\ntitle={CatFree3D: Category-Agnostic 3D Object Detection With Diffusion},\nauthor={Wenjing Bian and Zirui Wang and Andrea Vedaldi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=dzeSlsIc8Z}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dzeSlsIc8Z",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "eBXCDv8Doi",
        "title": "Maps from Motion (MfM): Generating 2D Semantic Maps from Sparse Multi-view Images",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "World-wide detailed 2D maps require enormous collective efforts. OpenStreetMap is the result of 11 million registered users manually annotating the GPS location of over 1.75 billion entries, including distinctive landmarks and common urban objects. At the same time, manual annotations can include errors and are slow to update, limiting the map's accuracy. Maps from Motion (MfM) is a step forward to automatize such time-consuming map making procedure by computing 2D maps of semantic objects directly from a collection of uncalibrated multi-view images. From each image, we extract a set of object detections, and estimate their spatial arrangement in a top-down local map centered in the reference frame of the camera that captured the image.\nAligning these local maps is not a trivial problem, since they provide incomplete, noisy fragments of the scene, and matching detections across them is unreliable because of the presence of repeated pattern and the limited appearance variability of urban objects. We address this with a novel graph-based framework, that encodes the spatial and semantic distribution of the objects detected in each image, and learns how to combine them to predict the objects' poses in a global reference system, while taking into account all possible detection matches and preserving the topology observed in each image. \nDespite the complexity of the problem, our best model achieves global 2D registration with an average accuracy within $4$ meters (i.e., below GPS accuracy) even on sparse sequences with strong viewpoint change, on which COLMAP has an $80\\%$ failure rate. \nWe provide extensive evaluation on synthetic and real-world data, showing how the method obtains a solution even in scenarios where standard optimization techniques fail. Find more information at matteot90.github.io/MapsFromMotion .",
        "keywords": "2D scene layout;Multi-view geometry;Object localization;Semantic maps",
        "primary_area": "",
        "supplementary_material": "/attachment/142198449781b342bb3939f33deb2bcfce136346.pdf",
        "author": "Matteo Toso;Stefano Fiorini;Stuart James;Alessio Del Bue",
        "authorids": "~Matteo_Toso1;~Stefano_Fiorini1;~Stuart_James1;~Alessio_Del_Bue2",
        "gender": "M;M;M;",
        "homepage": "https://www.iit.it/people-details/-/people/matteo-toso;;https://stuart-james.com;",
        "dblp": "225/4596;210/9032;19/10673;",
        "google_scholar": "xGqLC_MAAAAJ;2O-BN9YAAAAJ;VbRT0CwAAAAJ;",
        "orcid": "0000-0002-8990-7156;0000-0001-5432-7584;0000-0002-2649-2133;",
        "linkedin": "matteo-toso-71525b252/;;stuartajames/;",
        "or_profile": "~Matteo_Toso1;~Stefano_Fiorini1;~Stuart_James1;~Alessio_Del_Bue2",
        "aff": "Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;Durham University;",
        "aff_domain": "iit.it;iit.it;durham.ac.uk;",
        "position": "Postdoc;Postdoc;Assistant Professor;",
        "bibtex": "@inproceedings{\ntoso2025maps,\ntitle={Maps from Motion (MfM): Generating 2D Semantic Maps from Sparse Multi-view Images},\nauthor={Matteo Toso and Stefano Fiorini and Stuart James and Alessio Del Bue},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=eBXCDv8Doi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eBXCDv8Doi",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fTJrKaBKZk",
        "title": "AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools.",
        "keywords": "3D reconstruction;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/7bbf163b96dc2120d07f0723a1f152ab2ad21609.pdf",
        "author": "Xuqian Ren;Matias Turkulainen;Jiepeng Wang;Otto Seiskari;Iaroslav Melekhov;Juho Kannala;Esa Rahtu",
        "authorids": "~Xuqian_Ren1;~Matias_Turkulainen1;~Jiepeng_Wang1;~Otto_Seiskari1;~Iaroslav_Melekhov1;~Juho_Kannala5;~Esa_Rahtu1",
        "gender": "F;M;M;M;;M;",
        "homepage": "https://xuqianren.github.io;https://maturk.github.io;https://jiepengwang.github.io/;https://oseiskar.github.io/;https://imelekhov.com;https://users.aalto.fi/~kannalj1/;",
        "dblp": "299/7813;;227/6791;122/6326.html;195/5718;47/4656.html;",
        "google_scholar": "1j8cfOQAAAAJ;9ixpc8MAAAAJ;AoIVprMAAAAJ;6fr78PEAAAAJ;BXNprrsAAAAJ;c4mWQPQAAAAJ;",
        "orcid": "0000-0002-3811-0235;0009-0007-6931-2386;;0000-0001-6207-5671;;0000-0001-5088-4041;",
        "linkedin": ";;;otto-seiskari/;imelekhov/;;",
        "or_profile": "~Xuqian_Ren1;~Matias_Turkulainen1;~Jiepeng_Wang1;~Otto_Seiskari1;~Iaroslav_Melekhov1;~Juho_Kannala5;~Esa_Rahtu1",
        "aff": "Tampere University;Aalto University;China Telecom;Spectacular AI;Aalto University;Aalto University;",
        "aff_domain": "tuni.fi;aalto.fi;chinatelecom.cn;spectacularai.com;aalto.fi;aalto.fi;",
        "position": "PhD student;PhD student;Researcher;Researcher;Postdoc;Associate Professor;",
        "bibtex": "@inproceedings{\nren2025agsmesh,\ntitle={{AGS}-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones},\nauthor={Xuqian Ren and Matias Turkulainen and Jiepeng Wang and Otto Seiskari and Iaroslav Melekhov and Juho Kannala and Esa Rahtu},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=fTJrKaBKZk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fTJrKaBKZk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fTbOXM9uuV",
        "title": "Particle Rendering: Implicitly Aggregating Incident and Outgoing Light Fields for Novel View Synthesis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents Particle Rendering (PR), a new implicit rendering approach that extends Neural Radiance Fields (NeRF) by incorporating incident light along with traditional outgoing light modeling. In our framework, a 3D scene consists of a mass of particles, each offering a deeper understanding of light interactions by reflecting and emitting light in all directions. Our methodology involves a three-phase training pipeline: 1) Estimating the outgoing light field through a NeRF model; 2) Distilling the incident light field. A simple metric is introduced to assess the quality of the ray for better supervision; 3) Implicit rendering. We propose an implicit method to aggregate incident and outgoing fields that leverages Multilayer Perceptrons (MLP) to directly infer final pixel values, thus avoiding the limitation of traditional physically-based rendering techniques. The effectiveness of PR is demonstrated through state-of-the-art results in various challenging indoor and outdoor scenes, emphasizing its capability to handle complex lighting and reflective materials.",
        "keywords": "Neural Radiance Fields;Implicit Rendering;Lighting",
        "primary_area": "",
        "supplementary_material": "/attachment/4481e06654d0cdcbdf66cebeab04e8c19bcbb587.pdf",
        "author": "Tao Hu;Zhiwen Yan;Xiaogang Xu;Gim Hee Lee",
        "authorids": "~Tao_Hu1;~Zhiwen_Yan1;~Xiaogang_Xu2;~Gim_Hee_Lee1",
        "gender": "M;M;M;",
        "homepage": "https://tau-yihouxiang.github.io;;https://xiaogang00.github.io;https://www.comp.nus.edu.sg/~leegh/",
        "dblp": ";;118/2268-2;49/9455",
        "google_scholar": "xXUg31EAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.hk/citations?user=R65xDQwAAAAJ;https://scholar.google.com.sg/citations?user=7hNKrPsAAAAJ",
        "orcid": "0000-0001-6978-6994;;0000-0002-7928-7336;0000-0002-1583-0475",
        "linkedin": ";;;",
        "or_profile": "~Tao_Hu1;~Zhiwen_Yan1;~Xiaogang_Xu2;~Gim_Hee_Lee1",
        "aff": "National University of Singapore;national university of singaore, National University of Singapore;The Chinese University of Hong Kong;National University of Singapore",
        "aff_domain": "nus.edu.sg;u.nus.edu;cuhk.edu.hk;nus.edu.sg",
        "position": "Researcher;PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nhu2025particle,\ntitle={Particle Rendering: Implicitly Aggregating Incident and Outgoing Light Fields for Novel View Synthesis},\nauthor={Tao Hu and Zhiwen Yan and Xiaogang Xu and Gim Hee Lee},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=fTbOXM9uuV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fTbOXM9uuV",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "fWGjDx8PGx",
        "title": "Drivable 3D Gaussian Avatars",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present Drivable 3D Gaussian Avatars (D3GA), a multi-layered 3D controllable model for human bodies that utilizes 3D Gaussian primitives embedded into tetrahedral cages. The advantage of using cages compared to commonly employed linear blend skinning (LBS) is that primitives like 3D Gaussians are naturally re-oriented and their kernels are stretched via the deformation gradients of the encapsulating tetrahedron. Additional offsets are modeled for the tetrahedron vertices, effectively decoupling the low-dimensional driving poses from the extensive set of primitives to be rendered. This separation is achieved through the localized influence of each tetrahedron on 3D Gaussians, resulting in improved optimization. Using the cage-based deformation model, we introduce a compositional pipeline that decomposes an avatar into layers, such as garments, hands, or faces, improving the modeling of phenomena like garment sliding. These parts can be conditioned on different driving signals, such as keypoints for facial expressions or joint-angle vectors for garments and the body. Our experiments on two multi-view datasets with varied body shapes, clothes, and motions show higher-quality results. They surpass PSNR and SSIM metrics of other SOTA methods using the same data while offering greater flexibility and compactness.",
        "keywords": "Humans;face;body;pose;gesture;movement",
        "primary_area": "",
        "supplementary_material": "/attachment/b2bcaee23f9f6fe97d4f5f53cfff2e878976d4e4.zip",
        "author": "Wojciech Zielonka;Timur Bagautdinov;Shunsuke Saito;Michael Zollh\u00f6fer;Justus Thies;Javier Romero",
        "authorids": "~Wojciech_Zielonka1;~Timur_Bagautdinov1;~Shunsuke_Saito1;~Michael_Zollh\u00f6fer2;~Justus_Thies1;~Javier_Romero1",
        "gender": "M;M;M;;M;M",
        "homepage": "https://zielon.github.io/;;http://www-scf.usc.edu/~saitos/;;https://justusthies.github.io/;https://ps.is.tuebingen.mpg.de/person/jromero",
        "dblp": "84/6152;145/3196;21/5061;;145/9981;16/5949-2",
        "google_scholar": "aITcRswAAAAJ;oLi7xJ0AAAAJ;IolN_okAAAAJ;;;https://scholar.google.de/citations?user=Wx62iOsAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";bagautdinov/;;;;javier-romero-38b87331/",
        "or_profile": "~Wojciech_Zielonka1;~Timur_Bagautdinov1;~Shunsuke_Saito1;~Michael_Zollh\u00f6fer2;~Justus_Thies1;~Javier_Romero1",
        "aff": "Technische Universit\u00e4t Darmstadt;Reality Labs Research;Codec Avatars Lab;;Technische Universit\u00e4t Darmstadt;Meta",
        "aff_domain": "tu-darmstadt.de;meta.com;meta.com;;tu-darmstadt.de;meta.com",
        "position": "PhD student;Researcher;Researcher;;Full Professor;Researcher",
        "bibtex": "@inproceedings{\nzielonka2025drivable,\ntitle={Drivable 3D Gaussian Avatars},\nauthor={Wojciech Zielonka and Timur Bagautdinov and Shunsuke Saito and Michael Zollh{\\\"o}fer and Justus Thies and Javier Romero},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=fWGjDx8PGx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fWGjDx8PGx",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "grImaQQkFx",
        "title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models. Current methods generate scenes by iteratively stitching newly generated images with existing geometry, using pre-trained monocular depth estimators to lift the generated images to 3D. The predicted depth is fused with the existing scene representation through various alignment operations. In this work, we make two fundamental contributions to the field of 3D scene generation. First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene, thus prompting the need for alignment. We introduce a depth completion model to directly learn the 3D fusion process, resulting in improved geometric coherence of generated scenes. Second, we introduce a new benchmark to evaluate the geometric accuracy of scene generation methods. We show that the commonly used CLIP score between scene prompts and images is unsuitable for measuring the geometric quality of a scene and introduce a depth-based metric. Our benchmark thus offers an additional dimension to gauge the quality of generated scenes.",
        "keywords": "scene generation;depth inpainting",
        "primary_area": "",
        "supplementary_material": "/attachment/3506693217483faa97681e9a0bb730f596d4e894.zip",
        "author": "Paul Engstler;Andrea Vedaldi;Iro Laina;Christian Rupprecht",
        "authorids": "~Paul_Engstler1;~Andrea_Vedaldi1;~Iro_Laina1;~Christian_Rupprecht1",
        "gender": ";M;;M",
        "homepage": ";https://www.robots.ox.ac.uk/~vedaldi/;;http://chrirupp.github.io",
        "dblp": "317/6294;99/2825;;https://dblp.uni-trier.de/pid/76/744-1",
        "google_scholar": "ILc85z0AAAAJ;bRT7t28AAAAJ;;https://scholar.google.de/citations?user=IrYlproAAAAJ",
        "orcid": ";0000-0003-1374-2858;;",
        "linkedin": ";;;",
        "or_profile": "~Paul_Engstler1;~Andrea_Vedaldi1;~Iro_Laina1;~Christian_Rupprecht1",
        "aff": "University of Oxford;University of Oxford+Meta;;University of Oxford",
        "aff_domain": "ox.ac.uk;ox.ac.uk+meta.com;;ox.ac.uk",
        "position": "PhD student;Full Professor+Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\nengstler2025invisible,\ntitle={Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting},\nauthor={Paul Engstler and Andrea Vedaldi and Iro Laina and Christian Rupprecht},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=grImaQQkFx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=grImaQQkFx",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iKdpFUWltI",
        "title": "Pushing the Limits of LiDAR: Accurate Performance Analysis of Indoor 3D LiDARs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Light Detection and Ranging (LiDAR) technology has become crucial in robotics and autonomous systems for generating precise 3D environmental representations. However, challenges persist in achieving high accuracy and precision, especially in indoor environments. This paper rigorously analyzes the performance of various indoor LiDAR systems under different conditions. We present a novel experimental methodology, which quantifies LiDAR accuracy and precision, examining factors such as sensor type, environmental conditions, and target characteristics. Using an extensive dataset collected from nine distinct locations with more than 36000 LiDAR scans, combined with high-precision reference data from a FARO laser scanner, our analysis reveals significant insights into the accuracy and precision across different LiDAR models. The resulting public dataset, which include detailed point clouds and groundtruth labels, are expected to serve as a valuable resource for developing and validating advanced LiDAR processing techniques and benchmarks for various applications. The dataset will be publicly available at http://lidaraccuracy.github.io.",
        "keywords": "LiDAR Performance Evaluation;Indoor 3D mapping;Point cloud processing;Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/6dc1745344fabc41c5059f38a160d741fa15e360.zip",
        "author": "Xiting Zhao;S\u00f6ren Schwertfeger",
        "authorids": "~Xiting_Zhao1;~S\u00f6ren_Schwertfeger1",
        "gender": "M;",
        "homepage": "https://robotics.shanghaitech.edu.cn/people/zhxt;",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Xiting_Zhao1;~S\u00f6ren_Schwertfeger1",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\nzhao2025pushing,\ntitle={Pushing the Limits of Li{DAR}: Accurate Performance Analysis of Indoor 3D Li{DAR}s},\nauthor={Xiting Zhao and S{\\\"o}ren Schwertfeger},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=iKdpFUWltI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iKdpFUWltI",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "iu4MvXxnCz",
        "title": "Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As humans move around, performing their daily tasks, they are able to recall where they have positioned objects in their environment, even if these objects are currently out of their sight. In this paper, we aim to mimic this spatial cognition ability. We thus formulate the task of *Out of Sight, Not Out of Mind* -- 3D tracking active objects using observations captured through an egocentric camera. We introduce a  simple but effective approach to address this challenging problem, called Lift, Match, and Keep (LMK). LMK **lifts** partial 2D observations to 3D world coordinates, **matches** them over time using visual appearance, 3D location and interactions to form object tracks, and **keeps** these object tracks even when they go out-of-view of the camera.\nWe benchmark LMK on 100 long videos from EPIC-KITCHENS. Our results demonstrate that spatial cognition is critical for correctly locating objects over short and long time scales. E.g., for one long egocentric video, we estimate the 3D location of 50 active objects. \nAfter 120 seconds, 57\\% of the objects are correctly localized by LMK, compared to just 33\\% by a recent 3D method for egocentric videos and 17\\% by a general 2D tracking method.",
        "keywords": "Egocentric Video;3D Understanding",
        "primary_area": "",
        "supplementary_material": "/attachment/bf2e13bbc69d4aedce423bbb5ade74a3524e75b4.pdf",
        "author": "Chiara Plizzari;Shubham Goel;Toby Perrett;Jacob Chalk;Angjoo Kanazawa;Dima Damen",
        "authorids": "~Chiara_Plizzari1;~Shubham_Goel1;~Toby_Perrett1;~Jacob_Chalk1;~Angjoo_Kanazawa1;~Dima_Damen1",
        "gender": "F;;;M;F;F",
        "homepage": ";;;https://jacobchalk.github.io/;https://people.eecs.berkeley.edu/~kanazawa/;http://dimadamen.github.io/",
        "dblp": "272/9085;;173/5116;339/6566;119/1305;95/3618",
        "google_scholar": "OlK2hyIAAAAJ;;eWFLwEUAAAAJ;FPugmicAAAAJ;Ci-_QYIAAAAJ;https://scholar.google.co.uk/citations?user=OxL9Wn8AAAAJ",
        "orcid": "0000-0003-4984-7432;;;0000-0002-9751-8660;;0000-0001-8804-6238",
        "linkedin": "chiara-plizzari-395032183/?locale=en_US;;;jacob-chalk/;;dimadamen",
        "or_profile": "~Chiara_Plizzari1;~Shubham_Goel1;~Toby_Perrett1;~Jacob_Chalk1;~Angjoo_Kanazawa1;~Dima_Damen1",
        "aff": "Polytechnic Institute of Milan;;University of Bristol;University of Bristol;University of California, Berkeley;Google DeepMind+University of Bristol",
        "aff_domain": "polimi.it;;bristol.ac.uk;bristol.ac.uk;berkeley.edu;google.com+bristol.ac.uk",
        "position": "Postdoc;;Postdoc;PhD student;Assistant Professor;Researcher+Full Professor",
        "bibtex": "@inproceedings{\nplizzari2025spatial,\ntitle={Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind},\nauthor={Chiara Plizzari and Shubham Goel and Toby Perrett and Jacob Chalk and Angjoo Kanazawa and Dima Damen},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=iu4MvXxnCz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iu4MvXxnCz",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "j6AIRRxwbU",
        "title": "DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Human Mesh Recovery (HMR) is an important yet challenging problem with applications across various domains including motion capture, augmented reality, and biomechanics. Accurately predicting human pose parameters from a single image remains a challenging 3D computer vision task. In this work, we introduce DeforHMR, a novel regression-based monocular HMR framework designed to enhance the prediction of human pose parameters using deformable attention transformers. DeforHMR leverages a novel query-agnostic deformable cross-attention mechanism within the transformer decoder to effectively regress the visual features extracted from a frozen pretrained vision transformer (ViT) encoder. The proposed deformable cross-attention mechanism allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner. Equipped with a transformer decoder capable of spatially-nuanced attention, DeforHMR achieves state-of-the-art performance for single-frame regression-based methods on the widely used 3D HMR benchmarks 3DPW and RICH. By pushing the boundary on the field of 3D human mesh recovery through deformable attention, we introduce an new, effective paradigm for decoding local spatial information from large pretrained vision encoders in computer vision.",
        "keywords": "HMR;Computer Vision;Machine Learning;Pose Estimation;Transformer;Deformable Attention;3D",
        "primary_area": "",
        "supplementary_material": "/attachment/1aa3d164424a48c16d46427dd8b8064f7aa79f20.zip",
        "author": "Jaewoo Heo;George Hu;Zeyu Wang;Serena Yeung-Levy",
        "authorids": "~Jaewoo_Heo1;~George_Hu1;~Zeyu_Wang1;~Serena_Yeung-Levy1",
        "gender": "M;;;",
        "homepage": ";;;",
        "dblp": ";;132/7882-4.html;",
        "google_scholar": "uAmuZXkAAAAJ;7R7Qra4AAAAJ;https://scholar.google.com/citations?hl=en;",
        "orcid": ";;0000-0002-7057-1613;",
        "linkedin": "jaewoo-jeffrey-h/;;;",
        "or_profile": "~Jaewoo_Heo1;~George_Hu1;~Zeyu_Wang1;~Serena_Yeung-Levy1",
        "aff": "Computer Science Department, Stanford University+Stanford University;;Stanford University;",
        "aff_domain": "cs.stanford.edu+stanford.edu;;stanford.edu;",
        "position": "Intern+Undergrad student;;Postdoc;",
        "bibtex": "@inproceedings{\nheo2025deforhmr,\ntitle={Defor{HMR}: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery},\nauthor={Jaewoo Heo and George Hu and Zeyu Wang and Serena Yeung-Levy},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=j6AIRRxwbU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=j6AIRRxwbU",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "jHF0Xp9GVu",
        "title": "ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Implicit neural fields have made remarkable progress in reconstructing 3D surfaces from multiple images; however, they encounter challenges when it comes to separating individual objects within a scene. Previous approaches to this problem require ground-truth segmentation masks and introduce floating artifacts in occluded parts of the scene. \n\nWe address these challenges with ObjectCarver. ObjectCarver requires no ground-truth segmentation; all it needs is just a few user clicks in a single view. ObjectCarver also introduces a new loss function that prevents floaters and avoids inappropriate carving-out due to occlusion. \n\nFinally, ObjectCarver uses a simple initialization technique that significantly speeds up the process while preserving geometric details. We demonstrate qualitatively and quantitatively on multiple datasets (including a new dataset and benchmark with complete ground-truth) that ObjectCarver produces more accurate reconstructions of each object while minimizing artifacts",
        "keywords": "3D reconstruction;Scene decomposition;Mask propagation;Occlusion;Dataset",
        "primary_area": "",
        "supplementary_material": "/attachment/e81aa4b8d69c8fe2548d88c7ea0dc5fbe8b7fb2b.zip",
        "author": "Gemmechu Hassena;Jonathan Hyun Moon;Ryan M. Fujii;Andrew Yuen;Noah Snavely;Steve Marschner;Bharath Hariharan",
        "authorids": "~Gemmechu_Hassena1;~Jonathan_Hyun_Moon1;~Ryan_M._Fujii1;~Andrew_Yuen1;~Noah_Snavely1;~Steve_Marschner1;~Bharath_Hariharan3",
        "gender": "M;M;M;M;M;M;",
        "homepage": "https://gemmechu.github.io;;;;http://www.cs.cornell.edu/~snavely/;http://www.cs.cornell.edu/~srm/;",
        "dblp": ";;;;33/4636;;",
        "google_scholar": ";27-abyoAAAAJ;;;Db4BCX8AAAAJ;https://scholar.google.com.tw/citations?user=llo3F3QAAAAJ;",
        "orcid": "0009-0001-6294-618X;0009-0001-7873-4552;;;;;",
        "linkedin": ";jmoon0714/;ryan-fujii-668ba0277/;andrew-yuen-7132622b7/;;;",
        "or_profile": "~Gemmechu_Hassena1;~Jonathan_Hyun_Moon1;~Ryan_M._Fujii1;~Andrew_Yuen1;~Noah_Snavely1;~Steve_Marschner1;~Bharath_Hariharan3",
        "aff": "Cornell University;;Cornell University;Cornell University;Cornell University+Google;Department of Computer Science, Cornell University;",
        "aff_domain": "cornell.edu;;cornell.edu;cornell.edu;cornell.edu+google.com;cs.cornell.edu;",
        "position": "PhD student;;Undergrad student;Undergrad student;Professor+Researcher;Full Professor;",
        "bibtex": "@inproceedings{\nhassena2025objectcarver,\ntitle={ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects},\nauthor={Gemmechu Hassena and Jonathan Hyun Moon and Ryan M. Fujii and Andrew Yuen and Noah Snavely and Steve Marschner and Bharath Hariharan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=jHF0Xp9GVu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jHF0Xp9GVu",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "k0GvDVgCdo",
        "title": "Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Gaussian Garments, a novel approach for reconstructing realistic-looking, simulation-ready, garments from multi-view videos. \nOur method represents garments with a combination of a 3D mesh and a Gaussian texture that encodes both the color and high-frequency surface texture.\nWith this, it is able to model rich textures and surface detail including complex materials such as fur.\nAs part of our reconstruction process, we demonstrate how such a representation can be used to register a 3D mesh to multi-view videos.\nOur reconstructed Gaussian Garments can be animated on unseen body shapes and motions using physics-based simulation. \nWe show how a learned GNN-based physical simulator can be used to optimize the garment's material parameters to match its real behavior. \nFinally, we devise a procedure for automatically ordering virtual garments, allowing us to combine the captured individual garments into multi-garment outfits.",
        "keywords": "Gaussian Splatting;Garment Reconstruction;Cloth Simulation;Multi-view videos",
        "primary_area": "",
        "supplementary_material": "/attachment/1bc718c8378faa5f9b1a68d1c12b30a2a685a1c4.zip",
        "author": "Boxiang Rong;Artur Grigorev;Wenbo Wang;Michael J. Black;Bernhard Thomaszewski;Christina Tsalicoglou;Otmar Hilliges",
        "authorids": "~Boxiang_Rong1;~Artur_Grigorev1;~Wenbo_Wang5;~Michael_J._Black1;~Bernhard_Thomaszewski1;~Christina_Tsalicoglou1;~Otmar_Hilliges1",
        "gender": "M;M;M;;;F;M",
        "homepage": "https://ribosome-rbx.github.io/;;https://wenbwa.github.io;;https://n.ethz.ch/~bthomasz/;;https://ait.ethz.ch/people/hilliges/",
        "dblp": ";230/8122;;;;330/0134;82/2289",
        "google_scholar": ";https://scholar.google.ru/citations?user=xJP6avcAAAAJ;uhoiaBsAAAAJ;;IDw2HJAAAAAJ;7D10QQkAAAAJ;-epU9OsAAAAJ",
        "orcid": ";;;;;;0000-0002-5068-3474",
        "linkedin": "boxiang-rong/;;wenbo-wang-usyd/;;;christina-tsalicoglou/;",
        "or_profile": "~Boxiang_Rong1;~Artur_Grigorev1;~Wenbo_Wang5;~Michael_J._Black1;~Bernhard_Thomaszewski1;~Christina_Tsalicoglou1;~Otmar_Hilliges1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;University of Sydney;;Swiss Federal Institute of Technology;Google;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;usyd.edu.au;;ethz.ch;google.com;ethz.ch",
        "position": "MS student;PhD student;PhD student;;Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nrong2025gaussian,\ntitle={Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video},\nauthor={Boxiang Rong and Artur Grigorev and Wenbo Wang and Michael J. Black and Bernhard Thomaszewski and Christina Tsalicoglou and Otmar Hilliges},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=k0GvDVgCdo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=k0GvDVgCdo",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kgNkf7YSSM",
        "title": "Robustifying Point Cloud Networks by Refocusing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The ability to cope with out-of-distribution (OOD) corruptions and adversarial attacks is crucial in real-world safety-demanding applications. In this study, we develop a general mechanism to increase point clouds neural networks robustness based on focus analysis.\nRecent studies have revealed the phenomenon of \\textit{Overfocusing}, which leads to a performance drop. When the network is primarily influenced by small input regions, it becomes less robust and prone to misclassify under noise and corruptions.\nHowever, quantifying overfocusing is still vague and lacks clear definitions. Here, we provide a mathematical definition of \\textbf{focus}, \\textbf{overfocusing} and \\textbf{underfocusing}. The notions are general, but in this study, we specifically investigate the case of 3D point clouds.\nWe observe that corrupted sets result in a biased focus distribution compared to the clean training set.\nWe show that as focus distribution deviates from the one learned in the training phase - classification performance deteriorates.\nWe thus propose a parameter-free \\textbf{refocusing} algorithm that aims to unify all corruptions under the same distribution.\nWe validate our findings on a 3D zero-shot classification task, achieving SOTA in robust 3D classification on ModelNet-C dataset, and in adversarial defense against Shape-Invariant attack.",
        "keywords": "robustness;robust point-cloud classification;adversarial defense",
        "primary_area": "",
        "supplementary_material": "/attachment/edf815a65a8b3c10182005e03686088b9ceee0f0.pdf",
        "author": "Meir Yossef Levi;Guy Gilboa",
        "authorids": "~Meir_Yossef_Levi1;~Guy_Gilboa1",
        "gender": "M;M",
        "homepage": ";https://guygilboa.net.technion.ac.il/",
        "dblp": ";44/4220",
        "google_scholar": ";tH2_FEoAAAAJ",
        "orcid": ";0000-0001-8609-8253",
        "linkedin": "yossi-levi-a3755a190/?originalSubdomain=il;",
        "or_profile": "~Meir_Yossef_Levi1;~Guy_Gilboa1",
        "aff": "Technion - Israel Institute of Technology;Technion - Israel Institute of Technology, Technion",
        "aff_domain": "campus.technion.ac.il;technion.ac.il",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nlevi2025robustifying,\ntitle={Robustifying Point Cloud Networks by Refocusing},\nauthor={Meir Yossef Levi and Guy Gilboa},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=kgNkf7YSSM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kgNkf7YSSM",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kqhGAeCbKp",
        "title": "JADE: Joint-aware Latent Diffusion for 3D Human Generative Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Generative modeling of 3D human bodies have been studied extensively in computer vision. The core is to design a compact latent representation that is both expressive and semantically interpretable, yet existing approaches struggle to achieve both requirements. In this work, we introduce JADE, a generative framework that learns the variations of human shapes with fined-grained control. Our key insight is a joint-aware latent representation that decomposes human bodies into skeleton structures, modeled by joint positions, and local surface geometries, characterized by features attached to each joint. This disentangled latent space design enables geometric and semantic interpretation, facilitating users with flexible controllability. To generate coherent and plausible human shapes under our proposed decomposition, we also present a cascaded pipeline where two diffusions are employed to model the distribution of skeleton structures and local surface geometries respectively. Extensive experiments are conducted on public datasets, where we demonstrate the effectiveness of JADE framework in multiple tasks in terms of autoencoding reconstruction accuracy, editing controllability and generation quality compared with existing methods.",
        "keywords": "Human shape reconstruction;3D modeling;Diffusion",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haorui Ji;Rong Wang;Tao Jun Lin;Hongdong Li",
        "authorids": "~Haorui_Ji2;~Rong_Wang3;~Tao_Jun_Lin1;~Hongdong_Li1",
        "gender": "M;M;;M",
        "homepage": "https://www.researchgate.net/profile/Haorui-Ji;;;http://users.cecs.anu.edu.au/~hongdong/",
        "dblp": ";66/4610-2.html;;59/4859.html",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?hl=en",
        "orcid": ";0000-0002-1905-3175;;",
        "linkedin": ";;;",
        "or_profile": "~Haorui_Ji2;~Rong_Wang3;~Tao_Jun_Lin1;~Hongdong_Li1",
        "aff": "Australian National University;Australian National University;;Australian National University",
        "aff_domain": "anu.edu.au;anu.edu.au;;anu.edu.au",
        "position": "PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nji2025jade,\ntitle={{JADE}: Joint-aware Latent Diffusion for 3D Human Generative Modeling},\nauthor={Haorui Ji and Rong Wang and Tao Jun Lin and Hongdong Li},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=kqhGAeCbKp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kqhGAeCbKp",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "kryphH8cJP",
        "title": "SPAFormer: Sequential 3D Part Assembly with Transformers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce SPAFormer, an innovative model designed to overcome the combinatorial explosion challenge in the 3D Part Assembly (3D-PA) task. This task requires accurate prediction of each part's poses in sequential steps. As the number of parts increases, the possible assembly combinations increase exponentially, leading to a combinatorial explosion that severely hinders the efficacy of 3D-PA. SPAFormer addresses this problem by leveraging weak constraints from assembly sequences, effectively reducing the solution space's complexity. Since the sequence of parts conveys construction rules similar to sentences structured through words, our model explores both parallel and autoregressive generation. We further strengthen SPAFormer through knowledge enhancement strategies that utilize the attributes of parts and their sequence information, enabling it to capture the inherent assembly pattern and relationships among sequentially ordered parts. We also construct a more challenging benchmark named PartNet-Assembly covering 21 varied categories to more comprehensively validate the effectiveness of SPAFormer. Extensive experiments demonstrate the superior generalization capabilities of SPAFormer, particularly with multi-tasking and in scenarios requiring long-horizon assembly. Code is available at https://github.com/xuboshen/SPAFormer.",
        "keywords": "object assembly;3D point cloud;transformers",
        "primary_area": "",
        "supplementary_material": "/attachment/33ef1a10770dd02b1357469ce8214bd82e6cc85d.pdf",
        "author": "Boshen Xu;Sipeng Zheng;Qin Jin",
        "authorids": "~Boshen_Xu1;~Sipeng_Zheng1;~Qin_Jin1",
        "gender": "M;M;F",
        "homepage": "https://xuboshen.github.io/;https://github.com/zhengsipeng;https://www.jin-qin.com/index.html",
        "dblp": "293/8958;251/3691;47/2670",
        "google_scholar": "K2BnqoIAAAAJ;OonuDhcAAAAJ;8UkYbCMAAAAJ",
        "orcid": "0009-0000-1896-9600;;0000-0001-6486-6020",
        "linkedin": ";;qinjin/",
        "or_profile": "~Boshen_Xu1;~Sipeng_Zheng1;~Qin_Jin1",
        "aff": "Xiaomi Corporation+Renmin University of China;Beijing Academy of Artificial Intelligence;Renmin University of China",
        "aff_domain": "xiaomi.com+ruc.edu.cn;baai.ac.cn;ruc.edu.cn",
        "position": "Intern+PhD student;Researcher;Professor",
        "bibtex": "@inproceedings{\nxu2025spaformer,\ntitle={{SPAF}ormer: Sequential 3D Part Assembly with Transformers},\nauthor={Boshen Xu and Sipeng Zheng and Qin Jin},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=kryphH8cJP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kryphH8cJP",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lH7j1VauYG",
        "title": "HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "Current text-to-avatar methods often rely on implicit representations (e.g., NeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit and animate in graphics software. This paper introduces a novel framework for generating stylized head avatars from text guidance, which leverages locally learnable mesh deformation and 2D diffusion priors to achieve high-quality digital assets for attribute-preserving manipulation. Given a template mesh, our method represents mesh deformation with per-face Jacobians and adaptively modulates local deformation using a learnable vector field. This vector field enables anisotropic scaling while preserving the rotation of vertices, which can better express identity and geometric details. We also employ landmark- and contour-based regularization terms to balance the expressiveness and plausibility of generated head avatars from multiple views without relying on any specific shape prior. Our framework can generate realistic shapes and textures that can be further edited via text, while supporting seamless editing using the preserved attributes from the template mesh, such as 3DMM parameters, blendshapes, and UV coordinates. Extensive experiments demonstrate that our framework can generate diverse and expressive head avatars with high-quality meshes that artists can easily manipulate in 3D graphics software, facilitating downstream applications such as efficient asset creation and animation with preserved attributes.",
        "keywords": "head avatars;mesh deformation;text guidance;diffusion models",
        "primary_area": "",
        "supplementary_material": "/attachment/0cda5a950115583efb85907b1638321107e93e48.zip",
        "author": "Duotun Wang;Hengyu Meng;Zeyu Cai;Zhijing Shao;Qianxi Liu;Lin Wang;Mingming Fan;Xiaohang Zhan;Zeyu Wang",
        "authorids": "~Duotun_Wang1;~Hengyu_Meng1;~Zeyu_Cai2;~Zhijing_Shao1;~Qianxi_Liu1;~Lin_Wang2;~Mingming_Fan2;~Xiaohang_Zhan1;~Zeyu_Wang15",
        "gender": "M;M;M;M;F;M;M;M;M",
        "homepage": "https://www.duotun-wang.co.uk/;https://hengyumeng.github.io/;https://github.com/zcai0612;https://initialneil.github.io/;;https://dr.ntu.edu.sg/cris/rp/rp02550;https://www.mingmingfan.com/;https://xiaohangzhan.github.io/;https://zachzeyuwang.github.io/",
        "dblp": "225/8349;;;359/4184;;;50/6579.html;211/7010;132/7882-3.html",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;;;SReb2csAAAAJ;GAqZYGcAAAAJ;QfquhDEAAAAJ;q7NLPG0AAAAJ",
        "orcid": ";;0009-0006-5422-4044;0009-0008-3204-3271;0009-0008-1042-1630;0000-0002-7485-4493;0000-0002-0356-4712;0000-0003-2136-7592;0000-0001-5374-6330",
        "linkedin": ";;;;;;;xiaohang-zhan-%EF%BC%88%E8%A9%B9%E6%99%93%E8%88%AA%EF%BC%89-7659b2b8/;zachzeyuwang/",
        "or_profile": "~Duotun_Wang1;~Hengyu_Meng1;~Zeyu_Cai2;~Zhijing_Shao1;~Qianxi_Liu1;~Lin_Wang2;~Mingming_Fan2;~Xiaohang_Zhan1;~Zeyu_Wang15",
        "aff": "Hong Kong University of Science and Technology (Guangzhou);Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology (Guangzhou)+Prometheus Vision Technology Co., Ltd.;Hong Kong University of Science and Technology(Guangzhou);Nanyang Technological University;Hong Kong University of Science and Technology;Tencent AI Lab;The Hong Kong University of Science and Technology (Guangzhou)",
        "aff_domain": "hkust.edu;hkust-gz.edu.cn;hkust.edu;connect.hkust-gz.edu.cn+prometh.xyz;connect.hkust-gz.edu.cn;ntu.edu.sg;ust.hk;tencent.com;ust.hk",
        "position": "PhD student;MS student;MS student;PhD student+Researcher;MS student;Assistant Professor;Assistant Professor;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025headevolver,\ntitle={HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation},\nauthor={Duotun Wang and Hengyu Meng and Zeyu Cai and Zhijing Shao and Qianxi Liu and Lin Wang and Mingming Fan and Xiaohang Zhan and Zeyu Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=lH7j1VauYG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lH7j1VauYG",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "lzhYJt55Yf",
        "title": "INPC: Implicit Neural Point Clouds for Radiance Field Rendering",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We introduce a new approach for reconstruction and novel view synthesis of unbounded real-world scenes.\n\nIn contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which *implicitly* encodes the geometry in a continuous octree-based probability field and view-dependent appearance in a multi-resolution hash grid.\nThis allows for extraction of arbitrary *explicit* point clouds, which can be rendered using rasterization.\nIn doing so, we combine the benefits of both worlds and retain favorable behavior during optimization:\nOur novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving the fine geometric detail captured by volumetric neural fields.\nFurthermore, this representation does not depend on priors like structure-from-motion point clouds.\n\nOur method achieves state-of-the-art image quality on common benchmarks.\nFurthermore, we achieve fast inference at interactive frame rates, and can convert our trained model into a large, explicit point cloud to further enhance performance.",
        "keywords": "Novel View Synthesis;Point Clouds;Neural Rendering;Implicit Representations",
        "primary_area": "",
        "supplementary_material": "/attachment/17bf100fd40b9c8bcfe58847b9b9b7371b258248.zip",
        "author": "Florian Hahlbohm;Linus Franke;Moritz Kappel;Susana Castillo;Martin Eisemann;Marc Stamminger;Marcus Magnor",
        "authorids": "~Florian_Hahlbohm1;~Linus_Franke1;~Moritz_Kappel1;~Susana_Castillo1;~Martin_Eisemann1;~Marc_Stamminger1;~Marcus_Magnor1",
        "gender": "M;M;M;F;M;M;M",
        "homepage": "https://fhahlbohm.github.io/;;https://graphics.tu-bs.de/people/kappel;https://graphics.tu-bs.de/people/castillo;https://graphics.tu-bs.de/people/eisemann;https://www.lgdv.tf.fau.de/;https://graphics.tu-bs.de",
        "dblp": "372/5439.html;223/3776;260/8663;69/10121;60/1028;s/MarcStamminger.html;m/MarcusAMagnor",
        "google_scholar": "SLiSrCsAAAAJ;;https://scholar.google.com/citations?hl=en;https://scholar.google.de/citations?hl=en;4fN_h5QAAAAJ;https://scholar.google.de/citations?user=cx4AaqoAAAAJ;st_GV1QAAAAJ",
        "orcid": "0009-0004-8710-1433;0000-0001-8180-0963;0000-0001-9507-5141;0000-0003-1245-4758;0000-0002-8673-4405;0000-0001-8699-3442;0000-0003-0579-480X",
        "linkedin": ";;;susana-castillo-alejandre-162b2113/;;https://www.linkedin.com/MarcStammingerHasNoLinkedinProfileIFindThisTotallyAnnoying;",
        "or_profile": "~Florian_Hahlbohm1;~Linus_Franke1;~Moritz_Kappel1;~Susana_Castillo1;~Martin_Eisemann1;~Marc_Stamminger1;~Marcus_Magnor1",
        "aff": "Technische Universit\u00e4t Carolo-Wilhelmina zu Braunschweig;Friedrich-Alexander Universit\u00e4t Erlangen-N\u00fcrnberg;Technische Universit\u00e4t Braunschweig;Technische Universit\u00e4t Carolo-Wilhelmina Braunschweig;Technische Universit\u00e4t Carolo-Wilhelmina Braunschweig;University of Erlangen-Nuremberg;TU Braunschweig+Technische Universit\u00e4t Carolo-Wilhelmina Braunschweig",
        "aff_domain": "tu-braunschweig.de;fau.de;tu-braunschweig.de;tu-bs.de;tu-bs.de;fau.de;tu-bs.de+tu-bs.de",
        "position": "PhD student;PhD student;PhD student;Postdoc;Full Professor;Full Professor;Professor+Full Professor",
        "bibtex": "@inproceedings{\nhahlbohm2025inpc,\ntitle={{INPC}: Implicit Neural Point Clouds for Radiance Field Rendering},\nauthor={Florian Hahlbohm and Linus Franke and Moritz Kappel and Susana Castillo and Martin Eisemann and Marc Stamminger and Marcus Magnor},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=lzhYJt55Yf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lzhYJt55Yf",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "m1p0IQ8gZk",
        "title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce RealmDreamer, a technique for generating forward-facing 3D scenes from text descriptions. Our method optimizes a 3D Gaussian Splatting representation to match complex text prompts using pretrained diffusion models. Our key insight is to leverage 2D inpainting diffusion models conditioned on an initial scene estimate to provide low variance and high-fidelity estimates of unknown regions during 3D distillation. In conjunction, we imbue correct geometry with geometric distillation from a depth diffusion model, conditioned on samples from the inpainting model. We find that the initialization of the optimization is crucial, and provide a principled methodology for doing so. Notably, our technique doesn't require video or multi-view data and can synthesize various high-quality 3D scenes in different styles with complex layouts. Further, the generality of our method allows 3D synthesis from a single image. As measured by a comprehensive user study, our method outperforms all existing approaches, preferred by 88-95%. We encourage viewing the supplemental website and video. Project page: https://realmdreamer.github.io/",
        "keywords": "3D Generation;Diffusion Models;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/b705eeee788094d5142becffb6509e07ed0b73dd.zip",
        "author": "Jaidev Shriram;Alex Trevithick;Lingjie Liu;Ravi Ramamoorthi",
        "authorids": "~Jaidev_Shriram1;~Alex_Trevithick2;~Lingjie_Liu1;~Ravi_Ramamoorthi3",
        "gender": "M;;F;M",
        "homepage": "https://jaidevshriram.com;;https://lingjie0206.github.io/;https://cseweb.ucsd.edu/~ravir/",
        "dblp": ";;204/0052;88/6919",
        "google_scholar": ";;https://scholar.google.de/citations?user=HZPnJ9gAAAAJ;q0MzO6cAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Jaidev_Shriram1;~Alex_Trevithick2;~Lingjie_Liu1;~Ravi_Ramamoorthi3",
        "aff": ";;University of Pennsylvania;University of California, San Diego+NVIDIA",
        "aff_domain": ";;upenn.edu;ucsd.edu+nvidia.com",
        "position": ";;Assistant Professor;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nshriram2025realmdreamer,\ntitle={RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion},\nauthor={Jaidev Shriram and Alex Trevithick and Lingjie Liu and Ravi Ramamoorthi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=m1p0IQ8gZk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=m1p0IQ8gZk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "mZDHwtXnuZ",
        "title": "FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this work, we introduce FourieRF, a novel approach for achieving fast and high-quality reconstruction in the few-shot setting. Our method effectively parameterizes features through an explicit curriculum training procedure, incrementally increasing scene complexity during optimization. Experimental results show that the prior induced by our approach is both robust and adaptable across a wide variety of scenes, establishing FourieRF as a strong and versatile baseline for the few-shot rendering problem. While our approach significantly reduces artifacts, it may still lead to reconstruction errors in severely under-constrained scenarios, particularly where view occlusion leaves parts of the shape uncovered.  In the future, our method could be enhanced by integrating foundation models to complete missing parts using large data-driven priors.",
        "keywords": "NeRF;Few-shot Neural Rendering;Fourier Frequency",
        "primary_area": "",
        "supplementary_material": "/attachment/cfd807d65e73536795e9b6d06fd40d30c3553181.pdf",
        "author": "Diego Gomez;Bingchen Gong;Maks Ovsjanikov",
        "authorids": "~Diego_Gomez2;~Bingchen_Gong1;~Maks_Ovsjanikov1",
        "gender": "M;Not Specified;M",
        "homepage": "https://www.lix.polytechnique.fr/~gomez/;https://s2.hk/;http://www.lix.polytechnique.fr/~maks/",
        "dblp": ";150/6437;94/5668",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;A84X-3IAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0005-2847-8617;0000-0001-6459-6972;0000-0002-5867-4046",
        "linkedin": "diegogomezm/;;",
        "or_profile": "~Diego_Gomez2;~Bingchen_Gong1;~Maks_Ovsjanikov1",
        "aff": "\u00c9cole Polytechnique;\u00c9cole Polytechnique;Google+\u00c9cole Polytechnique",
        "aff_domain": "polytechnique.edu;polytechnique.edu;deepmind.com+polytechnique.edu",
        "position": "PhD student;Postdoc;Researcher+Full Professor",
        "bibtex": "@inproceedings{\ngomez2025fourierf,\ntitle={Fourie{RF}: Few-Shot Ne{RF}s via Progressive Fourier Frequency Control},\nauthor={Diego Gomez and Bingchen Gong and Maks Ovsjanikov},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=mZDHwtXnuZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mZDHwtXnuZ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "n04Dbq4Y8J",
        "title": "3D-GPT: Procedural 3D modeling with large language models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.",
        "keywords": "3D Generation; 3D Modeling; LLMs; Scene Generation",
        "primary_area": "",
        "supplementary_material": "/attachment/986604be850635d1b0e0f3a1de8d99b5c4e38e62.zip",
        "author": "Chunyi Sun;Junlin Han;Weijian Deng;Xinlong Wang;Zishan Qin;Stephen Gould",
        "authorids": "~Chunyi_Sun3;~Junlin_Han1;~Weijian_Deng1;~Xinlong_Wang2;~Zishan_Qin1;~Stephen_Gould1",
        "gender": "F;M;M;M;F;M",
        "homepage": ";https://junlinhan.github.io/;http://weijiandeng.xyz;;https://zishanqin.github.io/;http://users.cecs.anu.edu.au/~sgould/",
        "dblp": ";151/9037;198/1517;;125/2053;89/1569.html",
        "google_scholar": "e46WeckAAAAJ;5L0Uj_IAAAAJ;https://scholar.google.com.hk/citations?user=lReHnAEAAAAJ;DPz0DjYAAAAJ;https://scholar.google.com.au/citations?user=wGOQawgAAAAJ;YvdzeM8AAAAJ",
        "orcid": ";;;;0000-0002-9265-4583;0000-0001-8929-7899",
        "linkedin": ";;;;zishan-qin/;",
        "or_profile": "~Chunyi_Sun3;~Junlin_Han1;~Weijian_Deng1;~Xinlong_Wang2;~Zishan_Qin1;~Stephen_Gould1",
        "aff": "Australian National University;University of Oxford+GenAI, Meta;Australian National University;Beijing Academy of Artificial Intelligence;University of New South Wales+Commonwealth Scientific and Industrial Research Organisation, CSIRO;Australian National University",
        "aff_domain": "anu.edu.au;oxford.ac.uk+meta.com;anu.edu.au;baai.ac.cn;unsw.edu.au+data61.csiro.au;anu.edu.au",
        "position": "PhD student;PhD student+Researcher;Postdoc;Researcher;PhD student+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nsun2025dgpt,\ntitle={3D-{GPT}: Procedural 3D modeling with large language models},\nauthor={Chunyi Sun and Junlin Han and Weijian Deng and Xinlong Wang and Zishan Qin and Stephen Gould},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=n04Dbq4Y8J}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=n04Dbq4Y8J",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "nobySt7QFn",
        "title": "Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content.",
        "keywords": "3D Generation; Diffusion models; 3D Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/68b5d084d3792f202b854a338950f62707d60a1d.pdf",
        "author": "Zhiqi Li;Yiming Chen;Lingzhe Zhao;Peidong Liu",
        "authorids": "~Zhiqi_Li5;~Yiming_Chen9;~Lingzhe_Zhao1;~Peidong_Liu3",
        "gender": "M;M;M;",
        "homepage": "https://lizhiqi49.github.io;https://github.com/codejoker-c;https://github.com/LingzheZhao/;",
        "dblp": ";;234/8463;",
        "google_scholar": ";;mN764NsAAAAJ;",
        "orcid": ";;0009-0005-8000-1525;",
        "linkedin": ";;;",
        "or_profile": "~Zhiqi_Li5;~Yiming_Chen9;~Lingzhe_Zhao1;~Peidong_Liu3",
        "aff": "Westlake University;Westlake University;Westlake University;",
        "aff_domain": "westlake.edu;westlake.edu.cn;westlake.edu.cn;",
        "position": "PhD student;PhD student;PhD student;",
        "bibtex": "@inproceedings{\nli2025controllable,\ntitle={Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting},\nauthor={Zhiqi Li and Yiming Chen and Lingzhe Zhao and Peidong Liu},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=nobySt7QFn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nobySt7QFn",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "o1PvIlcBz9",
        "title": "OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "White Light Interferometry (WLI) is a precise optical tool for measuring the 3D topography of microstructures. However, conventional WLI cannot capture the natural color of a sample's surface, which is essential for many microscale research applications that require both 3D geometry and color information. Previous methods have attempted to overcome this limitation by modifying WLI hardware and analysis software, but these solutions are often costly. In this work, we address this challenge from a computer vision multi-modal reconstruction perspective for the first time. We introduce OpticFusion, a novel approach that uses an additional digital optical microscope (OM) to achieve 3D reconstruction with natural color textures using multi-view WLI and OM images. Our method employs a two-step data association process to obtain the poses of WLI and OM data. By leveraging the neural implicit representation, we fuse multi-modal data and apply color decomposition technology to extract the sample's natural color. Tested on our multi-modal dataset of various microscale samples, OpticFusion achieves detailed 3D reconstructions with color textures. Our method provides an effective tool for practical applications across numerous microscale research fields. The source code and our real-world dataset are available at https://github.com/zju3dv/OpticFusion.",
        "keywords": "multi-modal 3d reconstruction;neural implicit representation;white light interferometry;optical microscope.",
        "primary_area": "",
        "supplementary_material": "/attachment/31723f0ef0d3fe215df8993edec22914e90039e8.zip",
        "author": "Shuo Chen;Yijin Li;Guofeng Zhang",
        "authorids": "~Shuo_Chen10;~Yijin_Li1;~Guofeng_Zhang3",
        "gender": "M;M;M",
        "homepage": ";https://eugenelyj.github.io/;http://www.cad.zju.edu.cn/home/gfzhang",
        "dblp": ";178/6879;78/5389-1.html",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;F0xfpXAAAAAJ",
        "orcid": "0000-0001-6637-0109;0000-0003-3366-093X;0000-0001-5661-8430",
        "linkedin": ";;",
        "or_profile": "~Shuo_Chen10;~Yijin_Li1;~Guofeng_Zhang3",
        "aff": "Zhejiang University;Avolution AI;Zhejiang University",
        "aff_domain": "zju.edu.cn;avolutionai.com;zju.edu.cn",
        "position": "PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nchen2025opticfusion,\ntitle={OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy},\nauthor={Shuo Chen and Yijin Li and Guofeng Zhang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=o1PvIlcBz9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o1PvIlcBz9",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "o9fliwZ3iM",
        "title": "MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "We propose MeshUp, a technique that deforms a 3D mesh towards multiple\ntarget concepts, and intuitively controls the region where each concept is\nexpressed. Conveniently, the concepts can be defined as either text queries,\ne.g., \u201ca dog\u201d and \u201ca turtle,\u201d or inspirational images, and the local regions can\nbe selected as any number of vertices on the mesh. We can effectively control\nthe influence of the concepts and mix them together using a novel score\ndistillation approach, referred to as the Blended Score Distillation (BSD). BSD\noperates on each attention layer of the denoising U-Net of a diffusion model\nas it extracts and injects the per-objective activations into a unified denoising\npipeline from which the deformation gradients are calculated. To localize the\nexpression of these activations, we create a probabilistic Region of Interest\n(ROI) map on the surface of the mesh, and turn it into 3D-consistent masks\nthat we use to control the expression of these activations. We demonstrate\nthe effectiveness of BSD empirically and show that it can deform various\nmeshes towards multiple objectives.",
        "keywords": "geometry;deformation;diffusion;graphics;machine learning",
        "primary_area": "",
        "supplementary_material": "/attachment/c4bf95c8218dd29d7e2fb8ddb6259e123ec27204.zip",
        "author": "Hyunwoo Kim;Itai Lang;Thibault Groueix;Noam Aigerman;Vladimir Kim;Rana Hanocka",
        "authorids": "~Hyunwoo_Kim7;~Itai_Lang1;~Thibault_Groueix1;~Noam_Aigerman1;~Vladimir_Kim1;~Rana_Hanocka1",
        "gender": ";M;;;M;",
        "homepage": "https://3dl.cs.uchicago.edu/;https://itailang.github.io/;http://imagine.enpc.fr/~groueixt/index.html;;http://vova.kim;https://people.cs.uchicago.edu/~ranahanocka/",
        "dblp": ";179/4367;215/4301;;27/8698;167/2260",
        "google_scholar": ";q0bBhtsAAAAJ;https://scholar.google.fr/citations?user=45orDOEAAAAJ;;5S1kGcAAAAAJ;3Bk5C9EAAAAJ",
        "orcid": ";0000-0003-4066-4293;0000-0002-7984-8252;;;0000-0003-3214-3703",
        "linkedin": "hyun-woo-brian-kim-02b7561a9/;itailang;thibault-groueix-481761110/;;;",
        "or_profile": "~Hyunwoo_Kim7;~Itai_Lang1;~Thibault_Groueix1;~Noam_Aigerman1;~Vladimir_Kim1;~Rana_Hanocka1",
        "aff": "University of Chicago;University of Chicago;Adobe Systems;;Adobe Systems;University of Chicago",
        "aff_domain": "uchicago.edu;uchicago.edu;adobe.com;;adobe.com;uchicago.edu",
        "position": "Undergrad student;Postdoc;research scientist;;Research Scientist;Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025meshup,\ntitle={MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation},\nauthor={Hyunwoo Kim and Itai Lang and Thibault Groueix and Noam Aigerman and Vladimir Kim and Rana Hanocka},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=o9fliwZ3iM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o9fliwZ3iM",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "oGgtN2lMgP",
        "title": "Direct and Explicit 3D Generation from a Single Image",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current image-to-3D approaches suffer from high computational costs and lack scalability for high-resolution outputs. \nIn contrast, we introduce a novel framework to directly generate explicit surface geometry and texture using multi-view 2D depth and RGB images along with 3D Gaussian features using a repurposed Stable Diffusion model. \nWe introduce a depth branch into U-Net for efficient and high quality multi-view, cross-domain generation and incorporate epipolar attention into the latent-to-pixel decoder for pixel-level multi-view consistency depths. \nBy back-projecting the generated depth pixels into 3D space, we create a structured 3D representation that can be either rendered via Gaussian splatting or extracted to high-quality meshes, thereby leveraging additional novel view synthesis loss to further improve our performance. \nExtensive experiments demonstrate that our method surpasses existing baselines in geometry and texture quality while achieving significantly faster generation time.",
        "keywords": "Image-to-3D; Explicit 3D Representation; Diffusion Models",
        "primary_area": "",
        "supplementary_material": "/attachment/0e1e9e33c503c3a05d9bc2f07d0959c3e5284f34.zip",
        "author": "Haoyu Wu;Meher Gitika Karumuri;Chuhang Zou;Seungbae Bang;Yuelong Li;Dimitris Samaras;Sunil Hadap",
        "authorids": "~Haoyu_Wu2;~Meher_Gitika_Karumuri1;~Chuhang_Zou3;~Seungbae_Bang1;~Yuelong_Li2;~Dimitris_Samaras3;~Sunil_Hadap2",
        "gender": ";F;F;M;M;M;M",
        "homepage": "https://hao-yu-wu.github.io;;https://zouchuhang.github.io;https://sites.google.com/view/seungbaebang/home;;https://www.cs.stonybrook.edu/~samaras/;https://sunilhadap.github.io/",
        "dblp": ";;138/1927;165/9833;74/1280;s/DimitrisSamaras;70/4856",
        "google_scholar": "8AypXfoAAAAJ;;jBcKes0AAAAJ;https://scholar.google.com/citations?hl=en;pJdxUEIAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0003-2537-284X;0000-0003-1914-6782;;0000-0002-1373-0294;",
        "linkedin": ";meher-gitika-9028b311b/;chuhang-zou/;seungbae-bang-026846137/;;;",
        "or_profile": "~Haoyu_Wu2;~Meher_Gitika_Karumuri1;~Chuhang_Zou3;~Seungbae_Bang1;~Yuelong_Li2;~Dimitris_Samaras3;~Sunil_Hadap2",
        "aff": "Stony Brook University;Amazon;Meta;Amazon;Amazon;Stony Brook University;Amazon",
        "aff_domain": "cs.stonybrook.edu;amazon.com;meta.com;amazon.com;amazon.com;cs.stonybrook.edu;amazon.com",
        "position": "PhD student;Researcher;Researcher;Researcher;Researcher;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nwu2025direct,\ntitle={Direct and Explicit 3D Generation from a Single Image},\nauthor={Haoyu Wu and Meher Gitika Karumuri and Chuhang Zou and Seungbae Bang and Yuelong Li and Dimitris Samaras and Sunil Hadap},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=oGgtN2lMgP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oGgtN2lMgP",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "pIDl4wuZoG",
        "title": "CoE: Deep Coupled Embedding for Non-Rigid Point Cloud Correspondences",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The interest in matching non-rigidly deformed shapes represented as raw point clouds is rising due to the proliferation of low-cost 3D sensors. Yet, the task is challenging since point clouds are irregular and there is a lack of intrinsic shape information. We propose to tackle these challenges by learning a new shape representation -- a per-point high dimensional embedding, in an embedding space where semantically similar points share similar embeddings. The learned embedding has multiple beneficial properties: it is aware of the underlying shape geometry and is robust to shape deformations and various shape artefacts, such as noise and partiality. Consequently, this embedding can be directly employed to retrieve high-quality dense correspondences through a simple nearest neighbor search in the embedding space. Extensive experiments demonstrate new state-of-the-art results and robustness in numerous challenging non-rigid shape matching benchmarks and show its great potential in other shape analysis tasks, such as segmentation.",
        "keywords": "shape matching;point cloud;non-rigid deformation;3D representation",
        "primary_area": "",
        "supplementary_material": "/attachment/800252dd29bb8a9694d3b92afafadfb32d382117.pdf",
        "author": "Huajian Zeng;Maolin Gao;Daniel Cremers",
        "authorids": "~Huajian_Zeng1;~Maolin_Gao1;~Daniel_Cremers1",
        "gender": "M;M;M",
        "homepage": ";https://maolingao.github.io/;https://vision.in.tum.de/members/cremers",
        "dblp": ";205/3401;c/DanielCremers",
        "google_scholar": ";BupaOMMAAAAJ;cXQciMEAAAAJ",
        "orcid": ";;",
        "linkedin": "huajian-zeng-7282b8214/;;",
        "or_profile": "~Huajian_Zeng1;~Maolin_Gao1;~Daniel_Cremers1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence+Technische Universit\u00e4t M\u00fcnchen;Technical University Munich;Technical University Munich",
        "aff_domain": "mbzuai.ac.ae+tum.de;tum.de;tum.de",
        "position": "PhD student+MS student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nzeng2025coe,\ntitle={CoE: Deep Coupled Embedding for Non-Rigid Point Cloud Correspondences},\nauthor={Huajian Zeng and Maolin Gao and Daniel Cremers},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=pIDl4wuZoG}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pIDl4wuZoG",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "pa4RPUqzfJ",
        "title": "Open-Vocabulary Semantic Part Segmentation of 3D Human",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "3D part segmentation is still an open problem in the field of 3D vision and AR/VR. Due to limited 3D labeled data, traditional supervised segmentation methods fall short in generalizing to unseen shapes and categories. Recently, the advancement in vision-language models' zero-shot abilities has brought a surge in open-world 3D segmentation methods. While these methods show promising results for 3D scenes or objects, they do not generalize well to 3D humans. In this paper, we present the first open-vocabulary segmentation method capable of handling 3D human. Our framework can segment the human category into desired fine-grained parts based on the textual prompt. We design a simple segmentation pipeline, leveraging SAM to generate multi-view proposals in 2D and proposing a novel HumanCLIP model to create unified embeddings for visual and textual inputs. Compared with existing pre-trained CLIP models, the HumanCLIP model yields more accurate embeddings for human-centric contents. We also design a simple-yet-effective MaskFusion module, which classifies and fuses multi-view features into 3D semantic masks without complex voting and grouping mechanisms. The design of decoupling mask proposals and text input also significantly boosts the efficiency of per-prompt inference. Experimental results on various 3D human datasets show that our method outperforms current state-of-the-art open-vocabulary 3D segmentation methods by a large margin. In addition, we show that our method can be directly applied to various 3D representations including meshes, point clouds, and 3D Gaussian Splatting.",
        "keywords": "3D human segmentation;open-vocabulary",
        "primary_area": "",
        "supplementary_material": "/attachment/7e4a23231a147878463bb3bc7f87c7e8b7163ba1.zip",
        "author": "Keito Suzuki;Bang Du;Girish Krishnan;Kunyao Chen;Runfa Li;Truong Nguyen",
        "authorids": "~Keito_Suzuki1;~Bang_Du1;~Girish_Krishnan2;~Kunyao_Chen1;~Runfa_Li1;~Truong_Nguyen1",
        "gender": "M;;;M;M;M",
        "homepage": ";;;http://kunyao.github.io;https://blarklee.github.io/;http://videoprocessing.ucsd.edu/",
        "dblp": "271/6420;;;196/6258;393/4277;23/5856.html",
        "google_scholar": "RInf5DcAAAAJ;;;;vntCBHQAAAAJ;5rjfy4AAAAAJ",
        "orcid": ";;;0000-0003-4527-1114;;",
        "linkedin": ";bang-du-ucsd/;girk/;;;",
        "or_profile": "~Keito_Suzuki1;~Bang_Du1;~Girish_Krishnan2;~Kunyao_Chen1;~Runfa_Li1;~Truong_Nguyen1",
        "aff": "University of California, San Diego;University of California, San Diego+Google;University of California, San Diego;Qualcomm Inc, QualComm;University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu+google.com;ucsd.edu;qti.qualcomm.com;ucsd.edu;ucsd.edu",
        "position": "PhD student;PhD student+Intern;Undergrad student;Researcher;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nsuzuki2025openvocabulary,\ntitle={Open-Vocabulary Semantic Part Segmentation of 3D Human},\nauthor={Keito Suzuki and Bang Du and Girish Krishnan and Kunyao Chen and Runfa Li and Truong Nguyen},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=pa4RPUqzfJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pa4RPUqzfJ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "q3PAJAP8q0",
        "title": "DreamBeast: Distilling 3D Fantastic Animals with Part-Aware Knowledge Transfer",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D part-affinity implicit representation. This enables us to instantly generate part-affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to generate 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.",
        "keywords": "Score Distillation Sampling;3D Generation;Part-aware",
        "primary_area": "",
        "supplementary_material": "/attachment/397d2d0b9839b04c6ef5eda6369cf6a17921669b.zip",
        "author": "Runjia Li;Junlin Han;Luke Melas-Kyriazi;Chunyi Sun;Zhongrui Gui;Zhaochong An;Shuyang Sun;Philip Torr;Tomas Jakab",
        "authorids": "~Runjia_Li1;~Junlin_Han1;~Luke_Melas-Kyriazi1;~Chunyi_Sun3;~Zhongrui_Gui1;~Zhaochong_An1;~Shuyang_Sun1;~Philip_Torr1;~Tomas_Jakab1",
        "gender": "M;M;M;F;M;M;;;",
        "homepage": "https://runjiali-rl.github.io/;https://junlinhan.github.io/;https://lukemelas.github.io/;;https://github.com/g2zr004;https://zhaochongan.github.io/;;http://www.robots.ox.ac.uk/~tvg/;",
        "dblp": ";151/9037;228/5680;;375/1465;274/7063;;;222/2864",
        "google_scholar": "Zu3Rwn8AAAAJ;5L0Uj_IAAAAJ;https://scholar.google.com/citations?hl=en;e46WeckAAAAJ;HRnLy5wAAAAJ;;;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;0009-0006-8005-0471;;;;",
        "linkedin": "runjiali-b169451a9/;;;;;;;;",
        "or_profile": "~Runjia_Li1;~Junlin_Han1;~Luke_Melas-Kyriazi1;~Chunyi_Sun3;~Zhongrui_Gui1;~Zhaochong_An1;~Shuyang_Sun1;~Philip_Torr1;~Tomas_Jakab1",
        "aff": "University of Oxford;University of Oxford+GenAI, Meta;;Australian National University;University of Oxford;EPFL - EPF Lausanne+University of Copenhagen;;University of Oxford;University of Oxford",
        "aff_domain": "robots.ox.ac.uk;oxford.ac.uk+meta.com;;anu.edu.au;robots.ox.ac.uk;epfl.ch+diku.dk;;ox.ac.uk;ox.ac.uk",
        "position": "PhD student;PhD student+Researcher;;PhD student;Undergrad student;PhD student+PhD student;;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nli2025dreambeast,\ntitle={DreamBeast: Distilling 3D Fantastic Animals with Part-Aware Knowledge Transfer},\nauthor={Runjia Li and Junlin Han and Luke Melas-Kyriazi and Chunyi Sun and Zhongrui Gui and Zhaochong An and Shuyang Sun and Philip Torr and Tomas Jakab},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=q3PAJAP8q0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=q3PAJAP8q0",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qFWfgadJVQ",
        "title": "Interactive Humanoid: Online Full Body Human Motion Reaction Synthesis With Social Affordance Forecasting and Canonicalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We focus on the human-humanoid interaction problem optionally with an object. We propose a new task named online full-body motion reaction synthesis, which generates humanoid reactions based on the human actor's motions. The previous work only focuses on human interaction without objects and generates body reactions without hand. Besides, they also do not consider the task as an online setting, which means the reactor can only see the current information and cannot perceive the future actions of the actor.  To support the task of online full-body motion reaction synthesis, we construct two datasets named HHI and CoChair and propose a unified method.  Specifically, we encode the motion of human actors and objects from an interaction-centric view through a social affordance representation.\n Then we leverage a social affordance forecasting scheme to enable the reactor to predict based on the imagined future. We also use SE(3)-Equivariant Neural Networks to learn the local frame to canonicalize the social affordance.  Experiments demonstrate that our approach effectively generates high-quality reactions on HHI and CoChair. Furthermore, we also validate our method on existing human interaction datasets Interhuman and Chi3D in real-time at 25 fps.",
        "keywords": "Reaction Synthesis;SE(3)-Equivariant Neural Networks;Local Frame Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/608e2f6675259cb18e5d97dcecd0b641bebaaa0f.zip",
        "author": "Yunze Liu;Changxi Chen;Li Yi",
        "authorids": "~Yunze_Liu2;~Changxi_Chen1;~Li_Yi2",
        "gender": "M;F;M",
        "homepage": "https://yunzeliu.github.io;https://yunzeliu.github.io/;https://ericyi.github.io/",
        "dblp": ";;26/4239-1",
        "google_scholar": "xYVEg0cAAAAJ;;UyZL660AAAAJ",
        "orcid": "0009-0002-3148-8822;;",
        "linkedin": ";;",
        "or_profile": "~Yunze_Liu2;~Changxi_Chen1;~Li_Yi2",
        "aff": "Tsinghua University;;Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;;tsinghua.edu.cn",
        "position": "PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025interactive,\ntitle={Interactive Humanoid: Online Full Body Human Motion Reaction Synthesis With Social Affordance Forecasting and Canonicalization},\nauthor={Yunze Liu and Changxi Chen and Li Yi},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=qFWfgadJVQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qFWfgadJVQ",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "qkGKvk0NAX",
        "title": "Oblique-MERF: Revisiting and Improving MERF for Oblique Photography",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Neural radiance fields (NeRF) have established a new paradigm for 3D scene reconstruction, with subsequent work achieving high-quality real-time rendering. However, reconstructing large-scale scenes from oblique aerial photography presents unique challenges, such as varying spatial scale distributions and a constrained range of tilt angles, often resulting in high memory consumption and reduced rendering quality at extrapolated viewpoints. To address these issues, we propose a novel approach named Oblique-MERF to accommodate the distinctive characteristics of oblique photography datasets and support real-time rendering on various common devices. Firstly, an innovative adaptive occupancy plane is proposed to constrain the sampling space. Additionally, we propose a smoothness regularization loss for view-dependent color to enhance the  MLP's ability to generalize to untrained viewpoints. Experimental results demonstrate that Oblique-MERF reduces VRAM usage by approximately 40% while maintaining competitive rendering quality compared to baseline methods, and achieves higher frame rates with more realistic rendering even at untrained extrapolated viewpoints.",
        "keywords": "Neural Radiance Fields;Real-Time Rendering;Memory Efficient;Reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/1e8419521758059779332e055e188392ec93a675.zip",
        "author": "Xiaoyi Zeng;Kaiwen Song;Leyuan Yang;Bailin Deng;Juyong Zhang",
        "authorids": "~Xiaoyi_Zeng2;~Kaiwen_Song1;~Leyuan_Yang1;~Bailin_Deng2;~Juyong_Zhang1",
        "gender": "F;M;M;M;M",
        "homepage": "https://github.com/xiaoyi-Zeng;https://github.com/kevin2000A;https://github.com/paradise-yang;http://www.bdeng.me/;http://staff.ustc.edu.cn/~juyong/",
        "dblp": ";;;54/8293;38/5125",
        "google_scholar": ";;;zNXnH-UAAAAJ;_D9aUrgAAAAJ",
        "orcid": ";;;0000-0002-0158-7670;",
        "linkedin": ";;;;",
        "or_profile": "~Xiaoyi_Zeng2;~Kaiwen_Song1;~Leyuan_Yang1;~Bailin_Deng2;~Juyong_Zhang1",
        "aff": "University of Science and Technology of China;University of Science and Technology of China;University of Science and Technology of China;Cardiff University;University of Science and Technology of China",
        "aff_domain": "ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;cardiff.ac.uk;ustc.edu.cn",
        "position": "MS student;PhD student;Undergrad student;Senior Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nzeng2025obliquemerf,\ntitle={Oblique-{MERF}: Revisiting and Improving {MERF} for Oblique Photography},\nauthor={Xiaoyi Zeng and Kaiwen Song and Leyuan Yang and Bailin Deng and Juyong Zhang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=qkGKvk0NAX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qkGKvk0NAX",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rSI6XGVWqE",
        "title": "SurfR: Surface Reconstruction with Multi-scale Attention",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a fast and accurate surface reconstruction algorithm for unorganized point clouds using an implicit representation.  Recent learning methods are either single-object representations with small neural models that allow for high surface details but require per-object training or generalized representations that require larger models and generalize to newer shapes but lack details, and inference is slow. We propose a new implicit representation for general 3D shapes that is faster than all the baselines at their optimum resolution, with only a marginal loss in performance compared to the state-of-the-art. We achieve the best accuracy-speed trade-off using three key contributions. Many implicit methods extract features from the point cloud to classify whether a query point is inside or outside the object. First, to speed up the reconstruction, we show that this feature extraction does not need to use the query point at an early stage (lazy query). Second, we use a parallel multi-scale grid representation to develop robust features for different noise levels and input resolutions. Finally, we show that attention across scales can provide improved reconstruction results. The code will be made available.",
        "keywords": "Surface Reconstruction;Implicit Representation;Multi-scale attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siddhant Ranade;Gon\u00e7alo Dias Pais;Ross Tyler Whitaker;Pedro Miraldo;Jacinto Nascimento;Srikumar Ramalingam",
        "authorids": "~Siddhant_Ranade1;~Gon\u00e7alo_Dias_Pais1;~Ross_Tyler_Whitaker1;~Pedro_Miraldo2;~Jacinto_Nascimento2;~Srikumar_Ramalingam2",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://www.siddhantranade.com;;http://www.cs.utah.edu/~whitaker;https://pmiraldo.github.io/;;https://www.cs.utah.edu/~srikumar/",
        "dblp": ";300/8774;w/RossTWhitaker;71/10771;;17/4216",
        "google_scholar": ";EcZiAtYAAAAJ;FaOZ6EMAAAAJ;https://scholar.google.pt/citations?user=e4ZW_fgAAAAJ;https://scholar.google.pt/citations?user=9bMPdwoAAAAJ;6m1ptOgAAAAJ",
        "orcid": ";;;0000-0002-8551-2448;http://orcid.org/0000-0001-7468-5127;",
        "linkedin": ";goncalo-pais-1202004/;;;;srikumar-ramalingam-17728b22/",
        "or_profile": "~Siddhant_Ranade1;~Gon\u00e7alo_Dias_Pais1;~Ross_Tyler_Whitaker1;~Pedro_Miraldo2;~Jacinto_Nascimento2;~Srikumar_Ramalingam2",
        "aff": "University of Utah;Instituto Superior T\u00e9cnico;University of Utah+Department of Computer Science, University of North Carolina, Chapel Hill+University of Utah+University of Utah;Mitsubishi Electric Research Labs;T\u00e9cnico Lisboa;Google",
        "aff_domain": "utah.edu;tecnico.ulisboa.pt;utah.edu+cs.unc.edu++utah.edu;merl.com;ist.utl.pt;google.com",
        "position": "PhD student;PhD student;Full Professor+PhD student+Full Professor+;Principal Researcher;Assistant Professor;Research Scientist",
        "bibtex": "@inproceedings{\nranade2025surfr,\ntitle={SurfR: Surface Reconstruction with Multi-scale Attention},\nauthor={Siddhant Ranade and Gon{\\c{c}}alo Dias Pais and Ross Tyler Whitaker and Pedro Miraldo and Jacinto Nascimento and Srikumar Ramalingam},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=rSI6XGVWqE}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rSI6XGVWqE",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "rXlpYdCExk",
        "title": "A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining",
        "track": "main",
        "status": "3DV 2025 Oral",
        "tldr": "",
        "abstract": "3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build a large-scale dataset of 3DGS using the commonly used ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects from 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 2 GPU years on a TITAN XP GPU.\n\nWe utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce Gaussian-MAE, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that  (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks. Our dataset and model are publicly available at https://unique1i.github.io/ShapeSplat.",
        "keywords": "3D object dataset;self-supervised pretraining;representation learning;gaussian splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/e60daceb6486b7cf0a82a88eb8bcec38c0308a16.zip",
        "author": "Qi Ma;Yue Li;Bin Ren;Nicu Sebe;Ender Konukoglu;Theo Gevers;Luc Van Gool;Danda Pani Paudel",
        "authorids": "~Qi_Ma6;~Yue_Li12;~Bin_Ren2;~Nicu_Sebe1;~Ender_Konukoglu1;~Theo_Gevers1;~Luc_Van_Gool1;~Danda_Pani_Paudel3",
        "gender": "M;;;M;;M;;",
        "homepage": "https://qimaqi.github.io/;;;http://disi.unitn.it/~sebe/;http://www.vision.ee.ethz.ch/~kender;https://staff.science.uva.nl/th.gevers/;;",
        "dblp": ";;;20/3519;45/7041;12/6600;61/5017;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;https://scholar.google.it/citations?user=stFCYOAAAAAJ;https://scholar.google.ch/citations?user=OeEMrhQAAAAJ;yqsvxQgAAAAJ;https://scholar.google.be/citations?user=TwMib_QAAAAJ;",
        "orcid": "0009-0005-4028-6917;;;0000-0002-6597-7248;;;;",
        "linkedin": "qi-ma-27a655189/;;;;;theo-gevers-a215244/;;",
        "or_profile": "~Qi_Ma6;~Yue_Li12;~Bin_Ren2;~Nicu_Sebe1;~Ender_Konukoglu1;~Theo_Gevers1;~Luc_Van_Gool1;~Danda_Pani_Paudel3",
        "aff": "ETH Zurich;;;University of Trento;ETHZ - ETH Zurich;University of Amsterdam;Sofia Un. St. Kliment Ohridski;",
        "aff_domain": "vision.ee.ethz.ch;;;unitn.it;ethz.ch;ivi.uva.nl;insait.ai;",
        "position": "PhD student;;;Full Professor;Associate Professor;Full Professor;Full Professor;",
        "bibtex": "@inproceedings{\nma2025a,\ntitle={A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining},\nauthor={Qi Ma and Yue Li and Bin Ren and Nicu Sebe and Ender Konukoglu and Theo Gevers and Luc Van Gool and Danda Pani Paudel},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=rXlpYdCExk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rXlpYdCExk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "ri0q1OpyVk",
        "title": "HoleGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience.",
        "keywords": "Multimodal Generation; Audio-to-Gesture Generation; Accelerated Diffusion Generation; 3D Vision",
        "primary_area": "",
        "supplementary_material": "/attachment/23aae759cf8282827a4ebfd2685d7fa9904b3d62.zip",
        "author": "Yongkang Cheng;Shaoli Huang",
        "authorids": "~Yongkang_Cheng2;~Shaoli_Huang2",
        "gender": ";M",
        "homepage": ";",
        "dblp": ";80/8502",
        "google_scholar": ";o31BPFsAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Yongkang_Cheng2;~Shaoli_Huang2",
        "aff": ";Tencent AI Lab",
        "aff_domain": ";tencent.com",
        "position": ";Researcher",
        "bibtex": "@inproceedings{\ncheng2025holegest,\ntitle={HoleGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures},\nauthor={Yongkang Cheng and Shaoli Huang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=ri0q1OpyVk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ri0q1OpyVk",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sQe8zAYt5c",
        "title": "HMD^2: Environment-aware Motion Generation from Single Egocentric Head-Mounted Device",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper investigates the generation of realistic full-body human motion using a single head-mounted device with an outward-facing color camera and the ability to perform visual SLAM. To address the ambiguity of this setup, we present HMD$^2$, a novel system that balances motion reconstruction and generation. From a reconstruction standpoint, it aims to maximally utilize the camera streams to produce both analytical and learned features, including head motion, SLAM point cloud, and image embeddings. On the generative front, HMD$^2$ employs a multi-modal conditional motion diffusion model with a Transformer backbone to maintain temporal coherence of generated motions, and utilizes autoregressive inpainting to facilitate online motion inference with minimal latency (0.17 seconds). We show that our system provides an effective and robust solution that scales to a diverse dataset of over 200 hours of motion in complex indoor and outdoor environments.",
        "keywords": "Motion Estimation;Motion Generation;Ego-centric;Multi-modal;Head-mounted device",
        "primary_area": "",
        "supplementary_material": "/attachment/9a852efe6cecdca85cbe39fb90cdbd465c09b212.pdf",
        "author": "Vladimir Guzov;Yifeng Jiang;Fangzhou Hong;Gerard Pons-Moll;Richard Newcombe;Karen Liu;Yuting Ye;Lingni Ma",
        "authorids": "~Vladimir_Guzov1;~Yifeng_Jiang1;~Fangzhou_Hong1;~Gerard_Pons-Moll2;~Richard_A._Newcombe1;~Karen_Liu1;~Yuting_Ye2;~Lingni_Ma1",
        "gender": ";;M;;M;;F;F",
        "homepage": "https://virtualhumans.mpi-inf.mpg.de/people/Guzov.html;;;;;https://cs.stanford.edu/~karenliu;http://yutingye.info;",
        "dblp": "289/0612;;261/3476;;01/8653;;;126/4514",
        "google_scholar": "vFSOgzEAAAAJ;;mhaiL5MAAAAJ;;MhowvPkAAAAJ;i28fU0MAAAAJ;Zxb8sdkAAAAJ;https://scholar.google.nl/citations?user=eUAgpwkAAAAJ",
        "orcid": "0000-0003-1304-5577;;;;;0000-0001-5926-0905;0000-0003-2643-7457;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Vladimir_Guzov1;~Yifeng_Jiang1;~Fangzhou_Hong1;~Gerard_Pons-Moll2;~Richard_A._Newcombe1;~Karen_Liu1;~Yuting_Ye2;~Lingni_Ma1",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen+Saarland Informatics Campus, Max-Planck Institute;;Nanyang Technological University;;Meta, Reality Labs Research;Computer Science Department, Stanford University;Meta;Meta",
        "aff_domain": "uni-tuebingen.de+mpi-inf.mpg.de;;ntu.edu.sg;;meta.com;cs.stanford.edu;meta.com;meta.com",
        "position": "PhD student+PhD student;;PhD student;;Principal Researcher;Full Professor;Research Scientist;Researcher",
        "bibtex": "@inproceedings{\nguzov2025hmd,\ntitle={{HMD}{\\textasciicircum}2: Environment-aware Motion Generation from Single Egocentric Head-Mounted Device},\nauthor={Vladimir Guzov and Yifeng Jiang and Fangzhou Hong and Gerard Pons-Moll and Richard Newcombe and Karen Liu and Yuting Ye and Lingni Ma},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=sQe8zAYt5c}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sQe8zAYt5c",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "sRBNwH2Qal",
        "title": "CamCtrl3D: Single-Image Scene Exploration With Precise 3D Camera Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose a method for generating fly-through videos of a scene, from a single image and a given camera trajectory.\nWe build upon an image-to-video latent diffusion model. We condition its UNet denoiser on the camera trajectory, using four techniques.\n(1) We condition UNet's temporal blocks on raw camera extrinsics, similar to MotionCtrl.\n(2) We use images containing camera ray parameters, similar to CameraCtrl.\n(3) We re-project the initial image to subsequent frames and condition on the resulting video.\n(4) We introduce a global 3D representation using 2D$\\Leftrightarrow$3D transformers, which implicitly conditions on the camera poses.\nWe combine all conditions in a ContolNet-style architecture. We then propose a metric that evaluates overall video quality and the ability to preserve details with view changes, which we use to analyze the trade-offs of individual and combined conditions. Finally, we identify an optimal combination of conditions. We calibrate camera positions in our datasets for scale consistency across scenes, and we train our scene exploration model, CameraCtrl3D, demonstrating state-of-the-art results.\nExample videos generated by CameraCtrl3D are available at  https://camctrl3d.github.io/",
        "keywords": "latent video models;novel view synthesis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Stefan Popov;Amit Raj;Yuanzhen Li;Michael Krainin;William T. Freeman;Michael Rubinstein",
        "authorids": "~Stefan_Popov1;~Amit_Raj1;~Yuanzhen_Li1;~Michael_Krainin1;~William_T._Freeman1;~Michael_Rubinstein1",
        "gender": "M;M;F;;M;M",
        "homepage": "http://popov.im;https://amitraj93.github.io/;http://people.csail.mit.edu/yzli/;;https://billf.mit.edu/;http://people.csail.mit.edu/mrub/",
        "dblp": "72/5256;84/531;97/371;60/679;86/6650;16/1356",
        "google_scholar": "Glq3dWkAAAAJ;JVumcGEAAAAJ;k1eaag4AAAAJ;h_Izm-wAAAAJ;https://scholar.google.com.tw/citations?user=0zZnyMEAAAAJ;ttBdcmsAAAAJ",
        "orcid": ";;0000-0002-9831-8249;;;",
        "linkedin": "https://linkedin.com/in/stefanpopov;;yuanzhen-yz-li-5561655/;;;",
        "or_profile": "~Stefan_Popov1;~Amit_Raj1;~Yuanzhen_Li1;~Michael_Krainin1;~William_T._Freeman1;~Michael_Rubinstein1",
        "aff": "Google;Google DeepMind;Google;;Google+Massachusetts Institute of Technology+Massachusetts Institute of Technology;Google DeepMind",
        "aff_domain": "google.com;google.com;google.com;;google.com+mit.edu+mit.edu;google.com",
        "position": "Software Engineer;Researcher;Software Engineer;;Director+Full Professor+Professor;Principal Scientist",
        "bibtex": "@inproceedings{\npopov2025camctrld,\ntitle={CamCtrl3D: Single-Image Scene Exploration With Precise 3D Camera Control},\nauthor={Stefan Popov and Amit Raj and Yuanzhen Li and Michael Krainin and William T. Freeman and Michael Rubinstein},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=sRBNwH2Qal}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sRBNwH2Qal",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "tdEfYDta19",
        "title": "Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We tackle the problem of generating highly realistic and plausible mirror reflections using diffusion-based generative models. We formulate this problem as an image inpainting task, allowing for more user control over the placement of mirrors during the generation process. To enable this, we create SynMirror, a large-scale dataset of diverse synthetic scenes with objects placed in front of mirrors. SynMirror contains around 198k samples rendered from 66k unique 3D objects, along with their associated depth maps, normal maps and instance-wise segmentation masks, to capture relevant geometric properties of the scene. Using this dataset, we propose a novel depth-conditioned inpainting method called MirrorFusion, which generates high-quality, realistic, shape and appearance-aware reflections of real-world objects. MirrorFusion outperforms state-of-the-art methods on SynMirror, as demonstrated by extensive quantitative and qualitative analysis. To the best of our knowledge, we are the first to successfully tackle the challenging problem of generating controlled and faithful mirror reflections of an object in a scene using diffusion-based models. SynMirror and MirrorFusion open up new avenues for image editing and augmented reality applications for practitioners and researchers alike. The project page is available at: https://val.cds.iisc.ac.in/reflecting-reality.github.io/.",
        "keywords": "Diffusion-Models;Reflections",
        "primary_area": "",
        "supplementary_material": "/attachment/8f798d3cb287083ed8485ec7288e782a53756e31.pdf",
        "author": "Ankit Dhiman;Manan Shah;Rishubh Parihar;Yash Sanjay Bhalgat;LOKESH R BOREGOWDA;Venkatesh Babu Radhakrishnan",
        "authorids": "~Ankit_Dhiman1;~Manan_Shah1;~Rishubh_Parihar1;~Yash_Sanjay_Bhalgat1;~LOKESH_R_BOREGOWDA1;~Venkatesh_Babu_Radhakrishnan2",
        "gender": "M;M;M;M;M;M",
        "homepage": ";https://cs-mshah.github.io/;;https://yashbhalgat.github.io;;http://cds.iisc.ac.in/faculty/venky",
        "dblp": "227/7222;51/9584;264/3534;186/8383;;20/6289",
        "google_scholar": "https://scholar.google.co.in/citations?user=vPKjfz0AAAAJ;iIIm36YAAAAJ;RaRoJFYAAAAJ;https://scholar.google.com/citations?hl=en;;cVg7HrEAAAAJ",
        "orcid": ";0009-0003-2511-2551;;;;0000-0002-1926-1804",
        "linkedin": ";https://linkedin.com/in/manan-shah-2a5779212;;yashbhalgat/;lokesh-boregowda-613a90a2/;venkatesh-babu-radhakrishnan-16568939",
        "or_profile": "~Ankit_Dhiman1;~Manan_Shah1;~Rishubh_Parihar1;~Yash_Sanjay_Bhalgat1;~LOKESH_R_BOREGOWDA1;~Venkatesh_Babu_Radhakrishnan2",
        "aff": "Indian Institute of Science, Bangalore;Indian Institute of Science, Indian institute of science, Bangalore;Indian Institute of Science, Bangalore;University of Oxford;Samsung R&D India Bangalore ;Indian Institute of Science",
        "aff_domain": "iisc.ernet.in;iisc.ac.in;iisc.ac.in;ox.ac.uk;samsung.com;iisc.ac.in",
        "position": "PhD student;Researcher;PhD student;PhD student;Senior Director;Full Professor",
        "bibtex": "@inproceedings{\ndhiman2025reflecting,\ntitle={Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections},\nauthor={Ankit Dhiman and Manan Shah and Rishubh Parihar and Yash Sanjay Bhalgat and LOKESH R BOREGOWDA and Venkatesh Babu Radhakrishnan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=tdEfYDta19}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tdEfYDta19",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "u8OalqcKL5",
        "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Due to the rapid development of panorama cameras, the task of estimating panorama depth has attracted significant attention from the computer vision community, especially in applications such as robot sensing and autonomous driving. \nHowever, existing methods relying on different projection formats often encounter challenges, either struggling with distortion and discontinuity in the case of equirectangular, cubemap, and tangent projections, or experiencing a loss of texture details with the spherical projection. \nTo tackle these concerns, we present SphereFusion, an end-to-end framework that combines the strengths of various projection methods. \nSpecifically, SphereFusion initially employs 2D image convolution and mesh operations to extract two distinct types of features from the panorama image in both equirectangular and spherical projection domains. These features are then projected onto the spherical domain, where a gate fusion module selects the most reliable features for fusion. Finally, SphereFusion estimates panorama depth within the spherical domain.\nMeanwhile, SphereFusion employs a cache strategy to improve the efficiency of mesh operation.\nExtensive experiments on three public panorama datasets demonstrate that SphereFusion achieves competitive results with other state-of-the-art methods, while presenting the fastest inference speed at only 17 ms on a 512$\\times$1024 panorama image.",
        "keywords": "Panorama Depth Estimation;Equirectangular Projection;Spherical Projection;Cubemap Projection",
        "primary_area": "",
        "supplementary_material": "/attachment/65b4cd1b53ca20fc773fd58a8ba045192d014511.pdf",
        "author": "Qingsong Yan;Qiang Wang;Kaiyong Zhao;Jie Chen;Bo Li;Xiaowen Chu;Fei Deng",
        "authorids": "~Qingsong_Yan1;~Qiang_Wang14;~Kaiyong_Zhao1;~Jie_Chen3;~Bo_Li33;~Xiaowen_Chu2;~Fei_Deng2",
        "gender": "M;M;M;M;;M;M",
        "homepage": ";http://faculty.hitsz.edu.cn/wangqiang;;https://jchenhkg.github.io/;;https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/CHU-Xiaowen/xwchu;https://ieeexplore.ieee.org/author/37589876500",
        "dblp": ";64/5630-22;;92/6289-26;;24/2536;",
        "google_scholar": "QHT-c8AAAAAJ;6YzjcNgAAAAJ;https://scholar.google.com.hk/citations?user=j9u-fwpgKxQC;qrWi1RYAAAAJ;;https://scholar.google.com.hk/citations?user=v4rX24EAAAAJ;",
        "orcid": ";0000-0002-2986-967X;;;;0000-0001-9745-4372;",
        "linkedin": "qingsong-yan-a20630151/;;kaiyong-zhao-17a02b1a/;;;;",
        "or_profile": "~Qingsong_Yan1;~Qiang_Wang14;~Kaiyong_Zhao1;~Jie_Chen3;~Bo_Li33;~Xiaowen_Chu2;~Fei_Deng2",
        "aff": "Wuhan University;Harbin Institute of Technology;XGRIDS;Hong Kong Baptist University;;Hong Kong University of Science and Technology (Guangzhou);Wuhan University",
        "aff_domain": "whu.edu.cn;hit.edu.cn;xgrids.com;hkbu.edu.hk;;ust.hk;whu.edu.cn",
        "position": "PhD student;Associate Professor;CEO;Assistant Professor;;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nyan2025spherefusion,\ntitle={SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion},\nauthor={Qingsong Yan and Qiang Wang and Kaiyong Zhao and Jie Chen and Bo Li and Xiaowen Chu and Fei Deng},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=u8OalqcKL5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=u8OalqcKL5",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uIJyMIu1UY",
        "title": "Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The goal of this paper is to encode a 3D scene into an extremely compact representation from 2D images and to enable its transmittance, decoding and rendering in real-time across various platforms. Despite the progress in NeRFs and Gaussian Splats, their large model size and specialized renderers make it challenging to distribute free-viewpoint 3D content as easily as images.  To address this, we have designed a novel 3D representation that encodes the plenoptic function into sinusoidal function indexed dense volumes. This approach facilitates feature sharing across different locations, improving compactness over traditional spatial voxels. The memory footprint of the dense 3D feature grid can be further reduced using spatial decomposition techniques. This design combines the strengths of spatial hashing functions and voxel decomposition, resulting in a model size as small as 150 KB for each 3D scene. Moreover, PPNG features a lightweight rendering pipeline with only 300 lines of code that decodes its representation into standard GL textures and fragment shaders. This enables real-time rendering using the traditional GL pipeline, ensuring universal compatibility and efficiency across various platforms without additional dependencies.",
        "keywords": "NeRF;Efficient NeRF;Real-time NeRF",
        "primary_area": "",
        "supplementary_material": "/attachment/99180f57d5fd495a9d91b7818105c48dc1bdc7d0.pdf",
        "author": "Jae Yong Lee;Yuqun Wu;Chuhang Zou;Derek Hoiem;Shenlong Wang",
        "authorids": "~Jae_Yong_Lee1;~Yuqun_Wu1;~Chuhang_Zou3;~Derek_Hoiem1;~Shenlong_Wang1",
        "gender": "M;M;F;M;M",
        "homepage": "https://jyl.kr;http://yuqunw.github.io;https://zouchuhang.github.io;http://dhoiem.cs.illinois.edu/;https://shenlong.web.illinois.edu/",
        "dblp": ";;138/1927;08/6948;117/4842",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;jBcKes0AAAAJ;8Sfj7q8AAAAJ;QFpswmcAAAAJ",
        "orcid": "0000-0001-9256-7714;;0000-0003-2537-284X;;",
        "linkedin": "jae-yong-lee-72a8b18a/;;chuhang-zou/;;shenlong-wang-3496023b",
        "or_profile": "~Jae_Yong_Lee1;~Yuqun_Wu1;~Chuhang_Zou3;~Derek_Hoiem1;~Shenlong_Wang1",
        "aff": "Meta Facebook;Department of Computer Science;Meta;University of Illinois, Urbana Champaign+Reconstruct;University of Illinois, Urbana Champaign",
        "aff_domain": "meta.com;cs.illinois.edu;meta.com;illinois.edu+reconstructinc.com;illinois.edu",
        "position": "Computer Vision Engineer;PhD student;Researcher;Full Professor+Chief Scientist;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025plenoptic,\ntitle={Plenoptic {PNG}: Real-Time Neural Radiance Fields in 150 {KB}},\nauthor={Jae Yong Lee and Yuqun Wu and Chuhang Zou and Derek Hoiem and Shenlong Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=uIJyMIu1UY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uIJyMIu1UY",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "uuc5ULvg3w",
        "title": "DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reconstructing scenes and tracking motion are two sides of the same coin. Tracking points allow for geometric reconstruction [13], while geometric reconstruction of (dynamic) scenes allows for 3D tracking of points over time [22, 36]. The latter was recently also exploited for 2D point tracking to overcome occlusion ambiguities by lifting tracking directly into 3D [35]. However, above approaches either require offline processing or multi-view camera setups both unrealistic for real-world applications like robot navigation or mixed reality. We target the challenge of online 2D and 3D point tracking from unposed monocular camera input introducing Dynamic Online Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to reconstruct dynamic scenes in an online fashion. Our approach extends 3D Gaussians to capture new content and object motions while estimating camera movements from a single RGB frame. DynOMo stands out by enabling emergence of point trajectories through robust image feature reconstruction and a novel similarity-enhanced regularization term, without requiring any correspondence-level supervision. It sets the first baseline for online point tracking with monocular unposed cameras, achieving performance on par with existing methods. We aim to inspire the community to advance online point tracking and reconstruction, expanding the applicability to diverse real-world scenarios.",
        "keywords": "Point Tracking;Dynamic 3D Gaussian Splatting;Dynamic Online Monocular Reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/ff932b43cfcf821e6b966859e6183513ac7ab3d3.pdf",
        "author": "Jenny Seidenschwarz;Qunjie Zhou;Bardienus Pieter Duisterhof;Deva Ramanan;Laura Leal-Taix\u00e9",
        "authorids": "~Jenny_Seidenschwarz1;~Qunjie_Zhou1;~Bardienus_Pieter_Duisterhof1;~Deva_Ramanan1;~Laura_Leal-Taix\u00e91",
        "gender": "F;;M;M;F",
        "homepage": ";;https://bart-ai.com;https://www.cs.cmu.edu/~deva/;https://dvl.in.tum.de/team/lealtaixe/",
        "dblp": "285/4712;;243/5766;49/488;47/8483",
        "google_scholar": "7AiuDocAAAAJ;;LLsYMFYAAAAJ;9B8PoXUAAAAJ;tT2TC-UAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Jenny_Seidenschwarz1;~Qunjie_Zhou1;~Bardienus_Pieter_Duisterhof1;~Deva_Ramanan1;~Laura_Leal-Taix\u00e91",
        "aff": "Technical University of Munich;;Carnegie Mellon University;School of Computer Science, Carnegie Mellon University;NVIDIA+Technical University Munich",
        "aff_domain": "in.tum.de;;cmu.edu;cs.cmu.edu;nvidia.com+tum.de",
        "position": "PhD student;;PhD student;Full Professor;Principal Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nseidenschwarz2025dynomo,\ntitle={Dyn{OM}o: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction},\nauthor={Jenny Seidenschwarz and Qunjie Zhou and Bardienus Pieter Duisterhof and Deva Ramanan and Laura Leal-Taix{\\'e}},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=uuc5ULvg3w}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uuc5ULvg3w",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "v3XH8wGby3",
        "title": "UNIT: Unsupervised Online Instance Segmentation through Time",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Online object segmentation and tracking in Lidar point clouds enables autonomous agents to understand their surroundings and make safe decisions. Unfortunately, manual annotations for these tasks are prohibitively costly. We tackle this problem with the task of class-agnostic unsupervised online instance segmentation and tracking. To that end, we leverage an instance segmentation backbone and propose a new training recipe that enables the online tracking of objects. Our network is trained on pseudo-labels, eliminating the need for manual annotations. We conduct an evaluation using metrics adapted for temporal instance segmentation. Computing these metrics requires temporally-consistent instance labels. When unavailable, we construct these labels using the available 3D bounding boxes and semantic labels in the dataset. We compare our method against strong baselines and demonstrate its superiority across two different outdoor Lidar datasets.",
        "keywords": "unsupervised object segmentation",
        "primary_area": "",
        "supplementary_material": "/attachment/ec98a08dacb1394267e300d509095a3f37d83c54.pdf",
        "author": "Corentin Sautier;Gilles Puy;Alexandre Boulch;Renaud Marlet;Vincent Lepetit",
        "authorids": "~Corentin_Sautier1;~Gilles_Puy2;~Alexandre_Boulch1;~Renaud_Marlet1;~Vincent_Lepetit1",
        "gender": "M;;M;M;M",
        "homepage": "https://csautier.github.io/;;http://boulch.eu;http://imagine.enpc.fr/~marletr/;https://vincentlepetit.github.io",
        "dblp": "317/5523;;47/9368;61/5462;80/5556",
        "google_scholar": "xYDkHEsAAAAJ;;https://scholar.google.fr/citations?user=iJ3qFGAAAAAJ;2rclwh4AAAAJ;h0a5q3QAAAAJ",
        "orcid": ";;0000-0002-4196-9665;0000-0003-1612-1758;0000-0001-9985-4433",
        "linkedin": ";;https://linkedin.com/in/alexandre-boulch-0464b71b;renaud-marlet-9914ab/;vincent-lepetit-58a18bb/",
        "or_profile": "~Corentin_Sautier1;~Gilles_Puy2;~Alexandre_Boulch1;~Renaud_Marlet1;~Vincent_Lepetit1",
        "aff": "ENPC, Ecole Nationale des Ponts et Chausees;;Valeo;Inria+Valeo+Ecole des Ponts ParisTech;ENPC ParisTech",
        "aff_domain": "imagine.enpc.fr;;valeo.com;inria.fr+valeo.com+enpc.fr;enpc.fr",
        "position": "PhD student;;Research scientist;Researcher+Principal Scientist+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nsautier2025unit,\ntitle={{UNIT}: Unsupervised Online Instance Segmentation through Time},\nauthor={Corentin Sautier and Gilles Puy and Alexandre Boulch and Renaud Marlet and Vincent Lepetit},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=v3XH8wGby3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=v3XH8wGby3",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "w1M87lE7D2",
        "title": "Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in multimodal large language models (LLMs) have demonstrated significant potential across various domains, particularly in concept reasoning. However, their applications in understanding 3D environments remain limited, primarily offering textual or numerical outputs without generating dense, informative segmentation masks. This paper introduces Reason3D, a novel LLM designed for comprehensive 3D understanding. Reason3D processes point cloud data and text prompts to produce textual responses and segmentation masks, enabling advanced tasks such as 3D reasoning segmentation, hierarchical searching, express referring, and question answering with detailed mask outputs. We propose a hierarchical mask decoder that employs a coarse-to-fine approach to segment objects within expansive scenes. It begins with a coarse location estimation, followed by object mask estimation, using two unique tokens predicted by LLMs based on the textual query. Experimental results on large-scale ScanNet and Matterport3D datasets validate the effectiveness of our Reason3D across various tasks.",
        "keywords": "3D Reasoning Segmentation;3D Hierarchical Searching;3D Segmentation with LLM",
        "primary_area": "",
        "supplementary_material": "/attachment/7d5ad6da3b08f0884388fbb554a02660f16692d0.pdf",
        "author": "Kuan-Chih Huang;Xiangtai Li;Lu Qi;Shuicheng YAN;Ming-Hsuan Yang",
        "authorids": "~Kuan-Chih_Huang1;~Xiangtai_Li1;~Lu_Qi1;~Shuicheng_YAN3;~Ming-Hsuan_Yang1",
        "gender": "M;M;M;M;M",
        "homepage": "https://kuanchihhuang.github.io/;https://lxtgh.github.io/;https://www.luqi.info;https://yanshuicheng.ai/;https://faculty.ucmerced.edu/mhyang/",
        "dblp": "58/9427;239/4017;;y/ShuichengYan;79/3711.html",
        "google_scholar": "9tPZXEcAAAAJ;FL3ReD0AAAAJ;https://scholar.google.com.hk/citations?user=SSI90d4AAAAJ;https://scholar.google.com.hk/citations?user=DNuiPHwAAAAJ;p9-ohHsAAAAJ",
        "orcid": ";;;0000-0001-8906-3777;0000-0003-4848-2304",
        "linkedin": ";;;;minghsuanyang/",
        "or_profile": "~Kuan-Chih_Huang1;~Xiangtai_Li1;~Lu_Qi1;~Shuicheng_YAN3;~Ming-Hsuan_Yang1",
        "aff": "University of California, Merced;ByteDance Inc.;Insta360+Wuhan University;National University of Singapore;Google DeepMind+University of California at Merced",
        "aff_domain": "ucmerced.edu;bytedance.com;insta360.com+whu.edu.cn;nus.edu.sg;google.com+umcerced.edu",
        "position": "PhD student;Researcher;Principal Researcher+Full Professor;Full Professor;Senior Staff Research Scientist+Professor",
        "bibtex": "@inproceedings{\nhuang2025reasond,\ntitle={Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model},\nauthor={Kuan-Chih Huang and Xiangtai Li and Lu Qi and Shuicheng YAN and Ming-Hsuan Yang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=w1M87lE7D2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=w1M87lE7D2",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "w9a1XRUKpi",
        "title": "XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Comprehensive testing of autonomous systems through simulation is essential to ensure the safety of autonomous driving vehicles. This requires the generation of safety-critical scenarios that extend beyond the limitations of real-world data collection, as many of these scenarios are rare or rarely encountered on public roads.\nHowever, evaluating most existing novel view synthesis (NVS) methods relies on sporadic sampling of image frames from the training data, comparing the rendered images with ground-truth images. Unfortunately, this evaluation protocol falls short of meeting the actual requirements in closed-loop simulations. Specifically, the true application demands the capability to render novel views that extend beyond the original trajectory (such as cross-lane views), which are challenging to capture in the real world.\nTo address this, this paper presents a synthetic dataset for novel driving view synthesis evaluation, which is specifically designed for autonomous driving simulations. This unique dataset includes testing images captured by deviating from the training trajectory by $1-4$ meters. It comprises six sequences that cover various times and weather conditions. Each sequence contains $450$ training images, $120$ testing images, and their corresponding camera poses and intrinsic parameters. Leveraging this novel dataset, we establish the first realistic benchmark for evaluating existing NVS approaches under front-only and multicamera settings. The experimental findings underscore the significant gap in current approaches, revealing their inadequate ability to fulfill the demanding prerequisites of cross-lane or closed-loop simulation.\nOur dataset and code are released publicly on the project page: https://3d-aigc.github.io/XLD.",
        "keywords": "Dataset;Novel View Synthesis;Autonomous Driving",
        "primary_area": "",
        "supplementary_material": "/attachment/e83bc25213509413991653000e21400111eb68e6.pdf",
        "author": "Hao Li;Chenming Wu;Ming Yuan;Yan Zhang;Chen Zhao;Chunyu Song;Haocheng Feng;Errui Ding;Dingwen Zhang;Jingdong Wang",
        "authorids": "~Hao_Li35;~Chenming_Wu1;~Ming_Yuan3;~Yan_Zhang46;~Chen_Zhao9;~Chunyu_Song2;~Haocheng_Feng1;~Errui_Ding2;~Dingwen_Zhang2;~Jingdong_Wang1",
        "gender": ";M;M;M;;;;M;M;M",
        "homepage": ";https://chenming-wu.github.io/;;;;;;;https://zdw-nwpu.github.io/dingwenz.github.com/;https://jingdongwang2017.github.io/",
        "dblp": ";190/5879;;;;;;180/5531;150/6620;49/3441",
        "google_scholar": ";https://scholar.google.com.hk/citations?user=eOkkQWUAAAAJ;;https://scholar.google.com/citations?view_op=list_works;;;;1wzEtxcAAAAJ;;z5SPCmgAAAAJ",
        "orcid": ";0000-0001-8012-1547;;;;;;;;0000-0002-4888-4445",
        "linkedin": ";;%E6%98%8E-%E8%A2%81-8268b5310/;;;;;;;",
        "or_profile": "~Hao_Li35;~Chenming_Wu1;~Ming_Yuan3;~Yan_Zhang46;~Chen_Zhao9;~Chunyu_Song2;~Haocheng_Feng1;~Errui_Ding2;~Dingwen_Zhang2;~Jingdong_Wang1",
        "aff": ";Baidu;;;;;;Baidu;Northwest Polytechnical University Xi'an;Baidu",
        "aff_domain": ";baidu.com;;;;;;baidu.com;nwpu.edu.cn;baidu.com",
        "position": ";Researcher;;;;;;Director;Full Professor;Chief Scientist for Computer Vision",
        "bibtex": "@inproceedings{\nli2025xld,\ntitle={{XLD}: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis},\nauthor={Hao Li and Chenming Wu and Ming Yuan and Yan Zhang and Chen Zhao and Chunyu Song and Haocheng Feng and Errui Ding and Dingwen Zhang and Jingdong Wang},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=w9a1XRUKpi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=w9a1XRUKpi",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wgLwIY0Wk6",
        "title": "AG-MAE: Anatomically Guided Spatio-Temporal Masked Auto-Encoder for Online Hand Gesture Recognition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Hand gesture recognition plays a crucial role in the domain of computer vision, as it enhances human-computer interaction by enabling intuitive, touch-free control and communication. While offline methods have made significant advances in isolated gesture recognition, real-world applications demand online and continuous processing. Skeleton-based methods, though effective, face challenges due to the intricate nature of hand joints and the diverse 3D motions they induce. This paper introduces AG-MAE, a novel approach that integrates anatomical constraints to guide the self-supervised training of a spatio-temporal masked autoencoder, enhancing the learning of 3D keypoint representations. By incorporating anatomical knowledge, AG-MAE learns more discriminative features for hand poses and movements, subsequently improving online gesture recognition. Evaluation on standard datasets demonstrates the superiority of our approach and its potential for real-world applications. Code is available at: https://github.com/lambda-xyz-01/AGMAE.",
        "keywords": "Online gesture recognition;representation learning;self-supervised learning;bio-mechanical constraints",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Omar Ikne;Benjamin Allaert;Hazem Wannous",
        "authorids": "~Omar_Ikne1;~Benjamin_Allaert1;~Hazem_Wannous2",
        "gender": ";M;M",
        "homepage": "https://o-ikne.github.io/;;",
        "dblp": ";;97/5331.html",
        "google_scholar": "sbs8pg4AAAAJ;;WsdQ9Z0AAAAJ",
        "orcid": "0000-0002-6170-6854;0000-0002-4291-9803;0000-0001-8475-4309",
        "linkedin": "https://linkedin.com/in/omar-ikne-144319183;benjamin-allaert-80b300129/;hazemwannous/",
        "or_profile": "~Omar_Ikne1;~Benjamin_Allaert1;~Hazem_Wannous2",
        "aff": "IMT NORD EUROPE;Institut Mines-T\u00e9l\u00e9com Nord Europe;IMT Nord Europe, ",
        "aff_domain": "imt-nord-europe.fr;imt-nord-europde.fr;imt-nord-europe.fr",
        "position": "PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nikne2025agmae,\ntitle={{AG}-{MAE}: Anatomically Guided Spatio-Temporal Masked Auto-Encoder for Online Hand Gesture Recognition},\nauthor={Omar Ikne and Benjamin Allaert and Hazem Wannous},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=wgLwIY0Wk6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wgLwIY0Wk6",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "wxaQnHN1Vu",
        "title": "MotionDreamer: Exploring Semantic Video Diffusion features for Zero-Shot 3D Mesh Animation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Animation techniques bring digital 3D worlds and characters to life. However, manual animation is tedious and automated techniques are often specialized to narrow shape classes. In our work, we propose a technique for automatic re-animation of various 3D shapes based on a motion prior extracted from a video diffusion model. Unlike existing 4D generation methods, we focus solely on the motion, and we leverage an explicit mesh-based representation compatible with existing computer-graphics pipelines. Furthermore, our utilization of diffusion features enhances accuracy of our motion fitting. We analyze efficacy of these features for animation fitting and we experimentally validate our approach for two different diffusion models and four animation models. Finally, we demonstrate that our time-efficient zero-shot method achieves a superior performance re-animating a diverse set of 3D shapes when compared to existing techniques in a user study.",
        "keywords": "3D Animation;Text-to-Animation;Video Diffusion",
        "primary_area": "",
        "supplementary_material": "/attachment/169026faf1ea4b863cdd76df6450b8ec2e3f8cba.zip",
        "author": "Lukas Uzolas;Elmar Eisemann;Petr Kellnhofer",
        "authorids": "~Lukas_Uzolas1;~Elmar_Eisemann1;~Petr_Kellnhofer1",
        "gender": ";M;M",
        "homepage": "https://lukas.uzolas.com/;https://graphics.tudelft.nl/~eisemann/;http://kellnhofer.xyz",
        "dblp": "248/9451;65/2556;35/11357",
        "google_scholar": "d21AHFQAAAAJ;https://scholar.google.com.tw/citations?user=SFj3QkQAAAAJ;Lh54BvgAAAAJ",
        "orcid": "0000-0002-6857-8795;0000-0003-4153-065X;",
        "linkedin": ";elmar-eisemann-2237713/;",
        "or_profile": "~Lukas_Uzolas1;~Elmar_Eisemann1;~Petr_Kellnhofer1",
        "aff": "Delft University of Technology;Delft University of Technology;Delft University of Technology",
        "aff_domain": "tudelft.nl;tudelft.nl;tudelft.nl",
        "position": "PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nuzolas2025motiondreamer,\ntitle={MotionDreamer: Exploring Semantic Video Diffusion features for Zero-Shot 3D Mesh Animation},\nauthor={Lukas Uzolas and Elmar Eisemann and Petr Kellnhofer},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=wxaQnHN1Vu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wxaQnHN1Vu",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "xdiqbFlKsu",
        "title": "Lightplane: Highly-Scalable Components for Neural 3D Fields",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision.\n    However, current designs for these 2D-3D mapping are memory-intensive, posing a significant bottleneck for existing methods and hindering new applications.\n    In response, we propose a pair of highly scalable components for 3D neural fields: Lightplane Renderer and Splatter, which significantly reduce memory usage in 2D-3D mapping (over $\\bf{1000\\times}$).\n    These innovations enable the processing of vastly more and higher resolution images with significantly small memory and computational costs.\n We demonstrate their utility across various applications, from optimizing with image-level losses to enabling a versatile pipeline for scaling 3D reconstruction and generation.",
        "keywords": "Volumetric Rendering;3D Generation;3D Reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/84a29e8ee47da6a9fd2fa110eca401be4c48f55d.zip",
        "author": "Ang Cao;Justin Johnson;Andrea Vedaldi;David Novotny",
        "authorids": "~Ang_Cao1;~Justin_Johnson1;~Andrea_Vedaldi1;~David_Novotny1",
        "gender": "M;M;M;M",
        "homepage": "https://caoang327.github.io/;http://cs.stanford.edu/people/jcjohns/;https://www.robots.ox.ac.uk/~vedaldi/;https://d-novotny.github.io/",
        "dblp": "270/4548;04/3396;99/2825;161/9863",
        "google_scholar": "HtD-aVUAAAAJ;mS5k4CYAAAAJ;bRT7t28AAAAJ;2glXz7cAAAAJ",
        "orcid": ";;0000-0003-1374-2858;",
        "linkedin": ";;;",
        "or_profile": "~Ang_Cao1;~Justin_Johnson1;~Andrea_Vedaldi1;~David_Novotny1",
        "aff": "University of Michigan - Ann Arbor;World Labs;University of Oxford+Meta;Meta",
        "aff_domain": "umich.edu;worldlabs.ai;ox.ac.uk+meta.com;meta.com",
        "position": "PhD student;Cofounder;Full Professor+Researcher;Researcher",
        "bibtex": "@inproceedings{\ncao2025lightplane,\ntitle={Lightplane: Highly-Scalable Components for Neural 3D Fields},\nauthor={Ang Cao and Justin Johnson and Andrea Vedaldi and David Novotny},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=xdiqbFlKsu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xdiqbFlKsu",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yEm69udegI",
        "title": "Garment3DGen: 3D Garment Stylization and Texture Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. \nOur proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. \nWe leverage the recent progress of image-to-3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Carefully designed losses allow the base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. \nFinally, we generate high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. \nWith Garment3DGen users can generate the simulation-ready 3D garment of their choice without the need of artist intervention. \nWe present a plethora of quantitative and qualitative comparisons on various assets and demonstrate that Garment3DGen unlocks key applications ranging from sketch-to-simulated garments or interacting with the garments in VR. Code will be publicly available.",
        "keywords": "Garment generation;Cloth Simulation",
        "primary_area": "",
        "supplementary_material": "/attachment/3c12fcfc55e5403b9ca3c5a7df3d13ae78fab466.pdf",
        "author": "Nikolaos Sarafianos;Tuur Stuyck;Xiaoyu Xiang;YILEI LI;Jovan Popovi\u0107;Rakesh Ranjan",
        "authorids": "~Nikolaos_Sarafianos2;~Tuur_Stuyck1;~Xiaoyu_Xiang1;~YILEI_LI1;~Jovan_Popovi\u01071;~Rakesh_Ranjan2",
        "gender": "M;M;F;;M;",
        "homepage": "https://nsarafianos.github.io/;https://tuurstuyck.github.io/index.html#books;https://engineering.purdue.edu/people/xiaoyu.xiang.1;https://liyilui.github.io/personal_page/;;",
        "dblp": "169/3215;182/7143;241/5710;;;",
        "google_scholar": "O_TOBmAAAAAJ;buAnzJsAAAAJ;KTn1AoUAAAAJ;iTp5xFcAAAAJ;a3K-f2YAAAAJ;",
        "orcid": ";0000-0003-1892-2137;0000-0002-5999-9133;;;",
        "linkedin": ";tuurstuyck/;xiaoyuxiang/;;;",
        "or_profile": "~Nikolaos_Sarafianos2;~Tuur_Stuyck1;~Xiaoyu_Xiang1;~YILEI_LI1;~Jovan_Popovi\u01071;~Rakesh_Ranjan2",
        "aff": "Meta;Meta;Meta Facebook;Meta Facebook;Meta Facebook;",
        "aff_domain": "meta.com;meta.com;meta.com;fb.com;meta.com;",
        "position": "Research Scientist;Researcher;Researcher;Researcher;Researcher;",
        "bibtex": "@inproceedings{\nsarafianos2025garmentdgen,\ntitle={Garment3{DG}en: 3D Garment Stylization and Texture Generation},\nauthor={Nikolaos Sarafianos and Tuur Stuyck and Xiaoyu Xiang and YILEI LI and Jovan Popovi{\\'c} and Rakesh Ranjan},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=yEm69udegI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yEm69udegI",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "yiQHBPK6Ba",
        "title": "NeuHMR: Neural Rendering-Guided Human Motion Reconstruction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reconstructing 3D human movements from video sequences is an important task in the fields of computer vision, graphics, and biomechanics. Although much progress has been made to infer 3D human mesh based on visual contexts provided in video sequences, generalization to in-the-wild videos still remains challenging for existing human mesh recovery (HMR) methods. To overcome inaccurate prediction, they can perform a second step optimization that refines the inaccurate estimations continuously at test time. Most optimization methods seek fitting of the body joints in the image space with respect to pseudo ground truth predicted by an off-the-shelf key point detector. However, state-of-the-art detectors still introduce errors, especially for challenging poses. In this work, we rethink the dependency on the 2D key point fitting paradigm and present NeuHMR, an optimization-based mesh recovery framework based on recent advances in neural rendering. Our method builds on Human Neural Radiance Fields that allow the refinement of human motions through animatable 2D renderings. We evaluated our method on two common benchmarks and validated its effectiveness.",
        "keywords": "Human Mesh Recovery; 3D from 2D",
        "primary_area": "",
        "supplementary_material": "/attachment/2b41036f4958d868a6e005bdbc0a437692c0c909.pdf",
        "author": "Tiange Xiang;Kuan-Chieh Wang;Jaewoo Heo;Ehsan Adeli;Serena Yeung-Levy;Scott Delp;Li Fei-Fei",
        "authorids": "~Tiange_Xiang1;~Kuan-Chieh_Wang1;~Jaewoo_Heo1;~Ehsan_Adeli1;~Serena_Yeung-Levy1;~Scott_Delp1;~Li_Fei-Fei1",
        "gender": "M;;M;M;;M;F",
        "homepage": "https://tiangexiang.github.io/;https://wangkua1.github.io;;http://stanford.edu/~eadeli/;;https://nmbl.stanford.edu/people/scott-delp/;https://profiles.stanford.edu/fei-fei-li",
        "dblp": "245/7663;13/7562;;93/2941;;;79/2528",
        "google_scholar": ";https://scholar.google.ca/citations?user=LgMuT6IAAAAJ;uAmuZXkAAAAJ;7NX_J_cAAAAJ;;OEivUAQAAAAJ;rDfyQnIAAAAJ",
        "orcid": ";;;0000-0002-0579-7763;;;",
        "linkedin": ";;jaewoo-jeffrey-h/;eadeli;;;fei-fei-li-4541247/",
        "or_profile": "~Tiange_Xiang1;~Kuan-Chieh_Wang1;~Jaewoo_Heo1;~Ehsan_Adeli1;~Serena_Yeung-Levy1;~Scott_Delp1;~Li_Fei-Fei1",
        "aff": "Stanford University;Snap Inc.;Computer Science Department, Stanford University+Stanford University;Stanford University;;Stanford University;Stanford University",
        "aff_domain": "stanford.edu;snapchat.com;cs.stanford.edu+stanford.edu;stanford.edu;;stanford.edu;stanford.edu",
        "position": "PhD student;Researcher;Intern+Undergrad student;Assistant Professor;;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nxiang2025neuhmr,\ntitle={Neu{HMR}: Neural Rendering-Guided Human Motion Reconstruction},\nauthor={Tiange Xiang and Kuan-Chieh Wang and Jaewoo Heo and Ehsan Adeli and Serena Yeung-Levy and Scott Delp and Li Fei-Fei},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=yiQHBPK6Ba}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yiQHBPK6Ba",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zillAxIblY",
        "title": "PIR: Photometric Inverse Rendering with Shading Cues Modeling and Surface Reflectance Regularization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper addresses the problem of inverse rendering from photometric images. Existing approaches for this problem suffer from the effects of self-shadows, inter-reflections, and lack of constraints on the surface reflectance, leading to inaccurate decomposition of reflectance and illuminations due to the ill-posed nature of inverse rendering. In this work, we propose a new method for neural inverse rendering. Our method jointly optimizes the light source position to account for the self-shadows in the images, and computes indirect illumination using a differentiable rendering layer and an importance sampling strategy. To enhance surface reflectance decomposition, we introduce a new regularization by distilling DINO features to foster accurate and consistent material decomposition. Extensive experiments on synthetic and real datasets demonstrate that our method outperforms state-of-the-art methods in reflectance decomposition.",
        "keywords": "Neural Rendering;Inverse Rendering",
        "primary_area": "",
        "supplementary_material": "/attachment/fff7c3d5707488c02e13d48ccec7856dc3144a62.zip",
        "author": "Jingzhi Bao;Guanying Chen;Shuguang Cui",
        "authorids": "~Jingzhi_Bao1;~Guanying_Chen2;~Shuguang_Cui1",
        "gender": "M;M;M",
        "homepage": "https://www.linkedin.com/in/jingzhibao/;https://guanyingc.github.io/;https://sse.cuhk.edu.cn/en/content/1415",
        "dblp": ";217/1487;48/4914",
        "google_scholar": ";https://scholar.google.com.hk/citations?hl=en;https://scholar.google.com.hk/citations?user=1o_qvR0AAAAJ",
        "orcid": ";;0000-0003-2608-775X",
        "linkedin": ";;",
        "or_profile": "~Jingzhi_Bao1;~Guanying_Chen2;~Shuguang_Cui1",
        "aff": "University of Illinois, Urbana Champaign;Sun Yat-sen University;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "illinois.edu;sysu.edu.cn;cuhk.edu.cn",
        "position": "MS student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nbao2025pir,\ntitle={{PIR}: Photometric Inverse Rendering with Shading Cues Modeling and Surface Reflectance Regularization},\nauthor={Jingzhi Bao and Guanying Chen and Shuguang Cui},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=zillAxIblY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zillAxIblY",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    },
    {
        "id": "zjDXlrjJGU",
        "title": "Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision together is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates contact data (local depth maps) with multi-view images to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object's geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend often to fail to reconstruct such objects with fidelity. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation in both the virtual and real world on objects with glossy and reflective surfaces to demonstrate the effectiveness of our approach in improving reconstruction quality.",
        "keywords": "3D reconstruction;robotic manipulation;tactile sensing",
        "primary_area": "",
        "supplementary_material": "/attachment/7e0f74d378e435a91aa81476292da7f0295a5b71.pdf",
        "author": "Mauro Comi;Alessio Tonioni;Jonathan Tremblay;Max Yang;Valts Blukis;Yijiong Lin;Nathan F. Lepora;Laurence Aitchison",
        "authorids": "~Mauro_Comi2;~Alessio_Tonioni1;~Jonathan_Tremblay1;~Max_Yang1;~Valts_Blukis1;~Yijiong_Lin1;~Nathan_F._Lepora1;~Laurence_Aitchison1",
        "gender": "M;M;Not Specified;M;M;;;",
        "homepage": "https://maurocomi.com/;https://alessiotonioni.github.io/;https://jtremblay.org/;https://scholar.google.com/citations?user=WQQ1vz8AAAAJ&hl=en;;;https://www.lepora.com;http://www.gatsby.ucl.ac.uk/~laurence/",
        "dblp": ";203/8831;17/8925;;210/9692;;76/10010;155/1918.html",
        "google_scholar": "JKSK648AAAAJ;https://scholar.google.it/citations?user=ry_BLFUAAAAJ;https://scholar.google.ca/citations?user=zeS5UJEAAAAJ;;i9-GzNYAAAAJ;;fb2WiJgAAAAJ;",
        "orcid": ";;;;;;;",
        "linkedin": ";alessio-tonioni-16261668/?originalSubdomain=ch;;;valtsblukis/;;;",
        "or_profile": "~Mauro_Comi2;~Alessio_Tonioni1;~Jonathan_Tremblay1;~Max_Yang1;~Valts_Blukis1;~Yijiong_Lin1;~Nathan_F._Lepora1;~Laurence_Aitchison1",
        "aff": "University of Bristol;Google;NVIDIA;University of Bristol;NVIDIA;;University of Bristol;University of Bristol",
        "aff_domain": "bristol.ac.uk;google.com;nvidia.com;bristol.ac.uk;nvidia.com;;bristol.ac.uk;bristol.ac.uk",
        "position": "PhD student;Researcher;Researcher;PhD student;Researcher;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ncomi2025snapit,\ntitle={Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces},\nauthor={Mauro Comi and Alessio Tonioni and Jonathan Tremblay and Max Yang and Valts Blukis and Yijiong Lin and Nathan F. Lepora and Laurence Aitchison},\nbooktitle={International Conference on 3D Vision 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=zjDXlrjJGU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zjDXlrjJGU",
        "pdf_size": 0,
        "rating": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "replies_avg": [
            0,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ]
    }
]