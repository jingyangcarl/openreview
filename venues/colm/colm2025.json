[
    {
        "id": "0JzWiigkUy",
        "title": "BEARCUBS: A benchmark for computer-using web agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern web agents possess computer use abilities that allow them to interact with webpages by sending commands to a virtual keyboard and mouse. While such agents have considerable potential to assist human users with complex tasks, evaluating their capabilities in real-world settings poses a major challenge. To this end, we introduce BEARCUBS, a \u201csmallbut mighty\u201d benchmark of 111 information-seeking questions designed to evaluate a web agent\u2019s ability to search, browse, and identify factual information from the web. Unlike prior web agent benchmarks, solving BEARCUBS requires (1) accessing live web content rather than synthetic or simulated pages, which captures the unpredictability of real-world web interactions; and (2) performing a broad range of multimodal interactions (e.g., video understanding, 3D navigation) that cannot be bypassed via text-based workarounds. Each question in BEARCUBS has a corresponding short, unambiguous answer and a human-validated browsing trajectory, allowing for transparent evaluation of agent performance and strategies. A human study confirms that BEARCUBS questions are solvable but non-trivial (84.7% human accuracy), revealing domain knowledge gaps and overlooked details as common failure points. We find that ChatGPT Agent significantly outperforms other computer-using agents with an overall accuracy of 65.8% (compared to e.g., Operator\u2019s 23.4%), showcasing substantial progress in tasks involving real computer use, such as playing web games and navigating 3D environments. Nevertheless, closing the gap to human performance requires improvements in areas like fine control, complex data filtering, and execution speed. To facilitate future research, BEARCUBS will be updated periodically to replace invalid or contaminated questions, keeping the benchmark fresh for future generations of web agents.",
        "keywords": "computer-use agent;benchmark;multimodal",
        "primary_area": "",
        "supplementary_material": "/attachment/a29bd3e2638735810acd3292f411a4a586ca58c3.zip",
        "author": "Yixiao Song;Katherine Thai;Chau Minh Pham;Yapei Chang;Mazin Nadaf;Mohit Iyyer",
        "authorids": "~Yixiao_Song1;~Katherine_Thai1;~Chau_Minh_Pham1;~Yapei_Chang1;~Mazin_Nadaf1;~Mohit_Iyyer1",
        "gender": "F;;;F;M;M",
        "homepage": "https://yixiao-song.github.io;https://katherinethai.github.io/;;https://lilakk.github.io/;;http://cs.umass.edu/~miyyer",
        "dblp": "331/5829;;;316/9933;;148/9178",
        "google_scholar": "4OgciqMAAAAJ;;;qCjnm-UAAAAJ;;rBVA5tcAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "songyixiao/;;;ella-yapei-chang/;mazin-nadaf/;",
        "or_profile": "~Yixiao_Song1;~Katherine_Thai1;~Chau_Minh_Pham1;~Yapei_Chang1;~Mazin_Nadaf1;~Mohit_Iyyer1",
        "aff": "University of Massachusetts at Amherst;University of Massachusetts at Amherst;;University of Maryland, College Park+University of Massachusetts at Amherst;University of Maryland, College Park;University of Maryland, College Park",
        "aff_domain": "umass.edu;umass.edu;;umd.edu+umass.edu;umd.edu;umd.edu",
        "position": "PhD student;PhD student;;PhD student+PhD student;Undergrad student;Associate Professor",
        "bibtex": "@inproceedings{\nsong2025bearcubs,\ntitle={{BEARCUBS}: A benchmark for computer-using web agents},\nauthor={Yixiao Song and Katherine Thai and Chau Minh Pham and Yapei Chang and Mazin Nadaf and Mohit Iyyer},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=0JzWiigkUy}\n}",
        "github": "",
        "project": "",
        "reviewers": "osWr;ivpU;HtzH;KfhM",
        "site": "https://openreview.net/forum?id=0JzWiigkUy",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;5;4;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "0Qbwjd0fxB",
        "title": "Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language representation learning has emerged as a promising approach for sequential recommendation, thanks to its ability to learn generalizable representations. However, despite its advantages, this approach still struggles with data sparsity and a limited understanding of common-sense user preferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a framework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding $\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item textual descriptions. JEPA4Rec captures semantically rich and transferable representations, improving recommendation performance and reducing reliance on large-scale pre-training data. Specifically, JEPA4Rec represents items as text sentences by flattening descriptive information such as $\\textit{title, category}$, and other attributes. To encode these sentences, we employ a bidirectional Transformer encoder with modified embedding layers tailored for capturing item information in recommendation datasets. We apply masking to text sentences and use them to predict the representations of the unmasked sentences, helping the model learn generalizable item embeddings. To further improve recommendation performance and language understanding, we employ a two-stage training strategy incorporating self-supervised learning losses. Experiments on six real-world datasets demonstrate that JEPA4Rec consistently outperforms state-of-the-art methods, particularly in cross-domain, cross-platform, and low-resource scenarios.",
        "keywords": "language models;joint embedding predictive architecture;sequential recommendation",
        "primary_area": "",
        "supplementary_material": "/attachment/39c271ff9b08d923e1db47f3b6618f8d65046970.zip",
        "author": "Nguyen Anh Minh;Dung D. Le",
        "authorids": "~Nguyen_Anh_Minh1;~Dung_D._Le2",
        "gender": "M;M",
        "homepage": ";https://andrew-dungle.github.io/",
        "dblp": ";186/1477",
        "google_scholar": "l37PsIEAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Nguyen_Anh_Minh1;~Dung_D._Le2",
        "aff": "VinUniversity;VinUniversity",
        "aff_domain": "vinuni.edu.vn;vinuni.edu.vn",
        "position": "Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nminh2025learning,\ntitle={Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture},\nauthor={Nguyen Anh Minh and Dung D. Le},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=0Qbwjd0fxB}\n}",
        "github": "",
        "project": "",
        "reviewers": "fZrx;HQmU;F2XD",
        "site": "https://openreview.net/forum?id=0Qbwjd0fxB",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "0Y2zXLFBji",
        "title": "Impact-driven Context Filtering For Cross-file Code Completion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval-augmented generation (RAG) has recently demonstrated considerable potential for repository-level code completion, as it integrates cross-file knowledge with in-file preceding code to provide comprehensive contexts for generation. To better understand the contribution of the retrieved cross-file contexts, we introduce a likelihood-based metric to evaluate the impact of each retrieved code chunk on the completion. Our analysis reveals that, despite retrieving numerous chunks, only a small subset positively contributes to the completion, while some chunks even degrade performance. To address this issue, we leverage this metric to construct a repository-level dataset where each retrieved chunk is labeled as positive, neutral, or negative based on its relevance to the target completion. We then propose an adaptive retrieval context filtering framework, CODEFILTER, trained on this dataset to mitigate the harmful effects of negative retrieved contexts in code completion. Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks demonstrates that CODEFILTER consistently improves completion accuracy compared to approaches without filtering operations across various tasks. Additionally, CODEFILTER significantly reduces the length of the input prompt, enhancing computational efficiency while exhibiting strong generalizability across different models. These results underscore the potential of CODEFILTER to enhance the accuracy, efficiency, and attributability of repository-level code completion.",
        "keywords": "Code Completion; Adaptive Retreivla-augmented Generation; Large Language Model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yanzhou Li;Shangqing Liu;Kangjie Chen;Tianwei Zhang;Yang Liu",
        "authorids": "~Yanzhou_Li1;~Shangqing_Liu1;~Kangjie_Chen1;~Tianwei_Zhang1;~Yang_Liu36",
        "gender": "M;;M;M;M",
        "homepage": ";https://shangqing-liu.github.io/;https://kangjie.me;https://personal.ntu.edu.sg/tianwei.zhang/index.html;https://personal.ntu.edu.sg/yangliu/",
        "dblp": "238/7955;207/8653;204/3003;77/7902-4;51/3710-3",
        "google_scholar": "MxinsLMAAAAJ;Rl0-phkAAAAJ;vEPnP6oAAAAJ;9vpiYDIAAAAJ;https://scholar.google.com.sg/citations?hl=en",
        "orcid": ";;0000-0001-5099-7054;;0000-0001-7300-9215",
        "linkedin": ";;;;",
        "or_profile": "~Yanzhou_Li1;~Shangqing_Liu1;~Kangjie_Chen1;~Tianwei_Zhang1;~Yang_Liu36",
        "aff": "Nanyang Technological University;Nanyang Technological University;Nanyang Technological University+Nanyang Technological University;Nanyang Technological University;Nanyang Technological University",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg+ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "position": "PhD student;PhD student;Postdoc+PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nli2025impactdriven,\ntitle={Impact-driven Context Filtering For Cross-file Code Completion},\nauthor={Yanzhou Li and Shangqing Liu and Kangjie Chen and Tianwei Zhang and Yang Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=0Y2zXLFBji}\n}",
        "github": "",
        "project": "",
        "reviewers": "sXYr;75yn;s8bE;6TNq",
        "site": "https://openreview.net/forum?id=0Y2zXLFBji",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "0aHOVhkuOB",
        "title": "MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.",
        "keywords": "MLLM;Benchmark;Scientific Understanding Capabilities",
        "primary_area": "",
        "supplementary_material": "/attachment/9e5107c434a576e2bea79b55e38158745b152410.zip",
        "author": "Mohan Jiang;Jin Gao;Jiahao Zhan;Dequan Wang",
        "authorids": "~Mohan_Jiang1;~Jin_Gao3;~Jiahao_Zhan1;~Dequan_Wang1",
        "gender": "M;M;M;",
        "homepage": "https://mhjiang.site/;https://jingao.online;https://github.com/JohnZhan2023;",
        "dblp": ";;;",
        "google_scholar": "vkp6-osAAAAJ;HkVy8voAAAAJ;;",
        "orcid": "0009-0000-6184-2605;0009-0002-1129-8490;;",
        "linkedin": ";;;",
        "or_profile": "~Mohan_Jiang1;~Jin_Gao3;~Jiahao_Zhan1;~Dequan_Wang1",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;Fudan University;",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;fudan.edu.cn;",
        "position": "Undergrad student;PhD student;Undergrad student;",
        "bibtex": "@inproceedings{\njiang2025mac,\ntitle={{MAC}: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding},\nauthor={Mohan Jiang and Jin Gao and Jiahao Zhan and Dequan Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=0aHOVhkuOB}\n}",
        "github": "",
        "project": "",
        "reviewers": "DRu9;LGGS;3gnu",
        "site": "https://openreview.net/forum?id=0aHOVhkuOB",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;5;5",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.666666666666667,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "0zxugBcgF5",
        "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models, such as language models (LMs), to follow complex human preferences. In RLHF for LMs, we first train an LM using supervised fine-tuning, sample pairs of responses, obtain human feedback, and use the resulting data to train a reward model (RM). RL methods are then used to train the LM to maximize the reward given by the RM. As training progresses, the responses generated by the LM no longer resemble the responses seen by the RM during training, leading to the RM becoming inaccurate. The score given by the RM keeps increasing, but the learned behavior no longer matches the human preferences. This issue is known as overoptimization. We investigate overoptimization from the point of view of distribution shift and show that the shift results in an inconsistent estimate of the RM parameters, leading to an inconsistent estimate of the policy gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which iteratively off-policy corrects the RM using importance weighting, without requiring new labels or samples. This results in a more accurate RM, which empirically leads to an improved final policy. We validate our approach in experiments with summarization and chatbot datasets and show that it performs significantly better than standard RLHF methods and baselines.",
        "keywords": "alignment;reinforcement learning from human feedback;reinforcement learning;reward modeling",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Johannes Ackermann;Takashi Ishida;Masashi Sugiyama",
        "authorids": "~Johannes_Ackermann1;~Takashi_Ishida1;~Masashi_Sugiyama1",
        "gender": ";M;M",
        "homepage": "https://johannesack.github.io/;https://takashiishida.github.io/;http://www.ms.k.u-tokyo.ac.jp/sugi/",
        "dblp": "https://dblp.uni-trier.de/pid/249/9298;84/2290-1;35/1228",
        "google_scholar": "2HvSMI8AAAAJ;IzoyKyUAAAAJ;https://scholar.google.co.jp/citations?user=GkYIrlIAAAAJ",
        "orcid": ";;0000-0001-6658-6743",
        "linkedin": ";;",
        "or_profile": "~Johannes_Ackermann1;~Takashi_Ishida1;~Masashi_Sugiyama1",
        "aff": "The University of Tokyo+RIKEN;The University of Tokyo+RIKEN;RIKEN+The University of Tokyo",
        "aff_domain": "u-tokyo.ac.jp+riken.jp;tokyo.ac.jp+riken.jp;riken.jp+u-tokyo.ac.jp",
        "position": "PhD student+Researcher;Associate Professor+Research scientist;Director+Full Professor",
        "bibtex": "@inproceedings{\nackermann2025offpolicy,\ntitle={Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback},\nauthor={Johannes Ackermann and Takashi Ishida and Masashi Sugiyama},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=0zxugBcgF5}\n}",
        "github": "",
        "project": "",
        "reviewers": "fvYh;ruLM;4P8x;FVSR",
        "site": "https://openreview.net/forum?id=0zxugBcgF5",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6622661785325219
    },
    {
        "id": "12u7diwku0",
        "title": "ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decision-making. We present ALignment via Fine-grained Attributes, (ALFA) a framework that improves LLM question-asking by (i) decomposing the notion of a \u201cgood\u201d question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.",
        "keywords": "Information Seeking;Question Asking;Reliable LLM;Clinical Reasoning;Structured Rewards",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuyue Stella Li;Jimin Mun;Faeze Brahman;Pedram Hosseini;Bryceton G. Thomas;Jessica M. Sin;Bing Ren;Jonathan S. Ilgen;Yulia Tsvetkov;Maarten Sap",
        "authorids": "~Shuyue_Stella_Li1;~Jimin_Mun1;~Faeze_Brahman1;~Pedram_Hosseini1;~Bryceton_G._Thomas1;~Jessica_M._Sin1;~Bing_Ren1;~Jonathan_S._Ilgen1;~Yulia_Tsvetkov1;~Maarten_Sap1",
        "gender": "F;F;F;M;;F;;M;F;M",
        "homepage": "http://stellalisy.com/;https://jiminmun.github.io/;https://fabrahman.github.io;https://phosseini.github.io/;https://gme.dartmouth-hitchcock.org/pathology/our-residents;;;;https://homes.cs.washington.edu/~yuliats/;http://maartensap.com",
        "dblp": "312/6501;351/5635;276/6005;;;;;;75/8157;153/9519",
        "google_scholar": "CRfOlOEAAAAJ;xvq0n50AAAAJ;viCG2ikAAAAJ;yeKMUQYAAAAJ;;;ibkjcmsAAAAJ;;SEDPkrsAAAAJ;gFN4QUYAAAAJ",
        "orcid": ";0009-0007-7231-4550;;;;0000-0003-1390-2698;;0000-0003-4590-6570;0000-0002-4634-7128;",
        "linkedin": ";jimin-mun;;pedramhosseini/;;;;;;",
        "or_profile": "~Shuyue_Stella_Li1;~Jimin_Mun1;~Faeze_Brahman1;~Pedram_Hosseini1;~Bryceton_G._Thomas1;~Jessica_M._Sin1;~Bing_Ren1;~Jonathan_S._Ilgen1;~Yulia_Tsvetkov1;~Maarten_Sap1",
        "aff": "Department of Computer Science, University of Washington;Carnegie Mellon University;Allen Institute for Artificial Intelligence;Lavita AI;Dartmouth Hitchcock Medical Center ;Geisel School of Medicine at Dartmouth;Dartmouth Hitchcock Medical Center;University of Washington;Department of Computer Science, University of Washington;Carnegie Mellon University",
        "aff_domain": "cs.washington.edu;andrew.cmu.edu;allenai.org;lavita.ai;hitchcock.edu;dartmouth.edu;dartmouth.edu;uw.edu;cs.washington.edu;cmu.edu",
        "position": "PhD student;PhD student;Researcher;Researcher;Researcher;Assistant Professor;Assistant Professor;Full Professor;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025alfa,\ntitle={{ALFA}: Aligning {LLM}s to Ask Good Questions A Case Study in Clinical Reasoning},\nauthor={Shuyue Stella Li and Jimin Mun and Faeze Brahman and Pedram Hosseini and Bryceton G. Thomas and Jessica M. Sin and Bing Ren and Jonathan S. Ilgen and Yulia Tsvetkov and Maarten Sap},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=12u7diwku0}\n}",
        "github": "",
        "project": "",
        "reviewers": "cdw7;aB4k;3HyA;yWPb",
        "site": "https://openreview.net/forum?id=12u7diwku0",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "5;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6363636363636364
    },
    {
        "id": "17yFbHmblo",
        "title": "Noiser: Bounded Input Perturbations for Attributing Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Feature attribution (FA) methods are common post-hoc approaches that explain how Large Language Models (LLMs) make predictions. Accordingly, generating faithful attributions that reflect the actual inner behavior of the model is crucial. In this paper, we introduce Noiser, a perturbation-based FA method that imposes bounded noise on each input embedding and measures the robustness of the model against partially noised input to obtain the input attributions. Additionally, we propose an answerability metric that employs an instructed judge model to assess the extent to which highly scored tokens suffice to recover the predicted output. Through a comprehensive evaluation across six LLMs and three tasks, we demonstrate that Noiser consistently outperforms existing gradient-based, attention-based, and perturbation-based FA methods in terms of both faithfulness and answerability, making it a robust and effective approach for explaining language model predictions.",
        "keywords": "Feature Attribution;Post-hoc Explanations;Large Language Model;LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mohammad Reza Ghasemi Madani;Aryo Pradipta Gema;Yu Zhao;Gabriele Sarti;Pasquale Minervini;Andrea Passerini",
        "authorids": "~Mohammad_Reza_Ghasemi_Madani1;~Aryo_Pradipta_Gema1;~Yu_Zhao13;~Gabriele_Sarti1;~Pasquale_Minervini4;~Andrea_Passerini2",
        "gender": "M;M;M;M;;M",
        "homepage": "https://qasemii.github.io/;https://aryopg.github.io;;https://gsarti.com;;http://disi.unitn.it/~passerini/",
        "dblp": ";207/5260;;273/4259.html;;00/6186",
        "google_scholar": "dYvm1FYAAAAJ;Vf4Ij2MAAAAJ;QR0LL6gAAAAJ;https://scholar.google.it/citations?user=sK0B_08AAAAJ;;https://scholar.google.it/citations?user=IIXgkLoAAAAJ",
        "orcid": ";0009-0007-1163-3531;0000-0002-0074-091X;0000-0001-8715-2987;;0000-0002-2765-5395",
        "linkedin": "qasemii/;https://linkedin.com/in/aryopg;;gabrielesarti/;;",
        "or_profile": "~Mohammad_Reza_Ghasemi_Madani1;~Aryo_Pradipta_Gema1;~Yu_Zhao13;~Gabriele_Sarti1;~Pasquale_Minervini4;~Andrea_Passerini2",
        "aff": "University of Melbourne;Anthropic+University of Edinburgh;University of Edinburgh;University of Groningen;;University of Trento",
        "aff_domain": "unimelb.edu.au;anthropic.com+ed.ac.uk;ed.ac.uk;rug.nl;;unitn.it",
        "position": "PhD student;Intern+PhD student;PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nmadani2025noiser,\ntitle={Noiser: Bounded Input Perturbations for Attributing Large Language Models},\nauthor={Mohammad Reza Ghasemi Madani and Aryo Pradipta Gema and Yu Zhao and Gabriele Sarti and Pasquale Minervini and Andrea Passerini},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=17yFbHmblo}\n}",
        "github": "",
        "project": "",
        "reviewers": "CNV7;esjs;L9vK;7SGD",
        "site": "https://openreview.net/forum?id=17yFbHmblo",
        "pdf_size": 0,
        "rating": "6;6;7;9",
        "confidence": "3;4;3;5",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7385489458759963
    },
    {
        "id": "19fydz1QnW",
        "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Charts are essential to data analysis, transforming raw data into clear visual representations that support human decision-making. \nAlthough current vision-language models (VLMs) have made significant progress, they continue to struggle with chart comprehension due to training on datasets that lack diversity and real-world authenticity, or on automatically extracted underlying data tables of charts, which can contain numerous estimation errors. Furthermore, existing models only rely on supervised fine-tuning using these low-quality datasets, severely limiting their effectiveness. To address these issues, we first propose BigCharts, a dataset creation pipeline that generates visually diverse chart images by conditioning the rendering process on real-world charts sourced from multiple online platforms. \nUnlike purely synthetic datasets, BigCharts incorporates real-world data, ensuring authenticity and visual diversity, while still retaining accurate underlying data due to our proposed replotting process. Additionally, we introduce a comprehensive training framework that integrates supervised fine-tuning with Group Relative Policy Optimization (GRPO)-based reinforcement learning.  By introducing novel reward signals specifically designed for chart reasoning, our approach enhances model robustness and generalization across diverse chart styles and domains, resulting in a state-of-the-art chart reasoning model, BigCharts-R1. Extensive experiments demonstrate that our models surpass existing methods on multiple chart question-answering benchmarks compared to even larger open-source and closed-source models.",
        "keywords": "charts;chartqa;vision language models;multimodal",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ahmed Masry;Abhay Puri;Masoud Hashemi;Juan A. Rodriguez;Megh Thakkar;Khyati Mahajan;Vikas Yadav;Sathwik Tejaswi Madhusudhan;Alexandre Pich\u00e9;Dzmitry Bahdanau;Christopher Pal;David Vazquez;Enamul Hoque;Perouz Taslakian;Sai Rajeswar;Spandana Gella",
        "authorids": "~Ahmed_Masry1;~Abhay_Puri1;~Masoud_Hashemi1;~Juan_A._Rodriguez1;~Megh_Thakkar1;~Khyati_Mahajan1;~Vikas_Yadav2;~Sathwik_Tejaswi_Madhusudhan2;~Alexandre_Pich\u00e91;~Dzmitry_Bahdanau1;~Christopher_Pal1;~David_Vazquez1;~Enamul_Hoque2;~Perouz_Taslakian1;~Sai_Rajeswar2;~Spandana_Gella2",
        "gender": "M;M;M;;M;;M;M;;M;;M;;F;;F",
        "homepage": "https://ahmedmasryku.github.io/;https://abhaypuri.github.io/portfolio/;https://www.linkedin.com/in/masoud-hashemi-2b151635/;;http://megh-thakkar.github.io;;;https://www.linkedin.com/in/sat95;;;https://scholar.google.ca/citations?user=1ScWJOoAAAAJ&hl=en&oi=ao;http://www.david-vazquez.com;https://www.yorku.ca/enamulh/;http://www.perouz.com;;https://scholar.google.com/citations?user=fChTW6MAAAAJ&hl=en&oi=ao",
        "dblp": "287/6325;383/3753;;;92/6840;;;;;151/6504;45/1217;94/8653;71/4476.html;52/1849;;146/3968.html",
        "google_scholar": "XqPX5XcAAAAJ;https://scholar.google.ca/citations?user=s8vVSvIAAAAJ;https://scholar.google.ca/citations?user=jHX6JhoAAAAJ;;;;FyS1eswAAAAJ;wwVSfRsAAAAJ;;https://scholar.google.ca/citations?user=Nq0dVMcAAAAJ;https://scholar.google.ca/citations?user=1ScWJOoAAAAJ;1jHvtfsAAAAJ;https://scholar.google.ca/citations?user=NySeLFcAAAAJ;LJ7gHkQAAAAJ;;fChTW6MAAAAJ",
        "orcid": ";;0000-0002-6910-5367;;;;;;;;;0000-0002-2845-8158;;;;",
        "linkedin": "ahmed-masry-ku/;abhaypuri98/?originalSubdomain=ca;;;Megh-Thakkar;;vyf95/;sat95;;;;https://www.linkedin.com/company/david-vazquez/;;perouz/;;spandana-gella-313b7019/",
        "or_profile": "~Ahmed_Masry1;~Abhay_Puri1;~Masoud_Hashemi1;~Juan_A._Rodriguez1;~Megh_Thakkar1;~Khyati_Mahajan1;~Vikas_Yadav2;~Sathwik_Tejaswi_Madhusudhan2;~Alexandre_Pich\u00e91;~Dzmitry_Bahdanau1;~Christopher_Pal1;~David_Vazquez1;~Enamul_Hoque2;~Perouz_Taslakian1;~Sai_Rajeswar2;~Spandana_Gella2",
        "aff": "York University+ServiceNow Inc;ServiceNow Research;ServiceNow Inc;;Universit\u00e9 de Montr\u00e9al;;ServiceNow Inc;ServiceNow Inc;;ServiceNow Research;Polytechnique Montreal;ServiceNow research;York University;ServiceNow;;ServiceNow Inc",
        "aff_domain": "yorku.ca+servicenow.com;servicenow.com;servicenow.com;;umontreal.ca;;servicenow.com;servicenow.com;;servicenow.com;polymtl.ca;servicenow.com;yorku.ca;servicenow.com;;servicenow.com",
        "position": "PhD student+Intern;Researcher;Applied Research Scientist;;MS student;;Researcher;Researcher;;Research Scientist;Full Professor;Researcher;Associate Professor;Researcher;;Researcher",
        "bibtex": "@inproceedings{\nmasry2025bigchartsr,\ntitle={BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning},\nauthor={Ahmed Masry and Abhay Puri and Masoud Hashemi and Juan A. Rodriguez and Megh Thakkar and Khyati Mahajan and Vikas Yadav and Sathwik Tejaswi Madhusudhan and Alexandre Pich{\\'e} and Dzmitry Bahdanau and Christopher Pal and David Vazquez and Enamul Hoque and Perouz Taslakian and Sai Rajeswar and Spandana Gella},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=19fydz1QnW}\n}",
        "github": "",
        "project": "",
        "reviewers": "Lpvk;8ZTe;P2Yy;wCKY",
        "site": "https://openreview.net/forum?id=19fydz1QnW",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;5;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            16,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "1Pmuw08LoM",
        "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation---the degree of difference between a training sample and all other samples with the same prompt---in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.",
        "keywords": "creative writing generation;diversity;post-training",
        "primary_area": "",
        "supplementary_material": "/attachment/4ba9fd43d8499d750dec728005bede64a9ef32bf.zip",
        "author": "John Joon Young Chung;Vishakh Padmakumar;Melissa Roemmele;Yuqian Sun;Max Kreminski",
        "authorids": "~John_Joon_Young_Chung1;~Vishakh_Padmakumar1;~Melissa_Roemmele1;~Yuqian_Sun1;~Max_Kreminski1",
        "gender": "M;;F;F;Non-Binary",
        "homepage": "https://johnr0.github.io/;https://vishakhpk.github.io/;https://roemmele.github.io/;https://fakecheese.me/;https://mkremins.github.io",
        "dblp": "218/0917;285/5184;21/10890;;220/7713.html",
        "google_scholar": "ZHdXUCMAAAAJ;OeBKZ8AAAAAJ;l_nZmdkAAAAJ;6kg7oFMAAAAJ;Ja4R1yEAAAAJ",
        "orcid": "0000-0002-8492-2525;0000-0002-3396-3589;;0000-0002-4076-8140;0009-0002-6268-4033",
        "linkedin": ";;mroemmele/;;",
        "or_profile": "~John_Joon_Young_Chung1;~Vishakh_Padmakumar1;~Melissa_Roemmele1;~Yuqian_Sun1;~Max_Kreminski1",
        "aff": "Midjourney;New York University;Midjourney;Midjourney+Royal College of Art;Midjourney",
        "aff_domain": "midjourney.com;nyu.edu;midjourney.com;midjourney.com+network.rca.ac.uk;midjourney.com",
        "position": "Researcher;PhD student;Researcher;Researcher+PhD student;Researcher",
        "bibtex": "@inproceedings{\nchung2025modifying,\ntitle={Modifying Large Language Model Post-Training for Diverse Creative Writing},\nauthor={John Joon Young Chung and Vishakh Padmakumar and Melissa Roemmele and Yuqian Sun and Max Kreminski},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=1Pmuw08LoM}\n}",
        "github": "",
        "project": "",
        "reviewers": "HqMa;PMB2;Cvag;pc34",
        "site": "https://openreview.net/forum?id=1Pmuw08LoM",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "1w9Hay7tvm",
        "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.",
        "keywords": "Over-refusal;Safety;Instruction Tunning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhehao Zhang;Weijie Xu;Fanyou Wu;Chandan K. Reddy",
        "authorids": "~Zhehao_Zhang1;~Weijie_Xu1;~Fanyou_Wu1;~Chandan_K._Reddy1",
        "gender": "M;M;M;M",
        "homepage": "https://zzh-sjtu.github.io/zhehaozhang.github.io/;https://www.weijiexu.com;http://www.wufanyou.com;https://creddy.net/",
        "dblp": ";195/1675;229/8090;42/1341",
        "google_scholar": "QG-BAGwAAAAJ;lWjp-dQAAAAJ;C8WYCTAAAAAJ;LoXnMOIAAAAJ",
        "orcid": ";;;",
        "linkedin": ";weijie-xu-936b23101/;;",
        "or_profile": "~Zhehao_Zhang1;~Weijie_Xu1;~Fanyou_Wu1;~Chandan_K._Reddy1",
        "aff": "Ohio State University, Columbus+Dartmouth College;Amazon;Amazon;Virginia Tech+Amazon",
        "aff_domain": "osu.edu+dartmouth.edu;amazon.com;amazon.com;vt.edu+amazon.com",
        "position": "PhD student+MS student;Researcher;Researcher;Full Professor+Amazon Scholar",
        "bibtex": "@inproceedings{\nzhang2025falsereject,\ntitle={FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in {LLM}s via Structured Reasoning},\nauthor={Zhehao Zhang and Weijie Xu and Fanyou Wu and Chandan K. Reddy},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=1w9Hay7tvm}\n}",
        "github": "",
        "project": "",
        "reviewers": "td5z;3pGD;WxYz;pfUE",
        "site": "https://openreview.net/forum?id=1w9Hay7tvm",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "29jP6OsrIQ",
        "title": "Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to enrich semantic representations in vision-language contrastive learning. Unlike standard CLIP-style models that rely on a single text embedding, our method introduces multiple structured prompts, each containing a distinct adaptive token that captures diverse semantic aspects of the input text. We leverage a pretrained LLM as the text encoder within the CLIP framework, processing all prompts jointly in a single forward pass. The resulting prompt embeddings are combined into a unified text representation, enabling semantically richer alignment with visual features. To further promote semantic diversity and representation quality, we incorporate a diversity regularization loss and a negation-aware loss, encouraging specialization across prompts and improving contrastive discrimination. Our method achieves consistent improvements on both image-text and video-text retrieval benchmarks.",
        "keywords": "CLIP;contrastive learning;LLM embedding",
        "primary_area": "",
        "supplementary_material": "/attachment/5bc95b106630bdf9b944809b0efbe6b8eb7c0a1f.zip",
        "author": "Dahun Kim;Anelia Angelova",
        "authorids": "~Dahun_Kim1;~Anelia_Angelova1",
        "gender": ";",
        "homepage": ";https://research.google/people/aneliaangelova/",
        "dblp": "205/2487;46/3065",
        "google_scholar": "mHpN1xoAAAAJ;nkmDOPgAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Dahun_Kim1;~Anelia_Angelova1",
        "aff": "Google;Google+California Institute of Technology",
        "aff_domain": "google.com;google.com+caltech.edu",
        "position": "Research Scientist;Research Scientist+PhD student",
        "bibtex": "@inproceedings{\nkim2025contextadaptive,\ntitle={Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment},\nauthor={Dahun Kim and Anelia Angelova},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=29jP6OsrIQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "JaD2;TY9F;L9Jh",
        "site": "https://openreview.net/forum?id=29jP6OsrIQ",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "2H85485yAb",
        "title": "Truth-value judgment in language models: \u2018truth directions\u2019 are context sensitive",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described as uncovering a model\u2019s \u201cknowledge\u201d or \u201cbeliefs\u201d. We investigate this phenomenon, looking closely at the impact of context on the probes. Our experiments establish where in the LLM the probe\u2019s predictions are (most) sensitive to the presence of related sentences, and how to best characterize this kind of sensitivity. We do so by measuring different types of consistency errors that occur after probing an LLM whose inputs consist of hypotheses preceded by (negated) supporting and contradicting sentences. We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these truth-value directions influences the position of an entailed or contradicted sentence along that same direction. We find that the probes we test are generally context sensitive, but that contexts which should not affect the truth often still impact the probe outputs. Our experiments show that the type of errors depend on the layer, the model, and the kind of data. Finally, our results suggest that truth-value directions are causal mediators in the inference process that incorporates in-context information.",
        "keywords": "mechinterp;mechanistic interpretability;interpretability;truth directions;LLM beliefs;large language model;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Stefan F. Schouten;Peter Bloem;Ilia Markov;Piek Vossen",
        "authorids": "~Stefan_F._Schouten1;~Peter_Bloem1;~Ilia_Markov2;~Piek_Vossen2",
        "gender": ";M;M;M",
        "homepage": ";http://peterbloem.nl;https://ilia-markov.github.io/;https://vossen.info/",
        "dblp": ";151/0108;146/9620;",
        "google_scholar": ";https://scholar.google.nlcitations/?user=zVntAfQAAAAJ;;JvllTMIAAAAJ",
        "orcid": ";0000-0002-0189-5817;0000-0001-9533-748X;0000-0002-6238-5941",
        "linkedin": ";;ilia-markov-9b685680/;",
        "or_profile": "~Stefan_F._Schouten1;~Peter_Bloem1;~Ilia_Markov2;~Piek_Vossen2",
        "aff": ";Vrije Universiteit Amsterdam;Vrije Universiteit Amsterdam;Vrije Universiteit Amsterdam",
        "aff_domain": ";vu.nl;vu.nl;vu.nl",
        "position": ";Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nschouten2025truthvalue,\ntitle={Truth-value judgment in language models: {\\textquoteleft}truth directions{\\textquoteright} are context sensitive},\nauthor={Stefan F. Schouten and Peter Bloem and Ilia Markov and Piek Vossen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2H85485yAb}\n}",
        "github": "",
        "project": "",
        "reviewers": "PGfW;8KB7;BqG9;Gm35",
        "site": "https://openreview.net/forum?id=2H85485yAb",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "2JohTFaGbW",
        "title": "Language models align with brain regions that represent concepts across modalities",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Cognitive science and neuroscience have long faced the challenge of disentangling representations of language from representations of conceptual meaning. As the same problem arises in today's language models (LMs), we investigate the relationship between LM--brain alignment and two neural metrics: (1) the level of brain activation during processing of sentences, targeting linguistic processing, and (2) a novel measure of meaning consistency across input modalities, which quantifies how consistently a brain region responds to the same concept across paradigms (sentence, word cloud, image) using an fMRI dataset (Pereira et al., 2018). Our experiments show that both language-only and language-vision models predict the signal better in more meaning-consistent areas of the brain, even when these areas are not strongly sensitive to language processing, suggesting that LMs might internally represent cross-modal conceptual meaning.",
        "keywords": "LM\u2013brain alignment;fMRI;conceptual meaning;cognitive neuroscience",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Maria Ryskina;Greta Tuckute;Alexander Fung;Ashley Malkin;Evelina Fedorenko",
        "authorids": "~Maria_Ryskina1;~Greta_Tuckute1;~Alexander_Fung1;~Ashley_Malkin1;~Evelina_Fedorenko1",
        "gender": "F;F;M;F;F",
        "homepage": "http://ryskina.github.io;http://tuckute.com/;https://alexanderdfung.github.io/;;http://evlab.mit.edu",
        "dblp": "199/1730;240/4412;;;",
        "google_scholar": "s9kWzyQAAAAJ;https://scholar.google.dk/citations?user=pJB4fIEAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;1CgET20AAAAJ",
        "orcid": "0009-0002-5936-1380;;0000-0003-2205-5531;;",
        "linkedin": ";;;;",
        "or_profile": "~Maria_Ryskina1;~Greta_Tuckute1;~Alexander_Fung1;~Ashley_Malkin1;~Evelina_Fedorenko1",
        "aff": "Vector Institute+Massachusetts Institute of Technology;Massachusetts Institute of Technology+Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "vectorinstitute.ai+mit.edu;mit.edu+mit.edu;mit.edu;mit.edu;mit.edu",
        "position": "Postdoc+Affiliate;Postdoc+PhD student;Researcher;Intern;Associate Professor",
        "bibtex": "@inproceedings{\nryskina2025language,\ntitle={Language models align with brain regions that represent concepts across modalities},\nauthor={Maria Ryskina and Greta Tuckute and Alexander Fung and Ashley Malkin and Evelina Fedorenko},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2JohTFaGbW}\n}",
        "github": "",
        "project": "",
        "reviewers": "ygPR;tM7V;igwq;qokZ",
        "site": "https://openreview.net/forum?id=2JohTFaGbW",
        "pdf_size": 0,
        "rating": "4;6;7;7",
        "confidence": "5;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9847319278346618
    },
    {
        "id": "2Kl8Ztw6wk",
        "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end of user input and the start of audio output, resulting in suboptimal user experiences. This latency is particularly evident when LLMs are deployed as single-user voice assistants on consumer-grade hardware with limited computing capacity. We discovered that this latency is primarily dominated by the time it takes for the LLMs to generate the first sentence, which is required as input by the TTS systems that synthesize audio responses on a sentence-by-sentence basis. To address this bottleneck, we propose Predictive Generation (PredGen), a novel framework that mitigates\u2014or even eliminates\u2014this delay through speculative decoding at input time. PredGen generates candidate responses while the user is still speaking, enabling the system to begin TTS processing with minimal delay. Simulated experiments on the Lmsys and MT-Bench datasets show that the proposed method can effectively reduce the latency by around 2\u00d7 across a wide range of use cases, while incurring only minimal additional computation cost at input time\u2014computation that would otherwise go unused.",
        "keywords": "Large Language Models;Inference;Speculative Decoding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shufan Li;Aditya Grover",
        "authorids": "~Shufan_Li1;~Aditya_Grover1",
        "gender": "M;M",
        "homepage": ";https://aditya-grover.github.io",
        "dblp": "218/8196;162/5052",
        "google_scholar": ";oOhnPUgAAAAJ",
        "orcid": ";",
        "linkedin": "shufan-li-126b70187/;",
        "or_profile": "~Shufan_Li1;~Aditya_Grover1",
        "aff": "UCLA Computer Science Department, University of California, Los Angeles;University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;ucla.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025predgen,\ntitle={PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction},\nauthor={Shufan Li and Aditya Grover},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2Kl8Ztw6wk}\n}",
        "github": "",
        "project": "",
        "reviewers": "XMLi;evVf;2rYs;iE4i",
        "site": "https://openreview.net/forum?id=2Kl8Ztw6wk",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "4;4;3;2",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8181818181818182
    },
    {
        "id": "2YdSsi0bxK",
        "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM\u2019s performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications.",
        "keywords": "LLMs; Calibration; Copyright; Unlearning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tianyang Xu;Xiaoze Liu;Feijie Wu;Xiaoqian Wang;Jing Gao",
        "authorids": "~Tianyang_Xu3;~Xiaoze_Liu1;~Feijie_Wu1;~Xiaoqian_Wang1;~Jing_Gao1",
        "gender": ";Not Specified;;F;",
        "homepage": ";https://xz-liu.github.io/;https://harli.me/;https://engineering.purdue.edu/~joywang/;",
        "dblp": ";239/4537;246/4255;151/3215-1;",
        "google_scholar": ";MaIQOwsAAAAJ;https://scholar.google.com/citations?hl=en;I3tc214AAAAJ;",
        "orcid": ";0000-0002-9726-3397;0000-0003-0541-1901;;",
        "linkedin": ";xzliu/;;;",
        "or_profile": "~Tianyang_Xu3;~Xiaoze_Liu1;~Feijie_Wu1;~Xiaoqian_Wang1;~Jing_Gao1",
        "aff": "Purdue University;Purdue University+Amazon;Purdue University;Purdue University;",
        "aff_domain": "purdue.edu;purdue.edu+amazon.com;purdue.edu;purdue.edu;",
        "position": "PhD student;PhD student+Intern;PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nxu2025suv,\ntitle={{SUV}: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning},\nauthor={Tianyang Xu and Xiaoze Liu and Feijie Wu and Xiaoqian Wang and Jing Gao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2YdSsi0bxK}\n}",
        "github": "",
        "project": "",
        "reviewers": "eLus;UKgJ;Yg5w;VpA2",
        "site": "https://openreview.net/forum?id=2YdSsi0bxK",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "2ezugTT9kU",
        "title": "2 OLMo 2 Furious (COLM\u2019s Version)",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes a family of dense autoregressive language models at 7B, 13B, and 32B scales with fully released artifacts\u2014model weights, full training data, training code and recipes, training logs, and thousands of intermediate checkpoints. In this work, we describe our modified model architecture and training recipe, focusing on techniques for achieving better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e., specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\u00fclu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance-to-training compute, often matching or outperforming open-weight-only models like Llama 3.1, Qwen 2.5, and Gemma 2 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with open-weight-only models of comparable size and even some proprietary models like GPT-3.5 Turbo and GPT-4o Mini.",
        "keywords": "language model;pretraining;training stability;training data;instruction tuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Evan Pete Walsh;Luca Soldaini;Dirk Groeneveld;Kyle Lo;Shane Arora;Akshita Bhagia;Yuling Gu;Shengyi Huang;Matt Jordan;Nathan Lambert;Dustin Schwenk;Oyvind Tafjord;Taira Anderson;David Atkinson;Faeze Brahman;Christopher Clark;Pradeep Dasigi;Nouha Dziri;Allyson Ettinger;Michal Guerquin;David Heineman;Hamish Ivison;Pang Wei Koh;Jiacheng Liu;Saumya Malik;William Merrill;Lester James Validad Miranda;Jacob Morrison;Tyler Murray;Crystal Nam;Jake Poznanski;Valentina Pyatkin;Aman Rangapur;Michael Schmitz;Sam Skjonsberg;David Wadden;Christopher Wilhelm;Michael Wilson;Luke Zettlemoyer;Ali Farhadi;Noah A. Smith;Hannaneh Hajishirzi",
        "authorids": "~Evan_Pete_Walsh1;~Luca_Soldaini1;~Dirk_Groeneveld1;~Kyle_Lo1;~Shane_Arora1;~Akshita_Bhagia1;~Yuling_Gu1;~Shengyi_Huang1;~Matt_Jordan1;~Nathan_Lambert1;~Dustin_Schwenk1;~Oyvind_Tafjord2;~Taira_Anderson1;~David_Atkinson3;~Faeze_Brahman1;~Christopher_Clark1;~Pradeep_Dasigi1;~Nouha_Dziri2;~Allyson_Ettinger1;~Michal_Guerquin1;~David_Heineman1;~Hamish_Ivison1;~Pang_Wei_Koh1;~Jiacheng_Liu2;~Saumya_Malik1;~William_Merrill1;~Lester_James_Validad_Miranda1;~Jacob_Morrison2;~Tyler_Murray2;~Crystal_Nam1;~Jake_Poznanski1;~Valentina_Pyatkin1;~Aman_Rangapur1;~Michael_Schmitz1;~Sam_Skjonsberg1;~David_Wadden1;~Christopher_Wilhelm1;~Michael_Wilson4;~Luke_Zettlemoyer1;~Ali_Farhadi3;~Noah_A._Smith2;~Hannaneh_Hajishirzi1",
        "gender": "M;Non-Binary;;;;F;;M;M;M;;M;F;M;F;M;M;;F;;M;;M;M;F;M;M;;Not Specified;;M;;M;M;M;M;M;;M;M;;F",
        "homepage": "https://github.com/epwalsh/;https://soldaini.net;;https://kyleclo.github.io/;;https://akshitab.github.io/;;https://costa.sh/;https://www.cs.utexas.edu/~mjordan/;https://natolambert.com;;;;;https://fabrahman.github.io;https://chrisc36.github.io;https://pdasigi.github.io/;;https://aetting.github.io;https://michal.guerquin.com;https://davidheineman.com;https://hamishivi.github.io;http://cs.stanford.edu/~pangwei;https://github.com/liujch1998;https://www.linkedin.com/in/saumya-malik-983a11229/;http://lambdaviking.com;https://ljvmiranda921.github.io;;;;http://www.jakepoz.com;;https://amanrangapur.com;http://www.schmitztech.com;https://codeviking.net;https://dwadden.github.io/;https://www.semanticscholar.org/author/Chris-Wilhelm/2270038642;https://mewil.io/;https://www.cs.washington.edu/people/faculty/lsz/;https://homes.cs.washington.edu/~ali/;;https://homes.cs.washington.edu/~hannaneh/",
        "dblp": ";160/1741;185/7781;220/2020;;321/0726;194/1346;251/8731;236/5728;228/9584.html;208/4259;178/8640;;;276/6005;;27/7184;;165/0758;;336/4616;288/1956;10/10453;289/6273;;19/3512;224/9490;;220/2079;;400/6907;;;01/689-2;220/2080;239/4346;;;21/6793;37/5826;;52/1296",
        "google_scholar": ";3KPvwcgAAAAJ;KEhvGNMAAAAJ;VJS12uMAAAAJ;;fzH3_G4AAAAJ;;kl9YcpEAAAAJ;Zj7R8p0AAAAJ;O4jW7BsAAAAJ;4yiNcJyuYb4C;https://scholar.google.com/citations?hl=en;;;viCG2ikAAAAJ;CmzeVaEAAAAJ;https://scholar.google.com/citations?authorid=Bpd76vcAAAAJ;;;;JO2Q6CUAAAAJ;;Nn990CkAAAAJ;GJfoBZAAAAAJ;;CyjChJQAAAAJ;https://scholar.google.co.jp/citations?user=2RtnNKEAAAAJ;;;;;;QHHZO8kAAAAJ;VdDUILgAAAAJ;2xelBIYAAAAJ;BeTUvHIAAAAJ;;;https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ;jeOFRDsAAAAJ;;LOV6_WIAAAAJ",
        "orcid": ";0000-0001-6998-9863;0000-0002-8274-768X;;0000-0003-1199-770X;0000-0003-4848-3884;;;;0000-0002-9997-6817;;0000-0003-4190-5618;;;;;0000-0001-7127-1316;;;;;0000-0002-0069-7659;;0000-0003-3308-2869;;;;;0000-0003-0043-7192;;;;;;;;;;;;;",
        "linkedin": ";soldni/;mechanicaldirk/;kylelo/;;;yuling-gu/;costa-huang/;;nathan-lambert-55093468/;;;taira-anderson/;daa780;;;;;;;;;;liujch1998/;;william-merrill-15ab0743/;;;tcmurray/;crystalnam;;;;schmmd;https://linkedin.com/in/skone;david-wadden-0076a995/;;;luke-zettlemoyer-a0109b226/;;;",
        "or_profile": "~Evan_Pete_Walsh1;~Luca_Soldaini1;~Dirk_Groeneveld1;~Kyle_Lo1;~Shane_Arora1;~Akshita_Bhagia1;~Yuling_Gu1;~Shengyi_Huang1;~Matt_Jordan1;~Nathan_Lambert1;~Dustin_Schwenk1;~Oyvind_Tafjord2;~Taira_Anderson1;~David_Atkinson3;~Faeze_Brahman1;~Christopher_Clark1;~Pradeep_Dasigi1;~Nouha_Dziri2;~Allyson_Ettinger1;~Michal_Guerquin1;~David_Heineman1;~Hamish_Ivison1;~Pang_Wei_Koh1;~Jiacheng_Liu2;~Saumya_Malik1;~William_Merrill1;~Lester_James_Validad_Miranda1;~Jacob_Morrison2;~Tyler_Murray2;~Crystal_Nam1;~Jake_Poznanski1;~Valentina_Pyatkin1;~Aman_Rangapur1;~Michael_Schmitz1;~Sam_Skjonsberg1;~David_Wadden1;~Christopher_Wilhelm1;~Michael_Wilson4;~Luke_Zettlemoyer1;~Ali_Farhadi3;~Noah_A._Smith2;~Hannaneh_Hajishirzi1",
        "aff": "Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;New York University+Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Google DeepMind+Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Texas at Austin;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence+University of Washington;Allen Institute for Artificial Intelligence+Paul G. Allen School of Computer Science and Engineering, University of Washington;Allen Institute for Artificial Intelligence;New York University;Allen Institute for Artificial Intelligence;;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Google;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington+Meta Facebook+Meta;University of Washington;;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "allenai.org;allenai.org;allenai.org;allenai.org;allenai.org;allenai.org;nyu.edu+allenai.org;allenai.org;allenai.org;allenai.org;allenai.org;google.com+allenai.org;allenai.org;utexas.edu;allenai.org;allenai.org;allenai.org;;allenai.org;allenai.org;allenai.org;uw.edu;allenai.org+cs.washington.edu;allenai.org+cs.washington.edu;allenai.org;nyu.edu;allenai.org;;allenai.org;allenai.org;allenai.org;;allenai.org;allenai.org;allenai.org;google.com;allenai.org;allenai.org;cs.washington.edu+fb.com+meta.com;cs.uw.edu;;allenai.org+uw.edu",
        "position": "Researcher;Researcher;Principal Researcher;Researcher;Researcher;Researcher;PhD student+Predoctoral Young Investigator;Researcher;Researcher;Researcher;Researcher;Researcher+Researcher;Researcher;Lecturer;Researcher;Research Scientist;Research Scientist;;Researcher;Software Engineer;Researcher;PhD student;Visiting Research Scientist+Assistant Professor;Intern+PhD student;Researcher;Graduate student;Researcher;;Researcher;Legal counsel;Researcher;;Researcher;Researcher;Senior Engineering Manager;Researcher;Researcher;Engineer;Full Professor+Researcher+Researcher;Full Professor;;senior director+Associate Professor",
        "bibtex": "@inproceedings{\nwalsh2025,\ntitle={2 {OLM}o 2 Furious ({COLM}{\\textquoteright}s Version)},\nauthor={Evan Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Allyson Ettinger and Michal Guerquin and David Heineman and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James Validad Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Jake Poznanski and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2ezugTT9kU}\n}",
        "github": "",
        "project": "",
        "reviewers": "CBN5;5RBa;Maj8;9NGC",
        "site": "https://openreview.net/forum?id=2ezugTT9kU",
        "pdf_size": 0,
        "rating": "6;6;7;9",
        "confidence": "3;3;4;5",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            42,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9847319278346618
    },
    {
        "id": "2txrMBpw3q",
        "title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present RepoST, a scalable method to construct environments that provide execution feedback for repository-level code generation for both training and evaluation. Unlike existing works that aim to build entire repositories for execution, which is challenging for both human and LLMs, we provide execution feedback with sandbox testing, which isolates a given target function and its dependencies to a separate script for testing. Sandbox testing reduces the complexity of external dependencies and enables constructing environments at a large scale. We use our method to construct RepoST-Train, a large-scale train set with 7,415 functions from 832 repositories. Training with the execution feedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset, RepoST-Eval, and benchmark 12 code generation models. Code and datasets available at https://repost-code-gen.github.io/",
        "keywords": "code generation training;repo-level code generation",
        "primary_area": "",
        "supplementary_material": "/attachment/bc410e83bf051f2b0b987d2ac6fc926e03c3548d.zip",
        "author": "Yiqing Xie;Alex Xie;Divyanshu Sheth;Pengfei Liu;Daniel Fried;Carolyn Rose",
        "authorids": "~Yiqing_Xie1;~Alex_Xie1;~Divyanshu_Sheth1;~Pengfei_Liu1;~Daniel_Fried1;~Carolyn_Rose1",
        "gender": "Not Specified;M;M;M;M;F",
        "homepage": "https://veronicium.github.io;;;http://pfliu.com/;https://dpfried.github.io/;http://www.cs.cmu.edu/~cprose/",
        "dblp": "147/6506;338/2046;;34/3381-3;117/4804;r/CarolynPensteinRose",
        "google_scholar": "200mJh8AAAAJ;;pjLCdhIAAAAJ;oIz_CYEAAAAJ;sJDqACEAAAAJ;https://scholar.google.com.tw/citations?user=BMydCgcAAAAJ",
        "orcid": ";;;;;0000-0003-1128-5155",
        "linkedin": ";axie/;divyanshusheth;;;carolyn-rose-11226b23b/",
        "or_profile": "~Yiqing_Xie1;~Alex_Xie1;~Divyanshu_Sheth1;~Pengfei_Liu1;~Daniel_Fried1;~Carolyn_Rose1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;Apple;Shanghai Jiaotong University;Meta AI+Carnegie Mellon University;School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;apple.com;sjtu.edu;meta.com+cmu.edu;cs.cmu.edu",
        "position": "PhD student;Researcher;Researcher;Associate Professor;Researcher+Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nxie2025repost,\ntitle={Repo{ST}: Scalable Repository-Level Coding Environment Construction with Sandbox Testing},\nauthor={Yiqing Xie and Alex Xie and Divyanshu Sheth and Pengfei Liu and Daniel Fried and Carolyn Rose},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2txrMBpw3q}\n}",
        "github": "",
        "project": "",
        "reviewers": "dCkp;GgBH;qkFa",
        "site": "https://openreview.net/forum?id=2txrMBpw3q",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "2vDJiGUfhV",
        "title": "Don\u2019t lie to your friends: Learning what you know from collaborative self-play",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent's specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \\emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success with minimal effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \\emph{transfer} to improve tool use and selective prediction in single-agent scenarios.",
        "keywords": "self-play;calibration;tool use;multiagent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jacob Eisenstein;Reza Aghajani;Adam Fisch;Dheeru Dua;Fantine Huot;Mirella Lapata;Vicky Zayats;Jonathan Berant",
        "authorids": "~Jacob_Eisenstein1;~Reza_Aghajani2;~Adam_Fisch2;~Dheeru_Dua2;~Fantine_Huot1;~Mirella_Lapata1;~Vicky_Zayats1;~Jonathan_Berant1",
        "gender": "M;M;;F;F;F;F;M",
        "homepage": "https://jacobeisenstein.github.io;;;https://ddua.github.io/src/index.html;;https://homepages.inf.ed.ac.uk/mlap/;;http://www.cs.tau.ac.il/~joberant/",
        "dblp": "82/2305;;;194/5251;;59/6701;;31/8178",
        "google_scholar": "Wb_lnjAAAAAJ;nvlJ6SwAAAAJ;;RDky42sAAAAJ;79VvQLMAAAAJ;j67B9Q4AAAAJ;BVVJvoMAAAAJ;https://scholar.google.co.il/citations?user=xCYHonIAAAAJ",
        "orcid": ";0000-0002-7950-8266;;;;;;",
        "linkedin": ";;;;fantine/;;;",
        "or_profile": "~Jacob_Eisenstein1;~Reza_Aghajani2;~Adam_Fisch2;~Dheeru_Dua2;~Fantine_Huot1;~Mirella_Lapata1;~Vicky_Zayats1;~Jonathan_Berant1",
        "aff": "Google;Google;;Google;Google;Edinburgh University, University of Edinburgh;Google;Google+Tel Aviv University",
        "aff_domain": "google.com;google.com;;google.com;google.com;inf.ed.ac.uk;google.com;google.com+tau.ac.il",
        "position": "Research Scientist;Researcher;;Researcher;Researcher;Full Professor;Researcher;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\neisenstein2025dont,\ntitle={Don{\\textquoteright}t lie to your friends: Learning what you know from collaborative self-play},\nauthor={Jacob Eisenstein and Reza Aghajani and Adam Fisch and Dheeru Dua and Fantine Huot and Mirella Lapata and Vicky Zayats and Jonathan Berant},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=2vDJiGUfhV}\n}",
        "github": "",
        "project": "",
        "reviewers": "T7Se;GEzH;VkVL;osPm",
        "site": "https://openreview.net/forum?id=2vDJiGUfhV",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "38GehGepDd",
        "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005\\% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our code on GitHub and models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.",
        "keywords": "RADLAD;RADLADS;RWKV;Linear Attention;Conversion;LLM;SUPRA;MOHAWK;LolCATs;Hedgehog;DiJiang;Mamba in the Llama",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daniel Goldstein;Eric Alcaide;Janna Lu;Eugene Cheah",
        "authorids": "~Daniel_Goldstein2;~Eric_Alcaide2;~Janna_Lu1;~Eugene_Cheah1",
        "gender": "M;;;M",
        "homepage": ";;;https://substack.tech-talk-cto.com/",
        "dblp": ";;;",
        "google_scholar": ";;CuD3RzUAAAAJ;",
        "orcid": ";;0009-0009-1031-7742;0009-0002-8977-475X",
        "linkedin": "smerkyg/;;jannalu;eugene-cheah-a47791126/",
        "or_profile": "~Daniel_Goldstein2;~Eric_Alcaide2;~Janna_Lu1;~Eugene_Cheah1",
        "aff": "Recursal AI, Inc.;;George Mason University;Featherless AI+Recursal AI",
        "aff_domain": "recursal.ai;;gmu.edu;featherless.ai+recursal.ai",
        "position": "Researcher;;PhD student;CEO / Co-Founder+CEO / Co-Founder",
        "bibtex": "@inproceedings{\ngoldstein2025radlads,\ntitle={{RADLADS}: Rapid Attention Distillation to Linear Attention Decoders at Scale},\nauthor={Daniel Goldstein and Eric Alcaide and Janna Lu and Eugene Cheah},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=38GehGepDd}\n}",
        "github": "",
        "project": "",
        "reviewers": "1f3Y;emZF;CHRt",
        "site": "https://openreview.net/forum?id=38GehGepDd",
        "pdf_size": 0,
        "rating": "7;7;8",
        "confidence": "5;4;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "3BmPSFAdq3",
        "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive---LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost---estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.",
        "keywords": "Controlled generation;Bayesian inference;Approximate sampling;Decoding methods",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ben Lipkin;Benjamin LeBrun;Jacob Hoover Vigly;Jo\u00e3o Loula;David R. MacIver;Li Du;Jason Eisner;Ryan Cotterell;Vikash Mansinghka;Timothy J. O'Donnell;Alexander K. Lew;Tim Vieira",
        "authorids": "~Ben_Lipkin1;~Benjamin_LeBrun1;~Jacob_Hoover_Vigly1;~Jo\u00e3o_Loula1;~David_R._MacIver1;~Li_Du2;~Jason_Eisner1;~Ryan_Cotterell1;~Vikash_Mansinghka1;~Timothy_J._O'Donnell1;~Alexander_K._Lew1;~Tim_Vieira1",
        "gender": "M;;;M;M;M;M;;M;;M;M",
        "homepage": "https://benlipkin.github.io/;https://benlebrun.github.io/;;;;;http://cs.jhu.edu/~jason;;;https://mcqll.org/;http://alexlew.net;http://timvieira.github.io",
        "dblp": "346/0247;;;223/5836;;;37/3263;;23/1731;89/3188;242/4530;127/0214",
        "google_scholar": "zN6vxGUAAAAJ;;;;2wboagoAAAAJ;efDU43kAAAAJ;tjb2UccAAAAJ;;;iYjXhYwAAAAJ;TiF1WEsAAAAJ;Avtv7FkAAAAJ",
        "orcid": "0000-0001-7465-5315;;;;0000-0002-8635-3223;;0000-0002-8861-0772;;;0000-0002-5711-977X;;0000-0002-2043-1073",
        "linkedin": ";;;;;;;;;timothy-o-donnell-698b1b3/;;tim-vieira-608b0396/",
        "or_profile": "~Ben_Lipkin1;~Benjamin_LeBrun1;~Jacob_Hoover_Vigly1;~Jo\u00e3o_Loula1;~David_R._MacIver1;~Li_Du2;~Jason_Eisner1;~Ryan_Cotterell1;~Vikash_Mansinghka1;~Timothy_J._O'Donnell1;~Alexander_K._Lew1;~Tim_Vieira1",
        "aff": "Massachusetts Institute of Technology;Mila - Quebec Artificial Intelligence Institute;;Massachusetts Institute of Technology;CHI;Johns Hopkins University;Johns Hopkins University;;Massachusetts Institute of Technology;McGill University, Mila+McGill University;Massachusetts Institute of Technology;Department of Computer Science, ETHZ - ETH Zurich",
        "aff_domain": "mit.edu;mila.quebec;;mit.edu;chi-fro.org;cs.jhu.edu;jhu.edu;;mit.edu;mila.quebec+mcgill.ca;mit.edu;inf.ethz.ch",
        "position": "PhD student;Researcher;;PhD student;Researcher;PhD student;Full Professor;;Principal Research Scientist;Associate Professor+Associate Professor;PhD student;Postdoc",
        "bibtex": "@inproceedings{\nlipkin2025fast,\ntitle={Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling},\nauthor={Ben Lipkin and Benjamin LeBrun and Jacob Hoover Vigly and Jo{\\~a}o Loula and David R. MacIver and Li Du and Jason Eisner and Ryan Cotterell and Vikash Mansinghka and Timothy J. O'Donnell and Alexander K. Lew and Tim Vieira},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=3BmPSFAdq3}\n}",
        "github": "",
        "project": "",
        "reviewers": "cnre;qHSZ;VsDj;tTyg",
        "site": "https://openreview.net/forum?id=3BmPSFAdq3",
        "pdf_size": 0,
        "rating": "7;7;8;9",
        "confidence": "3;4;4;5",
        "wc_review": "",
        "rating_avg": [
            7.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8528028654224417
    },
    {
        "id": "3JiCl2A14H",
        "title": "SmolLM2: When Smol Goes Big \u2014 Data-Centric Training of a Fully Open Small Language Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models, while groundbreaking, are computationally expensive and difficult to deploy in resource-constrained settings. To address this challenge, small language models have emerged, but their performance critically depends on the quality and composition of the pretraining datasets\u2014yet many recent models, such as Qwen2.5-1.5B and Llama3.2-1B, remain opaque about their training data, limiting reproducibility and scientific understanding. In this paper, we document and publicly release SmolLM2, a fully transparent state-of-the-art ``small'' (1.7 billion parameter) language model (LM), along with its training datasets and code. To attain strong performance, we overtrain SmolLM2 on 11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally curate and release new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations and a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous one. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B, Llama3.2-1B, and Falcon3-1.6B. By releasing our model, datasets, and code, we aim to facilitate future research on LM development as well as applications of small LMs.",
        "keywords": "small language models;dataset;pretraining",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Loubna Ben allal;Anton Lozhkov;Elie Bakouch;Gabriel Martin Blazquez;Guilherme Penedo;Lewis Tunstall;Andr\u00e9s Marafioti;Agust\u00edn Piqueres Lajar\u00edn;Hynek Kydl\u00ed\u010dek;Vaibhav Srivastav;Joshua Lochner;Caleb Fahlgren;Xuan Son NGUYEN;Ben Burtenshaw;Cl\u00e9mentine Fourrier;Haojun Zhao;Hugo Larcher;Mathieu Morlon;Cyril Zakka;Colin Raffel;Leandro Von Werra;Thomas Wolf",
        "authorids": "~Loubna_Ben_allal1;~Anton_Lozhkov1;~Elie_Bakouch1;~Gabriel_Martin_Blazquez1;~Guilherme_Penedo1;~Lewis_Tunstall1;~Andr\u00e9s_Marafioti1;~Agust\u00edn_Piqueres_Lajar\u00edn1;~Hynek_Kydl\u00ed\u010dek1;~Vaibhav_Srivastav2;~Joshua_Lochner1;~Caleb_Fahlgren1;~Xuan_Son_NGUYEN3;~Ben_Burtenshaw1;~Cl\u00e9mentine_Fourrier1;~Haojun_Zhao1;~Hugo_Larcher1;~Mathieu_Morlon1;~Cyril_Zakka1;~Colin_Raffel1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "gender": "F;;M;M;M;M;;M;M;M;M;M;M;M;;M;M;;M;;M;M",
        "homepage": "https://loubnabnl.github.io/;;;https://gabrielmb.com;https://github.com/guipenedo;https://lewtun.github.io/blog/;;https://github.com/plaguss;https://me.hynky.name/;https://vaibhavs10.github.io;;https://calebfahlgren.com/;https://ngxson.com;https://www.uantwerpen.be/en/staff/benjamin-burtenshaw/;;;;https://github.com/glutamatt;https://cyrilzakka.github.io;http://colinraffel.com;https://github.com/lvwerra;https://thomwolf.io",
        "dblp": ";;;;;;228/9304;;;;;;;;;;;;;149/0082;223/1855;",
        "google_scholar": "reU1i-sAAAAJ;xlMMVCAAAAAJ;;;L-jmoJYAAAAJ;Hc6MI0QAAAAJ;;;;;;;;https://scholar.google.nl/citations?user=Dcav34UAAAAJ;;;;;AnQY3l8AAAAJ;I66ZBYwAAAAJ;https://scholar.google.com/citations?hl=en;D2H5EFEAAAAJ",
        "orcid": ";;;;;;;;;;0000-0002-5823-071X;;;;;;;;0000-0001-8446-2349;;;",
        "linkedin": "https://www.linkedin.com/mwlite/in/loubna-ben-allal-238690152;anton-lozhkov/;eliebak/;gabrielmbmb/;;lewis-tunstall/;;aguspiql/;;;;calebfahlgren/;;;;haojunzhao/;hlarcher;;;;lvwerra/;",
        "or_profile": "~Loubna_Ben_allal1;~Anton_Lozhkov1;~Elie_Bakouch1;~Gabriel_Martin_Blazquez1;~Guilherme_Penedo1;~Lewis_Tunstall1;~Andr\u00e9s_Marafioti1;~Agust\u00edn_Piqueres_Lajar\u00edn1;~Hynek_Kydl\u00ed\u010dek1;~Vaibhav_Srivastav2;~Joshua_Lochner1;~Caleb_Fahlgren1;~Xuan_Son_NGUYEN3;~Ben_Burtenshaw1;~Cl\u00e9mentine_Fourrier1;~Haojun_Zhao1;~Hugo_Larcher1;~Mathieu_Morlon1;~Cyril_Zakka1;~Colin_Raffel1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "aff": "Hugging Face;Hugging Face;Hugging Face;Hugging Face;HuggingFace;Hugging Face;Hugging Face;Hugging Face;Huggingface;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Department of Computer Science, University of Toronto+Hugging Face;Hugging Face;Hugging Face",
        "aff_domain": "hugggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;;huggingface.co;huggingface.co;huggingface.co;huggingface.co;cs.toronto.edu+huggingface.co;hf.co;huggingface.co",
        "position": "Researcher;Machine Learning Engineer;Researcher;Researcher;Researcher;Researcher;Researcher;Researcher;Intern;Researcher;Researcher;Product ML Engineer;Researcher;Researcher;;Researcher;Researcher;Researcher;Researcher;Associate Professor+Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nallal2025smollm,\ntitle={Smol{LM}2: When Smol Goes Big {\\textemdash} Data-Centric Training of a Fully Open Small Language Model},\nauthor={Loubna Ben allal and Anton Lozhkov and Elie Bakouch and Gabriel Martin Blazquez and Guilherme Penedo and Lewis Tunstall and Andr{\\'e}s Marafioti and Agust{\\'\\i}n Piqueres Lajar{\\'\\i}n and Hynek Kydl{\\'\\i}{\\v{c}}ek and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan Son NGUYEN and Ben Burtenshaw and Cl{\\'e}mentine Fourrier and Haojun Zhao and Hugo Larcher and Mathieu Morlon and Cyril Zakka and Colin Raffel and Leandro Von Werra and Thomas Wolf},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=3JiCl2A14H}\n}",
        "github": "",
        "project": "",
        "reviewers": "fgjE;sj9u;FMrj;8zt8",
        "site": "https://openreview.net/forum?id=3JiCl2A14H",
        "pdf_size": 0,
        "rating": "6;7;7;9",
        "confidence": "3;5;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            22,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3244428422615251
    },
    {
        "id": "3NjnRo6apU",
        "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 13 different fact verification models, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances.",
        "keywords": "factuality;fact verifier;attribution evaluation;LLM hallucination",
        "primary_area": "",
        "supplementary_material": "/attachment/eb70f4621cb8b5fcdd29c5acc930ae032dfcfcae.zip",
        "author": "Wooseok Seo;Seungju Han;Jaehun Jung;Benjamin Newman;Seungwon Lim;Seungbeen Lee;Ximing Lu;Yejin Choi;Youngjae Yu",
        "authorids": "~Wooseok_Seo1;~Seungju_Han2;~Jaehun_Jung1;~Benjamin_Newman1;~Seungwon_Lim1;~Seungbeen_Lee1;~Ximing_Lu1;~Yejin_Choi1;~Youngjae_Yu1",
        "gender": "M;M;M;;M;F;F;F;M",
        "homepage": ";https://seungjuhan.me;https://jaehunjung.com;http://blnewman.com;https://sngwonlim.github.io/;https://github.com/seunbite;https://gloriaximinglu.github.io/;https://yejinc.github.io/;https://yj-yu.github.io/home/",
        "dblp": ";;192/7707;126/5109;349/7859;;24/10879;89/579-1;188/6210",
        "google_scholar": "G-Tl_2YAAAAJ;g_anRqAAAAAJ;_bXzUGEAAAAJ;QehvrDoAAAAJ;L-Ikpu0AAAAJ;;https://scholar.google.com/citations?hl=en;vhP-tlcAAAAJ;https://scholar.google.co.kr/citations?user=WDO24ZYAAAAJ",
        "orcid": ";;0000-0002-0292-3074;;0009-0007-3887-6189;;;;",
        "linkedin": "wooseok-seo-aa617a22a/;seungju-han-66b85017a/;;;seungwon-lim-196802246/;;;;",
        "or_profile": "~Wooseok_Seo1;~Seungju_Han2;~Jaehun_Jung1;~Benjamin_Newman1;~Seungwon_Lim1;~Seungbeen_Lee1;~Ximing_Lu1;~Yejin_Choi1;~Youngjae_Yu1",
        "aff": "Yonsei University;Computer Science Department, Stanford University+NVIDIA;University of Washington;University of Washington;Yonsei University;Yonsei University;University of Washington;Computer Science Department, Stanford University+NVIDIA;Yonsei University",
        "aff_domain": "yonsei.ac.kr;cs.stanford.edu+nvidia.com;uw.edu;cs.washington.edu;yonsei.ac.kr;yonsei.ac.kr;cs.washington.edu;cs.stanford.edu+nvidia.com;yonsei.ac.kr",
        "position": "PhD student;PhD student+Researcher;PhD student;PhD student;PhD student;MS student;PhD student;Full Professor+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nseo2025verifying,\ntitle={Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers},\nauthor={Wooseok Seo and Seungju Han and Jaehun Jung and Benjamin Newman and Seungwon Lim and Seungbeen Lee and Ximing Lu and Yejin Choi and Youngjae Yu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=3NjnRo6apU}\n}",
        "github": "",
        "project": "",
        "reviewers": "i78y;TTKa;NJrb;6Tms",
        "site": "https://openreview.net/forum?id=3NjnRo6apU",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "3vxxB3Ar9r",
        "title": "One ruler to measure them all: Benchmarking multilingual long-context language models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the \"needle-in-a-haystack\" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines.",
        "keywords": "Multilingual;Benchmark;Long-context;Synthetic dataset",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yekyung Kim;Jenna Russell;Marzena Karpinska;Mohit Iyyer",
        "authorids": "~Yekyung_Kim1;~Jenna_Russell1;~Marzena_Karpinska1;~Mohit_Iyyer1",
        "gender": "F;F;;M",
        "homepage": ";https://jenna-russell.github.io/;;http://cs.umass.edu/~miyyer",
        "dblp": "151/5527;398/4161;;148/9178",
        "google_scholar": "https://scholar.google.co.kr/citations?user=6Rn81SsAAAAJ;ru5uMcUAAAAJ;;rBVA5tcAAAAJ",
        "orcid": ";;;",
        "linkedin": "yekyung-kim-b9413a91;jennifer-j-russell/;;",
        "or_profile": "~Yekyung_Kim1;~Jenna_Russell1;~Marzena_Karpinska1;~Mohit_Iyyer1",
        "aff": "University of Maryland, College Park+University of Massachusetts at Amherst;University of Maryland, College Park+University of Massachusetts at Amherst;;University of Maryland, College Park",
        "aff_domain": "umd.edu+umass.edu;umd.edu+umass.edu;;umd.edu",
        "position": "PhD student+PhD student;PhD student+PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nkim2025one,\ntitle={One ruler to measure them all: Benchmarking multilingual long-context language models},\nauthor={Yekyung Kim and Jenna Russell and Marzena Karpinska and Mohit Iyyer},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=3vxxB3Ar9r}\n}",
        "github": "",
        "project": "",
        "reviewers": "tfQ4;p1ae;UQwv;mDZF",
        "site": "https://openreview.net/forum?id=3vxxB3Ar9r",
        "pdf_size": 0,
        "rating": "7;7;7;9",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "3xErKrVAdG",
        "title": "Privately Learning from Graphs with Applications in Fine-tuning Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graphs offer unique insights into relationships between entities, complementing data modalities like text and images and enabling AI models to extend their capabilities beyond traditional tasks. However, learning from graphs often involves handling sensitive relations, raising significant privacy concerns. Existing privacy-preserving methods, such as DP-SGD, rely on gradient decoupling assumptions and are incompatible with relational learning due to the inherent dependencies between training samples. To address this challenge, we propose a privacy-preserving pipeline for relational learning that decouples dependencies in sampled relations during training, ensuring differential privacy through a tailored application of DP-SGD. We apply this approach to fine-tune large language models (LLMs), such as BERT and Llama2, on sensitive graph data while addressing the associated computational complexities. Our method is evaluated on four real-world text-attributed graphs, demonstrating significant improvements in relational learning tasks while maintaining robust privacy guarantees. Additionally, we analyze the trade-offs between privacy, utility, and computational efficiency, offering insights into the practical deployment of our approach for privacy-preserving relational learning. Code is available at https://github.com/Graph-COM/PvGaLM.",
        "keywords": "differential privacy;relational learning;private learning;language models;fine-tuning",
        "primary_area": "",
        "supplementary_material": "/attachment/ed5d9a838d52f64768239444d7c642ba54b0ab31.zip",
        "author": "Haoteng Yin;Rongzhe Wei;Eli Chien;Pan Li",
        "authorids": "~Haoteng_Yin1;~Rongzhe_Wei1;~Eli_Chien1;~Pan_Li2",
        "gender": "M;M;;",
        "homepage": "https://home.veritasyin.me/;https://jesson-wei.github.io/Rongzhe-Wei.github.io/;;",
        "dblp": "206/6804;259/6894;;https://dblp.org/pers/hd/l/Li_0005:Pan",
        "google_scholar": "https://scholar.google.com/citations?hl=en;di8ubMoAAAAJ;;IroP0EwAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;pan-li-b951105a/",
        "or_profile": "~Haoteng_Yin1;~Rongzhe_Wei1;~Eli_Chien1;~Pan_Li2",
        "aff": "Purdue University;Georgia Institute of Technology+Amazon;;Georgia Institute of Technology+Purdue University",
        "aff_domain": "purdue.edu;gatech.edu+amazon.com;;gatech.edu+purdue.edu",
        "position": "PhD student;PhD student+Intern;;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nyin2025privately,\ntitle={Privately Learning from Graphs with Applications in Fine-tuning Large Language Models},\nauthor={Haoteng Yin and Rongzhe Wei and Eli Chien and Pan Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=3xErKrVAdG}\n}",
        "github": "",
        "project": "",
        "reviewers": "pMGM;Uv6C;SESE;7KXH;n351;VoeN;qk3t",
        "site": "https://openreview.net/forum?id=3xErKrVAdG",
        "pdf_size": 0,
        "rating": "5;6;6;6;7;7;7",
        "confidence": "3;3;3;3;3;2;2",
        "wc_review": "",
        "rating_avg": [
            6.285714285714286,
            0.6998542122237652
        ],
        "confidence_avg": [
            2.7142857142857144,
            0.45175395145262565
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6454972243679029
    },
    {
        "id": "4Ns18bSoHo",
        "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advances leverage post-training to enhance model reasoning performance, which typically requires costly training pipelines and still suffers from inefficient, overly lengthy outputs. We introduce **Speculative Thinking**, a training-free framework that enables large reasoning models to guide smaller ones during inference at the reasoning level, distinct from speculative decoding, which operates at the token level. Our approach is based on two observations: (1) reasoning-supportive tokens such as $''wait''$ frequently appear after structural delimiters like $''\\n\\n''$, serving as signals for reflection or continuation; and (2) larger models exhibit stronger control over reflective behavior, reducing unnecessary backtracking while improving reasoning quality. By strategically delegating reflective steps to a more capable model, our method significantly boosts the reasoning accuracy of reasoning models while shortening their output. With the assistance of the 32B reasoning model, the 1.5B model\u2019s accuracy on $MATH500$ increases from 83.2\\% to 89.4\\%, marking a substantial improvement of 6.2\\%. Simultaneously, the average output length is reduced from $5439$ tokens to $4583$ tokens, representing a 15.7\\% decrease. Moreover, when applied to a non-reasoning model (Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0\\% to 81.8\\% on the same benchmark, achieving a relative improvement of 7.8\\%.",
        "keywords": "LLM Reasoning; LLM Inference; Speculative Decoding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Van Yang;Xiang Yue;Vipin Chaudhary;Xiaotian Han",
        "authorids": "~Van_Yang1;~Xiang_Yue1;~Vipin_Chaudhary2;~Xiaotian_Han1",
        "gender": "M;;M;M",
        "homepage": "https://uservan.github.io/;;https://engineering.case.edu/profiles/vxc204;https://ahxt.github.io/",
        "dblp": ";;c/VipinChaudhary.html;",
        "google_scholar": "5JmuGbcAAAAJ;;vJbjqpIAAAAJ;Uromx98AAAAJ",
        "orcid": "0009-0008-0891-7215;;0000-0001-9672-6225;",
        "linkedin": ";;vipin-chaudhary-379529/;",
        "or_profile": "~Van_Yang1;~Xiang_Yue1;~Vipin_Chaudhary2;~Xiaotian_Han1",
        "aff": "Case Western Reserve University;;Case Western Reserve University;Case Western Reserve University",
        "aff_domain": "case.edu;;case.edu;case.edu",
        "position": "PhD student;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025speculative,\ntitle={Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time},\nauthor={Van Yang and Xiang Yue and Vipin Chaudhary and Xiaotian Han},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=4Ns18bSoHo}\n}",
        "github": "",
        "project": "",
        "reviewers": "kGJs;Xtaq;UDes",
        "site": "https://openreview.net/forum?id=4Ns18bSoHo",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "4d69EwfKAr",
        "title": "Law of Vision Representation in MLLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce the \"Law of Vision Representation\" in multimodal large language models (MLLMs), revealing a strong correlation among cross-modal alignment, vision representation correspondence, and overall model performance. We quantify the these factors using the cross-modal Alignment and Correspondence score. Extensive experiments across fifteen distinct vision representation settings and evaluations on eight benchmarks show that the A and C scores correlate with performance following a quadratic relationship. By leveraging this relationship, we can identify and train the optimal vision representation for an MLLM, achieving a 99.7% reduction in computational cost without the need for repeated finetuning of the language model.",
        "keywords": "Multimodality Large Language Models; Computer Vision; Vision Representation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shijia Yang;Bohan Zhai;Quanzeng You;Jianbo Yuan;Hongxia Yang;Chenfeng Xu",
        "authorids": "~Shijia_Yang1;~Bohan_Zhai1;~Quanzeng_You3;~Jianbo_Yuan1;~Hongxia_Yang2;~Chenfeng_Xu1",
        "gender": "F;M;M;M;F;M",
        "homepage": ";;https://qzyou.github.io/;;https://www4.comp.polyu.edu.hk/~hongxyang/;https://www.chenfengx.com",
        "dblp": ";;33/9972.html;134/6790;;65/1881",
        "google_scholar": ";TAbgR14AAAAJ;c5KJsIgAAAAJ;https://scholar.google.com/citations?hl=en;iJlC5mMAAAAJ;RpqvaTUAAAAJ",
        "orcid": ";;0000-0003-3608-0607;;;0000-0002-4941-6985",
        "linkedin": "bronya-shijia-yang-762927193/;;quanzeng-you-5b98a55a/;;;",
        "or_profile": "~Shijia_Yang1;~Bohan_Zhai1;~Quanzeng_You3;~Jianbo_Yuan1;~Hongxia_Yang2;~Chenfeng_Xu1",
        "aff": "Stanford University;Snowflake;ByteDance;Amazon;Hong Kong Polytechnic University;University of California, Berkeley",
        "aff_domain": "stanford.edu;snowflake.com;bytedance.com;amazon.com;polyu.edu.hk;berkeley.edu",
        "position": "MS student;Researcher;Researcher;Principal Researcher;Full Professor;PhD student",
        "bibtex": "@inproceedings{\nyang2025law,\ntitle={Law of Vision Representation in {MLLM}s},\nauthor={Shijia Yang and Bohan Zhai and Quanzeng You and Jianbo Yuan and Hongxia Yang and Chenfeng Xu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=4d69EwfKAr}\n}",
        "github": "",
        "project": "",
        "reviewers": "ufKw;UhUv;Qc4i;s8cX",
        "site": "https://openreview.net/forum?id=4d69EwfKAr",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;4;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "4jdIxXBNve",
        "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''\u2014that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO),  a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. Specifically, using LCPO we derive Short Reasoning Models (SRMs), that exhibit similar reasoning patterns as full-length reasoning models, but can generate CoT lengths comparable to non-reasoning models. They demonstrate significant performance gains, for instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy.",
        "keywords": "reasoning llms;controllability;test-time comptue",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pranjal Aggarwal;Sean Welleck",
        "authorids": "~Pranjal_Aggarwal1;~Sean_Welleck1",
        "gender": "M;",
        "homepage": "https://github.com/Pranjal2041/;",
        "dblp": "163/0764;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;",
        "orcid": "0000-0002-2962-1535;",
        "linkedin": ";",
        "or_profile": "~Pranjal_Aggarwal1;~Sean_Welleck1",
        "aff": "School of Computer Science, Carnegie Mellon University;",
        "aff_domain": "cs.cmu.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\naggarwal2025l,\ntitle={L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning},\nauthor={Pranjal Aggarwal and Sean Welleck},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=4jdIxXBNve}\n}",
        "github": "",
        "project": "",
        "reviewers": "E128;2aPy;ivGY;wXkP",
        "site": "https://openreview.net/forum?id=4jdIxXBNve",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "4mxQmpnawk",
        "title": "Resona: Improving Context Copying in Linear Recurrence Models with Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent shifts in the space of large language model (LLM) research have shown an increasing focus on novel architectures to compete with prototypical Transformer-based models that have long dominated this space. Linear recurrent models have proven to be a viable competitor due to their computational efficiency. However, such models still demonstrate a sizeable gap compared to Transformers in terms of in-context learning among other tasks that require recalling information from a context. In this work, we introduce __Resona__, a simple and scalable framework for augmenting linear recurrent models with retrieval. __Resona__ augments models with the ability to integrate retrieved information from the provided input context, enabling tailored behaviour to diverse task requirements. Experiments on a variety of linear recurrent models demonstrate that __Resona__-augmented models observe significant performance gains on a variety of synthetic as well as real-world natural language tasks, highlighting its ability to act as a general purpose method to improve the in-context learning and language modelling abilities of linear recurrent LLMs.",
        "keywords": "Large Language Models;Linear Recurrent Models;In-Context Learning;Retrieval",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinyu Wang;Linrui Ma;Jerry Huang;Peng Lu;Prasanna Parthasarathi;Xiao-Wen Chang;Boxing Chen;Yufei Cui",
        "authorids": "~Xinyu_Wang17;~Linrui_Ma1;~Jerry_Huang1;~Peng_Lu6;~Prasanna_Parthasarathi2;~Xiao-Wen_Chang1;~Boxing_Chen1;~Yufei_Cui3",
        "gender": "M;M;;M;M;M;M;",
        "homepage": ";;;;https://www.cs.mcgill.ca/~pparth2/;https://www.cs.mcgill.ca/~chang;https://sites.google.com/site/chenboxing/Home;",
        "dblp": ";;;;211/7503;;12/1081;",
        "google_scholar": "gNQIDzQAAAAJ;;;c4xAa8gAAAAJ;https://scholar.google.co.in/citations?hl=en;;LiINs3gAAAAJ;",
        "orcid": ";;;;;;0000-0002-3170-4858;",
        "linkedin": "xinyu-wang-99b119200;linrui-ma-507467223/;;peng-lu-211b7617a/;prasanna-parthasarathi/;;;",
        "or_profile": "~Xinyu_Wang17;~Linrui_Ma1;~Jerry_Huang1;~Peng_Lu6;~Prasanna_Parthasarathi2;~Xiao-Wen_Chang1;~Boxing_Chen1;~Yufei_Cui3",
        "aff": "McGill University;Mila - Quebec Artificial Intelligence Institute;;University of Montreal;Huawei Technologies Ltd.;McGill University;Huawei Technologies Ltd.;",
        "aff_domain": "mail.mcgill.ca;mila.quebec;;umontreal.ca;huawei.com;mcgill.ca;huawei.com;",
        "position": "PhD student;MS student;;PhD student;Researcher;Associate Professor;Principal Researcher;",
        "bibtex": "@inproceedings{\nwang2025resona,\ntitle={Resona: Improving Context Copying in Linear Recurrence Models with Retrieval},\nauthor={Xinyu Wang and Linrui Ma and Jerry Huang and Peng Lu and Prasanna Parthasarathi and Xiao-Wen Chang and Boxing Chen and Yufei Cui},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=4mxQmpnawk}\n}",
        "github": "",
        "project": "",
        "reviewers": "UqFQ;9ueu;ymyb",
        "site": "https://openreview.net/forum?id=4mxQmpnawk",
        "pdf_size": 0,
        "rating": "7;8;8",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "52YBEzcI0l",
        "title": "Spike No More: Stabilizing the Pre-training of Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Loss spikes often occur during pre-training of large language models.\nThe spikes degrade the performance of large language models and sometimes ruin the pre-training.\nSince the pre-training needs a vast computational budget, we should avoid such spikes.\nBased on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers.\nOur findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut.\nWe conduct various experiments to empirically verify our theoretical analyses.\nExperimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.",
        "keywords": "stable training;llm;pre-training",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sho Takase;Shun Kiyono;Sosuke Kobayashi;Jun Suzuki",
        "authorids": "~Sho_Takase2;~Shun_Kiyono1;~Sosuke_Kobayashi1;~Jun_Suzuki1",
        "gender": ";M;M;M",
        "homepage": "https://takase.github.io/;https://butsugiri.github.io/;https://soskek.github.io/;https://www.nlp.ecei.tohoku.ac.jp/~jun/",
        "dblp": ";211/7611;185/5523;78/6923",
        "google_scholar": "https://scholar.google.co.jp/citations?user=2dvzFDYAAAAJ;LS3EdOoAAAAJ;VY6PqvsAAAAJ;https://scholar.google.co.jp/citations?user=XO5CrIsAAAAJ",
        "orcid": ";;;0000-0003-2108-1340",
        "linkedin": ";;;",
        "or_profile": "~Sho_Takase2;~Shun_Kiyono1;~Sosuke_Kobayashi1;~Jun_Suzuki1",
        "aff": "LINE Corporation;SB Intuitions;Tohoku University+Preferred Networks, Inc.;Tohoku University",
        "aff_domain": "linecorp.com;sbintuitions.co.jp;tohoku.ac.jp+preferred.jp;tohoku.ac.jp",
        "position": "Researcher;Researcher;Researcher+Researcher;Full Professor",
        "bibtex": "@inproceedings{\ntakase2025spike,\ntitle={Spike No More: Stabilizing the Pre-training of Large Language Models},\nauthor={Sho Takase and Shun Kiyono and Sosuke Kobayashi and Jun Suzuki},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=52YBEzcI0l}\n}",
        "github": "",
        "project": "",
        "reviewers": "ePfy;2gZh;4pzp",
        "site": "https://openreview.net/forum?id=52YBEzcI0l",
        "pdf_size": 0,
        "rating": "4;6;6",
        "confidence": "5;4;3",
        "wc_review": "",
        "rating_avg": [
            5.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "5PAF7PAY2Y",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art.",
        "keywords": "R1-Zero;Reinforcement Learning;Post-Training",
        "primary_area": "",
        "supplementary_material": "/attachment/d50bac1e8727c4425c63d8fa98bfd5d403479734.zip",
        "author": "Zichen Liu;Changyu Chen;Wenjun Li;Penghui Qi;Tianyu Pang;Chao Du;Wee Sun Lee;Min Lin",
        "authorids": "~Zichen_Liu1;~Changyu_Chen2;~Wenjun_Li1;~Penghui_Qi1;~Tianyu_Pang1;~Chao_Du1;~Wee_Sun_Lee1;~Min_Lin1",
        "gender": ";M;M;M;M;M;M;M",
        "homepage": ";;;;https://p2333.github.io/;https://duchao0726.github.io/;http://www.comp.nus.edu.sg/~leews/;https://linmin.me",
        "dblp": ";;;236/4626;202/2550;75/7523;86/1498;",
        "google_scholar": ";https://scholar.google.com.sg/citations?hl=en;https://scholar.google.com/citations?hl=zh-CN;CLRsGEMAAAAJ;wYDbtFsAAAAJ;QOp7xW0AAAAJ;https://scholar.google.com.sg/citations?user=8PCrLgwAAAAJ;BGONmkIAAAAJ",
        "orcid": ";;;;0000-0003-0639-6176;0000-0003-1244-6336;;",
        "linkedin": ";;wenjun-li-004375116/;;%E5%A4%A9%E5%AE%87-%E5%BA%9E-b3999017a/;duchao/;;min-lin-08a3a422/",
        "or_profile": "~Zichen_Liu1;~Changyu_Chen2;~Wenjun_Li1;~Penghui_Qi1;~Tianyu_Pang1;~Chao_Du1;~Wee_Sun_Lee1;~Min_Lin1",
        "aff": ";Singapore Management University;Singapore Management University;National University of Singapore+sea ai lab;Sea AI Lab;Sea AI Lab;National University of Singapore;Sea AI Lab",
        "aff_domain": ";smu.edu.sg;smu.edu.sg;nus.edu+sail.sea.com;sea.com;sea.com;nus.edu.sg;sea.com",
        "position": ";PhD student;PhD student;PhD student+Researcher;Senior Research Scientist;Senior Research Scientist;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nliu2025understanding,\ntitle={Understanding R1-Zero-Like Training: A Critical Perspective},\nauthor={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=5PAF7PAY2Y}\n}",
        "github": "",
        "project": "",
        "reviewers": "FjK5;dTZp;pF1u",
        "site": "https://openreview.net/forum?id=5PAF7PAY2Y",
        "pdf_size": 0,
        "rating": "5;9;10",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            8.0,
            2.160246899469287
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9819805060619656
    },
    {
        "id": "5UkUsRsWYx",
        "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The ability to acquire latent semantics is one of the key properties that determines the performance of language models. \nOne convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. \nPrevious studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. \nTo understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. \nInterestingly, we found that this approach produces both positive and negative effects on the downstream tasks. \nWe demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. \nSpecifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. \nIn contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.",
        "keywords": "LLM;pretraining;metadata conditioning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rei Higuchi;Ryotaro Kawata;Naoki Nishikawa;Kazusato Oko;Shoichiro Yamaguchi;Sosuke Kobayashi;Seiya Tokui;Kohei Hayashi;Daisuke Okanohara;Taiji Suzuki",
        "authorids": "~Rei_Higuchi1;~Ryotaro_Kawata1;~Naoki_Nishikawa1;~Kazusato_Oko1;~Shoichiro_Yamaguchi1;~Sosuke_Kobayashi1;~Seiya_Tokui1;~Kohei_Hayashi1;~Daisuke_Okanohara1;~Taiji_Suzuki1",
        "gender": "M;;M;M;M;M;M;M;M;M",
        "homepage": ";;https://sites.google.com/view/n-nishikawa;;;https://soskek.github.io/;https://www.beam2d.net/;https://sites.google.com/site/koheihayashi84;;http://ibis.t.u-tokyo.ac.jp/suzuki/",
        "dblp": ";399/5423.html;24/2962.html;;76/9374;185/5523;162/3213;84/1101.html;60/4701;08/312",
        "google_scholar": ";MJvufWcAAAAJ;https://scholar.google.co.jp/citations?user=JGwf2FAAAAAJ;;;VY6PqvsAAAAJ;qOkcVXMAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.co.jp/citations?user=14n6Z6IAAAAJ;x8osrBsAAAAJ",
        "orcid": "0009-0009-4113-819X;;;;;;;;;",
        "linkedin": ";ryotaro-kawata-deep/;;kazusatooko/;;;;koheih/;;",
        "or_profile": "~Rei_Higuchi1;~Ryotaro_Kawata1;~Naoki_Nishikawa1;~Kazusato_Oko1;~Shoichiro_Yamaguchi1;~Sosuke_Kobayashi1;~Seiya_Tokui1;~Kohei_Hayashi1;~Daisuke_Okanohara1;~Taiji_Suzuki1",
        "aff": "the University of Tokyo;The University of Tokyo;Graduate School of Information Science and Technology, The University of Tokyo+RIKEN Center for Advanced Intelligence Project, RIKEN;University of California, Berkeley;Preferred Networks, Inc.;Tohoku University+Preferred Networks, Inc.;Preferred Networks, Inc.;The University of Tokyo, Tokyo Institute of Technology+Preferred Networks, Inc.;Preferred Networks, Inc.;The University of Tokyo",
        "aff_domain": "g.ecc.u-tokyo.ac.jp;u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp+riken.jp;berkeley.edu;preferred.jp;tohoku.ac.jp+preferred.jp;preferred.jp;u-tokyo.ac.jp+preferred.jp;preferred.jp;u-tokyo.ac.jp",
        "position": "MS student;MS student;PhD student+Researcher;PhD student;Researcher;Researcher+Researcher;Researcher;Lecturer+Researcher;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nhiguchi2025when,\ntitle={When Does Metadata Conditioning ({NOT}) Work for Language Model Pre-Training? A Study with Context-Free Grammars},\nauthor={Rei Higuchi and Ryotaro Kawata and Naoki Nishikawa and Kazusato Oko and Shoichiro Yamaguchi and Sosuke Kobayashi and Seiya Tokui and Kohei Hayashi and Daisuke Okanohara and Taiji Suzuki},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=5UkUsRsWYx}\n}",
        "github": "",
        "project": "",
        "reviewers": "xHQg;StBC;dKwt;d5Xy",
        "site": "https://openreview.net/forum?id=5UkUsRsWYx",
        "pdf_size": 0,
        "rating": "5;6;6;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.13245323570650439
    },
    {
        "id": "5mICyyD4OF",
        "title": "MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While AI presents significant potential for enhancing music mixing and mastering workflows, current research predominantly emphasizes end-to-end automation or generation, often overlooking the collaborative and instructional dimensions vital for co-creative processes. This gap leaves artists, particularly amateurs seeking to develop expertise, underserved. To bridge this, we introduce MixAssist, a novel audio-language dataset capturing the situated, multi-turn dialogue between expert and amateur music producers during collaborative mixing sessions. Comprising 431 audio-grounded conversational turns derived from 7 in-depth sessions involving 12 producers, MixAssist provides a unique resource for training and evaluating audio-language models that can comprehend and respond to the complexities of real-world music production dialogues. Our evaluations, including automated LLM-as-a-judge assessments and human expert comparisons, demonstrate that fine-tuning models such as Qwen-Audio on MixAssist can yield promising results, with Qwen significantly outperforming other tested models in generating helpful, contextually relevant mixing advice. By focusing on co-creative instruction grounded in audio context, MixAssist enables the development of intelligent AI assistants designed to support and augment the creative process in music mixing.",
        "keywords": "audio-language dataset;dataset creation;co-creativity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Paul Clemens;Ana Marasovic",
        "authorids": "~Michael_Paul_Clemens1;~Ana_Marasovic1",
        "gender": "M;F",
        "homepage": "http://mclem.in;https://www.anamarasovic.com",
        "dblp": ";185/0950",
        "google_scholar": "kGcBokwAAAAJ;3W6OnfAAAAAJ",
        "orcid": "0000-0002-4507-8421;",
        "linkedin": "m-clem/;",
        "or_profile": "~Michael_Paul_Clemens1;~Ana_Marasovic1",
        "aff": "New Jersey Institute of Technology;University of Utah",
        "aff_domain": "njit.edu;utah.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nclemens2025mixassist,\ntitle={MixAssist: An Audio-Language Dataset for Co-Creative {AI} Assistance in Music Mixing},\nauthor={Michael Paul Clemens and Ana Marasovic},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=5mICyyD4OF}\n}",
        "github": "",
        "project": "",
        "reviewers": "jqzw;UNoT;ThtW",
        "site": "https://openreview.net/forum?id=5mICyyD4OF",
        "pdf_size": 0,
        "rating": "5;8;9",
        "confidence": "4;5;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            1.699673171197595
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.2773500981126146
    },
    {
        "id": "5wAfbEs34A",
        "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling test-time computation through detailed reasoning traces significantly enhances performance.Although these traces effectively facilitate knowledge distillation into smaller, instruction-tuned models, the precise nature of transferred reasoning remains unclear. In this study, we investigate to what extent distilled models internalize replicated stylistic patterns during reasoning. To this end, we systematically analyze reasoning traces, identifying structural and lexical patterns that characterize successful reasoning. We then introduce two new datasets -- a dataset of emergent reasoning traces and a synthetic dataset explicitly constructed to replicate these stylistic patterns -- to precisely examine their influence on distilled models' reasoning capabilities. We find that models trained on the synthetic traces achieve comparable performance, indicating that distilled reasoning abilities rely significantly on surface-level patterns. Surprisingly, we observe an increase in performance even when the synthetic traces are altered to lead to the wrong answer. Our findings highlight how stylistic patterns can be leveraged to efficiently enhance LM reasoning across diverse model families.",
        "keywords": "Reasoning;language models;stylistic mimicry;pivots;synthetic data;distillation;finetuning;metacognition",
        "primary_area": "",
        "supplementary_material": "/attachment/249d626d9fb3445366a1147e370bbc82ac834811.zip",
        "author": "Philip Lippmann;Jie Yang",
        "authorids": "~Philip_Lippmann1;~Jie_Yang1",
        "gender": ";M",
        "homepage": "https://www.wis.ewi.tudelft.nl/lippmann;http://yangjiera.github.io",
        "dblp": "331/3240;12/1198-28.html",
        "google_scholar": ";DAlsOOEAAAAJ",
        "orcid": "0000-0002-0139-1061;0000-0002-0350-0313",
        "linkedin": ";",
        "or_profile": "~Philip_Lippmann1;~Jie_Yang1",
        "aff": "Delft University of Technology;Delft University of Technology",
        "aff_domain": "tudelft.nl;tudelft.nl",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nlippmann2025style,\ntitle={Style over Substance: Distilled Language Models Reason Via Stylistic Replication},\nauthor={Philip Lippmann and Jie Yang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=5wAfbEs34A}\n}",
        "github": "",
        "project": "",
        "reviewers": "F6zJ;nWWk;CCPR;x9kt",
        "site": "https://openreview.net/forum?id=5wAfbEs34A",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "63JtmQL7dv",
        "title": "Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion.\nWe introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion.\nThis enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.",
        "keywords": "web agent;evaluation;distillation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lajanugen Logeswaran;Jaekyeom Kim;Sungryull Sohn;Creighton Glasscock;Honglak Lee",
        "authorids": "~Lajanugen_Logeswaran1;~Jaekyeom_Kim1;~Sungryull_Sohn1;~Creighton_Glasscock2;~Honglak_Lee2",
        "gender": "M;M;M;M;",
        "homepage": "https://sites.google.com/umich.edu/llajan/;https://jaekyeom.github.io/;;;",
        "dblp": "157/3603;228/6696;172/9884;;",
        "google_scholar": "dcv4kpIAAAAJ;8PR-AaoAAAAJ;https://scholar.google.com/citations?hl=en;;",
        "orcid": ";;;;",
        "linkedin": ";jaekyeom-kim-14157428;;creighton-glasscock-2a8a5b190/;",
        "or_profile": "~Lajanugen_Logeswaran1;~Jaekyeom_Kim1;~Sungryull_Sohn1;~Creighton_Glasscock2;~Honglak_Lee2",
        "aff": "LG AI Research;LG AI Research;LG AI Research;LG Corporation;",
        "aff_domain": "lgresearch.ai;lgresearch.ai;lgresearch.ai;lgresearch.ai;",
        "position": "Researcher;Researcher;Researcher;Intern;",
        "bibtex": "@inproceedings{\nlogeswaran2025scaling,\ntitle={Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation},\nauthor={Lajanugen Logeswaran and Jaekyeom Kim and Sungryull Sohn and Creighton Glasscock and Honglak Lee},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=63JtmQL7dv}\n}",
        "github": "",
        "project": "",
        "reviewers": "puzT;3xdz;3mSW;mfjm",
        "site": "https://openreview.net/forum?id=63JtmQL7dv",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "63c7hTrUCh",
        "title": "Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. Our source code is available at https://github.com/NUS-HPC-AI-Lab/DCS.",
        "keywords": "ensemble debiasing;accuracy imbalance;Heaviside step function;post-hoc correction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruixi Lin;Ziqiao Wang;Yang You",
        "authorids": "~Ruixi_Lin1;~Ziqiao_Wang2;~Yang_You1",
        "gender": ";M;M",
        "homepage": ";;https://www.comp.nus.edu.sg/~youy/",
        "dblp": ";;33/8167-1.html",
        "google_scholar": ";;jF4dPZwAAAAJ",
        "orcid": ";0009-0008-7785-174X;",
        "linkedin": ";ziqiao-wang-95a4b232b/;yang-you-0b92914b/",
        "or_profile": "~Ruixi_Lin1;~Ziqiao_Wang2;~Yang_You1",
        "aff": ";National University of Singapore;National University of Singapore",
        "aff_domain": ";u.nus.edu;nus.edu.sg",
        "position": ";PhD student;Professor",
        "bibtex": "@inproceedings{\nlin2025ensemble,\ntitle={Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy},\nauthor={Ruixi Lin and Ziqiao Wang and Yang You},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=63c7hTrUCh}\n}",
        "github": "",
        "project": "",
        "reviewers": "vy6v;HyPZ;unSp;kpFD",
        "site": "https://openreview.net/forum?id=63c7hTrUCh",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "6BGDGKZN7q",
        "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model is available at https://github.com/DISL-Lab/ReFeed.",
        "keywords": "Text Refinement;Text Summarization;LLMs;Reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Taewon Yun;Jihwan Oh;Hyangsuk Min;Yuho Lee;Jihwan Bang;Jason Cai;Hwanjun Song",
        "authorids": "~Taewon_Yun1;~Jihwan_Oh2;~Hyangsuk_Min1;~Yuho_Lee1;~Jihwan_Bang1;~Jason_Cai1;~Hwanjun_Song2",
        "gender": "M;M;F;M;M;;M",
        "homepage": ";https://github.com/hwaniz;https://scholar.google.com/citations?hl=en&user=U24LXHAAAAAJ;https://github.com/myyhlee;https://hwany-j.github.io/;;https://songhwanjun.github.io/",
        "dblp": "388/2074;;273/0234;;221/4643;;204/3381",
        "google_scholar": "https://scholar.google.co.kr/citations?user=Iou6pXkAAAAJ;https://scholar.google.co.kr/citations?user=71Ac4VwAAAAJ;https://scholar.google.com/citations?hl=en;;molKYzwAAAAJ;;Ijzuc-8AAAAJ",
        "orcid": ";;;;;;0000-0002-1105-0818",
        "linkedin": "ytaewon/;;hyangsukmin/;;jihwan-bang/;;",
        "or_profile": "~Taewon_Yun1;~Jihwan_Oh2;~Hyangsuk_Min1;~Yuho_Lee1;~Jihwan_Bang1;~Jason_Cai1;~Hwanjun_Song2",
        "aff": "Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Qualcomm Inc, QualComm;;Korea Advanced Institute of Science & Technology",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;qti.qualcomm.com;;kaist.ac.kr",
        "position": "MS student;MS student;PhD student;MS student;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\nyun2025refeed,\ntitle={ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback},\nauthor={Taewon Yun and Jihwan Oh and Hyangsuk Min and Yuho Lee and Jihwan Bang and Jason Cai and Hwanjun Song},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=6BGDGKZN7q}\n}",
        "github": "",
        "project": "",
        "reviewers": "7Nyd;qsDY;iDgk;rriG",
        "site": "https://openreview.net/forum?id=6BGDGKZN7q",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "6jZi4HSs6o",
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85\\% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.",
        "keywords": "Web Agents;Multimodal Large Language Models;Automatic Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tianci Xue;Weijian Qi;Tianneng Shi;Chan Hee Song;Boyu Gou;Dawn Song;Huan Sun;Yu Su",
        "authorids": "~Tianci_Xue1;~Weijian_Qi2;~Tianneng_Shi1;~Chan_Hee_Song1;~Boyu_Gou1;~Dawn_Song1;~Huan_Sun1;~Yu_Su2",
        "gender": "M;M;;;M;F;F;M",
        "homepage": "https://xuetianci.github.io/;;https://tnshi.com;https://chanh.ee;https://boyugou.github.io;;https://u.osu.edu/ihudas/people/;http://ysu1989.github.io",
        "dblp": "347/9360;309/0417;324/8332.html;249/5607;365/3954;s/DXSong;33/2952-1.html;38/1070-1",
        "google_scholar": "wVFSbzkAAAAJ;XeI20iUAAAAJ;ANBocsYAAAAJ;IdBL738AAAAJ;BgEYhp4AAAAJ;;wIFkulcAAAAJ;rIh5OqoAAAAJ",
        "orcid": ";0009-0004-2927-0693;;;;;;",
        "linkedin": ";;;;boyu-gou-b0a710238/;;huan-sun-81527924/?originalSubdomain=cn;",
        "or_profile": "~Tianci_Xue1;~Weijian_Qi2;~Tianneng_Shi1;~Chan_Hee_Song1;~Boyu_Gou1;~Dawn_Song1;~Huan_Sun1;~Yu_Su2",
        "aff": "Ohio State University, Columbus;Ohio State University, Columbus;University of California, Berkeley;The Ohio State University;Ohio State University, Columbus;University of California, Berkeley;The Ohio State University, Columbus;Ohio State University",
        "aff_domain": "osu.edu;osu.edu;berkeley.edu;osu.edu;osu.edu;berkeley.edu;osu.edu;osu.edu",
        "position": "PhD student;MS student;PhD student;PhD student;PhD student;Full Professor;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxue2025an,\ntitle={An Illusion of Progress? Assessing the Current State of Web Agents},\nauthor={Tianci Xue and Weijian Qi and Tianneng Shi and Chan Hee Song and Boyu Gou and Dawn Song and Huan Sun and Yu Su},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=6jZi4HSs6o}\n}",
        "github": "",
        "project": "",
        "reviewers": "BGA6;inKQ;9u2C;8d2e",
        "site": "https://openreview.net/forum?id=6jZi4HSs6o",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "2;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "6ox8XZGOqP",
        "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks \u2013 from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual\u2019s traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user\u2019s inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.\n\nIn this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query at a specific time point, we evaluate LLM chatbots\u2019 ability to identify the most suitable response according to the current state of the user\u2019s profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users\u2019 profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users\u2019 current situations and preferences, with frontier models such as GPT-4.5, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots.",
        "keywords": "personalization;long context;memory;conversational chatbot",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bowen Jiang;Zhuoqun Hao;Young Min Cho;Bryan Li;Yuan Yuan;Sihao Chen;Lyle Ungar;Camillo Jose Taylor;Dan Roth",
        "authorids": "~Bowen_Jiang2;~Zhuoqun_Hao1;~Young_Min_Cho1;~Bryan_Li1;~Yuan_Yuan7;~Sihao_Chen1;~Lyle_Ungar1;~Camillo_Jose_Taylor2;~Dan_Roth3",
        "gender": "F;F;M;;M;M;M;M;M",
        "homepage": "https://sites.google.com/seas.upenn.edu/bowenjiang;;https://jeffreych0.github.io/;https://manestay.github.io/;https://sites.google.com/view/yuanyuanweb/bio;https://sihaoc.github.io;http://www.cis.upenn.edu/~ungar/;https://www.cis.upenn.edu/~cjtaylor/;https://www.cis.upenn.edu/~danroth/",
        "dblp": "142/2975-1;;;243/6637;;;u/LyleHUngar;t/CamilloJTaylor.html;r/DanRoth",
        "google_scholar": "_6AHV9QAAAAJ;;BjdxOB4AAAAJ;;https://scholar.google.com.hk/citations?hl=zh-CN;PQ9dRCgAAAAJ;https://scholar.google.com.tw/citations?user=KCiDjbkAAAAJ;r50jBCUAAAAJ;E-bpPWgAAAAJ",
        "orcid": "0009-0005-0414-0435;0000-0003-0999-6425;0000-0002-3793-7257;;;;;;",
        "linkedin": "bowen-jiang-6946b2187/;;jeffrey-young-min-cho-888105180/;;yuanyuanvassar/;;;;dan-roth-8667361/",
        "or_profile": "~Bowen_Jiang2;~Zhuoqun_Hao1;~Young_Min_Cho1;~Bryan_Li1;~Yuan_Yuan7;~Sihao_Chen1;~Lyle_Ungar1;~Camillo_Jose_Taylor2;~Dan_Roth3",
        "aff": "University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;Microsoft;University of Pennsylvania;University of Pennsylvania;Oracle+University of Pennsylvania",
        "aff_domain": "upenn.edu;seas.upenn.edu;seas.upenn.edu;upenn.edu;seas.upenn.edu;microsoft.com;upenn.edu;upenn.edu;oracle.com+upenn.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;MS student;Researcher;Full Professor;Full Professor;Chief Scientist+Eduardo D. Glandt Distinguished Professor",
        "bibtex": "@inproceedings{\njiang2025know,\ntitle={Know Me, Respond to Me: Benchmarking {LLM}s for Dynamic User Profiling and Personalized Responses at Scale},\nauthor={Bowen Jiang and Zhuoqun Hao and Young Min Cho and Bryan Li and Yuan Yuan and Sihao Chen and Lyle Ungar and Camillo Jose Taylor and Dan Roth},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=6ox8XZGOqP}\n}",
        "github": "",
        "project": "",
        "reviewers": "VgcQ;4zES;zZwi",
        "site": "https://openreview.net/forum?id=6ox8XZGOqP",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "6vMRcaYbU7",
        "title": "Improving LLMs\u2018 Generalized Reasoning Abilities by Graph Problems",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have made remarkable strides in reasoning tasks, yet their performance often falters on novel and complex problems. Domain-specific continue-pretraining (CPT) methods, such as those tailored for mathematical reasoning,  have shown promise but lack transferability to broader reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning (GPR) to enhance LLMs' general reasoning capabilities. GPR tasks\u2014spanning pathfinding, network analysis, numerical computation, and topological reasoning\u2014require sophisticated logical and relational reasoning, making them ideal for teaching diverse reasoning patterns. To achieve this,  we introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes Chain-of-Thought, Program-of-Thought, Trace of Execution, and Real-world Graph Data. Using GraphPile, we train GraphMind on popular base models-Llama 3&3.1 and Gemma 2-achieving up to 4.9% higher accuracy in mathematical reasoning and up to 21.2% improvement in non-mathematical reasoning tasks, like logical and commonsense reasoning. By being the first to harness GPR for enhancing reasoning patterns and introducing the first dataset of its kind, our work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.",
        "keywords": "Large Language Models;Continue-Pretraining;Graph Problem Reasoning;General Reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qifan Zhang;Nuo Chen;Zehua Li;Miao Peng;Jing Tang;Jia Li",
        "authorids": "~Qifan_Zhang7;~Nuo_Chen1;~Zehua_Li2;~Miao_Peng1;~Jing_Tang5;~Jia_Li4",
        "gender": "M;M;M;M;M;M",
        "homepage": ";https://jerrynchen.github.io/;;https://gknl.github.io;https://sites.google.com/view/jtang;https://sites.google.com/view/lijia",
        "dblp": "44/8211-1;135/5622-1;;50/7223;83/663-4;23/6950-9",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;https://scholar.google.com/citations?hl=zh-CN;;WVtOGDMAAAAJ;https://scholar.google.com/citations?hl=en;1gSbcYoAAAAJ",
        "orcid": "0009-0003-4687-5793;;;0009-0002-7063-2014;0000-0002-0785-707X;0000-0002-6362-4385",
        "linkedin": ";;zehua-li-2a28b4350/;;;",
        "or_profile": "~Qifan_Zhang7;~Nuo_Chen1;~Zehua_Li2;~Miao_Peng1;~Jing_Tang5;~Jia_Li4",
        "aff": "Hong Kong University of Science and Technology;Alibaba Group+Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology (Guangzhou);The Hong Kong University of Science and Technology (Guangzhou)+Hong Kong University of Science and Technology;Hong Kong University of Science and Technology (Guangzhou)",
        "aff_domain": "hkust.edu;alibaba.com+hkust.edu;connect.hkust-gz.edu.cn;connect.hkust-gz.edu.cn;hkust-gz.edu.cn+ust.hk;ust.hk",
        "position": "PhD student;Researcher+PhD student;MS student;PhD student;Assistant Professor+Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025improving,\ntitle={Improving {LLM}s{\\textquoteleft} Generalized Reasoning Abilities by Graph Problems},\nauthor={Qifan Zhang and Nuo Chen and Zehua Li and Miao Peng and Jing Tang and Jia Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=6vMRcaYbU7}\n}",
        "github": "",
        "project": "",
        "reviewers": "o5Wk;imX7;bH1x;CqtP",
        "site": "https://openreview.net/forum?id=6vMRcaYbU7",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "6vTv9M9ZAA",
        "title": "Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks.\nPrior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning?\nThis work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. \nLLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones.\nOur data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance.\nAnalyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language.\nThe datasets and fine-tuned models will be publicly available.\nOur datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.",
        "keywords": "large language models; instruction tuning; synthetic data generation; cross-lingual datasets",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Youmi Ma;Sakae Mizuki;Kazuki Fujii;Taishi Nakamura;Masanari Ohi;Hinari Shimada;Taihei Shiotani;Koshiro Saito;Koki Maeda;Kakeru Hattori;Takumi Okamoto;Shigeki Ishida;Rio Yokota;Hiroya Takamura;Naoaki Okazaki",
        "authorids": "~Youmi_Ma2;~Sakae_Mizuki1;~Kazuki_Fujii1;~Taishi_Nakamura1;~Masanari_Ohi1;~Hinari_Shimada1;~Taihei_Shiotani1;~Koshiro_Saito1;~Koki_Maeda1;~Kakeru_Hattori1;~Takumi_Okamoto1;~Shigeki_Ishida1;~Rio_Yokota1;~Hiroya_Takamura1;~Naoaki_Okazaki2",
        "gender": ";M;M;;M;F;M;M;M;M;;M;M;M;M",
        "homepage": ";https://s-mizuki-nlp.github.io;;;https://sites.google.com/view/masanarioi/;https://hinarishimada.github.io/portfolio/;https://github.com/inatoihs;https://sites.google.com/view/koshiro-saito;https://sites.google.com/view/silviase/english;https://aya-se.vercel.app/;;https://www.wantedly.com/id/reborn27;https://www.rio.scrc.iir.isct.ac.jp/en/index.html;;http://www.chokkan.org/",
        "dblp": ";253/6986.html;;;371/4336;;395/5549;;331/0922;;;;61/7413;75/3612;49/4018",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=ryX_tAEAAAAJ;https://scholar.google.co.jp/citations?user=jHXLs2wAAAAJ;nbPQwgUAAAAJ;https://scholar.google.co.jp/citations?user=-f0Jt-isZzAC;;;;https://scholar.google.co.jp/citations?user=TOHpU1IAAAAJ;;;;klw9KE0AAAAJ;o57RFqgAAAAJ;",
        "orcid": ";;;;;;;;0009-0008-0529-3152;;;;0000-0001-7573-7873;0000-0002-3244-8294;",
        "linkedin": ";;kazuki-fujii/;taishi-nakamura/;masanari-oi-65745b2bb/;;;;;kakeru-hattori-973404240/;takumi-okamoto/;;rio-yokota-62857235/?originalSubdomain=jp;hiroya-takamura-7125b832/;",
        "or_profile": "~Youmi_Ma2;~Sakae_Mizuki1;~Kazuki_Fujii1;~Taishi_Nakamura1;~Masanari_Ohi1;~Hinari_Shimada1;~Taihei_Shiotani1;~Koshiro_Saito1;~Koki_Maeda1;~Kakeru_Hattori1;~Takumi_Okamoto1;~Shigeki_Ishida1;~Rio_Yokota1;~Hiroya_Takamura1;~Naoaki_Okazaki2",
        "aff": ";Institute of Science Tokyo+AIST, National Institute of Advanced Industrial Science and Technology;Institute of Science Tokyo;Institute of Science Tokyo;Institute of Science Tokyo;Tokyo Institute of Technology;Institute of Science Tokyo;Institute of Science Tokyo;Institute of Science Tokyo;Institute of Science Tokyo;Institute of Science Tokyo;Institute of Science Tokyo+Tokyo Institute of Technology;Institute of Science Tokyo;AIST, National Institute of Advanced Industrial Science and Technology;Institute of Science Tokyo",
        "aff_domain": ";titech.ac.jp+aist.go.jp;isct.ac.jp;titech.ac.jp;titech.ac.jp;titech.ac.jp;isct.ac.jp;titech.ac.jp;titech.ac.jp;m.isct.ac.jp;titech.ac.jp;isct.ac.jp+titech.ac.jp;isct.ac.jp;aist.go.jp;isct.ac.jp",
        "position": ";Researcher+Researcher;MS student;MS student;MS student;MS student;MS student;Undergrad student;PhD student;MS student;MS student;MS student+MS student;Full Professor;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nma2025building,\ntitle={Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models},\nauthor={Youmi Ma and Sakae Mizuki and Kazuki Fujii and Taishi Nakamura and Masanari Ohi and Hinari Shimada and Taihei Shiotani and Koshiro Saito and Koki Maeda and Kakeru Hattori and Takumi Okamoto and Shigeki Ishida and Rio Yokota and Hiroya Takamura and Naoaki Okazaki},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=6vTv9M9ZAA}\n}",
        "github": "",
        "project": "",
        "reviewers": "UvMA;WhHW;DRap;7i5D",
        "site": "https://openreview.net/forum?id=6vTv9M9ZAA",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "5;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            15,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.30151134457776363
    },
    {
        "id": "78lTuD6wiO",
        "title": "What is the Visual Cognition Gap between Humans and Multimodal LLMs?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning -- the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children. Inspired by the matrix reasoning tasks in Raven\u2019s Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA to evaluate the visual cognition capability of MLLMs and compare their performance with existing human visual cognition studies. Based on the training data of MaRs-VQA, we also finetune a baseline model Qwen2-VCog with multi-stage cognition reasoning annotations. Our comparative experiments with different baselines reveal a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of MaRs-VQA and the Qwen2-VCog baseline model will drive progress toward the next generation of MLLMs with human-like visual cognition abilities. MaRs-VQA is available at huggingface.co/datasets/IrohXu/VCog-Bench. The training code of Qwen2-VCog is available at github.com/IrohXu/Cognition-MLLM.",
        "keywords": "Visual Cognition;Wechsler Intelligence Scale for Children;Multimodal LLMs",
        "primary_area": "",
        "supplementary_material": "/attachment/5db99c4af702912c3a61960419156e79ebefdcb3.zip",
        "author": "Xu Cao;Yifan Shen;Bolin Lai;Wenqian Ye;Yunsheng Ma;Joerg Heintz;Jintai Chen;Meihuan Huang;Jianguo Cao;Aidong Zhang;James Matthew Rehg",
        "authorids": "~Xu_Cao4;~Yifan_Shen5;~Bolin_Lai1;~Wenqian_Ye1;~Yunsheng_Ma2;~Joerg_Heintz1;~Jintai_Chen1;~Meihuan_Huang1;~Jianguo_Cao1;~Aidong_Zhang2;~James_Matthew_Rehg1",
        "gender": "Non-Binary;;M;M;M;M;M;F;M;F;",
        "homepage": "https://www.linkedin.com/in/irohxu/;;https://bolinlai.github.io/;https://wenqian-ye.github.io/;https://ysma.me/;;https://whatashot.github.io/;;https://pediamedai.com/;https://engineering.virginia.edu/faculty/aidong-zhang;",
        "dblp": ";;250/6136;303/4407;159/8051;;249/3929;;99/10180;z/AidongZhang.html;",
        "google_scholar": "oXWRBrwAAAAJ;;lWrljmQAAAAJ;g0xeSH8AAAAJ;notEEFMAAAAJ;;https://scholar.google.com/citations?hl=en;sM_jWNkAAAAJ;dPPOpHUAAAAJ;O8XxkE4AAAAJ;",
        "orcid": "0000-0001-8739-5196;;0000-0001-7578-7336;0000-0002-6069-5153;;0000-0002-4542-390X;0000-0002-3199-2597;0000-0002-1550-9593;0000-0001-8781-5365;0000-0001-9723-3246;",
        "linkedin": ";;;;yunsheng-ma/;joergheintz/;jintai-chen-3a09921b0/;;;;",
        "or_profile": "~Xu_Cao4;~Yifan_Shen5;~Bolin_Lai1;~Wenqian_Ye1;~Yunsheng_Ma2;~Joerg_Heintz1;~Jintai_Chen1;~Meihuan_Huang1;~Jianguo_Cao1;~Aidong_Zhang2;~James_Matthew_Rehg1",
        "aff": "Stealth Mode Startup+Department of Computer Science, University of Illinois at Urbana-Champaign;;Georgia Institute of Technology+Meta;University of Virginia;Waymo+Purdue University+Bosch Research North America;University of Illinois Urbana Champaign;Hong Kong University of Science and Technology;Shenzhen University;PediaMed AI+Shenzhen Children's Hospital+Shenzhen Children's Hospital;University of Virginia;",
        "aff_domain": "unknown.ai+cs.illinois.edu;;gatech.edu+meta.com;virginia.edu;google.com+purdue.edu+bosch.com;illinois.edu;hkust-gz.edu.cn;szu.edu;pediamed.ai+szkid.com.cn+szkid.com.cn;virginia.edu;",
        "position": "Founder Engineer+PhD student;;PhD student+Intern;PhD student;Intern+PhD student+Intern;Researcher;Assistant Professor;Researcher;Principal Researcher+Chief Physician+Full Professor;Full Professor;",
        "bibtex": "@inproceedings{\ncao2025what,\ntitle={What is the Visual Cognition Gap between Humans and Multimodal {LLM}s?},\nauthor={Xu Cao and Yifan Shen and Bolin Lai and Wenqian Ye and Yunsheng Ma and Joerg Heintz and Jintai Chen and Meihuan Huang and Jianguo Cao and Aidong Zhang and James Matthew Rehg},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=78lTuD6wiO}\n}",
        "github": "",
        "project": "",
        "reviewers": "1KjF;9Mv5;dTKc;1UB9",
        "site": "https://openreview.net/forum?id=78lTuD6wiO",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "7HPuAkgdVm",
        "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agents\u2014mapping textual plans to GUI elements\u2014can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, \nthe agent\u2019s behavior can be compromised even when given correct task-solving plans. \nTo validate this vulnerability, we propose \\textit{VisualTrap}, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding \\textcolor{black}{to ensure practical feasibility of attacking.} \nEmpirical results show that VisualTrap can effectively hijack visual grounding with as little as 5\\% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, \\textit{e.g.,} being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor attack risks in GUI agents.",
        "keywords": "GUI Agent;Backdoor Attack",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziang Ye;Yang Zhang;Wentao Shi;Xiaoyu You;Fuli Feng;Tat-Seng Chua",
        "authorids": "~Ziang_Ye1;~Yang_Zhang24;~Wentao_Shi1;~Xiaoyu_You1;~Fuli_Feng1;~Tat-Seng_Chua2",
        "gender": "M;M;M;F;M;",
        "homepage": "https://whi497.github.io/leaf.github.io/;http://home.ustc.edu.cn/~zy2015/;https://swt-user.github.io/;http://www.fudan.edu.cn;https://fulifeng.github.io/;",
        "dblp": ";06/6785-72;120/6916-2;187/3021;183/9198;",
        "google_scholar": ";M9NcazMAAAAJ;https://scholar.google.com/citations?hl=en;6i3cMN0AAAAJ;https://scholar.google.com.sg/citations?user=QePM4u8AAAAJ;",
        "orcid": ";0000-0002-7863-5183;0000-0002-2616-6880;0000-0002-1980-9868;0000-0002-5828-9842;",
        "linkedin": ";;;;;",
        "or_profile": "~Ziang_Ye1;~Yang_Zhang24;~Wentao_Shi1;~Xiaoyu_You1;~Fuli_Feng1;~Tat-Seng_Chua2",
        "aff": "University of Science and Technology of China;National University of Singapore;Carnegie Mellon University+University of Science and Technology of China;East China University of Science and Technology;University of Science and Technology of China;",
        "aff_domain": "mail.ustc.edu.cn;nus.edu.sg;andrew.cmu.edu+ustc.edu.cn;ecust.edu.cn;ustc.edu.cn;",
        "position": "MS student;Postdoc;Intern+PhD student;Researcher;Full Professor;",
        "bibtex": "@inproceedings{\nye2025visualtrap,\ntitle={VisualTrap: A Stealthy Backdoor Attack on {GUI} Agents via Visual Grounding Manipulation},\nauthor={Ziang Ye and Yang Zhang and Wentao Shi and Xiaoyu You and Fuli Feng and Tat-Seng Chua},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=7HPuAkgdVm}\n}",
        "github": "",
        "project": "",
        "reviewers": "tzq5;D55U;X5Ti",
        "site": "https://openreview.net/forum?id=7HPuAkgdVm",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "7Vj78acKIp",
        "title": "Single-Pass Document Scanning for Question Answering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Handling extremely large documents for question answering is challenging: chunk-based embedding methods often lose track of important global context, while full-context transformers can be prohibitively expensive for hundreds of thousands of tokens. We propose a single-pass document scanning approach that processes the entire text in linear time, preserving global coherence while deciding which sentences are most relevant to the query. On 41 QA benchmarks, our single-pass scanner consistently outperforms chunk-based embedding methods and competes with large language models at a fraction of the computational cost. By conditioning on the entire preceding context without chunk breaks, the method preserves global coherence, which is especially important for long documents. Overall, single-pass document scanning offers a simple solution for question answering over massive text. All code, datasets, and model checkpoints are available at https://github.com/MambaRetriever/MambaRetriever",
        "keywords": "Long Document QA;State-Space Models;Information Extraction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weili Cao;Jianyou Wang;Youze Zheng;Longtian Bao;Qirui Zheng;Taylor Berg-Kirkpatrick;Ramamohan Paturi;Leon Bergen",
        "authorids": "~Weili_Cao1;~Jianyou_Wang1;~Youze_Zheng1;~Longtian_Bao1;~Qirui_Zheng1;~Taylor_Berg-Kirkpatrick1;~Ramamohan_Paturi1;~Leon_Bergen1",
        "gender": "M;M;M;M;;M;Not Specified;",
        "homepage": "https://weilicao.github.io/;;https://hgnzheng.github.io/;;https://qz07.github.io/;https://cseweb.ucsd.edu/~tberg/;https://cseweb.ucsd.edu/~paturi/;",
        "dblp": "371/2464.html;251/3315;392/9344;393/0201;;22/8160;p/RPaturi.html;136/8736",
        "google_scholar": "BDrdcwQAAAAJ;4nysj5kAAAAJ;jmWrTQcAAAAJ;https://scholar.google.com/citations?hl=en;aQXBhQQAAAAJ;mN6_BKAAAAAJ;;0FclEuAAAAAJ",
        "orcid": ";;;;0009-0008-2402-1427;;;",
        "linkedin": ";;;longtian-bao-3ba886223/;qirui-zheng-b47235263/;;;",
        "or_profile": "~Weili_Cao1;~Jianyou_Wang1;~Youze_Zheng1;~Longtian_Bao1;~Qirui_Zheng1;~Taylor_Berg-Kirkpatrick1;~Ramamohan_Paturi1;~Leon_Bergen1",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "position": "MS student;PhD student;Undergrad student;MS student;Undergrad student;Associate Professor;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\ncao2025singlepass,\ntitle={Single-Pass Document Scanning for Question Answering},\nauthor={Weili Cao and Jianyou Wang and Youze Zheng and Longtian Bao and Qirui Zheng and Taylor Berg-Kirkpatrick and Ramamohan Paturi and Leon Bergen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=7Vj78acKIp}\n}",
        "github": "",
        "project": "",
        "reviewers": "Dc8t;jppx;t7hJ;gFrf",
        "site": "https://openreview.net/forum?id=7Vj78acKIp",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "7ZwuGZCopw",
        "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in large language models (LLMs) have shown promise in medical applications such as disease diagnosis and treatment planning. However, most existing medical LLMs struggle with the deep reasoning required for complex medical problems, such as differential diagnosis and medication recommendations. We propose FineMedLM-o1, which leverages high-quality medical synthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and deep reasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in the medical domain for the first time, facilitating domain adaptation and ensuring reliable, accurate reasoning. Experimental results demonstrate that FineMedLM-o1 achieves a 23% average performance improvement over prior models on key medical benchmarks. Furthermore, the introduction of TTT provides an additional 14% performance boost, highlighting its effectiveness in enhancing medical reasoning capabilities. To support this process, we also propose a novel method for synthesizing medical dialogue. Compared to other open-source datasets, our dataset stands out as superior in both quality and complexity. The project and data will be released on GitHub.",
        "keywords": "LMs on diverse domains and novel applications",
        "primary_area": "",
        "supplementary_material": "",
        "author": "hongzhou yu;Tianhao Cheng;Yingwen Wang;Wen He;Qing Wang;Ying Cheng;Yuejie Zhang;Rui Feng;Xiaobo Zhang",
        "authorids": "~hongzhou_yu1;~Tianhao_Cheng1;~Yingwen_Wang2;~Wen_He2;~Qing_Wang23;~Ying_Cheng2;~Yuejie_Zhang2;~Rui_Feng2;~Xiaobo_Zhang1",
        "gender": "M;M;;F;F;F;F;;F",
        "homepage": "https://github.com/hongzhouyu;https://crazycth.link;;https://scholar.google.ca/citations?user=KHJMbHwAAAAJ&hl=en;https://user.qzone.qq.com/2797022015;;http://www.cs.fudan.edu.cn/?page_id=5518;;https://baike.baidu.com/item/%E5%BC%A0%E6%99%93%E6%B3%A2/61140240?fr=aladdin",
        "dblp": ";;;;;54/4536-5;09/5786;;",
        "google_scholar": "https://scholar.google.com.hk/citations?user=ubUMMojUqjsC;https://scholar.google.com/citations?hl=zh-CN;;;;wsmBf7oAAAAJ;;;https://scholar.google.ca/citations?user=gUdjVUkAAAAJ",
        "orcid": ";;;0000-0003-0440-8516;0009-0004-6252-2565;0000-0002-8964-3998;0000-0001-7993-7223;;0000-0002-8645-5414",
        "linkedin": ";;;;;;;;",
        "or_profile": "~hongzhou_yu1;~Tianhao_Cheng1;~Yingwen_Wang2;~Wen_He2;~Qing_Wang23;~Ying_Cheng2;~Yuejie_Zhang2;~Rui_Feng2;~Xiaobo_Zhang1",
        "aff": "Fudan University;Fudan University;;Fudan University;Fudan University;Tongji University+Fudan University;Fudan University;;Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;;fudan.edu;fudan.edu.cn;tongji.edu.cn+fudan.edu;fudan.edu.cn;;fudan.edu.cn",
        "position": "MS student;MS student;;PhD student;MS student;Assistant Professor+Postdoc;Full Professor;;Full Professor",
        "bibtex": "@inproceedings{\nyu2025finemedlmo,\ntitle={FineMed{LM}-o1: Enhancing Medical Knowledge Reasoning Ability of {LLM} from Supervised Fine-Tuning to Test-Time Training},\nauthor={hongzhou yu and Tianhao Cheng and Yingwen Wang and Wen He and Qing Wang and Ying Cheng and Yuejie Zhang and Rui Feng and Xiaobo Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=7ZwuGZCopw}\n}",
        "github": "",
        "project": "",
        "reviewers": "Xx4M;H2Gw;NP49;BAG1",
        "site": "https://openreview.net/forum?id=7ZwuGZCopw",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "3;5;5;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6363636363636364
    },
    {
        "id": "7evvwwdo3z",
        "title": "R2E-Gym: Procedural Environment Generation and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce R2EGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.1K tasks. R2EGym is powered by two main contributions: 1) SWEGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories.",
        "keywords": "SWE Agents ; Inference Time Scaling ; Test Generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Naman Jain;Jaskirat Singh;Manish Shetty;Tianjun Zhang;Liang Zheng;Koushik Sen;Ion Stoica",
        "authorids": "~Naman_Jain2;~Jaskirat_Singh1;~Manish_Shetty1;~Tianjun_Zhang1;~Liang_Zheng4;~Koushik_Sen2;~Ion_Stoica1",
        "gender": "M;;M;;M;M;M",
        "homepage": "https://naman-ntc.github.io/;https://1jsingh.github.io/;https://manishs.org;https://tianjunz.github.io;http://zheng-lab.cecs.anu.edu.au/;https://people.eecs.berkeley.edu/~ksen/;http://people.eecs.berkeley.edu/~istoica/",
        "dblp": ";74/2036;270/0520;;61/7360-1;https://dblp.uni-trier.de/pid/04/418.html;s/IonStoica",
        "google_scholar": "6oqV3v8AAAAJ;HAmEM_4AAAAJ;Fcu7r3YAAAAJ;UE9jz_MAAAAJ;https://scholar.google.com.au/citations?user=vNHqr3oAAAAJ;Vn3L_ioAAAAJ;vN-is70AAAAJ",
        "orcid": ";;;;;;",
        "linkedin": ";;;;liang-zheng-76341311a/;;ionstoica",
        "or_profile": "~Naman_Jain2;~Jaskirat_Singh1;~Manish_Shetty1;~Tianjun_Zhang1;~Liang_Zheng4;~Koushik_Sen2;~Ion_Stoica1",
        "aff": "University of California, Berkeley;Australian National University;University of California, Berkeley;University of California, Berkeley;Australian National University;UC Berkeley, University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;anu.edu.au;berkeley.edu;berkeley.edu;anu.edu.au;cs.berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Associate Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\njain2025regym,\ntitle={R2E-Gym: Procedural Environment Generation and Hybrid Verifiers for Scaling Open-Weights {SWE} Agents},\nauthor={Naman Jain and Jaskirat Singh and Manish Shetty and Tianjun Zhang and Liang Zheng and Koushik Sen and Ion Stoica},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=7evvwwdo3z}\n}",
        "github": "",
        "project": "",
        "reviewers": "gmVU;DvQh;fpRC;2kAH",
        "site": "https://openreview.net/forum?id=7evvwwdo3z",
        "pdf_size": 0,
        "rating": "7;8;8;9",
        "confidence": "4;4;2;4",
        "wc_review": "",
        "rating_avg": [
            8.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "7qhBXq0NLN",
        "title": "IMPersona: Evaluating Individual Level LLM Impersonation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As language models achieve increasingly human-like capabilities in conversational text generation, a critical question emerges: to what extent can these systems simulate the characteristics of specific individuals? To evaluate this, we introduce IMPersona, a framework for evaluating LMs at impersonating specific individuals' writing style and personal knowledge. Using supervised fine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate that even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can achieve impersonation abilities at concerning levels. In blind conversation experiments, participants (mis)identified our fine-tuned models with memory integration as human in \\textbf{44.44\\%} of interactions, compared to just \\textbf{25.00\\%} for the best prompting-based approach. We analyze these results to propose detection methods and defense strategies against such impersonation attempts. Our findings raise important questions about both the potential applications and risks of personalized language models, particularly regarding privacy, security, and the ethical deployment of such technologies in real-world contexts.",
        "keywords": "Impersonation;Language Models;Personalization;Stylistic Mimicry;Contextual Knowledge;AI Evaluation;Social Engineering;Ethical AI;Memory-Augmented Models;Human-AI Interaction",
        "primary_area": "",
        "supplementary_material": "/attachment/d096af9d90cf9de17d4e5c69167501cf28845909.zip",
        "author": "Quan Shi;Carlos E Jimenez;Stephen Dong;Brian Seo;Caden Yao;Adam Kelch;Karthik R Narasimhan",
        "authorids": "~Quan_Shi1;~Carlos_E_Jimenez1;~Stephen_Dong1;~Brian_Seo1;~Caden_Yao1;~Adam_Kelch1;~Karthik_R_Narasimhan1",
        "gender": ";M;M;M;M;M;M",
        "homepage": ";https://www.carlosejimenez.com;;;;https://akelch11.github.io/;http://www.karthiknarasimhan.com",
        "dblp": ";153/0588;;;;;147/0322",
        "google_scholar": ";Ue4wghAAAAAJ;;;;;euc0GX4AAAAJ",
        "orcid": ";0000-0001-9370-3909;;;;;",
        "linkedin": ";;stephendong/;brian-y-seo/;cytusi-yao-713316359/;adam-kelch-113334183/;",
        "or_profile": "~Quan_Shi1;~Carlos_E_Jimenez1;~Stephen_Dong1;~Brian_Seo1;~Caden_Yao1;~Adam_Kelch1;~Karthik_R_Narasimhan1",
        "aff": ";Princeton University;Cornell University;Princeton University;Monongalia County Schools;Independent Researcher;Princeton University",
        "aff_domain": ";princeton.edu;cornell.edu;princeton.edu;boe.mono.k12.wv.us;askelch.gmail.com;princeton.edu",
        "position": ";PhD student;MS student;Undergrad student;Independent Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nshi2025impersona,\ntitle={{IMP}ersona: Evaluating Individual Level {LLM} Impersonation},\nauthor={Quan Shi and Carlos E Jimenez and Stephen Dong and Brian Seo and Caden Yao and Adam Kelch and Karthik R Narasimhan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=7qhBXq0NLN}\n}",
        "github": "",
        "project": "",
        "reviewers": "3VQQ;WMAf;jt2C;YRPf",
        "site": "https://openreview.net/forum?id=7qhBXq0NLN",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "8ExXncFpf6",
        "title": "UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.",
        "keywords": "tokenization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Preston Firestone;Shubham Ugare;Gagandeep Singh;Sasa Misailovic",
        "authorids": "~Preston_Firestone1;~Shubham_Ugare1;~Gagandeep_Singh1;~Sasa_Misailovic1",
        "gender": "M;M;M;M",
        "homepage": ";https://shubhamugare.github.io/;https://ggndpsngh.github.io/;http://misailo.cs.illinois.edu/",
        "dblp": ";227/3014;64/3747-1;00/4734.html",
        "google_scholar": ";vpK-0x8AAAAJ;https://scholar.google.ch/citations?user=m4b2ruEAAAAJ;https://scholar.google.com.tw/citations?user=3qJQjIYAAAAJ",
        "orcid": ";;0000-0002-9299-2961;",
        "linkedin": "preston-firestone;;gagandeep-singh-1bb01b49/;",
        "or_profile": "~Preston_Firestone1;~Shubham_Ugare1;~Gagandeep_Singh1;~Sasa_Misailovic1",
        "aff": "University of Illinois, Urbana Champaign;Bloomberg+University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign",
        "aff_domain": "illinois.edu;bloomberg.net+uiuc.edu;illinois.edu;illinois.edu",
        "position": "MS student;Intern+PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nfirestone2025utf,\ntitle={{UTF}-8 Plumbing: Byte-level Tokenizers Unavoidably Enable {LLM}s to Generate Ill-formed {UTF}-8},\nauthor={Preston Firestone and Shubham Ugare and Gagandeep Singh and Sasa Misailovic},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8ExXncFpf6}\n}",
        "github": "",
        "project": "",
        "reviewers": "Vin2;uy5s;KwbY;x3kb;cvmT;MUTr",
        "site": "https://openreview.net/forum?id=8ExXncFpf6",
        "pdf_size": 0,
        "rating": "3;4;7;7;7;8",
        "confidence": "4;4;2;2;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.8257418583505538
        ],
        "confidence_avg": [
            3.1666666666666665,
            0.8975274678557507
        ],
        "replies_avg": [
            32,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6102571532587293
    },
    {
        "id": "8LoPjpvWde",
        "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \\ModelName~achieves only 39\\% execution accuracy, highlighting the benchmark's difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at https://github.com/xyzCS/SciReplicate-Bench and project homepage at  https://xyzcs.github.io/scireplicate.github.io/.",
        "keywords": "Code Generation;Agent;Scientific Paper Understanding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yanzheng Xiang;Hanqi Yan;Shuyin Ouyang;Lin Gui;Yulan He",
        "authorids": "~Yanzheng_Xiang2;~Hanqi_Yan2;~Shuyin_Ouyang1;~Lin_Gui3;~Yulan_He1",
        "gender": "M;;M;M;F",
        "homepage": ";;https://sites.google.com/view/shuyinouyang;;https://www.kcl.ac.uk/people/yulan-he",
        "dblp": "305/9849;;354/6551.html;34/8605-3;75/5430",
        "google_scholar": "VXUcODEAAAAJ;;GZ9j3UcAAAAJ;https://scholar.google.com.ph/citations?user=1b3Eyx4AAAAJ;https://scholar.google.co.uk/citations?user=SP9r32UAAAAJ",
        "orcid": ";;0009-0007-0056-3101;;0000-0003-3948-5845",
        "linkedin": "yanzheng-xiang-9aa572282/?originalSubdomain=uk;;;;yulan-he-277234a/?originalSubdomain=uk",
        "or_profile": "~Yanzheng_Xiang2;~Hanqi_Yan2;~Shuyin_Ouyang1;~Lin_Gui3;~Yulan_He1",
        "aff": "King's College London, University of London;;King's College London;King's College London, University of London;King's College London, University of London",
        "aff_domain": "kcl.ac.uk;;kcl.ac.uk;kcl.ac.uk;kcl.ac.uk",
        "position": "PhD student;;PhD student;Lecturer;Full Professor",
        "bibtex": "@inproceedings{\nxiang2025scireplicatebench,\ntitle={SciReplicate-Bench: Benchmarking {LLM}s in Agent-driven Algorithmic Reproduction from Research Papers},\nauthor={Yanzheng Xiang and Hanqi Yan and Shuyin Ouyang and Lin Gui and Yulan He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8LoPjpvWde}\n}",
        "github": "",
        "project": "",
        "reviewers": "saNg;fdpN;RNJE",
        "site": "https://openreview.net/forum?id=8LoPjpvWde",
        "pdf_size": 0,
        "rating": "5;5;7",
        "confidence": "3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.49999999999999983
    },
    {
        "id": "8N5H8DgfPw",
        "title": "Rethinking Associative Memory Mechanism in Induction Head",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Induction head mechanism is a part of the computational circuits for in-context learning (ICL) that enable large language models (LLMs) to adapt to new tasks without fine-tuning. Most existing work explains the training dynamics behind acquiring such a powerful mechanism. However, it is unclear how a transformer extract information from long contexts and then use it to coordinate with global knowledge acquired during pretraninig. This paper considers weight matrices as associative memory to investigate how an induction head functions over long contexts and balances in-context and global bigram knowledge in next token prediction. We theoretically analyze the representation of the learned associative memory in attention layers and the resulting logits when a transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of the trained transformer align with the theoretical results.",
        "keywords": "transformer;induction head;associative memory;positional encoding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuo Wang;Issei Sato",
        "authorids": "~Shuo_Wang30;~Issei_Sato2",
        "gender": "M;",
        "homepage": "https://www.ml.is.s.u-tokyo.ac.jp/members-en;https://www.ml.is.s.u-tokyo.ac.jp/issei-sato-en",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Shuo_Wang30;~Issei_Sato2",
        "aff": "The University of Tokyo;The University of Tokyo",
        "aff_domain": "u-tokyo.ac.jp;u-tokyo.ac.jp",
        "position": "MS student;Full Professor",
        "bibtex": "@inproceedings{\nwang2025rethinking,\ntitle={Rethinking Associative Memory Mechanism in Induction Head},\nauthor={Shuo Wang and Issei Sato},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8N5H8DgfPw}\n}",
        "github": "",
        "project": "",
        "reviewers": "1jjv;8sjk;dwGn;SFDP;Pgux",
        "site": "https://openreview.net/forum?id=8N5H8DgfPw",
        "pdf_size": 0,
        "rating": "4;6;6;7;8",
        "confidence": "4;3;2;3;2",
        "wc_review": "",
        "rating_avg": [
            6.2,
            1.32664991614216
        ],
        "confidence_avg": [
            2.8,
            0.7483314773547882
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7655318158241111
    },
    {
        "id": "8OqGNXKwo8",
        "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costs\u2014often overlooked\u2014stand as the \"elephant in the room\" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2\u00d7 boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL. Our source code and model are available at https://elliesql.github.io/.",
        "keywords": "Text-to-SQL;Routing;Cost-Efficiency;Large Language Model",
        "primary_area": "",
        "supplementary_material": "/attachment/5d463428cf69a22e085ad7c79d3aa77603ca0c36.zip",
        "author": "Yizhang Zhu;Runzhi JIANG;Boyan Li;Nan Tang;Yuyu Luo",
        "authorids": "~Yizhang_Zhu1;~Runzhi_JIANG1;~Boyan_Li2;~Nan_Tang3;~Yuyu_Luo1",
        "gender": "M;M;M;M;M",
        "homepage": "https://derrickzhuyz.github.io/;https://github.com/JiangRunzhi;https://github.com/BugMaker-Boyan;https://nantang.github.io/;https://luoyuyu.vip/",
        "dblp": "359/8583;;;27/104-1;185/9921.html",
        "google_scholar": "eAkUamUAAAAJ;;https://scholar.google.cz/citations?user=RZ2oElwAAAAJ;;FAjYJkQAAAAJ",
        "orcid": "0009-0004-5496-5008;;0009-0009-8391-4687;;0000-0001-9530-3327",
        "linkedin": ";;;;",
        "or_profile": "~Yizhang_Zhu1;~Runzhi_JIANG1;~Boyan_Li2;~Nan_Tang3;~Yuyu_Luo1",
        "aff": "The Hong Kong University of Science and Technology (Guangzhou);The Hong Kong University of Science and Technology (Guangzhou);Hong Kong University of Science and Technology;HKUST(GZ);The Hong Kong University of Science and Technology (Guangzhou)+Hong Kong University of Science and Technology",
        "aff_domain": "hkust-gz.edu.cn;hkust-gz.edu.cn;hkust-gz.edu.cn;hkust-gz.edu.cn;hkust-gz.edu.cn+ust.hk",
        "position": "MS student;MS student;PhD student;Associate Professor;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nzhu2025elliesql,\ntitle={Ellie{SQL}: Cost-Efficient Text-to-{SQL} with Complexity-Aware Routing},\nauthor={Yizhang Zhu and Runzhi JIANG and Boyan Li and Nan Tang and Yuyu Luo},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8OqGNXKwo8}\n}",
        "github": "",
        "project": "",
        "reviewers": "367x;npNW;b7ZP;8cMx",
        "site": "https://openreview.net/forum?id=8OqGNXKwo8",
        "pdf_size": 0,
        "rating": "5;6;6;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            27,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "8Pxdzsqvx9",
        "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JailDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge   during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JailDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.",
        "keywords": "Multi-modality;AI Security;Jailbreak;Attack Defense",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yi Nian;Shenzhe Zhu;Yuehan Qin;Li Li;Ziyi Wang;Chaowei Xiao;Yue Zhao",
        "authorids": "~Yi_Nian1;~Shenzhe_Zhu1;~Yuehan_Qin1;~Li_Li18;~Ziyi_Wang31;~Chaowei_Xiao2;~Yue_Zhao13",
        "gender": "M;M;;M;F;;M",
        "homepage": ";https://shenzhezhu.github.io/;;https://lili0415.github.io;https://www.zoe-wang.com/;;https://viterbi-web.usc.edu/~yzhao010/",
        "dblp": "250/9392;377/2133;311/0533;53/2189-91;;;48/76-16",
        "google_scholar": "RBTCmEkAAAAJ;WBZCniUAAAAJ;byAHud4AAAAJ;r4kIL4cAAAAJ;hUEHQEMAAAAJ;;https://scholar.google.ca/citations?user=zoGDYsoAAAAJ",
        "orcid": ";;;0009-0003-2007-2706;0009-0006-4821-1255;;0000-0003-3401-4921",
        "linkedin": ";;yuehan-qin-845071176/;;ziyi-zoe-wang-488122292/;;yzhao062/",
        "or_profile": "~Yi_Nian1;~Shenzhe_Zhu1;~Yuehan_Qin1;~Li_Li18;~Ziyi_Wang31;~Chaowei_Xiao2;~Yue_Zhao13",
        "aff": "University of Southern California;University of Toronto;University of Southern California;University of Southern California;University of Maryland, College Park;;University of Southern California",
        "aff_domain": "usc.edu;utoronto.ca;usc.edu;usc.edu;umd.edu;;usc.edu",
        "position": "MS student;Undergrad student;PhD student;PhD student;MS student;;Assistant Professor",
        "bibtex": "@inproceedings{\nnian2025jaildam,\ntitle={Jail{DAM}: Jailbreak Detection with Adaptive Memory for Vision-Language Model},\nauthor={Yi Nian and Shenzhe Zhu and Yuehan Qin and Li Li and Ziyi Wang and Chaowei Xiao and Yue Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8Pxdzsqvx9}\n}",
        "github": "",
        "project": "",
        "reviewers": "mPCF;SCGg;gGas",
        "site": "https://openreview.net/forum?id=8Pxdzsqvx9",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "2;3;5",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            1.247219128924647
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.944911182523068
    },
    {
        "id": "8wKec6faAT",
        "title": "Layers at Similar Depths Generate Similar Activations Across LLM Architectures",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "How do the latent spaces used by independently-trained LLMs relate to one another? We study the nearest neighbor relationships induced by activations at different layers of 24 open-weight LLMs, and find that they 1) tend to vary from layer to layer within a model, and 2) are approximately shared between corresponding layers of different models. Claim 2 shows that these nearest neighbor relationships are not arbitrary, as they are shared across models, but Claim 1 shows that they are not \"obvious\" either, as there is no single set of nearest neighbor relationships that is universally shared. Together, these suggest that LLMs generate a progression of activation geometries from layer to layer, but that this entire progression is largely shared between models, stretched and squeezed to fit into different architectures.",
        "keywords": "universality;representational similarity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Christopher Wolfram;Aaron Schein",
        "authorids": "~Christopher_Wolfram1;~Aaron_Schein1",
        "gender": "M;M",
        "homepage": "https://christopherwolfram.com;https://www.aaronschein.com",
        "dblp": ";126/8722",
        "google_scholar": ";CaHuRsgAAAAJ",
        "orcid": "0000-0002-7953-7833;0000-0002-5507-2904",
        "linkedin": "christopher-wolfram/;",
        "or_profile": "~Christopher_Wolfram1;~Aaron_Schein1",
        "aff": "University of Chicago;University of Chicago",
        "aff_domain": "uchicago.edu;uchicago.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nwolfram2025layers,\ntitle={Layers at Similar Depths Generate Similar Activations Across {LLM} Architectures},\nauthor={Christopher Wolfram and Aaron Schein},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8wKec6faAT}\n}",
        "github": "",
        "project": "",
        "reviewers": "BgXt;VG7E;Dzwk;j38m",
        "site": "https://openreview.net/forum?id=8wKec6faAT",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "8xSbwT3763",
        "title": "Pretrained Hybrids with MAD Skills",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While Transformers underpin modern large language models (LMs), there is a growing list of alternative architectures with new capabilities, promises, and tradeoffs. This makes choosing the right LM architecture challenging. Recently proposed hybrid architectures seek a best-of-all-worlds approach that reaps the benefits of all architectures. Hybrid design is difficult for two reasons: it requires manual expert-driven search, and new hybrids must be trained from scratch. We propose Manticore, a framework that addresses these challenges by automating the design of hybrid architectures while reusing pretrained models to create pretrained hybrids. Our approach augments ideas from differentiable Neural Architecture Search (NAS) by incorporating simple projectors that translate features between pretrained blocks from different architectures. We then fine-tune hybrids that combine pretrained models from different architecture families---such as the GPT series and Mamba---end-to-end. With Manticore, we enable LM selection without training multiple models, the construction of pretrained hybrids from existing pretrained models, and the ability to program pretrained hybrids to have certain capabilities. Manticore hybrids match existing manually designed hybrids, achieve strong performance on the Long Range Arena benchmark, and improve on pretrained transformers and state space models on various natural language tasks.",
        "keywords": "hybrid architectures;large language models;transformers;state space models;model merging;neural architecture search;mechanistic search",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicholas Roberts;Samuel Guo;Zhiqi Gao;Satya Sai Srinath Namburi GNVV;Sonia Cromp;Chengjun Wu;Chengyu Duan;Frederic Sala",
        "authorids": "~Nicholas_Roberts2;~Samuel_Guo1;~Zhiqi_Gao2;~Satya_Sai_Srinath_Namburi_GNVV1;~Sonia_Cromp1;~Chengjun_Wu1;~Chengyu_Duan1;~Frederic_Sala1",
        "gender": ";;;M;F;M;F;M",
        "homepage": ";;;;;;;https://pages.cs.wisc.edu/~fredsala/",
        "dblp": ";;;362/5934;;;;133/3602",
        "google_scholar": ";;;brolZJEAAAAJ;;;;9KhIkNkAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": ";samuel-guo-03570b148/;;namburi-gnvv-satya-sai-srinath/;sonia-cromp;%E6%89%BF%E9%9A%BD-%E5%90%B4-0bb0a2293/;chengyu-duan-874008244;",
        "or_profile": "~Nicholas_Roberts2;~Samuel_Guo1;~Zhiqi_Gao2;~Satya_Sai_Srinath_Namburi_GNVV1;~Sonia_Cromp1;~Chengjun_Wu1;~Chengyu_Duan1;~Frederic_Sala1",
        "aff": ";University of Wisconsin - Madison;;GE HealthCare;Department of Computer Science, University of Wisconsin - Madison;University of Wisconsin - Madison;University of Wisconsin - Madison;University of Wisconsin, Madison",
        "aff_domain": ";wisc.edu;;gehealthcare.com;cs.wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "position": ";PhD student;;Researcher;PhD student;Undergrad student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nroberts2025pretrained,\ntitle={Pretrained Hybrids with {MAD} Skills},\nauthor={Nicholas Roberts and Samuel Guo and Zhiqi Gao and Satya Sai Srinath Namburi GNVV and Sonia Cromp and Chengjun Wu and Chengyu Duan and Frederic Sala},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8xSbwT3763}\n}",
        "github": "",
        "project": "",
        "reviewers": "8jx8;mpR2;NHYJ;VvYH",
        "site": "https://openreview.net/forum?id=8xSbwT3763",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "8xofWL61S9",
        "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into _safe_ Rust that passes a set of test cases. \nWe introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o3, is able to solve only 19 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. Code and Data available [here](https://github.com/anirudhkhatry/CRUST-bench).",
        "keywords": "Code translation;Code generation;Software engineering;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anirudh Khatry;Robert Zhang;Jia Pan;Ziteng Wang;Qiaochu Chen;Greg Durrett;Isil Dillig",
        "authorids": "~Anirudh_Khatry1;~Robert_Zhang1;~Jia_Pan3;~Ziteng_Wang9;~Qiaochu_Chen1;~Greg_Durrett1;~Isil_Dillig1",
        "gender": ";Not Specified;M;M;F;M;F",
        "homepage": ";https://robertzhang.vercel.app;;https://ziteng.wang/;https://cs.nyu.edu/~qc1127/;http://www.cs.utexas.edu/~gdurrett/;https://www.cs.utexas.edu/~isil/",
        "dblp": ";203/4613-3;;;247/1177.html;69/7968;",
        "google_scholar": ";ewz2zM8AAAAJ;SAfr8x4AAAAJ;QcqE8tQAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ;",
        "orcid": ";0009-0001-8853-5813;;0009-0001-8487-8093;0000-0003-4680-5157;;",
        "linkedin": ";~rz;;;;;",
        "or_profile": "~Anirudh_Khatry1;~Robert_Zhang1;~Jia_Pan3;~Ziteng_Wang9;~Qiaochu_Chen1;~Greg_Durrett1;~Isil_Dillig1",
        "aff": ";University of Texas at Austin;, University of Texas at Austin;University of Texas at Austin;University of Alberta+New York University;University of Texas at Austin;University of Texas, Austin",
        "aff_domain": ";utexas.edu;cs.utexas.edu;utexas.edu;ualberta.ca+nyu.edu;utexas.edu;utexas.edu",
        "position": ";PhD student;PhD student;PhD student;Assistant Professor+Postdoc;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nkhatry2025crustbench,\ntitle={{CRUST}-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation},\nauthor={Anirudh Khatry and Robert Zhang and Jia Pan and Ziteng Wang and Qiaochu Chen and Greg Durrett and Isil Dillig},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=8xofWL61S9}\n}",
        "github": "",
        "project": "",
        "reviewers": "B1yq;1tuk;YdSU;eZRP",
        "site": "https://openreview.net/forum?id=8xofWL61S9",
        "pdf_size": 0,
        "rating": "7;7;8;8",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "90UrTTxp5O",
        "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices\u2014including decoding parameters, random seeds, prompt formatting, and even hardware and software configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that most reinforcement learning (RL) approaches yield only modest improvements\u2014far below prior claims\u2014and are prone to overfitting, especially on small-scale benchmarks like AIME\u201924. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization in the settings we study. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.",
        "keywords": "data curation;rl;grpo;math;reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andreas Hochlehnert;Hardik Bhatnagar;Vishaal Udandarao;Samuel Albanie;Ameya Prabhu;Matthias Bethge",
        "authorids": "~Andreas_Hochlehnert1;~Hardik_Bhatnagar1;~Vishaal_Udandarao1;~Samuel_Albanie2;~Ameya_Prabhu1;~Matthias_Bethge1",
        "gender": "M;;M;;M;M",
        "homepage": ";https://hrdkbhatnagar.github.io/;https://vishaal27.github.io/;;https://drimpossible.github.io/;https://bethgelab.org",
        "dblp": ";;247/4693;;181/4512;77/3005",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;jUOcawkAAAAJ;;0kK7sSAAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;",
        "linkedin": ";;vishaal-udandarao/?originalSubdomain=de;;;",
        "or_profile": "~Andreas_Hochlehnert1;~Hardik_Bhatnagar1;~Vishaal_Udandarao1;~Samuel_Albanie2;~Ameya_Prabhu1;~Matthias_Bethge1",
        "aff": "Google+University of Tuebingen;Centre for Integrative Neuroscience, AG Bethge;Eberhard-Karls-Universit\u00e4t T\u00fcbingen+University of Cambridge;;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;University of Tuebingen",
        "aff_domain": "google.com+uni-tuebingen.de;bethgelab.org;uni-tuebingen.de+cam.ac.uk;;uni-tuebingen.de;uni-tuebingen.de",
        "position": "Student Researcher+PhD student;PhD student;PhD student+PhD student;;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nhochlehnert2025a,\ntitle={A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility},\nauthor={Andreas Hochlehnert and Hardik Bhatnagar and Vishaal Udandarao and Samuel Albanie and Ameya Prabhu and Matthias Bethge},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=90UrTTxp5O}\n}",
        "github": "",
        "project": "",
        "reviewers": "2SGm;xRby;ja19",
        "site": "https://openreview.net/forum?id=90UrTTxp5O",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "99e72TkWTi",
        "title": "Visual Representations inside the Language Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite interpretability work analyzing VIT encoders and transformer activations, we don't yet understand why Multimodal Language Models (MLMs) struggle on perception-heavy tasks. We offer an under-studied perspective by examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the flow of visual information through the language model, finding that image value tokens encode sufficient information to perform several perception-heavy tasks zero-shot: segmentation, semantic correspondence, temporal correspondence, and referring expression detection. We find that while the language model does augment the visual information received from the projection of input visual encodings---which we reveal correlates with overall MLM perception capability---it contains less visual information on several tasks than the equivalent visual encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that the visual information corresponding to input-agnostic image key tokens in later layers of language models contains artifacts which reduce perception capability of the overall MLM. Next, we discuss controlling visual information in the language model, showing that adding a text prefix to the image input improves perception capabilities of visual representations. Finally, we reveal that if language models were able to better control their visual information, their perception would significantly improve; e.g., in 33.3% of Art Style questions in the BLINK benchmark, perception information present in the language model is not surfaced to the output! Our findings reveal insights into the role of key-value tokens in multimodal systems, paving the way for deeper mechanistic interpretability of MLMs and suggesting new directions for training their visual encoder and language model components.",
        "keywords": "multimodal language model;visual representation;mechanistic interpretability;vision-language",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Benlin Liu;Amita Kamath;Madeleine Grunde-McLaughlin;Winson Han;Ranjay Krishna",
        "authorids": "~Benlin_Liu1;~Amita_Kamath1;~Madeleine_Grunde-McLaughlin1;~Winson_Han1;~Ranjay_Krishna1",
        "gender": "M;F;F;M;M",
        "homepage": "https://liubl1217.github.io/;https://amitakamath.github.io/;https://madeleinegrunde.github.io/;;http://ranjaykrishna.com",
        "dblp": "274/0684;267/9823;271/8198;255/5528;167/3785",
        "google_scholar": "fNl-ZkIAAAAJ;B_ek5IIAAAAJ;wzqKsd4AAAAJ;;IcqahyAAAAAJ",
        "orcid": ";;;;0000-0001-8784-2531",
        "linkedin": ";;;winsonhan/;ranjay-krishna-1a344444/",
        "or_profile": "~Benlin_Liu1;~Amita_Kamath1;~Madeleine_Grunde-McLaughlin1;~Winson_Han1;~Ranjay_Krishna1",
        "aff": "Department of Computer Science, University of Washington;Department of Computer Science, University of Washington+UCLA Computer Science Department, University of California, Los Angeles;University of Washington;Ai2;University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu+cs.ucla.edu;uw.edu;allenai.org;cs.washington.edu",
        "position": "PhD student;PhD student+PhD student;PhD student;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025visual,\ntitle={Visual Representations inside the Language Model},\nauthor={Benlin Liu and Amita Kamath and Madeleine Grunde-McLaughlin and Winson Han and Ranjay Krishna},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=99e72TkWTi}\n}",
        "github": "",
        "project": "",
        "reviewers": "RSnv;SMbp;GABW;6is2",
        "site": "https://openreview.net/forum?id=99e72TkWTi",
        "pdf_size": 0,
        "rating": "4;5;6;7",
        "confidence": "4;5;4;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            28,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.2581988897471611
    },
    {
        "id": "9AFIz0YzD7",
        "title": "Gating is Weighting: Understanding Gated Linear Attention through In-context Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Linear attention methods offer a compelling alternative to softmax attention due to their efficiency in recurrent decoding. Recent research has focused on enhancing standard linear attention by incorporating gating while retaining its computational benefits. Such Gated Linear Attention (GLA) architectures include highly competitive models such as Mamba and RWKV. In this work, we investigate the in-context learning capabilities of the GLA model and make the following contributions. We show that a multilayer GLA can implement a general class of Weighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent weights. These weights are induced by the gating mechanism and the input, enabling the model to control the contribution of individual tokens to prediction. To further understand the mechanics of this weighting, we introduce a novel data model with multitask prompts and characterize the optimization landscape of learning a WPGD algorithm. We identify mild conditions under which there exists a unique global minimum, up to scaling invariance, and the associated WPGD algorithm is unique as well. Finally, we translate these findings to explore the optimization landscape of GLA and shed light on how gating facilitates context-aware learning and when it is provably better than vanilla linear attention.",
        "keywords": "linear attention;gating;in-context learning;weighted gradient descent;optimization landscape",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yingcong Li;Davoud Ataee Tarzanagh;Ankit Singh Rawat;Maryam Fazel;Samet Oymak",
        "authorids": "~Yingcong_Li1;~Davoud_Ataee_Tarzanagh1;~Ankit_Singh_Rawat1;~Maryam_Fazel1;~Samet_Oymak2",
        "gender": ";M;M;F;",
        "homepage": "https://yingcong-li.github.io/;https://tarzanagh.github.io/;https://ankitsrawat.github.io/home/;;",
        "dblp": "244/4435;;https://dblp.org/pers/hd/r/Rawat:Ankit_Singh;10/2309;",
        "google_scholar": "9uWgjIUAAAAJ;Djtvz_0AAAAJ;http://scholar.google.com/citations?user=U0_ab4cAAAAJ;vlN_kRoAAAAJ;",
        "orcid": ";0000-0003-1267-3889;;;",
        "linkedin": ";;;;",
        "or_profile": "~Yingcong_Li1;~Davoud_Ataee_Tarzanagh1;~Ankit_Singh_Rawat1;~Maryam_Fazel1;~Samet_Oymak2",
        "aff": "University of Michigan - Ann Arbor;Samsung;Google;University of Washington, Seattle;",
        "aff_domain": "umich.edu;samsung.com;google.com;uw.edu;",
        "position": "PhD student;Research Scientist;Research Scientist;Full Professor;",
        "bibtex": "@inproceedings{\nli2025gating,\ntitle={Gating is Weighting: Understanding Gated Linear Attention through In-context Learning},\nauthor={Yingcong Li and Davoud Ataee Tarzanagh and Ankit Singh Rawat and Maryam Fazel and Samet Oymak},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9AFIz0YzD7}\n}",
        "github": "",
        "project": "",
        "reviewers": "XH8t;qjca;Pxhm;Xg8p",
        "site": "https://openreview.net/forum?id=9AFIz0YzD7",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "9DCQAGBoII",
        "title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose **SPOC**, a *spontaneous self-correction* approach that enables LLMs to generate interleaved solutions and verifications in a *single inference pass*, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving absolute gains of 8.8\\% and 11.6\\% on MATH500, 10.0\\% and 20.0\\% on AMC23, and 3.3\\% and 6.7\\% on AIME24, respectively.",
        "keywords": "Large Language Models;Reasoning;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xutong Zhao;Tengyu Xu;Xuewei Wang;Zhengxing Chen;Di Jin;Liang Tan;Yen-Ting Lin;Zishun Yu;Zhuokai Zhao;Yun He;Sinong Wang;Han Fang;Sarath Chandar;Chen Zhu",
        "authorids": "~Xutong_Zhao1;~Tengyu_Xu1;~Xuewei_Wang1;~Zhengxing_Chen2;~Di_Jin1;~Liang_Tan1;~Yen-Ting_Lin2;~Zishun_Yu1;~Zhuokai_Zhao1;~Yun_He2;~Sinong_Wang1;~Han_Fang4;~Sarath_Chandar1;~Chen_Zhu2",
        "gender": ";;F;M;M;M;M;M;M;;M;M;M;M",
        "homepage": "https://tongtongx.github.io/;;;http://czxttkl.github.io;https://jind11.github.io/;;https://yentingl.com/;https://www.zishun.me;https://zhuokai-zhao.com/;https://heyunh2015.github.io/yunhe.github.io/;https://sites.google.com/site/snongwang/;https://www.hanfang.info/;http://sarathchandar.in/;http://www.cs.umd.edu/~chenzhu/",
        "dblp": "320/8138;;56/4632;;;;;320/4542.html;348/5348;02/6731-1;140/0795;209/7867;45/8542;59/10522-1.html",
        "google_scholar": ";;E_0IDdMAAAAJ;LjZLn2MAAAAJ;x5QTK9YAAAAJ;N3rZr9kAAAAJ;UWOV0tYAAAAJ;yhoPwYYAAAAJ;EGcdEjEAAAAJ;https://scholar.google.com/citations?view_op=list_works;CYMAfxsAAAAJ;;https://scholar.google.co.in/citations?user=yxWtZLAAAAAJ;m-om5O8AAAAJ",
        "orcid": ";;;;;;0000-0003-2970-2455;;0000-0001-8201-2977;0000-0001-9462-4583;;;;",
        "linkedin": ";;;;;liang-tan-6646a484/;yen-ting-lin-416732b3/;zishunyu/;zhuokai-zhao-a9385169/;;wang-s-simon-194512a7;;;",
        "or_profile": "~Xutong_Zhao1;~Tengyu_Xu1;~Xuewei_Wang1;~Zhengxing_Chen2;~Di_Jin1;~Liang_Tan1;~Yen-Ting_Lin2;~Zishun_Yu1;~Zhuokai_Zhao1;~Yun_He2;~Sinong_Wang1;~Han_Fang4;~Sarath_Chandar1;~Chen_Zhu2",
        "aff": "Mila;;Meta Facebook;Meta Facebook;Meta;Meta Facebook;National Taiwan University;University of Illinois, Chicago+FAIR (Fundamental AI Research);Meta;Meta;Meta Facebook;Meta AI;\u00c9cole Polytechnique de Montr\u00e9al;xAI+Meta Facebook",
        "aff_domain": "mila.quebec;;fb.com;fb.com;meta.com;meta.com;ntu.edu.tw;uic.edu+meta.com;meta.com;meta.com;fb.com;facebook.com;polymtl.ca;x.ai+meta.com",
        "position": "PhD student;;Machine Learning Engineer;Research Scientist;Researcher;Researcher;PhD student;PhD student+Intern;Researcher;Research Scientist ;Research scientist;Research Scientist Manager;Associate Professor;Member of Technical Staff+Researcher",
        "bibtex": "@inproceedings{\nzhao2025boosting,\ntitle={Boosting {LLM} Reasoning via Spontaneous Self-Correction},\nauthor={Xutong Zhao and Tengyu Xu and Xuewei Wang and Zhengxing Chen and Di Jin and Liang Tan and Yen-Ting Lin and Zishun Yu and Zhuokai Zhao and Yun He and Sinong Wang and Han Fang and Sarath Chandar and Chen Zhu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9DCQAGBoII}\n}",
        "github": "",
        "project": "",
        "reviewers": "MWdF;VrYp;Vfpt;Cxxp;VSGv",
        "site": "https://openreview.net/forum?id=9DCQAGBoII",
        "pdf_size": 0,
        "rating": "4;5;5;6;7",
        "confidence": "3;4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            5.4,
            1.0198039027185568
        ],
        "confidence_avg": [
            3.6,
            0.4898979485566356
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.08006407690254366
    },
    {
        "id": "9FES5yT9v3",
        "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While in-context learning is well-studied with decoder-only language models (LLMs), its utility for encoder-only models remains underexplored. We study in-context learning for encoder-only models for text retrieval tasks. Can incorporating in-context examples (query-document pairs) to the target query enhance retriever performance? Our approach, \\texttt{RARe}, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This approach achieves performance gains of up to +2.72\\% nDCG across open-domain retrieval datasets (BeIR, RAR-b) compared to using the target query only as an input. In particular, we find \\texttt{RARe} exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation for retrievers and lay the foundation for future work.",
        "keywords": "Information Retrieval;In-Context Learning;Representation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/7e64201c27f5bfd3e5b3b0d905eb10b59c777c4f.zip",
        "author": "Atula Tejaswi;Yoonsang Lee;sujay sanghavi;Eunsol Choi",
        "authorids": "~Atula_Tejaswi1;~Yoonsang_Lee1;~sujay_sanghavi1;~Eunsol_Choi1",
        "gender": ";;M;",
        "homepage": ";https://yoonsang-lee.github.io;https://sites.utexas.edu/sanghavi;https://eunsol.github.io/",
        "dblp": ";362/4502;69/4911.html;116/2765",
        "google_scholar": ";8f1o4hYAAAAJ;O-DazBUAAAAJ;6wulN88AAAAJ",
        "orcid": ";;;0000-0003-3607-9104",
        "linkedin": ";;;",
        "or_profile": "~Atula_Tejaswi1;~Yoonsang_Lee1;~sujay_sanghavi1;~Eunsol_Choi1",
        "aff": ";Princeton University+Seoul National University;University of Texas, Austin;New York University",
        "aff_domain": ";princeton.edu+snu.ac.kr;utexas.edu;nyu.edu",
        "position": ";PhD student+Undergrad student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntejaswi2025rare,\ntitle={{RAR}e: Retrieval Augmented Retrieval with In-Context Examples},\nauthor={Atula Tejaswi and Yoonsang Lee and sujay sanghavi and Eunsol Choi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9FES5yT9v3}\n}",
        "github": "",
        "project": "",
        "reviewers": "9t1y;neiN;cnSt;1TMR",
        "site": "https://openreview.net/forum?id=9FES5yT9v3",
        "pdf_size": 0,
        "rating": "5;5;5;6",
        "confidence": "5;5;3;5",
        "wc_review": "",
        "rating_avg": [
            5.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.5,
            0.8660254037844386
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "9ffYcEiNw9",
        "title": "M\u00b2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \\textbf{M\u00b2IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M\u00b2IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M\u00b2IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \\textbf{VLibrary}, a repository that stores trained M\u00b2IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M\u00b2IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\\% with substantial improvements in overall efficiency.",
        "keywords": "Large Vision-Language Model;In-context Learning;Representation Engineering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yanshu Li;Yi Cao;Hongyang He;Qisen Cheng;Xiang Fu;Xi Xiao;Tianyang Wang;Ruixiang Tang",
        "authorids": "~Yanshu_Li1;~Yi_Cao6;~Hongyang_He1;~Qisen_Cheng2;~Xiang_Fu5;~Xi_Xiao2;~Tianyang_Wang1;~Ruixiang_Tang1",
        "gender": "M;Not Specified;Not Specified;;M;M;M;M",
        "homepage": "https://github.com/kaamava;;;;https://xiangfu.site;;https://wangt0716.github.io/;https://www.ruixiangtang.net/",
        "dblp": ";;03/8056.html;;;;;239/1928",
        "google_scholar": ";hM0fzb4AAAAJ;y-21I6MAAAAJ;;3CzDreAAAAAJ;https://scholar.google.com.hk/citations?hl=zh-CN;QbTV0r0AAAAJ;T575jsoAAAAJ",
        "orcid": "0009-0002-4054-606X;;0009-0001-4889-8118;;0000-0002-0249-1013;0009-0000-0931-6982;0000-0003-3184-0566;",
        "linkedin": "yanshu-li-aa08282a6/;;;;;xi-xiao-4800272a5/;tianyang-wang-03a86a4a/;ruixiang-tang-91660717b/",
        "or_profile": "~Yanshu_Li1;~Yi_Cao6;~Hongyang_He1;~Qisen_Cheng2;~Xiang_Fu5;~Xi_Xiao2;~Tianyang_Wang1;~Ruixiang_Tang1",
        "aff": "Brown University;Soochow University;University of Warwick;;Boston University;Oak Ridge National Laboratory+University of Alabama at Birmingham;University of Alabama at Birmingham;Rutgers University",
        "aff_domain": "brown.edu;suda.edu.cn;warwick.ac.uk;;bu.edu;ornl.gov+uab.edu;uab.edu;rutgers.edu",
        "position": "MS student;MS student;PhD student;;Undergrad student;Research Intern+PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025miv,\ntitle={M{\\texttwosuperior}{IV}: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering},\nauthor={Yanshu Li and Yi Cao and Hongyang He and Qisen Cheng and Xiang Fu and Xi Xiao and Tianyang Wang and Ruixiang Tang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9ffYcEiNw9}\n}",
        "github": "",
        "project": "",
        "reviewers": "P1rJ;w71E;SUT6;Ed6E",
        "site": "https://openreview.net/forum?id=9ffYcEiNw9",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            28,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "9nQsDdquOY",
        "title": "BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Neural sentence embedding models for dense retrieval typically rely on binary relevance labels, treating query-document pairs as either relevant or irrelevant. However, real-world relevance often exists on a continuum, and recent advances in large language models (LLMs) have made it feasible to scale the generation of fine-grained graded relevance labels. In this work, we propose \\textbf{BiXSE}, a simple and effective pointwise training method that optimizes binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE interprets these scores as probabilistic targets, enabling granular supervision from a single labeled query-document pair per query. Unlike pairwise or listwise losses that require multiple annotated comparisons per query, BiXSE achieves strong performance with reduced annotation and compute costs by leveraging in-batch negatives. Extensive experiments across sentence embedding (MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently outperforms softmax-based contrastive learning (InfoNCE), and matches or exceeds strong pairwise ranking baselines when trained on LLM-supervised data. BiXSE offers a robust, scalable alternative for training dense retrieval models as graded relevance supervision becomes increasingly accessible.",
        "keywords": "sentence embeddings;dense retrieval;retrieval;ranking;synthetic data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Christos Tsirigotis;Vaibhav Adlakha;Joao Monteiro;Aaron Courville;Perouz Taslakian",
        "authorids": "~Christos_Tsirigotis1;~Vaibhav_Adlakha1;~Joao_Monteiro1;~Aaron_Courville3;~Perouz_Taslakian1",
        "gender": ";M;M;;F",
        "homepage": ";https://vaibhavad.github.io/;;;http://www.perouz.com",
        "dblp": "215/5173;264/5013;215/5354-2;56/1688;52/1849",
        "google_scholar": "https://scholar.google.com/citations?hl=en;0qmIfUoAAAAJ;https://scholar.google.ca/citations?hl=en;https://scholar.google.ca/citations?user=km6CP8cAAAAJ;LJ7gHkQAAAAJ",
        "orcid": ";;;;",
        "linkedin": "tsirif/;vaibhav-adlakha/;joao-monteiro-47180256/;;perouz/",
        "or_profile": "~Christos_Tsirigotis1;~Vaibhav_Adlakha1;~Joao_Monteiro1;~Aaron_Courville3;~Perouz_Taslakian1",
        "aff": "ServiceNow Research+University of Montreal+Mila, Quebec Artificial Intelligence Institute;McGill University;Apple+Autodesk;University of Montreal+Universit\u00e9 de Montr\u00e9al;ServiceNow",
        "aff_domain": "servicenow.com+umontreal.ca+mila.quebec;mcgill.ca;apple.com+autodesk.com;umontreal.ca+ ;servicenow.com",
        "position": "Intern+PhD student+Researcher;PhD student;Researcher+Researcher;Assistant Professor+Assistant Professor;Researcher",
        "bibtex": "@inproceedings{\ntsirigotis2025bixse,\ntitle={Bi{XSE}: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation},\nauthor={Christos Tsirigotis and Vaibhav Adlakha and Joao Monteiro and Aaron Courville and Perouz Taslakian},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9nQsDdquOY}\n}",
        "github": "",
        "project": "",
        "reviewers": "CF35;odwD;Dk2A;t3xV",
        "site": "https://openreview.net/forum?id=9nQsDdquOY",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;5;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            27,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "9pzNFfgtyk",
        "title": "Partial Perspectives: How LLMs Handle Logically Inconsistent Knowledge in Reasoning Tasks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Most natural language reasoning tasks in the research community assume consistent input knowledge. Nevertheless, real-world scenarios often involve inconsistent information, which might lead to divergent conclusions and are typically associated with varying levels of uncertainty. This raises a key research question: can large language models (LLMs) effectively handle uncertainty in their reasoning process to maximize knowledge consistency?\nIn this paper, we propose a framework for evaluating reasoning over inconsistent knowledge. Our approach models uncertainty via weights of logical rules, leveraging Markov logic networks (MLN), which integrate probabilistic reasoning with first-order logic. This enables us to quantify inconsistencies in knowledge bases, and hence rigorously evaluate LLM reasoning. We introduce two tasks using this framework: 1) QA, which involves answering questions by integrating inconsistent knowledge; and 2) knowledge rectification, where we aim to rectify language models' acquired knowledge to improve consistency. We curate a dataset of 3,000 MLN-formatted knowledge bases to implement these tasks. We evaluate state-of-the-art LLMs on these tasks and highlight their limitations in uncertainty-aware reasoning over inconsistent logical knowledge.",
        "keywords": "evaluation methodologies;reasoning;logical reasoning;calibration/uncertainty",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zichao Li;Ines Arous;Jackie CK Cheung",
        "authorids": "~Zichao_Li3;~Ines_Arous1;~Jackie_CK_Cheung1",
        "gender": ";;M",
        "homepage": ";https://inesarous.github.io/;http://cs.mcgill.ca/~jcheung/",
        "dblp": "95/147-3;207/8093;00/9012",
        "google_scholar": ";RWXHLa8AAAAJ;https://scholar.google.com.tw/citations?user=Um-wmYQAAAAJ",
        "orcid": ";0000-0001-7513-6197;",
        "linkedin": ";ines-arous/;",
        "or_profile": "~Zichao_Li3;~Ines_Arous1;~Jackie_CK_Cheung1",
        "aff": "McGill University;York University+, McGill University;McGill University+Mila Research Institute+Microsoft",
        "aff_domain": "mcgill.ca;yorku.ca+cs.mcgill.ca;mcgill.ca+mila.quebec+microsoft.com",
        "position": "PhD student;Assistant Professor+Postdoc;Associate Professor+Associate Professor+Consulting Researcher",
        "bibtex": "@inproceedings{\nli2025partial,\ntitle={Partial Perspectives: How {LLM}s Handle Logically Inconsistent Knowledge in Reasoning Tasks},\nauthor={Zichao Li and Ines Arous and Jackie CK Cheung},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9pzNFfgtyk}\n}",
        "github": "",
        "project": "",
        "reviewers": "sz1U;8jVR;rzYg;1Tov",
        "site": "https://openreview.net/forum?id=9pzNFfgtyk",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "9rwtezthwo",
        "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Improvements in language models are often driven by increasing the quality of the data we train them on, which can be limiting when strong supervision is not readily available. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual sample. We formulate the **delta learning hypothesis** to explain this phenomenon, positing that the relative quality _delta_ between points suffices to drive learning via preference tuning\u2014even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to ensure a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of T\u00fclu 3, a state-of-the-art open model that was tuned from the same base as our model while relying on vastly stronger supervisors (e.g., GPT-4o). Delta learning thus enables simpler and cheaper open recipes for state-of-the-art post-training, highlighting that models can learn a surprising amount from data that might typically be considered weak.",
        "keywords": "Preference tuning;LLM post-training;synthetic data;weak-to-strong generalization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Scott Geng;Hamish Ivison;Chun-Liang Li;Maarten Sap;Jerry Li;Ranjay Krishna;Pang Wei Koh",
        "authorids": "~Scott_Geng1;~Hamish_Ivison1;~Chun-Liang_Li1;~Maarten_Sap1;~Jerry_Li1;~Ranjay_Krishna1;~Pang_Wei_Koh1",
        "gender": ";;M;M;M;M;M",
        "homepage": "https://www.scottgeng.com/;https://hamishivi.github.io;http://chunliangli.github.io;http://maartensap.com;https://jerryzli.github.io/;http://ranjaykrishna.com;http://cs.stanford.edu/~pangwei",
        "dblp": "330/4056.html;288/1956;;153/9519;;167/3785;10/10453",
        "google_scholar": "jCg1gRoAAAAJ;;https://scholar.google.com.tw/citations?user=vqHIt_sAAAAJ;gFN4QUYAAAAJ;4zybTq4AAAAJ;IcqahyAAAAAJ;Nn990CkAAAAJ",
        "orcid": ";0000-0002-0069-7659;;;;0000-0001-8784-2531;",
        "linkedin": ";;;;;ranjay-krishna-1a344444/;",
        "or_profile": "~Scott_Geng1;~Hamish_Ivison1;~Chun-Liang_Li1;~Maarten_Sap1;~Jerry_Li1;~Ranjay_Krishna1;~Pang_Wei_Koh1",
        "aff": "University of Washington;University of Washington;Apple;Carnegie Mellon University;University of Washington;University of Washington;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "cs.washington.edu;uw.edu;apple.com;cmu.edu;washington.edu;cs.washington.edu;allenai.org+cs.washington.edu",
        "position": "PhD student;PhD student;Researcher;Assistant Professor;Associate Professor;Assistant Professor;Visiting Research Scientist+Assistant Professor",
        "bibtex": "@inproceedings{\ngeng2025the,\ntitle={The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains},\nauthor={Scott Geng and Hamish Ivison and Chun-Liang Li and Maarten Sap and Jerry Li and Ranjay Krishna and Pang Wei Koh},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=9rwtezthwo}\n}",
        "github": "",
        "project": "",
        "reviewers": "wem6;nk1W;Mjit;vyR3",
        "site": "https://openreview.net/forum?id=9rwtezthwo",
        "pdf_size": 0,
        "rating": "6;8;9;9",
        "confidence": "2;5;3;5",
        "wc_review": "",
        "rating_avg": [
            8.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            1.299038105676658
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6285393610547089
    },
    {
        "id": "AFMGbq39bQ",
        "title": "Readability \u2260 Learnability: Rethinking the Role of Simplicity in Training Small Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent studies suggest that very small language models (SLMs) can generate surprisingly coherent text when trained on simplified, child-directed corpora such as TinyStories. These findings have been interpreted as evidence that readability\u2014characterized by accessible vocabulary, familiar narrative structure, and simple syntax\u2014plays a key role in enabling such capabilities to emerge. In this paper, we challenge that interpretation. We construct synthetic datasets with matched structure but varied readability, and find that readability alone does not predict coherence or learning efficiency in SLMs. Models trained on complex, adult-level text perform comparably to those trained on simplified language, and even exhibit faster development of coherence during training. Instead, we show that statistical simplicity, as measured by n-gram diversity, is a stronger predictor of learnability. Our findings caution against the growing trend of anthropomorphizing language model training\u2014drawing parallels to human cognitive development without empirical basis\u2014and argue for more precise reasoning about what properties actually support capability emergence in small models.",
        "keywords": "child-directed language;developmentally inspired data;small language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ivan Lee;Taylor Berg-Kirkpatrick",
        "authorids": "~Ivan_Lee2;~Taylor_Berg-Kirkpatrick1",
        "gender": ";M",
        "homepage": "https://ivnle.github.io/;https://cseweb.ucsd.edu/~tberg/",
        "dblp": ";22/8160",
        "google_scholar": ";mN6_BKAAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ivan_Lee2;~Taylor_Berg-Kirkpatrick1",
        "aff": "University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "position": "PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nlee2025readability,\ntitle={Readability \\ensuremath{\\neq} Learnability: Rethinking the Role of Simplicity in Training Small Language Models},\nauthor={Ivan Lee and Taylor Berg-Kirkpatrick},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=AFMGbq39bQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "NYqh;nRzk;QjJm",
        "site": "https://openreview.net/forum?id=AFMGbq39bQ",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "AHhDpMMXtf",
        "title": "Understanding and Improving Noisy Embedding Techniques in Instruction Finetuning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in instructional fine-tuning have injected noise into embeddings, with NEFTune (Jain et al., 2024) setting benchmarks using uniform noise. Despite NEFTune\u2019s empirical findings that uniform noise outperforms Gaussian noise, the reasons for this remain unclear. This paper aims to clarify this by offering a thorough analysis, both theoretical and empirical, indicating comparable performance among these noise types. Additionally, we introduce a new fine-tuning method for language models, utilizing symmetric noise in embeddings. This method aims to enhance the model\u2019s function by more stringently regulating its local curvature, demonstrating superior performance over the current method, NEFTune. When fine-tuning the LLaMA-2-7B model using Alpaca, standard techniques yield a 29.79% score on AlpacaEval. However, our approach, SymNoise, increases this score significantly to 69.04%, using symmetric noisy embeddings. This is a 6.7% improvement over the state-of-the-art method, NEFTune (64.69%). Furthermore, when tested on various models and stronger baseline instruction datasets, such as Evol-Instruct, ShareGPT, OpenPlatypus, SymNoise consistently outperforms NEFTune. The current literature, including NEFTune, has underscored the importance of more in-depth research into the application of noise-based strategies in the fine-tuning of language models. Our approach, SymNoise, is another significant step towards this direction, showing notable improvement over the existing state-of-the-art method.",
        "keywords": "Instruction Finetuning;LLM;Domain Adaptation;Multi-Task Learning;Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abhay Yadav",
        "authorids": "~Abhay_Yadav1",
        "gender": "",
        "homepage": "https://aiem.jhu.edu/people/abhay-yadav/",
        "dblp": "07/8617",
        "google_scholar": "https://scholar.google.co.in/citations?user=pbRu-GQAAAAJ",
        "orcid": "",
        "linkedin": "abhay-yadav-bb671415/",
        "or_profile": "~Abhay_Yadav1",
        "aff": "Johns Hopkins University",
        "aff_domain": "jhu.edu",
        "position": "Researcher",
        "bibtex": "@inproceedings{\nyadav2025understanding,\ntitle={Understanding and Improving Noisy Embedding Techniques in Instruction Finetuning},\nauthor={Abhay Yadav},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=AHhDpMMXtf}\n}",
        "github": "",
        "project": "",
        "reviewers": "5zJn;7QRP;Yag6",
        "site": "https://openreview.net/forum?id=AHhDpMMXtf",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "ASS5YD4hL4",
        "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning\u2014even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model\u2019s adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code will be released upon acceptance.",
        "keywords": "Safety;Alignment;Large Language Models;Low-rank;Robustness",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gabriel Jacob Perin;Runjin Chen;Xuxi Chen;Nina S. T. Hirata;Zhangyang Wang;Junyuan Hong",
        "authorids": "~Gabriel_Jacob_Perin1;~Runjin_Chen1;~Xuxi_Chen1;~Nina_S._T._Hirata1;~Zhangyang_Wang1;~Junyuan_Hong1",
        "gender": "M;;Unspecified;F;M;M",
        "homepage": ";;;https://www.ime.usp.br/~nina/;https://vita-group.github.io;https://jyhong.gitlab.io/",
        "dblp": ";;267/9662;;119/4026;185/1316",
        "google_scholar": ";;afsDlKYAAAAJ;HnC89_0AAAAJ;pxFyKAIAAAAJ;7Cbv6doAAAAJ",
        "orcid": "0000-0002-5990-7497;;;0000-0001-9722-5764;;0000-0002-5718-5187",
        "linkedin": ";;;;;",
        "or_profile": "~Gabriel_Jacob_Perin1;~Runjin_Chen1;~Xuxi_Chen1;~Nina_S._T._Hirata1;~Zhangyang_Wang1;~Junyuan_Hong1",
        "aff": "International Business Machines+Universidade de S\u00e3o Paulo;;University of Texas at Austin;Universidade de S\u00e3o Paulo;University of Texas at Austin;University of Texas at Austin",
        "aff_domain": "ibm.com+usp.br;;utexas.edu;usp.br;utexas.edu;utexas.edu",
        "position": "Intern+Undergrad student;;PhD student;Associate Professor;Associate Professor;Postdoc",
        "bibtex": "@inproceedings{\nperin2025lox,\ntitle={LoX: Low-Rank Extrapolation Robustifies {LLM} Safety Against Fine-tuning},\nauthor={Gabriel Jacob Perin and Runjin Chen and Xuxi Chen and Nina S. T. Hirata and Zhangyang Wang and Junyuan Hong},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ASS5YD4hL4}\n}",
        "github": "",
        "project": "",
        "reviewers": "a4V1;NrFf;uZoX;3vEX",
        "site": "https://openreview.net/forum?id=ASS5YD4hL4",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Ah0U1r5Ldq",
        "title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs.\nIn this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks.\nOur approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance.\nExperimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.",
        "keywords": "Machine Translation;Long Context;Multi-Paradigm Translation Dataset Curation;Instruction Tuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Miguel Moura Ramos;Patrick Fernandes;Sweta Agrawal;Andre Martins",
        "authorids": "~Miguel_Moura_Ramos1;~Patrick_Fernandes1;~Sweta_Agrawal1;~Andre_Martins1",
        "gender": "M;;F;M",
        "homepage": ";https://coderpat.github.io;https://sweta20.github.io/;https://andre-martins.github.io/",
        "dblp": ";207/6964.html;210/7863.html;m/AndreFTMartins",
        "google_scholar": "YOPFOaIAAAAJ;;Avsw9IkAAAAJ;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Miguel_Moura_Ramos1;~Patrick_Fernandes1;~Sweta_Agrawal1;~Andre_Martins1",
        "aff": "Instituto Superior T\u00e9cnico;School of Computer Science, Carnegie Mellon University+Instituto Superior T\u00e9cnico;Google;Instituto Superior T\u00e9cnico+Unbabel",
        "aff_domain": "tecnico.ulisboa.pt;cs.cmu.edu+tecnico.ulisboa.pt;google.com;tecnico.ulisboa.pt+unbabel.com",
        "position": "PhD student;PhD student+PhD student;Researcher;Associate Professor+Research Scientist",
        "bibtex": "@inproceedings{\nramos2025multilingual,\ntitle={Multilingual Contextualization of Large Language Models for Document-Level Machine Translation},\nauthor={Miguel Moura Ramos and Patrick Fernandes and Sweta Agrawal and Andre Martins},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Ah0U1r5Ldq}\n}",
        "github": "",
        "project": "",
        "reviewers": "eZn7;BHRV;cWq6;QBKw",
        "site": "https://openreview.net/forum?id=Ah0U1r5Ldq",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;5;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "AivRDOFi5H",
        "title": "Language Models Fail to Introspect About Their Knowledge of Language",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "There has been recent interest in whether large language models (LLMs) can introspect about their own internal states. Such abilities would make LLMs more interpretable, and also validate the use of standard introspective methods in linguistics to evaluate grammatical knowledge in models (e.g., asking \"Is this sentence grammatical?\").\nWe systematically investigate emergent introspection across 21 open-source LLMs, in two domains where introspection is of theoretical interest: grammatical knowledge and word prediction.\nCrucially, in both domains, a model\u2019s internal linguistic knowledge can\nbe theoretically grounded in direct measurements of string probability. We then evaluate whether models' responses to metalinguistic prompts faithfully reflect their internal knowledge.\nWe propose a new measure of introspection: the degree to which a model\u2019s prompted responses predict its own string probabilities, beyond what would be predicted by another model with nearly identical internal knowledge.\nWhile both metalinguistic prompting and probability comparisons lead to high task accuracy, we do not find evidence that LLMs have privileged \"self-access\". By using general tasks, controlling for model similarity, and evaluating a wide range of open-source models, we show that LLMs cannot introspect, and add new evidence to the argument that prompted responses should not be conflated with models' linguistic generalizations.",
        "keywords": "introspection;linguistic acceptability judgments;syntax;grammaticality;surprisal;metalinguistic;metacognition",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Siyuan Song;Jennifer Hu;Kyle Mahowald",
        "authorids": "~Siyuan_Song1;~Jennifer_Hu1;~Kyle_Mahowald1",
        "gender": "M;;M",
        "homepage": ";https://jennhu.github.io/;https://mahowak.github.io",
        "dblp": ";217/1862;38/11196",
        "google_scholar": "9d_X-zoAAAAJ;;XUmFLVUAAAAJ",
        "orcid": ";0000-0003-4075-6876;",
        "linkedin": ";;",
        "or_profile": "~Siyuan_Song1;~Jennifer_Hu1;~Kyle_Mahowald1",
        "aff": "University of Texas at Austin;Harvard University;The University of Texas at Austin",
        "aff_domain": "utexas.edu;harvard.edu;utexas.edu",
        "position": "Undergrad student;Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nsong2025language,\ntitle={Language Models Fail to Introspect About Their Knowledge of Language},\nauthor={Siyuan Song and Jennifer Hu and Kyle Mahowald},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=AivRDOFi5H}\n}",
        "github": "",
        "project": "",
        "reviewers": "n8wV;PXdC;KaWq;gFfK",
        "site": "https://openreview.net/forum?id=AivRDOFi5H",
        "pdf_size": 0,
        "rating": "4;7;8;8",
        "confidence": "5;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.6393596310755
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6137949055234262
    },
    {
        "id": "Atyk8lnIQQ",
        "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Employing language models as evaluators of long-form output (LLM-as-a-Judge) has become the \\textit{de facto} standard for automatic evaluation. However, most LLM judges have been optimized exclusively for English outputs, with strategies for enhancing judges' multilingual evaluation capabilities remaining largely unexplored in the current literature. This has created a disparity in the quality of automatic evaluation methods for other languages, ultimately hindering the development of models with better multilingual capabilities. To bridge this gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from 3B to 14B parameters that can provide both direct assessment and pairwise comparison feedback on multilingual outputs. M-Prometheus models outperform state-of-the-art open LLM judges on multilingual reward benchmarks spanning more than 20 languages, as well as on literary machine translation evaluation covering 4 language pairs. Furthermore, we find M-Prometheus models can be used with quality-aware decoding methods to significantly improve generated outputs, showcasing their utility for the development of better multilingual models. Crucially, through extensive ablations, we identify key strategies for training an effective multilingual judge. Our findings highlight the significance of model size and base model selection, and the advantages of using natively multilingual data rather than translated data. We release our models, training dataset, and code to reproduce our experiments.",
        "keywords": "automatic evaluation;llm-as-a-judge;multilinguality",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jos\u00e9 Pombal;Dongkeun Yoon;Patrick Fernandes;Ian Wu;Seungone Kim;Ricardo Rei;Graham Neubig;Andre Martins",
        "authorids": "~Jos\u00e9_Pombal1;~Dongkeun_Yoon1;~Patrick_Fernandes1;~Ian_Wu3;~Seungone_Kim1;~Ricardo_Rei1;~Graham_Neubig1;~Andre_Martins1",
        "gender": ";M;;M;M;;M;M",
        "homepage": ";;https://coderpat.github.io;;https://github.com/SeungoneKim;;http://phontron.com;https://andre-martins.github.io/",
        "dblp": ";330/5116;207/6964.html;;324/2064.html;;03/8155;m/AndreFTMartins",
        "google_scholar": "Tt_-ImgAAAAJ;;;QdfBJmAAAAAJ;https://scholar.google.co.kr/citations?user=qEf3e3EAAAAJ;;wlosgkoAAAAJ;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ",
        "orcid": ";0000-0002-4544-7346;;;;;;",
        "linkedin": "jos%C3%A9-maria-prc-pombal/;;;ianyhwu/;seungone-kim-09b551264/;;;",
        "or_profile": "~Jos\u00e9_Pombal1;~Dongkeun_Yoon1;~Patrick_Fernandes1;~Ian_Wu3;~Seungone_Kim1;~Ricardo_Rei1;~Graham_Neubig1;~Andre_Martins1",
        "aff": "Instituto Superior T\u00e9cnico+Unbabel;Korea Advanced Institute of Science & Technology;School of Computer Science, Carnegie Mellon University+Instituto Superior T\u00e9cnico;Carnegie Mellon University+C3 AI;Carnegie Mellon University+Meta Facebook;;Carnegie Mellon University;Instituto Superior T\u00e9cnico+Unbabel",
        "aff_domain": "tecnico.ulisboa.pt+unbabel.com;kaist.ac.kr;cs.cmu.edu+tecnico.ulisboa.pt;cmu.edu+c3.ai;cmu.edu+meta.com;;cmu.edu;tecnico.ulisboa.pt+unbabel.com",
        "position": "PhD student+Researcher;PhD student;PhD student+PhD student;PhD student+Researcher;PhD student+Intern;;Associate Professor;Associate Professor+Research Scientist",
        "bibtex": "@inproceedings{\npombal2025mprometheus,\ntitle={M-Prometheus: A Suite of Open Multilingual {LLM} Judges},\nauthor={Jos{\\'e} Pombal and Dongkeun Yoon and Patrick Fernandes and Ian Wu and Seungone Kim and Ricardo Rei and Graham Neubig and Andre Martins},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Atyk8lnIQQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "n3M7;oHAe;Aagm;P6dq",
        "site": "https://openreview.net/forum?id=Atyk8lnIQQ",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;4;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7071067811865476
    },
    {
        "id": "AwRFhS5grK",
        "title": "Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) are typically multilingual due to pretraining on diverse multilingual corpora. But can these models relate corresponding concepts across languages, i.e., be crosslingual? This study evaluates state-of-the-art LLMs on inherently crosslingual tasks. We observe that while these models show promising surface-level crosslingual abilities on machine translation and embedding space analyses, they struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain-specific (Harry Potter quiz and TOFU benchmark) contexts. Since simple inference-time mitigation methods offer only limited improvement, we propose fine-tuning of LLMs on mixed-language data, which effectively reduces these gaps, even when using out-of-domain datasets like WikiText. Our findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs.\nOur code is available at https://github.com/google-research/crosslingual-knowledge-barriers.",
        "keywords": "Large Language Models;Multilingual;Crosslingual Knowledge Barrier",
        "primary_area": "",
        "supplementary_material": "/attachment/616e1b96cbe3d33d5f1d781241b1a6de30c0b1e3.zip",
        "author": "Lynn Chua;Badih Ghazi;Yangsibo Huang;Pritish Kamath;Ravi Kumar;Pasin Manurangsi;Amer Sinha;Chulin Xie;Chiyuan Zhang",
        "authorids": "~Lynn_Chua1;~Badih_Ghazi1;~Yangsibo_Huang2;~Pritish_Kamath2;~Ravi_Kumar1;~Pasin_Manurangsi2;~Amer_Sinha1;~Chulin_Xie1;~Chiyuan_Zhang1",
        "gender": "F;;F;M;M;M;M;F;M",
        "homepage": ";https://sites.google.com/view/badihghazi/home;https://hazelsuko07.github.io/yangsibo/;https://pritishkamath.github.io/;https://sites.google.com/site/ravik53/;https://pasin30055.github.io/;;;http://pluskid.org",
        "dblp": "143/4392;125/2134;;https://dblp.org/pers/k/Kamath:Pritish.html;k/RaviKumar.html;133/2059;;245/4284;21/8315",
        "google_scholar": "D2SXVSYAAAAJ;GBJLTN8AAAAJ;NMPUDa0AAAAJ;1JFARhUAAAAJ;J_XhIsgAAAAJ;35hM-PkAAAAJ;;WeJnzAgAAAAJ;l_G2vr0AAAAJ",
        "orcid": ";;;;0000-0002-2203-2586;;;;",
        "linkedin": "chua-lynn/;badih-ghazi-608379132/;;;ravi-kumar-a3a9631;;amersinha/;;",
        "or_profile": "~Lynn_Chua1;~Badih_Ghazi1;~Yangsibo_Huang2;~Pritish_Kamath2;~Ravi_Kumar1;~Pasin_Manurangsi2;~Amer_Sinha1;~Chulin_Xie1;~Chiyuan_Zhang1",
        "aff": "Google;Google;Google;Google Research;Google;Google;Research, Google;University of Illinois, Urbana Champaign;Google",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;research.google.com;illinois.edu;google.com",
        "position": "Researcher;Researcher;Research Scientist;Research Scientist;Research Scientist;Research Scientist;Researcher;PhD student;Research Scientist",
        "bibtex": "@inproceedings{\nchua2025crosslingual,\ntitle={Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models},\nauthor={Lynn Chua and Badih Ghazi and Yangsibo Huang and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Amer Sinha and Chulin Xie and Chiyuan Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=AwRFhS5grK}\n}",
        "github": "",
        "project": "",
        "reviewers": "ar6x;jfyw;riAs;JPW1",
        "site": "https://openreview.net/forum?id=AwRFhS5grK",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.17407765595569782
    },
    {
        "id": "B5E3ijlLML",
        "title": "Exposing and Patching the Flaws of Large Language Models in Social Character Simulation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) are increasingly used for social character simulations, enabling applications in role-playing agents and Computational Social Science (CSS). However, their inherent flaws\u2014such as inconsistencies in simulated roles\u2014raise concerns about their reliability and trustworthiness. In this paper, we systematically investigate these flaws and explore potential solutions. To assess the reliability of LLM-based simulations, we introduce TrustSim, a benchmark dataset covering 10 CSS-related topics. Through experiments on 14 LLMs, we uncover persistent inconsistencies in simulated roles and find that higher general model performance does not necessarily correlate with greater simulation reliability. To mitigate these flaws, we propose Adaptive Learning Rate Based ORPO (AdaORPO), a reinforcement learning-based algorithm that improves simulation consistency across seven LLMs. Our study not only exposes critical weaknesses in LLM-driven social character simulations but also offers a pathway toward more robust and trustworthy simulations, laying the foundation for future advancements in this field.",
        "keywords": "Social simulation;Large language model;reliability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yue Huang;Zhengqing Yuan;Yujun Zhou;Kehan Guo;Xiangqi Wang;Haomin Zhuang;Weixiang Sun;Lichao Sun;Jindong Wang;Yanfang Ye;Xiangliang Zhang",
        "authorids": "~Yue_Huang9;~Zhengqing_Yuan2;~Yujun_Zhou1;~Kehan_Guo1;~Xiangqi_Wang1;~Haomin_Zhuang1;~Weixiang_Sun1;~Lichao_Sun1;~Jindong_Wang4;~Yanfang_Ye1;~Xiangliang_Zhang1",
        "gender": ";M;M;M;M;;M;M;M;;F",
        "homepage": "https://howiehwong.github.io/;https://dlyuangod.github.io/zhengqingyuan/;https://yujunzhou.github.io/;https://kehanguo2.github.io/KehanGuo/;https://xiangqiwang77.github.io/mainpage/;https://zhmzm.github.io/;https://weixiang-sun.github.io/;https://lichao-sun.github.io/;https://jd92.wang/;http://yes-lab.org/;https://sites.nd.edu/xiangliang-zhang/",
        "dblp": ";;162/3265-2;;;344/1798;;121/0780-1.html;19/2969-1;;74/1890-1",
        "google_scholar": "HvzvvqQAAAAJ;dTu07l8AAAAJ;t0c7rQQAAAAJ;t8iRCLUAAAAJ;https://scholar.google.com/citations?view_op=list_works;vXllNroAAAAJ;zizf0i0AAAAJ;WhGUE7AAAAAJ;hBZ_tKsAAAAJ;egjr888AAAAJ;BhRJe4wAAAAJ",
        "orcid": "0000-0002-8315-7972;;0000-0003-1376-5187;;;;;;0000-0002-4833-0880;;0000-0002-3574-5665",
        "linkedin": ";;yujun-zhou-zyj/;kehan98/;;;;lichao-sun-b273a290/;jindong-wang/;;",
        "or_profile": "~Yue_Huang9;~Zhengqing_Yuan2;~Yujun_Zhou1;~Kehan_Guo1;~Xiangqi_Wang1;~Haomin_Zhuang1;~Weixiang_Sun1;~Lichao_Sun1;~Jindong_Wang4;~Yanfang_Ye1;~Xiangliang_Zhang1",
        "aff": "University of Notre Dame+IBM Research;University of Notre Dame;University of Notre Dame;University of Notre Dame;University of Notre Dame;University of Notre Dame;University of Notre Dame+Northeastern University;Lehigh University;William & Mary;University of Notre Dame;University of Notre Dame",
        "aff_domain": "nd.edu+us.ibm.com;nd.edu;nd.edu;nd.edu;nd.edu;nd.edu;nd.edu+neu.edu.cn;lehigh.edu;wm.edu;nd.edu;nd.edu",
        "position": "PhD student+Intern;PhD student;PhD student;PhD student;PhD student;PhD student;PhD student+Undergrad student;Assistant Professor;Assistant Professor;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nhuang2025exposing,\ntitle={Exposing and Patching the Flaws of Large Language Models in Social Character Simulation},\nauthor={Yue Huang and Zhengqing Yuan and Yujun Zhou and Kehan Guo and Xiangqi Wang and Haomin Zhuang and Weixiang Sun and Lichao Sun and Jindong Wang and Yanfang Ye and Xiangliang Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=B5E3ijlLML}\n}",
        "github": "",
        "project": "",
        "reviewers": "2Gns;Lamc;vTuT",
        "site": "https://openreview.net/forum?id=B5E3ijlLML",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "BLonuGXDFu",
        "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Achieving robust textual comprehension and in-context learning requires language models capable of interpreting entire document contexts. However, scaling these models directly to long contexts remains technically challenging, prompting a surge of \u201cextension\u201d strategies. To date, rigorous comparisons among these approaches have been complicated by inconsistent base models, training data, and evaluation metrics, limiting our understanding of how long-context performance may differ from standard benchmarks.\nIn this work, we introduce a controlled extension protocol and a standardized evaluation pipeline, enabling an apples-to-apples comparison across diverse long-context methods. Through extensive experiments, we uncover three key insights: \n(1) perplexity emerges as a helpful (albeit imperfect) indicator for gauging model quality on lengthy-context tasks, \n(2) approximate attention mechanisms exhibit systematic performance deficits on long-context benchmarks, \nand (3) exact fine-tuning remains robust within its extension range, although extrapolation beyond that range continues to pose challenges.\nAll codebases, trained models, and checkpoints will be released, fostering transparency and accelerating progress in this critical area of AI research. Our results not only help clarify the current landscape of long-context modeling but also offer guidance for building more capable, context-aware language models.",
        "keywords": "Controlled Study;Long Context;Extension;Benchmark;Analysis",
        "primary_area": "",
        "supplementary_material": "/attachment/7196b614579be1cfe679bde4ae69ad6ad3203024.zip",
        "author": "Yi Lu;Jing Nathan Yan;Songlin Yang;Justin T Chiu;Siyu Ren;Fei Yuan;Wenting Zhao;Zhiyong Wu;Alexander M Rush",
        "authorids": "~Yi_Lu7;~Jing_Nathan_Yan1;~Songlin_Yang1;~Justin_T_Chiu1;~Siyu_Ren1;~Fei_Yuan2;~Wenting_Zhao1;~Zhiyong_Wu3;~Alexander_M_Rush1",
        "gender": ";;F;;M;;;;M",
        "homepage": ";https://nathanyanjing.github.io/;https://sustcsonglin.github.io;;https://drsy.github.io/;;;;http://rush.seas.harvard.edu/",
        "dblp": ";;;;;;41/10049-2.html;;http://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=",
        "google_scholar": ";;1chlis0AAAAJ;;jkJDyrkAAAAJ;;sycHskQAAAAJ;;LIjnUGgAAAAJ",
        "orcid": ";;;;;;;;0000-0002-9900-1606",
        "linkedin": ";;;;;;;;sasha-rush-a69b6917/",
        "or_profile": "~Yi_Lu7;~Jing_Nathan_Yan1;~Songlin_Yang1;~Justin_T_Chiu1;~Siyu_Ren1;~Fei_Yuan2;~Wenting_Zhao1;~Zhiyong_Wu3;~Alexander_M_Rush1",
        "aff": ";Google+Cornell University;Massachusetts Institute of Technology;;Meituan;;Cornell University;;Cornell University+School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": ";google.com+cornell.edu;mit.edu;;meituan.com;;cornell.edu;;cornell.edu+seas.harvard.edu",
        "position": ";Researcher+PhD student;PhD student;;Researcher;;PhD student;;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nlu2025a,\ntitle={A Controlled Study on Long Context Extension and Generalization in {LLM}s},\nauthor={Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M Rush},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=BLonuGXDFu}\n}",
        "github": "",
        "project": "",
        "reviewers": "hLQf;uEtu;RShr;LDP3",
        "site": "https://openreview.net/forum?id=BLonuGXDFu",
        "pdf_size": 0,
        "rating": "4;6;6;8",
        "confidence": "3;4;3;5",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8528028654224418
    },
    {
        "id": "BM192Ps5Nv",
        "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.",
        "keywords": "Quantization;Reasoning;Large Language models;Accuracy",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruikang Liu;Yuxuan Sun;Manyi Zhang;Haoli Bai;Xianzhi Yu;Tiezheng YU;Chun Yuan;Lu Hou",
        "authorids": "~Ruikang_Liu1;~Yuxuan_Sun4;~Manyi_Zhang2;~Haoli_Bai2;~Xianzhi_Yu1;~Tiezheng_YU1;~Chun_Yuan1;~Lu_Hou2",
        "gender": "M;;F;M;M;M;M;",
        "homepage": "https://github.com/ruikangliu;https://github.com/snowdusky;https://www.sigs.tsinghua.edu.cn/;https://haolibai.github.io;https://yuxianzhi.github.io;https://tysonyu.github.io;https://www.sigs.tsinghua.edu.cn/fg3/105064.jhtml;",
        "dblp": "253/3082.html;;;195/9712;259/5254.html;;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=zh-CN;;;tGnJRYQAAAAJ;https://scholar.google.com.hk/citations?user=JK7nNekAAAAJ;https://scholar.google.com.hk/citations?user=fYdxi2sAAAAJ;",
        "orcid": ";;;;;;;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Ruikang_Liu1;~Yuxuan_Sun4;~Manyi_Zhang2;~Haoli_Bai2;~Xianzhi_Yu1;~Tiezheng_YU1;~Chun_Yuan1;~Lu_Hou2",
        "aff": "Tsinghua University;Meituan+Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Hong Kong University of Science and Technology;Tsinghua University;",
        "aff_domain": "tsinghua.edu.cn;meituan.com+huawei.com;huawei.com;huawei.com;huawei.com;ust.hk;tsinghua.edu.cn;",
        "position": "MS student;Researcher+Researcher;Researcher;Researcher;Researcher;PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nliu2025quantization,\ntitle={Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models},\nauthor={Ruikang Liu and Yuxuan Sun and Manyi Zhang and Haoli Bai and Xianzhi Yu and Tiezheng YU and Chun Yuan and Lu Hou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=BM192Ps5Nv}\n}",
        "github": "",
        "project": "",
        "reviewers": "kP3E;v9cz;QCfM;xjtt",
        "site": "https://openreview.net/forum?id=BM192Ps5Nv",
        "pdf_size": 0,
        "rating": "6;6;8;9",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            1.299038105676658
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.19245008972987526
    },
    {
        "id": "Bs5Jb285qv",
        "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 85\\% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems.",
        "keywords": "inverse reinforcement learning;reinforcement learning with human feedback;LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jared Joselowitz;Ritam Majumdar;Arjun Jagota;Matthieu Bou;Nyal Patel;Satyapriya Krishna;Sonali Parbhoo",
        "authorids": "~Jared_Joselowitz2;~Ritam_Majumdar2;~Arjun_Jagota1;~Matthieu_Bou1;~Nyal_Patel1;~Satyapriya_Krishna2;~Sonali_Parbhoo2",
        "gender": ";;M;M;M;M;",
        "homepage": ";;;;;http://satyapriyakrishna.com/;",
        "dblp": ";;;;;251/9225;",
        "google_scholar": ";OR0l6-sAAAAJ;;;;Q5bfPlkAAAAJ;FwEz5s4AAAAJ",
        "orcid": ";;;;;;",
        "linkedin": "jaredjoselowitz/;;arjun-jagota-884927129?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;matthieu-bou/;nyal-patel/;satyapriya-krishna-50553084/;",
        "or_profile": "~Jared_Joselowitz2;~Ritam_Majumdar2;~Arjun_Jagota1;~Matthieu_Bou1;~Nyal_Patel1;~Satyapriya_Krishna2;~Sonali_Parbhoo2",
        "aff": "Ufonia;Imperial College London;Imperial College London;Imperial College London;Imperial College London;Harvard University;Imperial College London+Harvard University",
        "aff_domain": "ufonia.com;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;harvard.edu;imperial.ac.uk+harvard.edu",
        "position": "Engineer;PhD student;Researcher;MS student;MS student;PhD student;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\njoselowitz2025insights,\ntitle={Insights from the Inverse: Reconstructing {LLM} Training Goals Through Inverse Reinforcement Learning},\nauthor={Jared Joselowitz and Ritam Majumdar and Arjun Jagota and Matthieu Bou and Nyal Patel and Satyapriya Krishna and Sonali Parbhoo},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Bs5Jb285qv}\n}",
        "github": "",
        "project": "",
        "reviewers": "2sxD;rmRq;q1ub;di3L",
        "site": "https://openreview.net/forum?id=Bs5Jb285qv",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "BuXZtHTefA",
        "title": "The Negation Bias in Large Language Models: Investigating bias reflected in linguistic markers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models trained on large-scale uncontrolled corpora often encode stereotypes and biases, which can be displayed through harmful text generation or biased associations. However, do they also pick up subtler linguistic patterns that can potentially reinforce and communicate biases and stereotypes, as humans do? We aim to bridge theoretical insights from social science with bias research in NLP by designing controlled, theoretically motivated LLM experiments to elicit this type of bias. Our case study is negation bias, the bias that humans have towards using negation to describe situations that challenge common stereotypes. We construct an evaluation dataset containing negated and affirmed stereotypical and anti-stereotypical sentences and evaluate the performance of eight language models using perplexity as a metric for measuring model surprisal. We find that the autoregressive decoder models in our experiment exhibit this bias, while we do not find evidence for it among the stacked encoder models.",
        "keywords": "bias;evaluation;perplexity;fairness;negation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yishan Wang;Pia Sommerauer;Jelke Bloem",
        "authorids": "~Yishan_Wang1;~Pia_Sommerauer1;~Jelke_Bloem1",
        "gender": "F;F;M",
        "homepage": ";https://piasommerauer.github.io/;https://www.uva.nl/profiel/b/l/j.bloem/j.bloem.html",
        "dblp": ";220/0959;151/8437.html",
        "google_scholar": ";5UxZbeAAAAAJ;https://scholar.google.nl/citations?user=Eoc5JrgAAAAJ",
        "orcid": ";0000-0003-3593-1465;0000-0003-2221-0554",
        "linkedin": "askyishan/;;jelke-bloem-2a3a5a16",
        "or_profile": "~Yishan_Wang1;~Pia_Sommerauer1;~Jelke_Bloem1",
        "aff": "Eindhoven University of Technology;Vrije Universiteit Amsterdam;University of Amsterdam",
        "aff_domain": "tue.nl;vu.nl;uva.nl",
        "position": "PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025the,\ntitle={The Negation Bias in Large Language Models: Investigating bias reflected in linguistic markers},\nauthor={Yishan Wang and Pia Sommerauer and Jelke Bloem},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=BuXZtHTefA}\n}",
        "github": "",
        "project": "",
        "reviewers": "Ykig;qKBW;iwy7;gxew",
        "site": "https://openreview.net/forum?id=BuXZtHTefA",
        "pdf_size": 0,
        "rating": "5;7;7;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "C5mb473GMY",
        "title": "Resource-efficient Inference with Foundation Model Programs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The inference-time resource costs of large language and vision models present a growing challenge in production deployments. \nWe propose the use of ***foundation model programs***, i.e., programs that can invoke foundation models with varying resource costs and performance, as an approach to this problem. Specifically, we present a method that translates a task into a program, then learns a policy for resource allocation that, on each input, selects foundation model \"backends\" for each program module. The policy uses smaller, cheaper backends to handle simpler subtasks, while allowing more complex subtasks to leverage larger, more capable models. We evaluate the method on two new \"streaming\" visual question-answering tasks in which a system answers a question on a sequence of inputs, receiving ground-truth feedback after each answer. Compared to monolithic multi-modal models, our implementation achieves up to 98\\% resource savings with minimal accuracy loss, demonstrating its potential for scalable and resource-efficient multi-modal inference. The source code and the benchmarks are available at [GitHub](https://github.com/Flitternie/FMProgramming).",
        "keywords": "Neurosymblic Programming;Multimodal LMs;Agent Programming;LLM Computational Efficiency;Multi-modal Reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lunyiu Nie;Zhimin Ding;Kevin Yu;Marco Cheung;Chris Jermaine;Swarat Chaudhuri",
        "authorids": "~Lunyiu_Nie1;~Zhimin_Ding1;~Kevin_Yu4;~Marco_Cheung1;~Chris_Jermaine1;~Swarat_Chaudhuri2",
        "gender": "M;M;M;M;M;",
        "homepage": "https://flitternie.github.io/;;;;https://www.cs.rice.edu/~cmj4/;",
        "dblp": "320/5241;;;;j/ChrisJermaine;",
        "google_scholar": "IwXgGTsAAAAJ;MZWEfNgAAAAJ;;;;",
        "orcid": ";;;;;",
        "linkedin": ";;kevinyu3;marco-cheung-031054313/;;",
        "or_profile": "~Lunyiu_Nie1;~Zhimin_Ding1;~Kevin_Yu4;~Marco_Cheung1;~Chris_Jermaine1;~Swarat_Chaudhuri2",
        "aff": "University of Texas at Austin;Rice University;, University of Texas at Austin;University of Texas at Austin;Rice University;",
        "aff_domain": "utexas.edu;rice.edu;cs.utexas.edu;utexas.edu;rice.edu;",
        "position": "PhD student;PhD student;Undergrad student;Undergrad student;Professor;",
        "bibtex": "@inproceedings{\nnie2025resourceefficient,\ntitle={Resource-efficient Inference with Foundation Model Programs},\nauthor={Lunyiu Nie and Zhimin Ding and Kevin Yu and Marco Cheung and Chris Jermaine and Swarat Chaudhuri},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=C5mb473GMY}\n}",
        "github": "",
        "project": "",
        "reviewers": "MUgH;PfjH;3qCW;Y93Q",
        "site": "https://openreview.net/forum?id=C5mb473GMY",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "CB3CeOWo0J",
        "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recognizing the information flows and operations comprising data science and machine learning Python notebooks is critical for evaluating, reusing, and adapting notebooks for new tasks. Investigating a notebook via re-execution often is impractical due to the challenges of resolving data and software dependencies. While Large Language Models (LLMs) pre-trained on large codebases have demonstrated effectiveness in understanding code without running it, we observe that they fail to understand some realistic notebooks due to hallucinations and long-context challenges. To address these issues, we propose a notebook understanding task yielding an information flow graph and corresponding cell execution dependency graph for a notebook, and demonstrate the effectiveness of a pincer strategy that uses limited syntactic analysis to assist full comprehension of the notebook using an LLM. Our Capture and Resolve Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O set$\\textemdash$the flows of information into or out of cells via variables$\\textemdash$then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning, thereby identifying the true data inputs and outputs of each cell. We evaluate and demonstrate the effectiveness of our approach using an annotated dataset of 50 representative, highly up-voted Kaggle notebooks that together represent 3454 actual cell inputs and outputs. The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves average $F_1$ scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies. Moreover, 37 out of the 50 (74%) individual information flow graphs and 41 out of 50 (82%) cell execution dependency graphs match the ground truth exactly.",
        "keywords": "notebook understanding;LLM;YesWorkflow;data flow;provenance",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Meng Li;Timothy M. McPhillips;Dingmin Wang;Shin-Rong Tsai;Bertram Lud\u00e4scher",
        "authorids": "~Meng_Li25;~Timothy_M._McPhillips1;~Dingmin_Wang1;~Shin-Rong_Tsai1;~Bertram_Lud\u00e4scher1",
        "gender": "F;;M;F;",
        "homepage": ";;http://www.dingmin.wang;https://cindytsai.github.io/;",
        "dblp": ";;206/1677;;",
        "google_scholar": ";;zpidC7IAAAAJ;;",
        "orcid": "0000-0002-2542-2141;;;0000-0003-4635-6259;",
        "linkedin": ";;dimmy0302/;shin-rong-tsai-80854924a/;",
        "or_profile": "~Meng_Li25;~Timothy_M._McPhillips1;~Dingmin_Wang1;~Shin-Rong_Tsai1;~Bertram_Lud\u00e4scher1",
        "aff": "University of Illinois, Urbana Champaign;;Amazon;University of Illinois, Urbana Champaign;",
        "aff_domain": "illinois.edu;;amazon.com;illinois.edu;",
        "position": "PhD student;;Applied Scientist;Researcher;",
        "bibtex": "@inproceedings{\nli2025crabs,\ntitle={{CRABS}: A syntactic-semantic pincer strategy for bounding {LLM} interpretation of Python notebooks},\nauthor={Meng Li and Timothy M. McPhillips and Dingmin Wang and Shin-Rong Tsai and Bertram Lud{\\\"a}scher},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CB3CeOWo0J}\n}",
        "github": "",
        "project": "",
        "reviewers": "dByL;TYCf;TcJh;rU2s",
        "site": "https://openreview.net/forum?id=CB3CeOWo0J",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "CJ2FmPmoDE",
        "title": "Efficient Process Reward Model Training via Active Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. \nTo address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs.\nDuring training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss w.r.t. the labels and update the PRM\u2019s weights.\nWe compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduce 50\\% annotation, but achieving the comparable or even better performance. \nBeyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60\\% of the data.\nA subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0\\%) and PRMBench (65.5\\%) compared with same sized models.",
        "keywords": "Process Reward Model;Active Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Keyu Duan;Zichen Liu;Xin Mao;Tianyu Pang;Changyu Chen;Qiguang Chen;Michael Qizhe Shieh;Longxu Dou",
        "authorids": "~Keyu_Duan1;~Zichen_Liu1;~Xin_Mao3;~Tianyu_Pang1;~Changyu_Chen2;~Qiguang_Chen1;~Michael_Qizhe_Shieh1;~Longxu_Dou1",
        "gender": "M;;M;M;M;M;;M",
        "homepage": "https://kduan.live;;;https://p2333.github.io/;;https://scholar.google.com/citations?user=8j8AfF0AAAAJ;;https://longxudou.github.io/",
        "dblp": ";;;202/2550;;292/9953;;229/2829",
        "google_scholar": "fGW4ClMAAAAJ;;-PqtbtoAAAAJ;wYDbtFsAAAAJ;https://scholar.google.com.sg/citations?hl=en;8j8AfF0AAAAJ;;flgPmvkAAAAJ",
        "orcid": "0000-0002-1902-5545;;;0000-0003-0639-6176;;0000-0002-9154-7858;;",
        "linkedin": ";;;%E5%A4%A9%E5%AE%87-%E5%BA%9E-b3999017a/;;;;longxu-dou-6b167410a/",
        "or_profile": "~Keyu_Duan1;~Zichen_Liu1;~Xin_Mao3;~Tianyu_Pang1;~Changyu_Chen2;~Qiguang_Chen1;~Michael_Qizhe_Shieh1;~Longxu_Dou1",
        "aff": "national university of singaore, National University of Singapore;;ByteDance Inc.+NTU;Sea AI Lab;Singapore Management University;Harbin Institute of Technology;;Sea AI Lab",
        "aff_domain": "u.nus.edu;;bytedance.com+ntu.sg;sea.com;smu.edu.sg;hit.edu.cn;;sea.com",
        "position": "PhD student;;Researcher+Postdoc;Senior Research Scientist;PhD student;PhD student;;Researcher",
        "bibtex": "@inproceedings{\nduan2025efficient,\ntitle={Efficient Process Reward Model Training via Active Learning},\nauthor={Keyu Duan and Zichen Liu and Xin Mao and Tianyu Pang and Changyu Chen and Qiguang Chen and Michael Qizhe Shieh and Longxu Dou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CJ2FmPmoDE}\n}",
        "github": "",
        "project": "",
        "reviewers": "YDbb;p2Cc;hjfr;ZZ8c",
        "site": "https://openreview.net/forum?id=CJ2FmPmoDE",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;2;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8181818181818182
    },
    {
        "id": "CNWlNF8VOm",
        "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Membership inference attacks serves as useful tool for fair use of language models, such as detecting potential copyright infringement and auditing data leakage. However, many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4. In this work, we introduce N-Gram Coverage Attack, a membership inference attack that relies **solely** on text outputs from the target model, enabling attacks on completely black-box models. We leverage the observation that models are more likely to memorize and subsequently generate text patterns that were commonly observed in their training data. Specifically, to make a prediction on a candidate member, N-Gram Coverage Attack first obtains multiple model generations conditioned on a prefix of the candidate. It then uses n-gram overlap metrics to compute and aggregate the similarities of these outputs with the ground truth suffix; high similarities indicate likely membership. We first demonstrate on a diverse set of existing benchmarks that N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks --- despite having access to only text outputs.  Interestingly, we find that the success rate of our method scales with the attack compute budget --- as we increase the number of sequences generated from the target model conditioned on the prefix, attack performance tends to improve. Having verified the accuracy of our method, we use it to investigate previously unstudied closed OpenAI models on multiple domains. We find that more recent models, such as GPT-4o, exhibit increased robustness to membership inference, suggesting an evolving trend toward improved privacy protections.",
        "keywords": "membership inference;membership inference attack;privacy;memorization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Skyler Hallinan;Jaehun Jung;Melanie Sclar;Ximing Lu;Abhilasha Ravichander;Sahana Ramnath;Yejin Choi;Sai Praneeth Karimireddy;Niloofar Mireshghallah;Xiang Ren",
        "authorids": "~Skyler_Hallinan1;~Jaehun_Jung1;~Melanie_Sclar1;~Ximing_Lu1;~Abhilasha_Ravichander2;~Sahana_Ramnath2;~Yejin_Choi1;~Sai_Praneeth_Karimireddy1;~Niloofar_Mireshghallah1;~Xiang_Ren1",
        "gender": "M;M;F;F;;F;F;M;;M",
        "homepage": "https://skylerhallinan.com/;https://jaehunjung.com;https://msclar.github.io;https://gloriaximinglu.github.io/;https://www.cs.cmu.edu/~aravicha/;;https://yejinc.github.io/;https://spkreddy.org;;https://shanzhenren.github.io/",
        "dblp": "256/6863;192/7707;274/6796;24/10879;170/4795.html;252/5822;89/579-1;217/3342;;36/360-1",
        "google_scholar": "mO_tZ94AAAAJ;_bXzUGEAAAAJ;4uNPtZgAAAAJ;https://scholar.google.com/citations?hl=en;6vLsKGsAAAAJ;YuRzzf0AAAAJ;vhP-tlcAAAAJ;wKJeOQoAAAAJ;;_moJlrIAAAAJ",
        "orcid": ";0000-0002-0292-3074;;;;;;;;",
        "linkedin": "skyler-hallinan/;;melanie-sclar-077047b5/;;abhilasha-ravichander-57524958;;;;;xren7",
        "or_profile": "~Skyler_Hallinan1;~Jaehun_Jung1;~Melanie_Sclar1;~Ximing_Lu1;~Abhilasha_Ravichander2;~Sahana_Ramnath2;~Yejin_Choi1;~Sai_Praneeth_Karimireddy1;~Niloofar_Mireshghallah1;~Xiang_Ren1",
        "aff": "University of Southern California;University of Washington;Meta Facebook+University of Washington, Seattle;University of Washington;University of Washington+School of Computer Science, Carnegie Mellon University;University of Southern California;Computer Science Department, Stanford University+NVIDIA;University of Southern California;;University of Southern California",
        "aff_domain": "usc.edu;uw.edu;meta.com+uw.edu;cs.washington.edu;uw.edu+cs.cmu.edu;usc.edu;cs.stanford.edu+nvidia.com;usc.edu;;usc.edu",
        "position": "PhD student;PhD student;Researcher+PhD student;PhD student;Postdoc+MS student;PhD student;Full Professor+Researcher;Assistant Professor;;Associate Professor",
        "bibtex": "@inproceedings{\nhallinan2025the,\ntitle={The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage},\nauthor={Skyler Hallinan and Jaehun Jung and Melanie Sclar and Ximing Lu and Abhilasha Ravichander and Sahana Ramnath and Yejin Choi and Sai Praneeth Karimireddy and Niloofar Mireshghallah and Xiang Ren},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CNWlNF8VOm}\n}",
        "github": "",
        "project": "",
        "reviewers": "avj5;Yujw;zvq9",
        "site": "https://openreview.net/forum?id=CNWlNF8VOm",
        "pdf_size": 0,
        "rating": "4;6;9",
        "confidence": "5;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            2.0548046676563256
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            34,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3973597071195132
    },
    {
        "id": "CODs4jSGhN",
        "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training framework that leverages mutual enhancement between a white-box small language model (SLM) and a black-box large language model (LLM) for RAG. Specifically, the SLM decomposes complex queries into simpler sub-questions, thus enhancing the accuracy of the retrieval and facilitating more effective reasoning by the black-box LLM. Concurrently, the black-box LLM provides feedback signals to improve the SLM's decomposition capability. We observe that Collab-RAG relies solely on supervision from an affordable black-box LLM without additional distillation from frontier LLMs, yet demonstrates strong generalization across multiple black-box LLMs. Experimental evaluations across five multi-hop QA datasets demonstrate that Collab-RAG substantially outperforms existing black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition, highlighting the efficiency of Collab-RAG in improving reasoning and retrieval for complex questions. Our implementation is available at \\url{https://github.com/ritaranx/Collab-RAG/}",
        "keywords": "large language model;retrieval augmented generation;complex question answering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ran Xu;Wenqi Shi;Yuchen Zhuang;Yue Yu;Joyce C. Ho;Haoyu Wang;Carl Yang",
        "authorids": "~Ran_Xu4;~Wenqi_Shi1;~Yuchen_Zhuang1;~Yue_Yu2;~Joyce_C._Ho1;~Haoyu_Wang6;~Carl_Yang1",
        "gender": "F;F;M;M;;M;M",
        "homepage": "https://ritaranx.github.io/;https://profiles.utsouthwestern.edu/profile/231462/wenqi-shi.html;https://night-chen.github.io/;https://yueyu1030.github.io;;https://wanghaoyu0408.github.io/;https://cs.emory.edu/~jyang71/",
        "dblp": "71/1270-2;16/4475;191/5231.html;;;50/8499-4;305/0254",
        "google_scholar": "mcC5NzwAAAAJ;4qkrZTAAAAAJ;T-f6XlEAAAAJ;zQ3Jh6UAAAAJ;;https://scholar.google.com.hk/citations?user=5Lw9_jcAAAAJ;mOINlwcAAAAJ",
        "orcid": ";0000-0001-8972-7342;;0000-0002-3683-5208;;0000-0001-7485-6213;0000-0001-9145-4531",
        "linkedin": "ran-rita-xu-4568a9159/;;;;;;",
        "or_profile": "~Ran_Xu4;~Wenqi_Shi1;~Yuchen_Zhuang1;~Yue_Yu2;~Joyce_C._Ho1;~Haoyu_Wang6;~Carl_Yang1",
        "aff": "Emory University+Tencent AI Lab+Google;University of Texas Southwestern Medical Center;Google DeepMind+Georgia Institute of Technology;Meta;;State University of New York at Albany;Emory University",
        "aff_domain": "emory.edu+tencent.com+google.com;utsouthwestern.edu;google.com+gatech.edu;meta.com;;albany.edu;emory.edu",
        "position": "PhD student+Intern+Intern;Assistant Professor;Researcher+PhD student;Researcher;;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025collabrag,\ntitle={Collab-{RAG}: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box {LLM} Collaboration},\nauthor={Ran Xu and Wenqi Shi and Yuchen Zhuang and Yue Yu and Joyce C. Ho and Haoyu Wang and Carl Yang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CODs4jSGhN}\n}",
        "github": "",
        "project": "",
        "reviewers": "k4rg;vqF1;YbMU;sLb3;j6Ne",
        "site": "https://openreview.net/forum?id=CODs4jSGhN",
        "pdf_size": 0,
        "rating": "6;6;7;7;7",
        "confidence": "3;4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.6,
            0.48989794855663565
        ],
        "confidence_avg": [
            4.0,
            0.6324555320336759
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6454972243679027
    },
    {
        "id": "CPJ9EAeYfd",
        "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question:  can we use MLA\u2019s benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4\u00d7 compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6\u00d7 compression have less than 0.1\\% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at \\url{https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models}.",
        "keywords": "MLA;Multi-head Attention;LLM;Efficient",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Guihong Li;Mehdi Rezagholizadeh;Mingyu Yang;Vikram Appia;Emad Barsoum",
        "authorids": "~Guihong_Li1;~Mehdi_Rezagholizadeh1;~Mingyu_Yang5;~Vikram_Appia1;~Emad_Barsoum1",
        "gender": "Unspecified;M;M;M;",
        "homepage": "https://liguihong.github.io/;;https://mingyuyng.github.io/;;",
        "dblp": "143/6649.html;;;;",
        "google_scholar": ";MvXlF6kAAAAJ;xF9-nI0AAAAJ;WvtDA4MAAAAJ;",
        "orcid": "0000-0001-8537-8632;;0000-0003-1301-6493;;",
        "linkedin": ";;;;",
        "or_profile": "~Guihong_Li1;~Mehdi_Rezagholizadeh1;~Mingyu_Yang5;~Vikram_Appia1;~Emad_Barsoum1",
        "aff": "Advanced Micro Devices;Advanced Micro Devices+Huawei Technologies Ltd.;University of Michigan - Ann Arbor;Advanced Micro Devices;",
        "aff_domain": "amd.com;amd.com+huawei.com;umich.edu;amd.com;",
        "position": "Researcher;Principal Researcher+Principal Researcher;PhD student;Researcher;",
        "bibtex": "@inproceedings{\nli2025xecomla,\ntitle={X-Eco{MLA}: Upcycling Pre-Trained Attention into {MLA} for Efficient and Extreme {KV} Compression},\nauthor={Guihong Li and Mehdi Rezagholizadeh and Mingyu Yang and Vikram Appia and Emad Barsoum},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CPJ9EAeYfd}\n}",
        "github": "",
        "project": "",
        "reviewers": "5zCs;VttQ;YRgt",
        "site": "https://openreview.net/forum?id=CPJ9EAeYfd",
        "pdf_size": 0,
        "rating": "4;5;6",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            5.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "CYiXNIQegF",
        "title": "Correctness-Guaranteed Code Generation via Constrained Decoding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language Models (LMs) are increasingly being used for code generation, but ensuring the correctness of generated programs remains a significant challenge. Although imperfect code may be acceptable during software development with human oversight, domains such as video games and robotics require one-shot correctness for runtime-critical components. We present a constrained decoding algorithm for generating semantically correct programs that incorporates a context-sensitive parser, which, at each step, outputs a regular expression that satisfies a critical non-extensible property to guide the generation of the next token sequence that can continue to a correct program. To build such a context-sensitive parser, we propose a framework of a dynamic tree of parsers (ToP) during parsing, where each parser corresponds to a modular context-free grammar enriched with contextual information such as variable scopes and type constraints, with tree branches representing ambiguity in the future code segment. We demonstrate our approach through sLua, a strongly typed variant of Lua, showing that our method can generate semantically correct programs conforming to any prescribed scripting API. We further show that, with careful design, our semantic guarantees extend to runtime correctness, as validated in the application of generating game mechanics for a roguelike video game.",
        "keywords": "code generation;constrained decoding;correctness;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lingxiao Li;salar rahili;Yiwei Zhao",
        "authorids": "~Lingxiao_Li1;~salar_rahili1;~Yiwei_Zhao1",
        "gender": "M;M;M",
        "homepage": "http://people.csail.mit.edu/lingxiao/;;",
        "dblp": ";;",
        "google_scholar": ";7WMhnzAAAAAJ;FiMEAdYAAAAJ",
        "orcid": ";;",
        "linkedin": ";;evan-yiwei-zhao-18584a105/",
        "or_profile": "~Lingxiao_Li1;~salar_rahili1;~Yiwei_Zhao1",
        "aff": "Netflix;NetFlix;NetFlix",
        "aff_domain": "netflix.com;netflix.com;netflix.com",
        "position": "Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nli2025correctnessguaranteed,\ntitle={Correctness-Guaranteed Code Generation via Constrained Decoding},\nauthor={Lingxiao Li and salar rahili and Yiwei Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CYiXNIQegF}\n}",
        "github": "",
        "project": "",
        "reviewers": "b3mK;kWD4;GqNm;YK73",
        "site": "https://openreview.net/forum?id=CYiXNIQegF",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "3;4;2;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.0909090909090909
    },
    {
        "id": "CaWkEqUjxs",
        "title": "Transformers are Efficient Compilers, Provably",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Transformer-based large language models (LLMs) have demonstrated surprisingly robust performance across a wide range of language-related tasks, including programming language understanding and generation. In this paper, we take the first steps towards a formal investigation of using transformers as compilers from an expressive power perspective. To this end, we introduce a representative programming language, **Mini-Husky**, which encapsulates key features of modern C-like languages. We show that if the input code sequence has a bounded depth in both the Abstract Syntax Tree (AST) and type inference (reasonable assumptions based on the *clean code principle*), then the number of parameters required by transformers depends only on the *logarithm of the input sequence length* to handle compilation tasks, such as AST construction, symbol resolution, and type analysis. A significant technical challenge stems from the fact that transformers operate at a low level, where each layer processes the input sequence as raw vectors without explicitly associating them with predefined structure or meaning. In contrast, high-level compiler tasks necessitate managing intricate relationships and structured program information. Our primary technical contribution is the development of a domain-specific language, **Cybertron**, which generates formal proofs of the transformer's expressive power, scaling to address compiler tasks. We further establish that recurrent neural networks (RNNs) require at least a linear number of parameters relative to the input sequence, leading to an exponential separation between transformers and RNNs. Finally, we empirically validate our theoretical results by comparing transformers and RNNs on compiler tasks within **Mini-Husky**.",
        "keywords": "Transformers;Expressive Power;Programming Language;Attention Mechanism;Compiler",
        "primary_area": "",
        "supplementary_material": "/attachment/9e3b0798f1a0435b5281691be7542214bae68a5d.zip",
        "author": "Xiyu Zhai;Runlong Zhou;Liao Zhang;Simon Shaolei Du",
        "authorids": "~Xiyu_Zhai1;~Runlong_Zhou1;~Liao_Zhang4;~Simon_Shaolei_Du1",
        "gender": "M;M;;M",
        "homepage": ";https://vectorzhou.com;;http://simonshaoleidu.com",
        "dblp": "203/8195;;;176/5602",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;OttawxUAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Xiyu_Zhai1;~Runlong_Zhou1;~Liao_Zhang4;~Simon_Shaolei_Du1",
        "aff": "University of Washington;University of Washington;;University of Washington",
        "aff_domain": "uw.edu;cs.washington.edu;;washington.edu",
        "position": "Postdoc;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nzhai2025transformers,\ntitle={Transformers are Efficient Compilers, Provably},\nauthor={Xiyu Zhai and Runlong Zhou and Liao Zhang and Simon Shaolei Du},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CaWkEqUjxs}\n}",
        "github": "",
        "project": "",
        "reviewers": "Nhkd;7WyJ;a8E2;UePS",
        "site": "https://openreview.net/forum?id=CaWkEqUjxs",
        "pdf_size": 0,
        "rating": "5;6;8;8",
        "confidence": "5;3;4;2",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.299038105676658
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6024640760767093
    },
    {
        "id": "CdRauNXD1w",
        "title": "Stuffed Mamba: Oversized States Lead to the Inability to Forget",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in recurrent architectures, such as Mamba and RWKV, have showcased strong language capabilities. Unlike transformer-based models, these architectures encode all contextual information into a fixed-size state, leading to great inference efficiency. However, this approach can cause information interference, where different token data conflicts, resulting in performance degradation and incoherent outputs beyond a certain context length. To prevent this, most RNNs incorporate mechanisms designed to \"forget\" earlier tokens. In this paper, we reveal that Mamba-based models struggle to effectively forget earlier tokens even with built-in forgetting mechanisms. We demonstrate that this issue stems from training on contexts that are too short for the state size, enabling the model to perform well without needing to learn how to forget. Then, we show that the minimum training length required for the model to learn forgetting scales linearly with the state size, and the maximum context length for accurate retrieval of a 5-digit passkey scales exponentially with the state size, indicating that the model retains some information beyond the point where forgetting begins. These findings highlight a critical limitation in current RNN architectures and provide valuable insights for improving long-context modeling. Our work suggests that future RNN designs must account for the interplay between state size, training length, and forgetting mechanisms to achieve robust performance in long-context tasks.",
        "keywords": "state space models;mamba;long-context modeling;linear attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yingfa Chen;Xinrong Zhang;Shengding Hu;Xu Han;Zhiyuan Liu;Maosong Sun",
        "authorids": "~Yingfa_Chen1;~Xinrong_Zhang1;~Shengding_Hu2;~Xu_Han2;~Zhiyuan_Liu1;~Maosong_Sun1",
        "gender": "M;F;;;M;M",
        "homepage": "https://www.github.com/chen-yingfa;;;;http://nlp.csai.tsinghua.edu.cn/~lzy;https://www.cs.tsinghua.edu.cn/csen/info/1312/4394.htm",
        "dblp": ";;;;53/3245-1;95/3291-1",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com.hk/citations?user=IvTrgR0AAAAJ;;;dT0v5u0AAAAJ;https://scholar.google.com.tw/citations?user=zIgT0HMAAAAJ",
        "orcid": ";0009-0001-3963-8891;;;0000-0002-7709-2543;",
        "linkedin": ";;;;;",
        "or_profile": "~Yingfa_Chen1;~Xinrong_Zhang1;~Shengding_Hu2;~Xu_Han2;~Zhiyuan_Liu1;~Maosong_Sun1",
        "aff": "Tsinghua University;ByteDance Inc.;;;Tsinghua University;Tsinghua University",
        "aff_domain": "tsinghua.edu.cn;bytedance.com;;;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;Researcher;;;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nchen2025stuffed,\ntitle={Stuffed Mamba: Oversized States Lead to the Inability to Forget},\nauthor={Yingfa Chen and Xinrong Zhang and Shengding Hu and Xu Han and Zhiyuan Liu and Maosong Sun},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=CdRauNXD1w}\n}",
        "github": "",
        "project": "",
        "reviewers": "pm8h;rF9s;cVoT;DDLr",
        "site": "https://openreview.net/forum?id=CdRauNXD1w",
        "pdf_size": 0,
        "rating": "4;5;7;8",
        "confidence": "4;4;3;5",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.5811388300841898
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.223606797749979
    },
    {
        "id": "DAozI4etUp",
        "title": "Multi-Agent Systems Execute Arbitrary Malicious Code",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-agent systems coordinate LLM-based agents to perform tasks on users' behalf. In real-world applications, multi-agent systems will inevitably interact with untrusted inputs, such as malicious Web content, files, email attachments, and more.\n\nUsing several recently proposed multi-agent frameworks as concrete examples, we demonstrate that adversarial content can hijack control and communication within the system to invoke unsafe agents and functionalities.  This results in a complete security breach, up to execution of arbitrary malicious code on the user's device or exfiltration of sensitive data from the user's containerized environment.  For example, **when agents are instantiated with GPT-4o, Web-based attacks successfully cause the multi-agent system execute arbitrary malicious code in 58-90% of trials** (depending on the orchestrator).  In some model-orchestrator configurations, the attack success rate is 100%.  We also demonstrate that these attacks succeed even if individual agents are not susceptible to direct or indirect prompt injection, and even if they refuse to perform harmful actions.  We hope that these results will motivate development of trust and security models for multi-agent systems before they are widely deployed.",
        "keywords": "multi-agent systems;LLMs;AI security;prompt injection;control flow hijacking",
        "primary_area": "",
        "supplementary_material": "/attachment/732db52e05322dcb44766df874d843221176e3b2.zip",
        "author": "Harold Triedman;Rishi Dev Jha;Vitaly Shmatikov",
        "authorids": "~Harold_Triedman2;~Rishi_Dev_Jha1;~Vitaly_Shmatikov1",
        "gender": "M;M;",
        "homepage": "https://haltriedman.com;http://rishijha.com/;",
        "dblp": ";359/6028.html;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;v8oRh6YAAAAJ;",
        "orcid": "0000-0001-5231-7059;;",
        "linkedin": "hal-triedman/;;",
        "or_profile": "~Harold_Triedman2;~Rishi_Dev_Jha1;~Vitaly_Shmatikov1",
        "aff": "Department of Computer Science, Cornell University+Wikimedia;Cornell Tech;",
        "aff_domain": "cs.cornell.edu+wikimedia.org;cornell.edu;",
        "position": "PhD student+Intern;PhD student;",
        "bibtex": "@inproceedings{\ntriedman2025multiagent,\ntitle={Multi-Agent Systems Execute Arbitrary Malicious Code},\nauthor={Harold Triedman and Rishi Dev Jha and Vitaly Shmatikov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=DAozI4etUp}\n}",
        "github": "",
        "project": "",
        "reviewers": "AHJL;cVYY;AGN9;4AuC",
        "site": "https://openreview.net/forum?id=DAozI4etUp",
        "pdf_size": 0,
        "rating": "6;6;6;8",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "DDtwtoAMjA",
        "title": "On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding and mitigating biases is critical for the adoption of large language models (LLMs) in high-stakes decision-making. We introduce Admissions and Hiring, decision tasks with hypothetical applicant profiles where a person's race can be inferred from their name, as simplified test beds for racial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit strong biases. Gemma grants admission to 26% more White than Black applicants, and LLaMA hires 60% more Asian than White applicants. We demonstrate that these biases are resistant to prompt engineering: multiple prompting strategies all fail to promote fairness. In contrast, using distributed alignment search, we can identify \"race subspaces\" within model activations and intervene on them to debias model decisions. Averaging the representation across all races within the subspaces reduces Gemma's bias by 37-57%. Finally, we examine the generalizability of Gemma's race subspaces, and find limited evidence for generalization, where changing the prompt format can affect the race representation. Our work suggests mechanistic approaches may provide a promising venue for improving the fairness of LLMs, but a universal race representation remains elusive.",
        "keywords": "fairness and bias;interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dang Nguyen;Chenhao Tan",
        "authorids": "~Dang_Nguyen4;~Chenhao_Tan1",
        "gender": ";M",
        "homepage": ";https://chenhaot.com/",
        "dblp": ";95/8314",
        "google_scholar": ";https://scholar.google.com.tw/citations?user=KGMaP18AAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Dang_Nguyen4;~Chenhao_Tan1",
        "aff": ";University of Chicago",
        "aff_domain": ";uchicago.edu",
        "position": ";Associate Professor",
        "bibtex": "@inproceedings{\nnguyen2025on,\ntitle={On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions},\nauthor={Dang Nguyen and Chenhao Tan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=DDtwtoAMjA}\n}",
        "github": "",
        "project": "",
        "reviewers": "hTCd;KL13;yTeN;8C6H",
        "site": "https://openreview.net/forum?id=DDtwtoAMjA",
        "pdf_size": 0,
        "rating": "5;7;8;10",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.8027756377319946
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8006407690254357
    },
    {
        "id": "DW8U8ZWa1U",
        "title": "SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships and not dynamic awareness of motion and space---i.e. reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos---even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.",
        "keywords": "spatial reasoning;vqa;multimodal language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arijit Ray;Jiafei Duan;Ellis L Brown II;Reuben Tan;Dina Bashkirova;Rose Hendrix;Kiana Ehsani;Aniruddha Kembhavi;Bryan A. Plummer;Ranjay Krishna;Kuo-Hao Zeng;Kate Saenko",
        "authorids": "~Arijit_Ray1;~Jiafei_Duan1;~Ellis_L_Brown_II1;~Reuben_Tan1;~Dina_Bashkirova1;~Rose_Hendrix1;~Kiana_Ehsani1;~Aniruddha_Kembhavi1;~Bryan_A._Plummer1;~Ranjay_Krishna1;~Kuo-Hao_Zeng3;~Kate_Saenko1",
        "gender": "M;M;;M;F;F;F;M;;M;M;F",
        "homepage": "https://arijitray1993.github.io/;https://duanjiafei.com/;;https://cs-people.bu.edu/rxtan/;https://cs-people.bu.edu/dbash/;;https://ehsanik.github.io/;https://anikem.github.io/;;http://ranjaykrishna.com;https://kuohaozeng.github.io;http://ai.bu.edu",
        "dblp": "164/9384;275/9973.html;;247/5782;;236/4851;198/0910;81/7583;;167/3785;185/0743;88/2754",
        "google_scholar": "VE-ZVW0AAAAJ;d1WCSJIAAAAJ;;;qvUTYsUAAAAJ;TIPqRC0AAAAJ;RScZCLEAAAAJ;JnUevM0AAAAJ;;IcqahyAAAAAJ;SRWelkkAAAAJ;https://scholar.google.com.tw/citations?user=9xDADY4AAAAJ",
        "orcid": "0000-0002-4175-0655;;;;0000-0003-3557-3215;;;;;0000-0001-8784-2531;;0000-0002-5704-7614",
        "linkedin": ";jiafei-duan-a69b11112/;;;;;kiana-ehsani-1b81b0162/;;;ranjay-krishna-1a344444/;%E5%9C%8B%E8%B1%AA-%E6%9B%BE-0165b7b9/?locale=en_US;",
        "or_profile": "~Arijit_Ray1;~Jiafei_Duan1;~Ellis_L_Brown_II1;~Reuben_Tan1;~Dina_Bashkirova1;~Rose_Hendrix1;~Kiana_Ehsani1;~Aniruddha_Kembhavi1;~Bryan_A._Plummer1;~Ranjay_Krishna1;~Kuo-Hao_Zeng3;~Kate_Saenko1",
        "aff": "Google+Boston University;University of Washington;;Microsoft Research;Google+Boston University;Allen Institute for Artificial Intelligence;Vercept;Wayve;;University of Washington;Allen Institute for Artificial Intelligence;Meta AI+Boston University",
        "aff_domain": "google.com+bu.edu;uw.edu;;research.microsoft.com;google.com+bu.edu;allenai.org;vercept.com;wayve.ai;;cs.washington.edu;allenai.org;meta.com+bu.edu",
        "position": "Intern+PhD student;PhD student;;Researcher;Researcher+PhD student;Research Engineer;Researcher;Director;;Assistant Professor;Research Scientist;Researcher+Full Professor",
        "bibtex": "@inproceedings{\nray2025sat,\ntitle={{SAT}: Dynamic Spatial Aptitude Training for Multimodal Language Models},\nauthor={Arijit Ray and Jiafei Duan and Ellis L Brown II and Reuben Tan and Dina Bashkirova and Rose Hendrix and Kiana Ehsani and Aniruddha Kembhavi and Bryan A. Plummer and Ranjay Krishna and Kuo-Hao Zeng and Kate Saenko},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=DW8U8ZWa1U}\n}",
        "github": "",
        "project": "",
        "reviewers": "MPJk;Q2hV;VDnM;2SN7",
        "site": "https://openreview.net/forum?id=DW8U8ZWa1U",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "2;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "DktAODDdbt",
        "title": "Evaluating Large Language Models as Expert Annotators",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Textual data annotation, the process of labeling or tagging text with relevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct alternatives to human annotators for general domains natural language processing (NLP) tasks, their effectiveness on annotation tasks in domains requiring expert knowledge remains underexplored.\nIn this paper, we investigate: whether top-performing LLMs, which might be perceived as having expert-level proficiency in academic and professional benchmarks, can serve as direct alternatives to human expert annotators?\nTo this end, we evaluate both individual LLMs and multi-agent approaches across three highly specialized domains: finance, biomedicine, and law.\nSpecifically, we propose a multi-agent discussion framework to simulate a group of human annotators, where LLMs are tasked to engage in discussions by considering others\u2019 annotations and justifications before finalizing their labels.\nAdditionally, we incorporate reasoning models (*e.g.*, o3-mini) to enable a more comprehensive comparison.\nOur empirical results reveal that:\n*(1)* Individual LLMs equipped with inference-time techniques (*e.g.*, chain-of-thought (CoT), self-consistency) show only marginal or even negative performance gains, contrary to prior literature suggesting their broad effectiveness.\n*(2)* Overall, reasoning models do not demonstrate statistically significant improvements over non-reasoning models in most settings.\nThis suggests that extended long CoT provides relatively limited benefits for data annotation in specialized domains.\n*(3)* Certain model behaviors emerge in the multi-agent discussion environment.\nFor instance, Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even when other agents provide correct annotations or valid reasoning.",
        "keywords": "LLMs-as-expert-annotators;reasoning models;multi-agent framework",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yu-Min Tseng;Wei-Lin Chen;Chung-Chi Chen;Hsin-Hsi Chen",
        "authorids": "~Yu-Min_Tseng1;~Wei-Lin_Chen1;~Chung-Chi_Chen1;~Hsin-Hsi_Chen2",
        "gender": "F;;M;M",
        "homepage": "https://www.ymtseng.com;https://wlchen0206.github.io/;https://nlpfin.github.io/;http://nlg.csie.ntu.edu.tw/advisor.php",
        "dblp": "358/8816;72/7187;177/6602;84/3130.html",
        "google_scholar": "https://scholar.google.com.tw/citations?user=Fwog7uUAAAAJ;https://scholar.google.com.tw/citations?user=Hrbne1wAAAAJ;sJwWSg8AAAAJ;CRth4q4AAAAJ",
        "orcid": "0009-0002-6012-6492;;0000-0003-3680-9277;0000-0001-9757-9423",
        "linkedin": "ym-tseng/;;chungchichen/;",
        "or_profile": "~Yu-Min_Tseng1;~Wei-Lin_Chen1;~Chung-Chi_Chen1;~Hsin-Hsi_Chen2",
        "aff": "Virginia Polytechnic Institute and State University+University of Virginia, Charlottesville+National Taiwan University;University of Virginia, Charlottesville;AIST, National Institute of Advanced Industrial Science and Technology;National Taiwan University",
        "aff_domain": "vt.edu+virginia.edu+ntu.edu.tw;virginia.edu;aist.go.jp;ntu.edu.tw",
        "position": "PhD student+Visiting Graduate Student+MS student;PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\ntseng2025evaluating,\ntitle={Evaluating Large Language Models as Expert Annotators},\nauthor={Yu-Min Tseng and Wei-Lin Chen and Chung-Chi Chen and Hsin-Hsi Chen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=DktAODDdbt}\n}",
        "github": "",
        "project": "",
        "reviewers": "XTfE;CsN1;6CbB;jNni",
        "site": "https://openreview.net/forum?id=DktAODDdbt",
        "pdf_size": 0,
        "rating": "4;6;7;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.479019945774904
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.29277002188455997
    },
    {
        "id": "DmhcCRIfvq",
        "title": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% \uff5e 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models.",
        "keywords": "large language models;reinforcement learning;workflow generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fan Nie;Lan Feng;Haotian Ye;Weixin Liang;Pan Lu;Huaxiu Yao;Alexandre Alahi;James Zou",
        "authorids": "~Fan_Nie1;~Lan_Feng1;~Haotian_Ye1;~Weixin_Liang1;~Pan_Lu2;~Huaxiu_Yao1;~Alexandre_Alahi3;~James_Zou1",
        "gender": ";M;M;;;M;M;",
        "homepage": ";https://alan-lanfeng.github.io/;https://haotianye.com;https://ai.stanford.edu/~wxliang/;;http://huaxiuyao.mystrikingly.com;https://vita.epfl.ch/;",
        "dblp": ";231/7529;284/0539;231/1803;;197/1635;48/3455;",
        "google_scholar": ";8-QJ-kkAAAAJ;VU4chlsAAAAJ;7z9P1jYAAAAJ;;A20BZnQAAAAJ;UIhXQ64AAAAJ;23ZXZvEAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": ";;;weixin-liang-2562aa154/;;huaxiuyao/;;",
        "or_profile": "~Fan_Nie1;~Lan_Feng1;~Haotian_Ye1;~Weixin_Liang1;~Pan_Lu2;~Huaxiu_Yao1;~Alexandre_Alahi3;~James_Zou1",
        "aff": ";EPFL - EPF Lausanne;Stanford University;Stanford University;;Department of Computer Science, University of North Carolina at Chapel Hill;EPFL;Stanford University",
        "aff_domain": ";epfl.ch;stanford.edu;stanford.edu;;cs.unc.edu;epfl.ch;stanford.edu",
        "position": ";PhD student;PhD student;PhD student;;Assistant Professor;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nnie2025weakforstrong,\ntitle={Weak-for-Strong:  Training Weak Meta-Agent to Harness Strong Executors},\nauthor={Fan Nie and Lan Feng and Haotian Ye and Weixin Liang and Pan Lu and Huaxiu Yao and Alexandre Alahi and James Zou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=DmhcCRIfvq}\n}",
        "github": "",
        "project": "",
        "reviewers": "vbtG;iWtW;4Sc7",
        "site": "https://openreview.net/forum?id=DmhcCRIfvq",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            30,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "E7Tu5yjqXw",
        "title": "Language Model Personalization via Reward Factorization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF approaches assume a universal preference model and fail to account for individual user preferences, limiting their effectiveness in personalized applications. We introduce a framework that extends RLHF to enable user personalization by leveraging the assumption that user preferences lie in a low-dimensional space. Instead of training a separate model per user, we represent user-specific rewards as a linear combination of base reward functions. Using only 10 user responses, our method can infer user-specific rewards and align LLM outputs accordingly. We validate our approach through experiments with both synthetic and real users, demonstrating significant personalization achieved by our method. In human evaluations, our method achieves a 67% win rate over default GPT-4o responses.",
        "keywords": "RLHF;Personalization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Idan Shenfeld;Felix Faltings;Pulkit Agrawal;Aldo Pacchiano",
        "authorids": "~Idan_Shenfeld1;~Felix_Faltings1;~Pulkit_Agrawal1;~Aldo_Pacchiano1",
        "gender": "M;M;M;M",
        "homepage": "https://www.linkedin.com/in/idan-shenfeld/;;https://people.eecs.berkeley.edu/~pulkitag/;https://www.aldopacchiano.ai",
        "dblp": ";277/1699;149/2672;129/6338",
        "google_scholar": ";r3FyXZoAAAAJ;UpZmJI0AAAAJ;no_BfYgAAAAJ",
        "orcid": ";;;",
        "linkedin": ";felix-faltings-73b886127;;",
        "or_profile": "~Idan_Shenfeld1;~Felix_Faltings1;~Pulkit_Agrawal1;~Aldo_Pacchiano1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Boston University",
        "aff_domain": "mit.edu;mit.edu;mit.edu;bu.edu",
        "position": "PhD student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nshenfeld2025language,\ntitle={Language Model Personalization via Reward Factorization},\nauthor={Idan Shenfeld and Felix Faltings and Pulkit Agrawal and Aldo Pacchiano},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=E7Tu5yjqXw}\n}",
        "github": "",
        "project": "",
        "reviewers": "iLD4;Fo71;wGH9;fVG8",
        "site": "https://openreview.net/forum?id=E7Tu5yjqXw",
        "pdf_size": 0,
        "rating": "4;5;7;8",
        "confidence": "3;3;5;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.5811388300841898
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7627700713964739
    },
    {
        "id": "ED5diyzc1C",
        "title": "LLM-based Multi-Agents System Attack via Continuous Optimization with Discrete Efficient Search",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have demonstrated remarkable capability in complex tasks. However, emerging evidence indicates significant security vulnerabilities within these systems. \nIn this paper, we introduce three novel and practical attack scenarios that allow only a single intervention on one agent from the MAS. However, previous methods struggle to achieve success. Thus, we propose Continuous Optimization with Discrete Efficient Search (CODES), a token-level jailbreak method that combines continuous-space optimization with discrete-space search to efficiently generate self-replicating attack prompts. Through CODES, malicious content propagates across multiple agents, compromising the entire MAS. In the three realistic threat scenarios\u2014ranging from triggering offensive outputs across an entire agent cohort to bypassing multi-level safeguard modules, CODES demonstrate effectiveness. Our findings underscore the urgent need for more robust safety mechanisms tailored to MAS and highlight the importance of developing resilient alignment strategies to defend against this new class of adversarial threats.",
        "keywords": "multi-agent system;adversarial attack;LLM-based jailbreak",
        "primary_area": "",
        "supplementary_material": "/attachment/cdaee438f8055e1a86b8c8b46c158682d6257dde.zip",
        "author": "Weichen Yu;Kai Hu;Tianyu Pang;Chao Du;Min Lin;Matt Fredrikson",
        "authorids": "~Weichen_Yu1;~Kai_Hu2;~Tianyu_Pang1;~Chao_Du1;~Min_Lin1;~Matt_Fredrikson1",
        "gender": "F;M;M;M;M;M",
        "homepage": "https://weichen-yu.github.io/;https://github.com/hukkai;https://p2333.github.io/;https://duchao0726.github.io/;https://linmin.me;https://cs.cmu.edu/~mfredrik",
        "dblp": "325/1209;;202/2550;75/7523;;38/2612",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;;wYDbtFsAAAAJ;QOp7xW0AAAAJ;BGONmkIAAAAJ;https://scholar.google.com.tw/citations?user=tMYCvLAAAAAJ",
        "orcid": "0009-0003-7935-2358;;0000-0003-0639-6176;0000-0003-1244-6336;;",
        "linkedin": ";;%E5%A4%A9%E5%AE%87-%E5%BA%9E-b3999017a/;duchao/;min-lin-08a3a422/;",
        "or_profile": "~Weichen_Yu1;~Kai_Hu2;~Tianyu_Pang1;~Chao_Du1;~Min_Lin1;~Matt_Fredrikson1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;Sea AI Lab;Sea AI Lab;Sea AI Lab;Gray Swan AI+Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;sea.com;sea.com;sea.com;grayswan.ai+cmu.edu",
        "position": "PhD student;PhD student;Senior Research Scientist;Senior Research Scientist;Principal Researcher;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nyu2025llmbased,\ntitle={{LLM}-based Multi-Agents System Attack via Continuous Optimization with Discrete Efficient Search},\nauthor={Weichen Yu and Kai Hu and Tianyu Pang and Chao Du and Min Lin and Matt Fredrikson},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ED5diyzc1C}\n}",
        "github": "",
        "project": "",
        "reviewers": "3nxL;9qgi;6z7R;78LB",
        "site": "https://openreview.net/forum?id=ED5diyzc1C",
        "pdf_size": 0,
        "rating": "4;5;7;8",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.5811388300841898
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7302967433402215
    },
    {
        "id": "EFxC34XbDh",
        "title": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pre-training is notoriously compute-intensive and academic researchers are notoriously under-resourced. It is, therefore, commonly assumed that academics can't pre-train models. In this paper, we seek to clarify this assumption. We first survey academic researchers to learn about their available compute and then empirically measure the time to replicate models on such resources. We introduce a benchmark to measure the time to pre-train models on given GPUs and also identify ideal settings for maximizing training speed. We run our benchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on our experiments. Our results reveal a brighter picture for academic pre-training: for example, although Pythia-1B was originally trained on 64 GPUs for 3 days, we find it is also possible to replicate this model (with the same hyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude with a cost-benefit analysis to help clarify the trade-offs between price and pre-training time. We believe our benchmark will help academic researchers conduct experiments that require training larger models on more data. We include our codebase in supplementary materials and will fully release it.",
        "keywords": "pre-training;training efficiency;benchmarking;hardware;GPUs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Apoorv Khandelwal;Tian Yun;Nihal V. Nayak;Jack Merullo;Stephen Bach;Chen Sun;Ellie Pavlick",
        "authorids": "~Apoorv_Khandelwal1;~Tian_Yun2;~Nihal_V._Nayak1;~Jack_Merullo2;~Stephen_Bach1;~Chen_Sun1;~Ellie_Pavlick1",
        "gender": "M;M;;;M;M;F",
        "homepage": "http://apoorvkh.com;https://tttyuntian.github.io/;https://nihalnayak.github.io/;;http://stephenbach.net;https://chensun.me;http://cs.brown.edu/people/epavlick/",
        "dblp": "126/9502-1;33/303;203/9278;;90/1077;01/6072-2;141/4059",
        "google_scholar": "gwUMjlsAAAAJ;https://scholar.google.com/citations?hl=en;Bx497RMAAAAJ;;hs6pGXoAAAAJ;vQa7heEAAAAJ;sFyrSa8AAAAJ",
        "orcid": ";0000-0003-1671-5484;0000-0002-3150-1997;;0000-0003-3857-3560;;",
        "linkedin": ";tian-yun-83b385146/;;;;;",
        "or_profile": "~Apoorv_Khandelwal1;~Tian_Yun2;~Nihal_V._Nayak1;~Jack_Merullo2;~Stephen_Bach1;~Chen_Sun1;~Ellie_Pavlick1",
        "aff": "Brown University;Brown University+Meta;School of Engineering and Applied Sciences, Harvard University+Brown University;;Brown University+Snorkel AI;Brown University+Google;Brown University",
        "aff_domain": "brown.edu;brown.edu+meta.com;seas.harvard.edu+brown.edu;;cs.brown.edu+snorkel.ai;brown.edu+google.com;brown.edu",
        "position": "PhD student;PhD student+Intern;Postdoc+PhD student;;Assistant Professor+Researcher;Assistant Professor+Research Scientist;Associate Professor",
        "bibtex": "@inproceedings{\nkhandelwal2025k,\ntitle={\\$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources},\nauthor={Apoorv Khandelwal and Tian Yun and Nihal V. Nayak and Jack Merullo and Stephen Bach and Chen Sun and Ellie Pavlick},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=EFxC34XbDh}\n}",
        "github": "",
        "project": "",
        "reviewers": "DTxJ;ufEP;Ja5N;QD5y",
        "site": "https://openreview.net/forum?id=EFxC34XbDh",
        "pdf_size": 0,
        "rating": "6;6;7;9",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "EJGlOybbDB",
        "title": "CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language models often struggle with cross-mode knowledge retrieval \u2013 the ability to access knowledge learned in one format (mode) when queried in another.\nWe demonstrate that models trained on multiple data sources (e.g., Wikipedia and TinyStories) exhibit significantly reduced accuracy when retrieving knowledge in a format different from its original training mode.\nThis paper quantitatively investigates this phenomenon through a controlled study of random token sequence memorization across different modes.\nWe first explore dataset rewriting as a solution, revealing that effective cross-mode retrieval requires prohibitively extensive rewriting efforts that follow a sigmoid-like relationship.\nAs an alternative, we propose CASCADE, a novel pretraining algorithm that uses cascading datasets with varying sequence lengths and computing losses on only the second half of each training sequence to capture knowledge at different scales.\nOur experiments demonstrate that CASCADE outperforms dataset rewriting approaches, even when compressed into a single model with a unified loss function.\nThis work provides both qualitative evidence of cross-mode retrieval limitations and a practical solution to enhance language models' ability to access knowledge independently of its presentational format.",
        "keywords": "large language models;pretraining;knowledge retrieval;spurious correlations",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Runlong Zhou;Yi Zhang",
        "authorids": "~Runlong_Zhou1;~Yi_Zhang1",
        "gender": "M;M",
        "homepage": "https://vectorzhou.com;https://yi-zhang.me",
        "dblp": ";64/6544-74",
        "google_scholar": "https://scholar.google.com/citations?hl=en;lc6CVqEAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Runlong_Zhou1;~Yi_Zhang1",
        "aff": "University of Washington;Microsoft",
        "aff_domain": "cs.washington.edu;microsoft.com",
        "position": "PhD student;Researcher",
        "bibtex": "@inproceedings{\nzhou2025cascade,\ntitle={{CASCADE} Your Datasets for Cross-Mode Knowledge Retrieval of Language Models},\nauthor={Runlong Zhou and Yi Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=EJGlOybbDB}\n}",
        "github": "",
        "project": "",
        "reviewers": "RKvZ;KQtE;jyit;cKFW",
        "site": "https://openreview.net/forum?id=EJGlOybbDB",
        "pdf_size": 0,
        "rating": "4;6;6;7",
        "confidence": "3;2;3;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3244428422615251
    },
    {
        "id": "EP7mAqx2BO",
        "title": "Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become essential for improving language model capabilities, but traditional approaches rely on the assumption that human preferences follow a transitive Bradley-Terry model.\nThis assumption fails to capture the non-transitive nature of populational human preferences.\nNash learning from human feedback (NLHF), targeting non-transitive preferences, is a problem of computing the Nash equilibrium (NE) of the two-player constant-sum game defined by the human preference.\nWe introduce Extragradient preference optimization (EGPO), a novel algorithm for NLHF achieving last-iterate linear convergence to the NE of KL-regularized games and polynomial convergence to the NE of original games, while being robust to noise.\nUnlike previous approaches that rely on nested optimization, we derive an equivalent implementation using gradients of an online variant of the identity preference optimization (IPO) loss, enabling more faithful implementation for neural networks.\nOur empirical evaluations demonstrate EGPO's superior performance over baseline methods when training for the same number of epochs, as measured by pairwise win-rates using the ground truth preference.\nThese results validate both the theoretical strengths and practical advantages of EGPO for language model alignment with non-transitive human preferences.",
        "keywords": "reinforcement learning;reinforcement learning from human feedback;large language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Runlong Zhou;Maryam Fazel;Simon Shaolei Du",
        "authorids": "~Runlong_Zhou1;~Maryam_Fazel1;~Simon_Shaolei_Du1",
        "gender": "M;F;M",
        "homepage": "https://vectorzhou.com;;http://simonshaoleidu.com",
        "dblp": ";10/2309;176/5602",
        "google_scholar": "https://scholar.google.com/citations?hl=en;vlN_kRoAAAAJ;OttawxUAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Runlong_Zhou1;~Maryam_Fazel1;~Simon_Shaolei_Du1",
        "aff": "University of Washington;University of Washington, Seattle;University of Washington",
        "aff_domain": "cs.washington.edu;uw.edu;washington.edu",
        "position": "PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhou2025extragradient,\ntitle={Extragradient Preference Optimization ({EGPO}): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback},\nauthor={Runlong Zhou and Maryam Fazel and Simon Shaolei Du},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=EP7mAqx2BO}\n}",
        "github": "",
        "project": "",
        "reviewers": "PkxM;Z3uq;DDAh;GcQP",
        "site": "https://openreview.net/forum?id=EP7mAqx2BO",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7071067811865476
    },
    {
        "id": "EfTuzTijDo",
        "title": "NoWag: A Unified Framework for Shape Preserving Com- pression of Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) exhibit remarkable performance across various natural language processing tasks but suffer from immense computational and memory demands, limiting their deployment in resource-constrained environments. To address this challenge, we propose NoWag (Normalized Weight and Activation Guided Compression), a unified framework for one-shot shape preserving compression algorithms. We apply NoWag to compress Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models using two popular shape-preserving techniques: vector quantization (NoWag-VQ) and unstructured/semi-structured pruning (NoWag-P). Our results show that NoWag-VQ significantly outperforms state-of-the-art one-shot vector quantization methods, while NoWag-P performs competitively against leading pruning techniques. These findings highlight underlying commonalities between these compression paradigms and suggest promising directions for future research. Our code is available at https://github.com/LawrenceRLiu/NoWag",
        "keywords": "Quantization;Vector Quantization LLMs;Compression;Sparsity;Pruning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lawrence Ray Liu;Inesh Chakrabarti;Yixiao Li;Mengdi Wang;Tuo Zhao;Lin Yang",
        "authorids": "~Lawrence_Ray_Liu1;~Inesh_Chakrabarti1;~Yixiao_Li2;~Mengdi_Wang1;~Tuo_Zhao2;~Lin_Yang12",
        "gender": ";F;;F;;",
        "homepage": ";;https://yxli2123.github.io;http://mwang.princeton.edu;;",
        "dblp": ";;;;;",
        "google_scholar": "Ven0INYAAAAJ;;;;;",
        "orcid": "0009-0005-7579-5084;;;;;",
        "linkedin": "lawrencerliu/;inesh-chakrabarti-878602183/;yixiao-li-90710b209/;;;",
        "or_profile": "~Lawrence_Ray_Liu1;~Inesh_Chakrabarti1;~Yixiao_Li2;~Mengdi_Wang1;~Tuo_Zhao2;~Lin_Yang12",
        "aff": "University of California, Los Angeles;University of California, Los Angeles+University of California, Los Angeles;Georgia Institute of Technology;Princeton University;;",
        "aff_domain": "ucla.edu;ucla.edu+ucla.edu;gatech.edu;princeton.edu;;",
        "position": "MS student;MS student+Undergrad student;PhD student;Full Professor;;",
        "bibtex": "@inproceedings{\nliu2025nowag,\ntitle={NoWag: A Unified Framework for Shape Preserving Com- pression of Large Language Models},\nauthor={Lawrence Ray Liu and Inesh Chakrabarti and Yixiao Li and Mengdi Wang and Tuo Zhao and Lin Yang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=EfTuzTijDo}\n}",
        "github": "",
        "project": "",
        "reviewers": "H7VY;sxCa;5R73;yN5X",
        "site": "https://openreview.net/forum?id=EfTuzTijDo",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "FeAM2RVO8l",
        "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: (1) use model and data size to predict an intermediate loss, then (2) use it to predict task performance. We train a set of small-scale \"ladder\" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1\\% of the compute used for the target models. On four multiple-choice tasks formatted as ranked classification, we can predict the accuracy of both target models within 2 points of absolute error. We find that tasks with higher prediction error also have higher variance in the metrics over model checkpoints. We also contrast multiple design choices for predicting accuracy, and present recommendations for extending our method to new models and tasks.",
        "keywords": "scaling law;model ladder;downstream tasks",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Akshita Bhagia;Jiacheng Liu;Alexander Wettig;David Heineman;Oyvind Tafjord;Ananya Harsh Jha;Luca Soldaini;Noah A. Smith;Dirk Groeneveld;Pang Wei Koh;Jesse Dodge;Hannaneh Hajishirzi",
        "authorids": "~Akshita_Bhagia1;~Jiacheng_Liu2;~Alexander_Wettig1;~David_Heineman1;~Oyvind_Tafjord2;~Ananya_Harsh_Jha2;~Luca_Soldaini1;~Noah_A._Smith2;~Dirk_Groeneveld1;~Pang_Wei_Koh1;~Jesse_Dodge1;~Hannaneh_Hajishirzi1",
        "gender": "F;M;;M;M;M;Non-Binary;;;M;M;F",
        "homepage": "https://akshitab.github.io/;https://github.com/liujch1998;https://www.cs.princeton.edu/~awettig/;https://davidheineman.com;;;https://soldaini.net;;;http://cs.stanford.edu/~pangwei;http://www.cs.cmu.edu/~jessed/;https://homes.cs.washington.edu/~hannaneh/",
        "dblp": "321/0726;289/6273;302/0235;336/4616;178/8640;;160/1741;;185/7781;10/10453;49/11425;52/1296",
        "google_scholar": "fzH3_G4AAAAJ;GJfoBZAAAAAJ;N_jSE08AAAAJ;JO2Q6CUAAAAJ;https://scholar.google.com/citations?hl=en;KK_RffoAAAAJ;3KPvwcgAAAAJ;;KEhvGNMAAAAJ;Nn990CkAAAAJ;nHy_1doAAAAJ;LOV6_WIAAAAJ",
        "orcid": "0000-0003-4848-3884;0000-0003-3308-2869;;;0000-0003-4190-5618;;0000-0001-6998-9863;;0000-0002-8274-768X;;;",
        "linkedin": ";liujch1998/;alexander-wettig/;;;ananyaharshjha/;soldni/;;mechanicaldirk/;;;",
        "or_profile": "~Akshita_Bhagia1;~Jiacheng_Liu2;~Alexander_Wettig1;~David_Heineman1;~Oyvind_Tafjord2;~Ananya_Harsh_Jha2;~Luca_Soldaini1;~Noah_A._Smith2;~Dirk_Groeneveld1;~Pang_Wei_Koh1;~Jesse_Dodge1;~Hannaneh_Hajishirzi1",
        "aff": "Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence+Paul G. Allen School of Computer Science and Engineering, University of Washington;Princeton University+Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Google DeepMind+Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence;;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence+University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "allenai.org;allenai.org+cs.washington.edu;princeton.edu+allenai.org;allenai.org;google.com+allenai.org;uw.edu;allenai.org;;allenai.org;allenai.org+cs.washington.edu;allenai.org;allenai.org+uw.edu",
        "position": "Researcher;Intern+PhD student;PhD student+Intern;Researcher;Researcher+Researcher;PhD student;Researcher;;Principal Researcher;Visiting Research Scientist+Assistant Professor;Researcher;senior director+Associate Professor",
        "bibtex": "@inproceedings{\nbhagia2025establishing,\ntitle={Establishing Task Scaling Laws via Compute-Efficient Model Ladders},\nauthor={Akshita Bhagia and Jiacheng Liu and Alexander Wettig and David Heineman and Oyvind Tafjord and Ananya Harsh Jha and Luca Soldaini and Noah A. Smith and Dirk Groeneveld and Pang Wei Koh and Jesse Dodge and Hannaneh Hajishirzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=FeAM2RVO8l}\n}",
        "github": "",
        "project": "",
        "reviewers": "Jq1a;ExxQ;s6ea;Dg36",
        "site": "https://openreview.net/forum?id=FeAM2RVO8l",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "FqXXtSZWEZ",
        "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pretraining data curation is a cornerstone in Large Language Model (LLM) development, leading to growing research on quality filtering of large web corpora. From statistical quality flags to LLM-based labelling systems, datasets are divided into categories, frequently reducing to a binary: those passing the filters are deemed as valuable examples, others are discarded as useless or detrimental. However, a more detailed understanding of the contribution of different kinds of texts to model performance is still largely lacking. In this article, we present the first study utilising _registers_ or _genres_\u2014a widely used standard in corpus linguistics to model linguistic variation\u2014to curate pretraining datasets and investigate the effect of register on the performance of LLMs. We train small generative models with register classified data and evaluate them using standard benchmarks, and show that the register of pretraining data substantially affects model performance. We uncover surprising relationships between the pretraining material and the resulting models: using the _News_ register results in subpar performance, and on the contrary, including the _Opinion_ class, covering texts such as reviews and opinion blogs, is highly beneficial. While a model trained on the entire unfiltered dataset outperforms those trained on datasets limited to a single register, combining well-performing registers such as _How-to-Instructions_, _Informational Description_, and _Opinion_ leads to major improvements. Furthermore, analysis of individual benchmark results reveals key differences in the strengths and drawbacks of specific register classes as pretraining data: _How-to-Instructions_ excels at physical reasoning and sentence completion while barely crossing random baselines on world-knowledge benchmarks, while _Narrative_ boosts performance on social interaction tasks but struggles with scientific questions. These findings show that register is an important explainer of model variation and can facilitate more deliberate and detailed future data selection practices.",
        "keywords": "Register;Genre;Large Language Models;NLP;LLM evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amanda Myntti;Erik Henriksson;Veronika Laippala;Sampo Pyysalo",
        "authorids": "~Amanda_Myntti1;~Erik_Henriksson1;~Veronika_Laippala1;~Sampo_Pyysalo2",
        "gender": ";M;;M",
        "homepage": ";https://erikhenriksson.fi;https://turkunlp.org/;",
        "dblp": ";;;",
        "google_scholar": ";;;GUHpTS0AAAAJ",
        "orcid": ";;;",
        "linkedin": "amanda-myntti-a31609230;;;",
        "or_profile": "~Amanda_Myntti1;~Erik_Henriksson1;~Veronika_Laippala1;~Sampo_Pyysalo2",
        "aff": "University of Turku+University of Turku;University of Turku;University of Turku;University of Turku",
        "aff_domain": "utu.fi+utu.fi;utu.fi;utu.fi;utu.fi",
        "position": "PhD student+MS student;Postdoc;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nmyntti2025register,\ntitle={Register Always Matters: Analysis of {LLM} Pretraining Data Through the Lens of Language Variation},\nauthor={Amanda Myntti and Erik Henriksson and Veronika Laippala and Sampo Pyysalo},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=FqXXtSZWEZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "syA5;4S5f;W4AW;ak3C",
        "site": "https://openreview.net/forum?id=FqXXtSZWEZ",
        "pdf_size": 0,
        "rating": "2;6;7;7",
        "confidence": "4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            2.0615528128088303
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.48507125007266594
    },
    {
        "id": "GFPoM8Ylp8",
        "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. \nThe extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce **LongCodeBench** (**LCB**), a benchmark to test LLM coding abilities in long-context scenarios. \nOur benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (**LongCodeQA**) and bug fixing (**LongSWE-Bench**) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.",
        "keywords": "Large language models (LLMs);Benchmarking;Long-context;Coding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Stefano Rando;Luca Romani;Alessio Sampieri;Luca Franco;John Yang;Yuta Kyuragi;Fabio Galasso;Tatsunori Hashimoto",
        "authorids": "~Stefano_Rando1;~Luca_Romani1;~Alessio_Sampieri1;~Luca_Franco1;~John_Yang3;~Yuta_Kyuragi1;~Fabio_Galasso1;~Tatsunori_Hashimoto1",
        "gender": "M;M;M;M;M;M;M;M",
        "homepage": ";;;https://fraluca.github.io/;https://john-b-yang.github.io/;;https://fgalasso.bitbucket.io/;https://thashim.github.io",
        "dblp": ";;304/2196;304/2582;;;48/3897;",
        "google_scholar": ";;lzFb_ZUAAAAJ;https://scholar.google.com/citations?hl=it;71G11ksAAAAJ;0zN_288AAAAJ;https://scholar.google.de/citations?user=2gSuGBEAAAAJ;5ygiTwsAAAAJ",
        "orcid": ";;;0000-0003-0107-6755;;0000-0002-6078-384X;0000-0003-1875-7813;",
        "linkedin": "https://it.linkedin.com/in/stefano-rando-94b852183?original_referer=https%3A%2F%2Fwww.google.com%2F;luca-romani-523039251/;;luca-franco-968819196/;jyang20/;yuta-kyuragi-589128294?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;fabio-galasso-61141b32/;",
        "or_profile": "~Stefano_Rando1;~Luca_Romani1;~Alessio_Sampieri1;~Luca_Franco1;~John_Yang3;~Yuta_Kyuragi1;~Fabio_Galasso1;~Tatsunori_Hashimoto1",
        "aff": "Panasonic;University of Roma \"La Sapienza\";ItalAI;ItalAI;Stanford University;Stanford University+Panasonic R&D Company of America;University of Roma \"La Sapienza\";Stanford University",
        "aff_domain": "us.panasonic.com;uniroma1.it;uniroma1.it;italailabs.com;stanford.edu;stanford.edu+us.panasonic.com;uniroma1.it;stanford.edu",
        "position": "Researcher;PhD student;Principal Researcher;Principal Researcher;PhD student;Visiting Scholar+AI Research Engineer;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nrando2025longcodebench,\ntitle={LongCodeBench: Evaluating Coding {LLM}s at 1M Context Windows},\nauthor={Stefano Rando and Luca Romani and Alessio Sampieri and Luca Franco and John Yang and Yuta Kyuragi and Fabio Galasso and Tatsunori Hashimoto},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=GFPoM8Ylp8}\n}",
        "github": "",
        "project": "",
        "reviewers": "YF7D;rfFo;5MXK;KV24",
        "site": "https://openreview.net/forum?id=GFPoM8Ylp8",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;2;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            1.0897247358851685
        ],
        "replies_avg": [
            8,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6622661785325219
    },
    {
        "id": "GQNojroNCH",
        "title": "Breakpoint: Stress-testing systems-level reasoning in LLM agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Benchmarks for large language models (LLMs) have predominantly assessed short-horizon, localized reasoning. \nExisting long-horizon suites (e.g. SWE-lancer) rely on manually curated issues, so expanding or tuning difficulty demands expensive human effort and evaluations quickly saturate.\nHowever, many real-world tasks, such as software engineering or scientific research, require agents to rapidly comprehend and manipulate novel, complex structures dynamically; evaluating these capabilities requires the ability to construct large and varied sets of problems for agents to solve.\nWe introduce Breakpoint, a benchmarking methodology that automatically generates code-repair tasks by adversarially corrupting functions within real-world software repositories. Breakpoint systematically controls task difficulty along two different dimensions: local reasoning (characterized by code complexity metrics such as cyclomatic complexity) and system-level reasoning (characterized by call-graph centrality and the number of simultaneously corrupted interdependent functions).\nIn experiments across more than 900 generated tasks we demonstrate that Breakpoint's methodology can scale to arbitrary difficulty, with state-of-the-art models' success rates ranging from 55\\% on the easiest tasks down to 0\\% on the hardest. We analyze how static parameters control task difficulty, characterize how improvements in models and inference-time budgets affect local versus system-level reasoning, and evaluate the strategies models use to gather information and iterate on solutions, demonstrating Breakpoint\u2019s effectiveness as a comprehensive evaluation suite for understanding agent behavior and capabilities.",
        "keywords": "LLMS;coding benchmarks;evaluation;inverse problems;long-horizon reasoning;systems-level comprehension;robustness evaluation;code corruption;software engineering;long-horizon",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaivalya Hariharan;Uzay Girit;Zifan Wang;Jacob Andreas",
        "authorids": "~Kaivalya_Hariharan1;~Uzay_Girit1;~Zifan_Wang8;~Jacob_Andreas1",
        "gender": "M;M;M;M",
        "homepage": ";https://uzpg.me;https://chry-santhemum.github.io/website/;http://web.mit.edu/jda/www",
        "dblp": ";344/2052;;97/8154",
        "google_scholar": ";qvZ_Q_IAAAAJ;;dnZ8udEAAAAJ",
        "orcid": ";;;",
        "linkedin": "kaivalya-hariharan-44a698204;uzay-girit-a208161a2/;;",
        "or_profile": "~Kaivalya_Hariharan1;~Uzay_Girit1;~Zifan_Wang8;~Jacob_Andreas1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "position": "MS student;Undergrad student;Undergrad student;Associate Professor",
        "bibtex": "@inproceedings{\nhariharan2025breakpoint,\ntitle={Breakpoint: Stress-testing systems-level reasoning in {LLM} agents},\nauthor={Kaivalya Hariharan and Uzay Girit and Zifan Wang and Jacob Andreas},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=GQNojroNCH}\n}",
        "github": "",
        "project": "",
        "reviewers": "zGgD;tkG9;CJEp",
        "site": "https://openreview.net/forum?id=GQNojroNCH",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9999999999999997
    },
    {
        "id": "GanmYQ0RpE",
        "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security Threats",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present DoomArena, a security evaluation framework for AI agents. DoomArena is designed on three principles: 1) It is a \\emph{plug-in} framework and integrates easily into realistic agentic frameworks like Browsergym (for web agents) and $\\tau$-bench (for tool calling agents); 2) It is \\emph{configurable} and allows for detailed threat modeling, allowing configuration of specific components of the agentic framework being attackable, and specifying targets for the attacker; and 3) It is \\emph{modular} and decouples the development of attacks from details of the environment in which the agent is deployed, allowing for the same attacks to be applied across multiple environments. We illustrate several advantages of our framework, including enabling the development of generic attacker agents, the ability to easily combine several previously published attacks to enable comprehensive and fine-grained security testing, and the ability to analyze trade-offs between various vulnerabilities. We apply DoomArena to state-of-the-art (SOTA) web and tool-calling agents and find a number of surprising results: 1) SOTA agents have varying levels of vulnerability to different threat models (malicious user vs malicious environment), and there is no Pareto dominant agent across all threat models; 2) When multiple attacks are applied to an agent, they often combine constructively; 3) Guardrail model-based defenses seem to fail, while defenses based on powerful SOTA LLMs work much better.",
        "keywords": "security;agents",
        "primary_area": "",
        "supplementary_material": "",
        "author": "L\u00e9o Boisvert;Abhay Puri;Gabriel Huang;Mihir Bansal;Chandra Kiran Reddy Evuru;Avinandan Bose;Maryam Fazel;Quentin Cappart;Alexandre Lacoste;Alexandre Drouin;Krishnamurthy Dj Dvijotham",
        "authorids": "~L\u00e9o_Boisvert1;~Abhay_Puri1;~Gabriel_Huang1;~Mihir_Bansal1;~Chandra_Kiran_Reddy_Evuru1;~Avinandan_Bose1;~Maryam_Fazel1;~Quentin_Cappart1;~Alexandre_Lacoste1;~Alexandre_Drouin2;~Krishnamurthy_Dj_Dvijotham1",
        "gender": "M;M;M;;M;M;F;M;M;M;",
        "homepage": "https://leo-boisvert.com/;https://abhaypuri.github.io/portfolio/;;;;https://avinandan22.github.io/;;https://qcappart.github.io/;;https://alexdrouin.com;",
        "dblp": ";383/3753;;;355/1221;305/7490;10/2309;164/5606;59/6239.html;117/3861;",
        "google_scholar": "2XdDpIgAAAAJ;https://scholar.google.ca/citations?user=s8vVSvIAAAAJ;https://scholar.google.ca/citations?hl=en;;;https://scholar.google.com/citations?pli=1;vlN_kRoAAAAJ;QJQMYLsAAAAJ;;https://scholar.google.ca/citations?user=LR6aJcEAAAAJ;",
        "orcid": ";;;;;;;0000-0002-8742-0774;;0000-0001-7718-0319;",
        "linkedin": ";abhaypuri98/?originalSubdomain=ca;;;ckevuru/;;;;;drouinalexandre/;",
        "or_profile": "~L\u00e9o_Boisvert1;~Abhay_Puri1;~Gabriel_Huang1;~Mihir_Bansal1;~Chandra_Kiran_Reddy_Evuru1;~Avinandan_Bose1;~Maryam_Fazel1;~Quentin_Cappart1;~Alexandre_Lacoste1;~Alexandre_Drouin2;~Krishnamurthy_Dj_Dvijotham1",
        "aff": "\u00c9cole Polytechnique de Montr\u00e9al, Universit\u00e9 de Montr\u00e9al;ServiceNow Research;ServiceNow Inc;;ServiceNow Inc;Department of Computer Science;University of Washington, Seattle;\u00c9cole Polytechnique de Montr\u00e9al, Universit\u00e9 de Montr\u00e9al;ServiceNow;Laval university+ServiceNow Research ;",
        "aff_domain": "polymtl.ca;servicenow.com;servicenow.com;;servicenow.com;cs.washington.edu;uw.edu;polymtl.ca;servicenow.com;ulaval.ca+servicenow.com;",
        "position": "PhD student;Researcher;Researcher;;Researcher;PhD student;Full Professor;Associate Professor;Research Scientist;Adjunct Professor+Research Scientist;",
        "bibtex": "@inproceedings{\nboisvert2025doomarena,\ntitle={DoomArena: A framework for Testing {AI} Agents Against Evolving Security Threats},\nauthor={L{\\'e}o Boisvert and Abhay Puri and Gabriel Huang and Mihir Bansal and Chandra Kiran Reddy Evuru and Avinandan Bose and Maryam Fazel and Quentin Cappart and Alexandre Lacoste and Alexandre Drouin and Krishnamurthy Dj Dvijotham},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=GanmYQ0RpE}\n}",
        "github": "",
        "project": "",
        "reviewers": "QaTp;vWM4;oLEo",
        "site": "https://openreview.net/forum?id=GanmYQ0RpE",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Gu0XSax2YS",
        "title": "Adaptive Layer-skipping in Pre-trained LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, limited attention has been paid to a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive computation in LLMs without modifying their original parameters. Applied to Llama-3-8B, it skips 8 out of 32 layers while maintaining full benchmark performance. Our experiments reveal that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Despite the computational savings, FlexiDepth does not yet achieve wall-clock speedup due to varied skipping patterns and I/O overhead. To inspire future work and advance research on practical speedup, we open-sourced FlexiDepth and a dataset documenting its layer allocation patterns.",
        "keywords": "Large language models;Layer-skipping;Conditional Computation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xuan Luo;Weizhi Wang;Xifeng Yan",
        "authorids": "~Xuan_Luo2;~Weizhi_Wang1;~Xifeng_Yan1",
        "gender": "M;M;",
        "homepage": "https://www.luoxuan.space/;https://victorwz.github.io;https://sites.cs.ucsb.edu/~xyan/",
        "dblp": ";98/6969;y/XifengYan",
        "google_scholar": ";UC2_V1MAAAAJ;XZV2eogAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Xuan_Luo2;~Weizhi_Wang1;~Xifeng_Yan1",
        "aff": "University of California, Santa Barbara;University of California, Santa Barbara;UC Santa Barbara",
        "aff_domain": "ucsb.edu;ucsb.edu;ucsb.edu",
        "position": "PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nluo2025adaptive,\ntitle={Adaptive Layer-skipping in Pre-trained {LLM}s},\nauthor={Xuan Luo and Weizhi Wang and Xifeng Yan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Gu0XSax2YS}\n}",
        "github": "",
        "project": "",
        "reviewers": "Anmw;BFv9;upxR",
        "site": "https://openreview.net/forum?id=Gu0XSax2YS",
        "pdf_size": 0,
        "rating": "6;8;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "H6Ae8Po6fS",
        "title": "Adversarial Training of Reward Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reward modeling has emerged as a promising approach for the scalable alignment of language models. However, contemporary reward models (RMs) often lack robustness, awarding high rewards to low-quality, out-of-distribution (OOD) samples. This can lead to reward hacking, where policies exploit unintended shortcuts to maximize rewards, undermining alignment. To address this challenge, we introduce Adv-RM, a novel adversarial training framework that automatically identifies adversarial examples \u2014 responses that receive high rewards from the target RM but are OOD and of low quality. By leveraging reinforcement learning, Adv-RM trains a policy to generate adversarial examples that reliably expose vulnerabilities in large state-of-the-art reward models such as Nemotron 340B RM. Incorporating these adversarial examples into the reward training process improves the robustness of RMs, mitigating reward hacking and enhancing downstream performance in RLHF. We demonstrate that Adv-RM significantly outperforms conventional RM training, increasing stability and enabling more effective RLHF training in both synthetic and real-data settings. We will open-source all code and data.",
        "keywords": "reward models;robustness;RLHF",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexander Bukharin;Haifeng Qian;Shengyang Sun;Adithya Renduchintala;Soumye Singhal;Zhilin Wang;Oleksii Kuchaiev;Olivier Delalleau;Tuo Zhao",
        "authorids": "~Alexander_Bukharin1;~Haifeng_Qian1;~Shengyang_Sun4;~Adithya_Renduchintala2;~Soumye_Singhal1;~Zhilin_Wang2;~Oleksii_Kuchaiev1;~Olivier_Delalleau1;~Tuo_Zhao2",
        "gender": "M;M;M;M;M;;;M;",
        "homepage": "https://abukharin3.github.io;https://sites.google.com/view/haifengqian;http://www.cs.toronto.edu/~ssy/;https://arendu.github.io/;;;http://www.kuchaev.com;;",
        "dblp": "294/6372;61/6767;173/5093;;245/4872;53/10643;;68/2192;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=NktP1NQAAAAJ;;https://scholar.google.com/citations?hl=en;OmMgSQsAAAAJ;qmmIGnwAAAAJ;https://scholar.google.ca/citations?user=zqLpO2QAAAAJ;",
        "orcid": ";0000-0002-7189-6903;;0000-0002-1896-3679;;;;0000-0002-0610-7226;",
        "linkedin": ";haifengqian;;;;;oleksiikuchaiev/;odelalleau;",
        "or_profile": "~Alexander_Bukharin1;~Haifeng_Qian1;~Shengyang_Sun4;~Adithya_Renduchintala2;~Soumye_Singhal1;~Zhilin_Wang2;~Oleksii_Kuchaiev1;~Olivier_Delalleau1;~Tuo_Zhao2",
        "aff": "NVIDIA+Georgia Institute of Technology;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;",
        "aff_domain": "nvidia.com+gatech.edu;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;",
        "position": "Researcher+PhD student;Principal Applied Scientist;Researcher;Applied Research Scientist;Researcher;Applied Scientist;Principal Researcher;Researcher;",
        "bibtex": "@inproceedings{\nbukharin2025adversarial,\ntitle={Adversarial Training of Reward Models},\nauthor={Alexander Bukharin and Haifeng Qian and Shengyang Sun and Adithya Renduchintala and Soumye Singhal and Zhilin Wang and Oleksii Kuchaiev and Olivier Delalleau and Tuo Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=H6Ae8Po6fS}\n}",
        "github": "",
        "project": "",
        "reviewers": "afoe;ALVK;3VZA;qJrb",
        "site": "https://openreview.net/forum?id=H6Ae8Po6fS",
        "pdf_size": 0,
        "rating": "6;6;6;8",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "H6so82c2Sw",
        "title": "Arctic-Embed 2.0: Multilingual Retrieval Without Compromise",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents the training methodology of Snowflake Arctic-Embed 2.0, a set of open-source text embedding models built for effective and efficient multilingual retrieval. While prior works have suffered from degraded English retrieval quality, Arctic-Embed 2.0 delivers competitive retrieval quality on multilingual and English-only benchmarks, and supports Matryoshka Representation Learning (MRL) for efficient embedding storage with significantly lower compressed quality degradation compared to alternatives. Beyond describing the design and implementation details, we highlight critical research questions encountered during development, including the mechanisms of cross-lingual transfer in retrieval pre-training and what we term the \"English performance gap\" - the systematic quality difference between specialized English-only models and multilingual alternatives. Through targeted experiments addressing these questions, we derive insights from both positive and negative results, contributing to a broader understanding of multilingual embedding models and aiming to stimulate further research on improving cross-lingual representation quality while maintaining strong monolingual performance.",
        "keywords": "Multilingual Retrieval;Dense Retrieval;Cross-lingual Transfer",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Puxuan Yu;Luke Merrick;Gaurav Nuti;Daniel F Campos",
        "authorids": "~Puxuan_Yu1;~Luke_Merrick1;~Gaurav_Nuti1;~Daniel_F_Campos1",
        "gender": "M;;M;M",
        "homepage": "https://people.cs.umass.edu/~pxyu/;https://lukemerrick.com;;https://spacemanidol.github.io/",
        "dblp": "222/1251;;;64/1314",
        "google_scholar": "S102tmcAAAAJ;tvCQBs0AAAAJ;;nlHptm8AAAAJ",
        "orcid": "0000-0001-7913-8632;;;0000-0002-5138-8426",
        "linkedin": ";https://linkedin.com/in/lukesmerrick/;gauravnuti;spacemanidol/",
        "or_profile": "~Puxuan_Yu1;~Luke_Merrick1;~Gaurav_Nuti1;~Daniel_F_Campos1",
        "aff": "Snowflake;Snowflake, Inc.;Snowflake;Snowflake",
        "aff_domain": "snowflake.com;snowflake.com;snowflake.com;snowflake.com",
        "position": "Researcher;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nyu2025arcticembed,\ntitle={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\nauthor={Puxuan Yu and Luke Merrick and Gaurav Nuti and Daniel F Campos},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=H6so82c2Sw}\n}",
        "github": "",
        "project": "",
        "reviewers": "wM18;XDLM;P4sz;aVcC;HHZh",
        "site": "https://openreview.net/forum?id=H6so82c2Sw",
        "pdf_size": 0,
        "rating": "5;6;7;7;7",
        "confidence": "5;4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.4,
            0.7999999999999999
        ],
        "confidence_avg": [
            4.2,
            0.39999999999999997
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8750000000000003
    },
    {
        "id": "HAjgxcHpzc",
        "title": "Hardware-Efficient Attention for Fast Decoding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The combination of excessive data movement, an expanding key-value cache, and the limited parallelism inherent in incremental decoding severely bottleneck attention. We explore the design of hardware-efficient attention optimized for LLM decoding. We examine how arithmetic intensity, parallelization, and model quality interact and assess whether the current architecture fully capitalizes on modern hardware. To maximize hardware-effiency, we first propose Group Tied Attention (GTA), a simple attention variant that combines and reuses key and value states to reduce memory transfers during incremental decoding while preserving model quality. We then introduce Group Latent Attention (GLA), a parallel-friendly latent attention combined with low-level optimization designed for fast decoding while showing high model quality. We empirically demonstrate the efficacy of these inference-aware variants in language modeling experiments, showing that GTA matches grouped query attention (GQA) quality with roughly 2x smaller KV cache, and GLA matches multi-head latent attention (MLA) but is easier to shard. Our optimized attention kernel for GLA is up to 2x faster than FlashMLA.",
        "keywords": "Inference;Engineering for large LMs;Compute efficient LM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ted Zadouri;Hubert Strauss;Tri Dao",
        "authorids": "~Ted_Zadouri1;~Hubert_Strauss1;~Tri_Dao1",
        "gender": ";;",
        "homepage": ";;https://tridao.me/",
        "dblp": ";;206/7018",
        "google_scholar": ";;NQRw0bQAAAAJ",
        "orcid": ";;",
        "linkedin": "www.linkedin.com/in/tedzed;hubert-strauss;",
        "or_profile": "~Ted_Zadouri1;~Hubert_Strauss1;~Tri_Dao1",
        "aff": "University of California, Los Angeles;Princeton University;Princeton University",
        "aff_domain": "ucla.edu;princeton.edu;princeton.edu",
        "position": "MS student;Research Engineer;Assistant Professor",
        "bibtex": "@inproceedings{\nzadouri2025hardwareefficient,\ntitle={Hardware-Efficient Attention for Fast Decoding},\nauthor={Ted Zadouri and Hubert Strauss and Tri Dao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=HAjgxcHpzc}\n}",
        "github": "",
        "project": "",
        "reviewers": "TDTP;J2Hz;CoaW;R9T9",
        "site": "https://openreview.net/forum?id=HAjgxcHpzc",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "4;4;4;5",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "HL5X5uX0RD",
        "title": "Customize Multi-modal RAI Guardrails with Precedent-based predictions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on \"precedents\", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.",
        "keywords": "Customizable Guardrail",
        "primary_area": "",
        "supplementary_material": "/attachment/919f1022d31c4c78d061053ab43b078255df0fa3.zip",
        "author": "Cheng-Fu Yang;Thanh Tran;Christos Christodoulopoulos;Weitong Ruan;Rahul Gupta;Kai-Wei Chang",
        "authorids": "~Cheng-Fu_Yang1;~Thanh_Tran1;~Christos_Christodoulopoulos1;~Weitong_Ruan1;~Rahul_Gupta3;~Kai-Wei_Chang1",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://joeyy5588.github.io/;https://thanhdtran.github.io/;http://christos-c.com/;;;http://kwchang.net",
        "dblp": "51/8564;181/2525-5;82/1906;171/0466;;18/2428",
        "google_scholar": "https://scholar.google.com.tw/citations?user=cJ5oowQAAAAJ;mTbsDNYAAAAJ;oZORQtwAAAAJ;89pcoJsAAAAJ;1CFrm2YAAAAJ;fqDBtzYAAAAJ",
        "orcid": ";;0000-0001-7708-0051;;;0000-0001-5365-0072",
        "linkedin": ";;christos-christodoulopoulos-376b9831/;;;kai-wei-chang-41239040",
        "or_profile": "~Cheng-Fu_Yang1;~Thanh_Tran1;~Christos_Christodoulopoulos1;~Weitong_Ruan1;~Rahul_Gupta3;~Kai-Wei_Chang1",
        "aff": "University of California, Los Angeles;Amazon;Amazon;Amazon;Amazon;University of California, Los Angeles+Amazon",
        "aff_domain": "cs.ucla.edu;amazon.com;amazon.co.uk;amazon.com;amazon.com;ucla.edu+amazon.com",
        "position": "PhD student;Researcher;Researcher;Researcher;Researcher;Associate Professor+Researcher",
        "bibtex": "@inproceedings{\nyang2025customize,\ntitle={Customize Multi-modal {RAI} Guardrails with Precedent-based predictions},\nauthor={Cheng-Fu Yang and Thanh Tran and Christos Christodoulopoulos and Weitong Ruan and Rahul Gupta and Kai-Wei Chang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=HL5X5uX0RD}\n}",
        "github": "",
        "project": "",
        "reviewers": "3C7s;zikp;Hnir;oTxU",
        "site": "https://openreview.net/forum?id=HL5X5uX0RD",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "HbwkIDWQgN",
        "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI O1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, partial reward from AI feedback, n-gram similarity, and syntax check rewards, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained14B-parameter model significantly outperforms larger proprietary models, e.g. O3-Mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.",
        "keywords": "Text-to-SQL;Reinforcement Learning;Database",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mohammadreza Pourreza;Shayan Talaei;Ruoxi Sun;Xingchen Wan;Hailong Li;Azalia Mirhoseini;Amin Saberi;Sercan O Arik",
        "authorids": "~Mohammadreza_Pourreza1;~Shayan_Talaei1;~Ruoxi_Sun2;~Xingchen_Wan1;~Hailong_Li2;~Azalia_Mirhoseini3;~Amin_Saberi1;~Sercan_O_Arik1",
        "gender": "M;M;F;M;M;;;M",
        "homepage": ";;;https://xingchen.one;;;https://www.stanford.edu/~saberi;https://www.sercanarik.com/",
        "dblp": "338/7789;322/8865;72/7683;255/7214;;;28/4017;",
        "google_scholar": "https://scholar.google.ca/citations?user=_rOg88EAAAAJ;DXuAYKwAAAAJ;ut1-7LAAAAAJ;6KkohssAAAAJ;;;;",
        "orcid": ";0009-0005-6697-8487;;0000-0003-0074-0597;;;;0000-0001-6333-1729",
        "linkedin": ";shayan-talaei-6b65a0229/;;;hailong-li-b3a56aa6/;;;",
        "or_profile": "~Mohammadreza_Pourreza1;~Shayan_Talaei1;~Ruoxi_Sun2;~Xingchen_Wan1;~Hailong_Li2;~Azalia_Mirhoseini3;~Amin_Saberi1;~Sercan_O_Arik1",
        "aff": "Google;Stanford University;Google;Google;Google;;Stanford University;Google",
        "aff_domain": "google.com;stanford.edu;google.com;google.com;google.com;;stanford.edu;google.com",
        "position": "Researcher;PhD student;Google;Research Scientist;Researcher;;Full Professor;Research Scientist",
        "bibtex": "@inproceedings{\npourreza2025reasoningsql,\ntitle={Reasoning-{SQL}: Reinforcement Learning with {SQL} Tailored Partial Rewards for Reasoning-Enhanced Text-to-{SQL}},\nauthor={Mohammadreza Pourreza and Shayan Talaei and Ruoxi Sun and Xingchen Wan and Hailong Li and Azalia Mirhoseini and Amin Saberi and Sercan O Arik},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=HbwkIDWQgN}\n}",
        "github": "",
        "project": "",
        "reviewers": "KUeS;ChoJ;sLu2;yRRV",
        "site": "https://openreview.net/forum?id=HbwkIDWQgN",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "HyPeYU9JR6",
        "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, Needle-In-A-Haystack, and RULER demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.",
        "keywords": "Large Language Models;KV cache compression;Sentence-level semantic caching;Inference efficiency;Long-context inference",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuxuan Zhu;Ali Falahati;David H. Yang;Mohammad Mohammadi Amiri",
        "authorids": "~Yuxuan_Zhu3;~Ali_Falahati2;~David_H._Yang1;~Mohammad_Mohammadi_Amiri1",
        "gender": "M;M;M;",
        "homepage": "https://zzbright1998.github.io/;https://ali-falahati.github.io/;https://davidhy514.github.io/;",
        "dblp": "146/0939;352/9733;;",
        "google_scholar": "qmZnKSYAAAAJ;;qbkNeWoAAAAJ;",
        "orcid": "0000-0001-5373-4452;0009-0001-3064-4187;0000-0003-1708-1153;",
        "linkedin": "yuxuanzhu1998/;ali-falahati80/;david-yang-1986b8b1/;",
        "or_profile": "~Yuxuan_Zhu3;~Ali_Falahati2;~David_H._Yang1;~Mohammad_Mohammadi_Amiri1",
        "aff": "Rensselaer Polytechnic Institute;University of Waterloo;Rensselaer Polytechnic Institute;",
        "aff_domain": "rpi.edu;uwaterloo.ca;rpi.edu;",
        "position": "PhD student;MS student;PhD student;",
        "bibtex": "@inproceedings{\nzhu2025sentencekv,\ntitle={Sentence{KV}: Efficient {LLM} Inference via Sentence-Level Semantic {KV} Caching},\nauthor={Yuxuan Zhu and Ali Falahati and David H. Yang and Mohammad Mohammadi Amiri},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=HyPeYU9JR6}\n}",
        "github": "",
        "project": "",
        "reviewers": "ecqc;pn6t;tRUR;W49T",
        "site": "https://openreview.net/forum?id=HyPeYU9JR6",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "4;5;5;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.75,
            0.4330127018922193
        ],
        "replies_avg": [
            29,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "I95XCwHdSE",
        "title": "Exploring Large Language Model Agents for Piloting Social Experiments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Computational social experiments, which typically employ agent-based modeling to create testbeds for piloting social experiments, not only provide a computational solution to the major challenges faced by traditional experimental methods, but have also gained widespread attention in various research fields. Despite their significance, their broader impact is largely limited by the underdeveloped intelligence of their core component, i.e., agents. To address this limitation, we develop a framework grounded in well-established social science theories and practices, consisting of three key elements: (i) large language model (LLM)-driven experimental agents, serving as \"silicon participants\", (ii) methods for implementing various interventions or treatments, and (iii) tools for collecting behavioral, survey, and interview data. We evaluate its effectiveness by replicating three representative experiments, with results demonstrating strong alignment, both quantitatively and qualitatively, with real-world evidence. This work provides the first framework for designing LLM-driven agents to pilot social experiments, underscoring the transformative potential of LLMs and their agents in computational social science.",
        "keywords": "LLM Agents;Social Simulation;Computational Social Experiments",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jinghua Piao;Yuwei Yan;Nian Li;Jun Zhang;Yong Li",
        "authorids": "~Jinghua_Piao1;~Yuwei_Yan1;~Nian_Li1;~Jun_Zhang22;~Yong_Li7",
        "gender": "F;M;M;M;M",
        "homepage": ";https://github.com/PinkGranite;https://scholar.google.com/citations?user=mZBzFC4AAAAJ&hl=en;https://github.com/Sweetnow;http://fi.ee.tsinghua.edu.cn/~liyong/",
        "dblp": "252/6101;169/9076;31/2019\u200b;;",
        "google_scholar": "FfWpCk0AAAAJ;;mZBzFC4AAAAJ;58AEX6UAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0003-2256-4256;0009-0002-3687-4933;0000-0003-4689-2289;0000-0003-1232-3475;",
        "linkedin": ";;;;",
        "or_profile": "~Jinghua_Piao1;~Yuwei_Yan1;~Nian_Li1;~Jun_Zhang22;~Yong_Li7",
        "aff": "Dept. of Electronic Engineering, Tsinghua University;Hong Kong University of Science and Technology;Shenzhen International Graduate School, Tsinghua University;Tsinghua University;Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;hkust-gz.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;PhD student;PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\npiao2025exploring,\ntitle={Exploring Large Language Model Agents for Piloting Social Experiments},\nauthor={Jinghua Piao and Yuwei Yan and Nian Li and Jun Zhang and Yong Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=I95XCwHdSE}\n}",
        "github": "",
        "project": "",
        "reviewers": "LCsr;njT7;KjSi",
        "site": "https://openreview.net/forum?id=I95XCwHdSE",
        "pdf_size": 0,
        "rating": "5;6;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "IAoSG4Q2xC",
        "title": "Hyperparameter Loss Surfaces Are Simple Near their Optima",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Hyperparameters greatly impact models' capabilities; however, modern models are too large for extensive search. Instead, researchers design recipes that train well across scales based on their understanding of the hyperparameters. Despite this importance, few tools exist for understanding the hyperparameter loss surface. We discover novel structure in it and propose a new theory yielding such tools. The loss surface is complex, but as you approach the optimum simple structure emerges. It becomes characterized by a few basic features, like its effective dimension and the best possible loss. To uncover this *asymptotic regime*, we develop a novel technique based on random search. Within this regime, the best scores from random search take on a new distribution we discover. Its parameters are exactly the features defining the loss surface in the asymptotic regime. From these features, we derive a new asymptotic law for random search that can explain and extrapolate its convergence. These new tools enable new analyses, such as confidence intervals for the best possible performance or determining the effective number of hyperparameters. We make these tools available at: https://github.com/nicholaslourie/opda.",
        "keywords": "experimental design;hyperparameters;scaling laws",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicholas Lourie;He He;Kyunghyun Cho",
        "authorids": "~Nicholas_Lourie1;~He_He2;~Kyunghyun_Cho1",
        "gender": ";;M",
        "homepage": ";;http://kyunghyuncho.me",
        "dblp": "218/5457;;41/9736",
        "google_scholar": ";;https://scholar.google.fi/citations?user=0RAmmIAAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Nicholas_Lourie1;~He_He2;~Kyunghyun_Cho1",
        "aff": "New York University;;Genentech+New York University",
        "aff_domain": "nyu.edu;;gene.com+nyu.edu",
        "position": "PhD student;;Executive Director of Frontier Research+Professor",
        "bibtex": "@inproceedings{\nlourie2025hyperparameter,\ntitle={Hyperparameter Loss Surfaces Are Simple Near their Optima},\nauthor={Nicholas Lourie and He He and Kyunghyun Cho},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=IAoSG4Q2xC}\n}",
        "github": "",
        "project": "",
        "reviewers": "DSqh;rHdb;rVAh;tyYN",
        "site": "https://openreview.net/forum?id=IAoSG4Q2xC",
        "pdf_size": 0,
        "rating": "4;4;4;7",
        "confidence": "2;2;2;1",
        "wc_review": "",
        "rating_avg": [
            4.75,
            1.299038105676658
        ],
        "confidence_avg": [
            1.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "IC2WwhUfQg",
        "title": "Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The malicious usage of large language models (LLMs) has motivated the detection of LLM-generated texts. Previous work in topological data analysis shows that the persistent homology dimension (PHD) of text embeddings can serve as a more robust and promising score than other zero-shot methods. However, effectively detecting short LLM-generated texts remains a challenge. This paper presents Short-PHD, a zero-shot LLM-generated text detection method tailored for short texts. Short-PHD stabilizes the estimation of the previous PHD method for short texts by inserting off-topic content before the given input text and identifies LLM-generated text based on an established detection threshold. Experimental results on both public and generated datasets demonstrate that Short-PHD outperforms existing zero-shot methods in short LLM-generated text detection. The implementation codes of this study are available online.",
        "keywords": "large language model;zero-shot detection;short text;topological data analysis",
        "primary_area": "",
        "supplementary_material": "/attachment/ae45d7449848df03bc6c863802520e24986cd3af.zip",
        "author": "Dongjun Wei;Minjia Mao;Xiao Fang;Michael Chau",
        "authorids": "~Dongjun_Wei1;~Minjia_Mao1;~Xiao_Fang5;~Michael_Chau1",
        "gender": ";M;M;",
        "homepage": ";;http://dalab.info;http://www.business.hku.hk/\uff5emchau/",
        "dblp": ";273/0031;;",
        "google_scholar": "x22wN64AAAAJ;I9ej03sAAAAJ;5RZOkYsAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Dongjun_Wei1;~Minjia_Mao1;~Xiao_Fang5;~Michael_Chau1",
        "aff": "University of Hong Kong;University of Delaware;University of Delaware;University of Hong Kong",
        "aff_domain": "hku.hk;udel.edu;udel.edu;hku.hk",
        "position": "PhD student;PhD student;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwei2025shortphd,\ntitle={Short-{PHD}: Detecting Short {LLM}-generated Text with Topological Data Analysis After Off-topic Content Insertion},\nauthor={Dongjun Wei and Minjia Mao and Xiao Fang and Michael Chau},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=IC2WwhUfQg}\n}",
        "github": "",
        "project": "",
        "reviewers": "nqRY;9J1C;hvbd",
        "site": "https://openreview.net/forum?id=IC2WwhUfQg",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "IXwgE8hyJs",
        "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .",
        "keywords": "Speech Language Models;Scaling Analysis;Speech-Text Interleaving",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gallil Maimon;Michael Hassid;Amit Roth;Yossi Adi",
        "authorids": "~Gallil_Maimon1;~Michael_Hassid1;~Amit_Roth1;~Yossi_Adi1",
        "gender": ";M;M;M",
        "homepage": "https://pages.cs.huji.ac.il/gallilmaimon/;;;http://adiyoss.github.io/",
        "dblp": "322/9010;306/7698;;171/0957.html",
        "google_scholar": "x9iloggAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.co.il/citations?user=aUHJ154AAAAJ;https://scholar.google.co.il/citations?user=4W-HuYYAAAAJ",
        "orcid": "0000-0001-6400-3954;;;0000-0003-2237-3898",
        "linkedin": "gallil-maimon-969143255;;;yossi-adi-31a32858?trk=nav_responsive_tab_profile_pic",
        "or_profile": "~Gallil_Maimon1;~Michael_Hassid1;~Amit_Roth1;~Yossi_Adi1",
        "aff": "Hebrew University of Jerusalem;Meta;Hebrew University of Jerusalem;Hebrew University of Jerusalem+Meta",
        "aff_domain": "huji.ac.il;meta.com;huji.ac.il;huji.ac.il+meta.com",
        "position": "PhD student;Intern;MS student;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nmaimon2025scaling,\ntitle={Scaling Analysis of Interleaved Speech-Text Language Models},\nauthor={Gallil Maimon and Michael Hassid and Amit Roth and Yossi Adi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=IXwgE8hyJs}\n}",
        "github": "",
        "project": "",
        "reviewers": "sDgi;LQ6D;T9if;5iR4",
        "site": "https://openreview.net/forum?id=IXwgE8hyJs",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "Itxz7S4Ip3",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
        "keywords": "chain of thought;reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shibo Hao;Sainbayar Sukhbaatar;DiJia Su;Xian Li;Zhiting Hu;Jason E Weston;Yuandong Tian",
        "authorids": "~Shibo_Hao1;~Sainbayar_Sukhbaatar1;~DiJia_Su1;~Xian_Li1;~Zhiting_Hu3;~Jason_E_Weston1;~Yuandong_Tian1",
        "gender": "M;M;;;M;;M",
        "homepage": "https://ber666.github.io/;;;;http://zhiting.ucsd.edu;;http://yuandong-tian.com",
        "dblp": "302/1341;56/10550;;82/1763-3.html;134/4031;;t/YuandongTian",
        "google_scholar": "xwbHbUQAAAAJ;ri1sE34AAAAJ;;v_sIgawAAAAJ;N7_xhHoAAAAJ;;0mgEF28AAAAJ",
        "orcid": ";;;;;;0000-0003-4202-4847",
        "linkedin": ";;;;;;yuandongtian",
        "or_profile": "~Shibo_Hao1;~Sainbayar_Sukhbaatar1;~DiJia_Su1;~Xian_Li1;~Zhiting_Hu3;~Jason_E_Weston1;~Yuandong_Tian1",
        "aff": "University of California, San Diego;Meta AI;;Facebook AI;University of California, San Diego+Amazon;;Meta AI (FAIR)",
        "aff_domain": "ucsd.edu;meta.com;;fb.com;ucsd.edu+amazon.com;;meta.com",
        "position": "PhD student;Research Scientist;;Principal Researcher;Assistant Professor+Researcher;;Research Scientist",
        "bibtex": "@inproceedings{\nhao2025training,\ntitle={Training Large Language Models to Reason in a Continuous Latent Space},\nauthor={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason E Weston and Yuandong Tian},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Itxz7S4Ip3}\n}",
        "github": "",
        "project": "",
        "reviewers": "NeU4;eCiG;BZuh;Hbwo",
        "site": "https://openreview.net/forum?id=Itxz7S4Ip3",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "IyOC5GCzv4",
        "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Diversity is essential for language models to generate creative outputs. Temperature-based sampling is a common strategy to increase diversity. However, for tasks that require high precision, e.g., mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$ or top-$p$ lowers reasoning quality. \nWe demonstrate that the loss of accuracy is caused by sampling incorrect continuations in sensitive positions when entropy is high. To address this, in this paper, we propose selective sampling, a method that dynamically switches between greedy and high-temperature sampling based on a sampling risk metric. This risk metric estimates the likelihood of output errors when applying high temperature sampling on the current token position. We train a lightweight classifier on a small subset of verifiable problems to predict sampling risk. The classifier can be integrated with the base language model with minimal latency overhead. Experiments on mathematical reasoning tasks show that selective sampling improves the quality-diversity trade-off, even under high-temperature settings.",
        "keywords": "Natural Language Processing;Large Language Models;Text Generation;Sampling Methods;Truncation Sampling;Stochastic Sampling;Min-p Sampling;Top-p Sampling;Temperature Sampling;Decoding Methods;LLMs reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sergey Troshin;Wafaa Mohammed;Yan Meng;Christof Monz;Antske Fokkens;Vlad Niculae",
        "authorids": "~Sergey_Troshin1;~Wafaa_Mohammed1;~Yan_Meng3;~Christof_Monz1;~Antske_Fokkens1;~Vlad_Niculae2",
        "gender": "M;F;F;M;F;M",
        "homepage": "https://scholar.google.com/citations?user=1dRuao4AAAAJ&hl=en;https://wafaa014.github.io;https://yanmeng-nlp.com/;https://staff.fnwi.uva.nl/c.monz/;;https://vene.ro",
        "dblp": ";326/0640;;m/ChristofMonz;41/9013;40/10489",
        "google_scholar": "1dRuao4AAAAJ;https://scholar.google.com/citations?hl=en;;0r3PWLQAAAAJ;El5nmZUAAAAJ;7_3UAgQAAAAJ",
        "orcid": ";;;;0000-0002-6628-6916;",
        "linkedin": ";wafaamohammed/;;;;",
        "or_profile": "~Sergey_Troshin1;~Wafaa_Mohammed1;~Yan_Meng3;~Christof_Monz1;~Antske_Fokkens1;~Vlad_Niculae2",
        "aff": "University of Amsterdam;University of Amsterdam;University of Amsterdam;University of Amsterdam;VU University Amsterdam;University of Amsterdam",
        "aff_domain": "uva.nl;uva.nl;uva.nl;ivi.uva.nl;vu.nl;uva.nl",
        "position": "PhD student;PhD student;PhD student;Full Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntroshin2025control,\ntitle={Control the Temperature: Selective Sampling for Diverse and High-Quality {LLM} Outputs},\nauthor={Sergey Troshin and Wafaa Mohammed and Yan Meng and Christof Monz and Antske Fokkens and Vlad Niculae},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=IyOC5GCzv4}\n}",
        "github": "",
        "project": "",
        "reviewers": "vo4Q;s7Xi;YhHJ",
        "site": "https://openreview.net/forum?id=IyOC5GCzv4",
        "pdf_size": 0,
        "rating": "5;7;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            1.247219128924647
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "JMxRn7orEk",
        "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce \ud83c\udff9 CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request\u2014under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.",
        "keywords": "LLM;Evaluation;Benchmark;Personalization;Preferences;Interactions",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tae Soo Kim;Yoonjoo Lee;Yoonah Park;Jiho Kim;Young-Ho Kim;Juho Kim",
        "authorids": "~Tae_Soo_Kim3;~Yoonjoo_Lee1;~Yoonah_Park1;~Jiho_Kim3;~Young-Ho_Kim1;~Juho_Kim2",
        "gender": "M;F;;M;M;M",
        "homepage": "https://taesookim.com;http://yoonjoolee.com/;;https://jihokim.dev;http://younghokim.net;https://juhokim.com/",
        "dblp": ";31/10855;;;;64/3462-1",
        "google_scholar": ";;;https://scholar.google.com/citations?hl=en;https://scholar.google.co.kr/citations?user=b8er4DAAAAAJ;2dDAbMgAAAAJ",
        "orcid": ";0000-0001-7491-986X;;0000-0002-1434-4440;;0000-0001-6348-4127",
        "linkedin": ";;yoonah-park-50ba57249?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;nghtctrl;;",
        "or_profile": "~Tae_Soo_Kim3;~Yoonjoo_Lee1;~Yoonah_Park1;~Jiho_Kim3;~Young-Ho_Kim1;~Juho_Kim2",
        "aff": "Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Seoul National University;Calvin University;NAVER AI Lab;Korea Advanced Institute of Science & Technology",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;snu.ac.kr;calvin.edu;navercorp.com;kaist.ac.kr",
        "position": "PhD student;PhD student;Undergrad student;Undergrad student;Research Scientist;Associate Professor",
        "bibtex": "@inproceedings{\nkim2025cupid,\ntitle={{CUPID}: Evaluating Personalized and Contextualized Alignment of {LLM}s from Interactions},\nauthor={Tae Soo Kim and Yoonjoo Lee and Yoonah Park and Jiho Kim and Young-Ho Kim and Juho Kim},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=JMxRn7orEk}\n}",
        "github": "",
        "project": "",
        "reviewers": "CQeC;sNUJ;puYD",
        "site": "https://openreview.net/forum?id=JMxRn7orEk",
        "pdf_size": 0,
        "rating": "6;8;8",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.49999999999999983
    },
    {
        "id": "JloZnCwhmk",
        "title": "Understanding Layer Significance in LLM Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Aligning large language models (LLMs) through supervised fine-tuning is essential for tailoring them to specific applications. Recent studies suggest that alignment primarily adjusts a model's presentation style rather than its foundational knowledge, indicating that only certain components of the model are significantly impacted. To uncover how alignment affects model behavior at a granular level, we propose identifying which layers within LLMs are most critical to the alignment process. Our approach, named ILA, involves learning a binary mask for the parameter changes in each layer during alignment, as an indicator of layer significance. Experimental results reveal that, despite substantial differences in alignment datasets, the important layers of a model identified by ILA exhibit nearly 90\\% overlap, highlighting fundamental patterns in LLM alignment. The results also indicate that freezing non-essential layers improves overall model performance, while selectively tuning the most critical layers significantly enhances fine-tuning efficiency with minimal performance loss. Finally, we discuss how these findings extend from LLM alignment to reasoning. The source code is available at https://github.com/moukamisama/ILA.",
        "keywords": "Layer Significance;Language Model Alignment",
        "primary_area": "",
        "supplementary_material": "/attachment/a2cf8e7304066a7a65b379b1ad9fc3b59cb017d6.zip",
        "author": "Guangyuan SHI;ZEXIN LU;Xiaoyu DONG;Wenlong Zhang;Xuanyu Zhang;Yujie Feng;Xiao-Ming Wu",
        "authorids": "~Guangyuan_SHI1;~ZEXIN_LU1;~Xiaoyu_DONG4;~Wenlong_Zhang3;~Xuanyu_Zhang1;~Yujie_Feng1;~Xiao-Ming_Wu1",
        "gender": "M;M;;M;M;M;F",
        "homepage": ";;;https://wenlongzhang0517.github.io/;;;http://www4.comp.polyu.edu.hk/~csxmwu/",
        "dblp": ";;;;;116/3380;98/2898-3",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;https://scholar.google.com/citations?hl=zh-CN;;https://scholar.google.com.hk/citations?user=UnMImiUAAAAJ;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.com.hk/citations?hl=zh-CN;3KbaUFkAAAAJ",
        "orcid": "0000-0001-7401-8009;0000-0002-2205-7812;;;;0000-0002-6819-525X;",
        "linkedin": ";;;;;;",
        "or_profile": "~Guangyuan_SHI1;~ZEXIN_LU1;~Xiaoyu_DONG4;~Wenlong_Zhang3;~Xuanyu_Zhang1;~Yujie_Feng1;~Xiao-Ming_Wu1",
        "aff": "The Hong Kong Polytechnic University;Huawei Technologies Ltd.;;Shanghai AI Laboratory;DXM;Hong Kong Polytechnic University;Hong Kong Polytechnic University",
        "aff_domain": "polyu.edu.hk;huawei.com;;pjlab.org.cn;duxiaoman.com;polyu.edu.hk;polyu.edu.hk",
        "position": "PhD student;Researcher;;Researcher;Researcher;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nshi2025understanding,\ntitle={Understanding Layer Significance in {LLM} Alignment},\nauthor={Guangyuan SHI and ZEXIN LU and Xiaoyu DONG and Wenlong Zhang and Xuanyu Zhang and Yujie Feng and Xiao-Ming Wu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=JloZnCwhmk}\n}",
        "github": "",
        "project": "",
        "reviewers": "1beu;pE9b;EcV9;K9M2",
        "site": "https://openreview.net/forum?id=JloZnCwhmk",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "JsaXxGOXfU",
        "title": "Mitigating Modal Imbalance in Multimodal Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Foundation models (FMs) deployed in real-world tasks such as computer-use agents must integrate diverse modalities. How good are FMs at performing *joint reasoning*, simultaneously reasoning over multiple modalities, especially when the modalities interact and relate to each other to form *cross-modal context*? To better understand this problem, we study FMs on *cross-modal conflicts*: scenarios where conflicting evidence is presented across modalities. This allows us to examine whether FMs prioritize one modality over another or reason jointly to reconcile the conflict. Our experiments reveal that FMs can recognize conflicts in *unimodal contexts*, composed of a single modality, 90% of the time, but the ratio falls as low as 3% when evidence is split across modalities -- similar observations hold in *cross-lingual contexts*, composed of multiple languages. We trace this failure to *cross-modal attention imbalance*, showing that FMs exhibit extreme asymmetry in attention scores, disproportionately prioritizing certain modalities. We show that cross-modal attention imbalance does not go away by simply scaling up multimodal or multilingual datasets blindly, since they lack training examples that explicitly require cross-modal reasoning. We demonstrate that even a simple and scalable method of explicitly combining multiple modalities within each training instance significantly reduces attention imbalance. Reduced attention imbalance directly translates to improved downstream performance on several vision-language benchmarks. Our findings underscore the importance of systematically addressing cross-modal contexts to build reliable foundation models.",
        "keywords": "multilingual language models;multimodal language models;cross-modal reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chen Henry Wu;Neil Kale;Aditi Raghunathan",
        "authorids": "~Chen_Henry_Wu1;~Neil_Kale1;~Aditi_Raghunathan1",
        "gender": ";M;F",
        "homepage": ";https://neilkale.github.io/;https://www.cs.cmu.edu/~aditirag/",
        "dblp": ";;166/1409",
        "google_scholar": ";OrQXY9sAAAAJ;Ch9iRwQAAAAJ",
        "orcid": ";;",
        "linkedin": ";neil-kale/;",
        "or_profile": "~Chen_Henry_Wu1;~Neil_Kale1;~Aditi_Raghunathan1",
        "aff": ";School of Computer Science, Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": ";cs.cmu.edu;cmu.edu",
        "position": ";MS student;Assistant Professor",
        "bibtex": "@inproceedings{\nwu2025mitigating,\ntitle={Mitigating Modal Imbalance in Multimodal Reasoning},\nauthor={Chen Henry Wu and Neil Kale and Aditi Raghunathan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=JsaXxGOXfU}\n}",
        "github": "",
        "project": "",
        "reviewers": "gdZy;BCiN;878T;93V9",
        "site": "https://openreview.net/forum?id=JsaXxGOXfU",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;4;5",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9045340337332909
    },
    {
        "id": "K7kwRv5mj1",
        "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.",
        "keywords": "Many-Shot In-Context Learning;In-Context Learning;Large Language Models;Compute-Optimal In-Context Learning;Demonstration Selection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shahriar Golchin;Yanfei Chen;Rujun Han;Manan Gandhi;Tianli Yu;Swaroop Mishra;Mihai Surdeanu;Rishabh Agarwal;Chen-Yu Lee;Tomas Pfister",
        "authorids": "~Shahriar_Golchin1;~Yanfei_Chen1;~Rujun_Han1;~Manan_Gandhi2;~Tianli_Yu1;~Swaroop_Mishra1;~Mihai_Surdeanu1;~Rishabh_Agarwal2;~Chen-Yu_Lee2;~Tomas_Pfister1",
        "gender": "M;M;M;M;M;M;;M;;M",
        "homepage": "https://shahriargolchin.github.io/;https://sites.google.com/site/yanfeichen1990;https://rujunhan.github.io/publications/;;;https://swarooprm.github.io/;http://surdeanu.info/mihai/;https://agarwl.github.io;https://chl260.github.io/;http://tomas.pfister.fi",
        "dblp": ";58/8788;228/5627;;80/4618;249/2784;18/3479;;04/656;14/8360",
        "google_scholar": "iBl-Yc8AAAAJ;qyua6O4AAAAJ;HOF4Q8EAAAAJ;;bZaq0-0AAAAJ;-7LK2SwAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=aH8AJu4AAAAJ;uWPUSEgAAAAJ;ahSpJOAAAAAJ",
        "orcid": ";;;;;;;;;0009-0004-4088-8718",
        "linkedin": "shahriar-golchin;;;manan-g/;tianliyu;;;;chenyulee260/;",
        "or_profile": "~Shahriar_Golchin1;~Yanfei_Chen1;~Rujun_Han1;~Manan_Gandhi2;~Tianli_Yu1;~Swaroop_Mishra1;~Mihai_Surdeanu1;~Rishabh_Agarwal2;~Chen-Yu_Lee2;~Tomas_Pfister1",
        "aff": "Google;Google;Google;Google;Google Cloud AI;Microsoft+Google;University of Arizona;Meta Facebook+McGill University+Google DeepMind;Google;Google",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;microsoft.com+google.com;arizona.edu;meta.com+mcgill.ca+google.com;google.com;google.com",
        "position": "Intern;Software Engineer;Researcher;Researcher;Staff Software Engineer;Researcher+Researcher;Full Professor;Researcher+Adjunct Professor+Research Scientist;Research Scientist;Head of Research @ Cloud AI",
        "bibtex": "@inproceedings{\ngolchin2025towards,\ntitle={Towards Compute-Optimal Many-Shot In-Context Learning},\nauthor={Shahriar Golchin and Yanfei Chen and Rujun Han and Manan Gandhi and Tianli Yu and Swaroop Mishra and Mihai Surdeanu and Rishabh Agarwal and Chen-Yu Lee and Tomas Pfister},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=K7kwRv5mj1}\n}",
        "github": "",
        "project": "",
        "reviewers": "wBuF;KMMK;nzUp;MXr4",
        "site": "https://openreview.net/forum?id=K7kwRv5mj1",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "KI1WQ6rLiy",
        "title": "HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Interactive AI Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To address the growing safety risks as AI agents become increasingly autonomous in their interactions with human users and environments, we present HAICOSYSTEM, a framework examining AI agent safety within diverse and complex social interactions. HAICOSYSTEM features a modular sandbox environment that simulates multi-turn interactions between users and AI agents. We then develop a comprehensive multi-dimensional evaluation framework that uses metrics covering operational, content-related, societal, and legal risks to examine the safety of AI agents in these interactions. Through running over 8K simulations based on 132 scenarios across seven domains (e.g., healthcare, finance, education), we show that state-of-the-art LLMs exhibit safety risks in 62% of cases, particularly during tool use with malicious users, highlighting the importance of evaluating and addressing AI agent safety in dynamic human-AI-environment interactions.",
        "keywords": "AI Safety;Multi-Agent Systems;Human-AI Interaction;Social Simulation",
        "primary_area": "",
        "supplementary_material": "/attachment/03da59a388eb322b34c88124a41ad0962f77ba22.zip",
        "author": "Xuhui Zhou;Hyunwoo Kim;Faeze Brahman;Liwei Jiang;Hao Zhu;Ximing Lu;Frank F. Xu;Bill Yuchen Lin;Yejin Choi;Niloofar Mireshghallah;Ronan Le Bras;Maarten Sap",
        "authorids": "~Xuhui_Zhou1;~Hyunwoo_Kim3;~Faeze_Brahman1;~Liwei_Jiang2;~Hao_Zhu1;~Ximing_Lu1;~Frank_F._Xu1;~Bill_Yuchen_Lin2;~Yejin_Choi1;~Niloofar_Mireshghallah1;~Ronan_Le_Bras1;~Maarten_Sap1",
        "gender": "M;M;F;F;M;F;M;;F;;M;M",
        "homepage": "https://xuhuizhou.github.io/;http://hyunwookim.com;https://fabrahman.github.io;https://liweijiang.me;http://www.zhuhao.me;https://gloriaximinglu.github.io/;https://frankxfz.me/;;https://yejinc.github.io/;;https://rlebras.github.io/index.html;http://maartensap.com",
        "dblp": ";02/8768-2;276/6005;;10/3520-6;24/10879;190/4519;;89/579-1;;;153/9519",
        "google_scholar": "CKyX_Y8AAAAJ;https://scholar.google.co.kr/citations?user=PAXFuxsAAAAJ;viCG2ikAAAAJ;lcPsDgUAAAAJ;-3yFcsMAAAAJ;https://scholar.google.com/citations?hl=en;1hXyfIkAAAAJ;;vhP-tlcAAAAJ;;8dXLDSsAAAAJ;gFN4QUYAAAAJ",
        "orcid": ";0009-0002-2714-1287;;;;;;;;;;",
        "linkedin": ";hyunw-kim/;;;;;;;;;;",
        "or_profile": "~Xuhui_Zhou1;~Hyunwoo_Kim3;~Faeze_Brahman1;~Liwei_Jiang2;~Hao_Zhu1;~Ximing_Lu1;~Frank_F._Xu1;~Bill_Yuchen_Lin2;~Yejin_Choi1;~Niloofar_Mireshghallah1;~Ronan_Le_Bras1;~Maarten_Sap1",
        "aff": "Carnegie Mellon University;NVIDIA;Allen Institute for Artificial Intelligence;University of Washington;Stanford University;University of Washington;Microsoft;;Computer Science Department, Stanford University+NVIDIA;;Allen Institute for Artificial Intelligence;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;nvidia.com;allenai.org;washington.edu;stanford.edu;cs.washington.edu;microsoft.com;;cs.stanford.edu+nvidia.com;;allenai.org;cmu.edu",
        "position": "PhD student;Postdoc;Researcher;PhD student;Postdoc;PhD student;Researcher;;Full Professor+Researcher;;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nzhou2025haicosystem,\ntitle={{HAICOSYSTEM}: An Ecosystem for Sandboxing Safety Risks in Interactive {AI} Agents},\nauthor={Xuhui Zhou and Hyunwoo Kim and Faeze Brahman and Liwei Jiang and Hao Zhu and Ximing Lu and Frank F. Xu and Bill Yuchen Lin and Yejin Choi and Niloofar Mireshghallah and Ronan Le Bras and Maarten Sap},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=KI1WQ6rLiy}\n}",
        "github": "",
        "project": "",
        "reviewers": "ReNx;CEzF;bvmr;Zh9p",
        "site": "https://openreview.net/forum?id=KI1WQ6rLiy",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "KQhUEoPmJy",
        "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified by instruction tuning.\nHowever, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity.\nWe propose a two-step causal experimental approach to disentangle these factors.\nFirst, we finetune models multiple times using different random seeds to study how training randomness affects over $30$ cognitive biases.\nSecond, we introduce \\emph{cross-tuning} -- swapping instruction datasets between models to isolate bias sources.\nThis swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent.\nOur findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data.\nThese insights suggest that understanding biases in finetuned models requires considering their pretraining origins, especially given their high post-finetuning variability.\nThis perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs. See our code and models at: https://itay1itzhak.github.io/planted-in-pretraining.",
        "keywords": "bias;cognitive biases;large language models;LLM biases;bias analysis;instruction tuning;pretraining biases;causal experiments;bias evaluation;machine learning biases;model robustness;language model interpretability;bias sources",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Itay Itzhak;Yonatan Belinkov;Gabriel Stanovsky",
        "authorids": "~Itay_Itzhak1;~Yonatan_Belinkov1;~Gabriel_Stanovsky1",
        "gender": "M;M;M",
        "homepage": "https://itay1itzhak.github.io/;https://www.belinkov.com;https://gabrielstanovsky.github.io/",
        "dblp": "300/4099;136/8705;166/1740",
        "google_scholar": "knwqPdoAAAAJ;https://scholar.google.com/citations?authorid=K-6ujU4AAAAJ;AtkvBFYAAAAJ",
        "orcid": "0009-0000-5114-5451;;",
        "linkedin": ";;",
        "or_profile": "~Itay_Itzhak1;~Yonatan_Belinkov1;~Gabriel_Stanovsky1",
        "aff": "Technion - Israel Institute of Technology;Technion;Hebrew University of Jerusalem",
        "aff_domain": "campus.technion.ac.il;technion.ac.il;huji.ac.il",
        "position": "PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nitzhak2025planted,\ntitle={Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in {LLM}s},\nauthor={Itay Itzhak and Yonatan Belinkov and Gabriel Stanovsky},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=KQhUEoPmJy}\n}",
        "github": "",
        "project": "",
        "reviewers": "mav9;Eywr;VEAg;BmJn",
        "site": "https://openreview.net/forum?id=KQhUEoPmJy",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "4;2;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "Kd97lfFfTu",
        "title": "Not All Data Are Unlearned Equally",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability- and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
        "keywords": "LLM unlearning;analysis;frequency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aravind Krishnan;Siva Reddy;Marius Mosbach",
        "authorids": "~Aravind_Krishnan1;~Siva_Reddy1;~Marius_Mosbach1",
        "gender": "M;M;M",
        "homepage": ";http://sivareddy.in;https://mariusmosbach.com",
        "dblp": "201/3308.html;64/8153;228/8380",
        "google_scholar": ";;https://scholar.google.de/citations?user=O7RwHEkAAAAJ",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Aravind_Krishnan1;~Siva_Reddy1;~Marius_Mosbach1",
        "aff": "Saarland Univeristy;ServiceNow Inc+Mila, McGill University+Mila, McGill University;McGill University+Mila - Quebec Artificial Intelligence Institute",
        "aff_domain": "uni-saarbruecken.de;servicenow.com+cs.mcgill.ca+mila.quebec;mcgill.ca+mila.quebec",
        "position": "Researcher;Researcher+Assistant Professor+Assistant Professor;Postdoc+Postdoc",
        "bibtex": "@inproceedings{\nkrishnan2025not,\ntitle={Not All Data Are Unlearned Equally},\nauthor={Aravind Krishnan and Siva Reddy and Marius Mosbach},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Kd97lfFfTu}\n}",
        "github": "",
        "project": "",
        "reviewers": "45nH;qvTu;vCPa;rsSN",
        "site": "https://openreview.net/forum?id=Kd97lfFfTu",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "KtGsJm8bOC",
        "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines\u2014including sparse and dense retrievers combined with frontier LLMs\u2014reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.",
        "keywords": "Retrieval-Augmented Generation;Retrieval;Summarization;Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rohan Phanse;Ej Zhou;Kejian Shi;WENCAI ZHANG;Yixin Liu;Yilun Zhao;Arman Cohan",
        "authorids": "~Rohan_Phanse1;~Ej_Zhou1;~Kejian_Shi2;~WENCAI_ZHANG1;~Yixin_Liu2;~Yilun_Zhao1;~Arman_Cohan1",
        "gender": "M;;;M;;;M",
        "homepage": ";;;https://github.com/dadyita;https://yixinl7.github.io/;https://yilunzhao.github.io/;http://www.armancohan.com",
        "dblp": ";;;;140/7348-3;271/8391-1;160/1727",
        "google_scholar": ";;;;sFtxaMkAAAAJ;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0000-7654-5689;;;;;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Rohan_Phanse1;~Ej_Zhou1;~Kejian_Shi2;~WENCAI_ZHANG1;~Yixin_Liu2;~Yilun_Zhao1;~Arman_Cohan1",
        "aff": "Yale University;;;Alibaba Group;Yale University;Yale University;Yale University+Allen Institute for Artificial Intelligence",
        "aff_domain": "yale.edu;;;alibaba-inc.com;yale.edu;yale.edu;yale.edu+allenai.org",
        "position": "Undergrad student;;;Researcher;PhD student;PhD student;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nphanse2025msrs,\ntitle={{MSRS}: Benchmarking Multi-Source Retrieval-Augmented Generation},\nauthor={Rohan Phanse and Ej Zhou and Kejian Shi and WENCAI ZHANG and Yixin Liu and Yilun Zhao and Arman Cohan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=KtGsJm8bOC}\n}",
        "github": "",
        "project": "",
        "reviewers": "JeWr;Witb;QKAz;aSkS",
        "site": "https://openreview.net/forum?id=KtGsJm8bOC",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "L2NPhLAKEd",
        "title": "In-context Ranking Preference Optimization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent developments in Direct Preference Optimization (DPO) allow large language models (LLMs) to function as implicit ranking models by maximizing the margin between preferred and non-preferred responses. In practice, user feedback on such lists typically involves identifying a few relevant items in context rather than providing detailed pairwise comparisons for every possible item pair. Besides, many complex information retrieval tasks, such as conversational agents and summarization systems, critically depend on ranking the highest-quality outputs at the top, further emphasizing the need to support natural and flexible forms of user feedback. To address the challenge of limited and sparse pairwise feedback in the in-context setting, we propose an In-context Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs based on ranking lists constructed during inference. To further capture the natural and flexible forms of feedback, IRPO extends the DPO objective by incorporating both the relevance of items and their positions in the list. Modeling these aspects jointly is non-trivial, as ranking metrics are inherently discrete and non-differentiable, making direct optimization challenging. To overcome this, IRPO introduces a differentiable objective based on positional aggregation of pairwise item preferences, enabling effective gradient-based optimization of discrete ranking metrics. We further provide theoretical insights showing that IRPO (i) automatically emphasizes items with greater disagreement between the model and the reference ranking, and (ii) shows its gradient's linkage to an importance sampling estimator, resulting in an unbiased gradient estimator with reduced variance. Empirical evaluations demonstrate that IRPO outperforms standard DPO approaches in ranking performance, highlighting its effectiveness and efficiency in aligning LLMs with direct in-context ranking preferences.",
        "keywords": "direct preference optimization;large language model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junda Wu;Rohan Surana;Zhouhang Xie;Yiran Shen;Yu Xia;Tong Yu;Ryan A. Rossi;Prithviraj Ammanabrolu;Julian McAuley",
        "authorids": "~Junda_Wu1;~Rohan_Surana1;~Zhouhang_Xie1;~Yiran_Shen2;~Yu_Xia9;~Tong_Yu3;~Ryan_A._Rossi2;~Prithviraj_Ammanabrolu1;~Julian_McAuley1",
        "gender": "M;M;M;;M;;;M;M",
        "homepage": "https://scholar.google.com/citations?user=_iKeQFwAAAAJ&hl=en;;https://zhouhanxie.github.io/;;https://andree-9.github.io/;https://www.linkedin.com/in/tong-yu-42790744;;http://prithvirajva.com;http://cseweb.ucsd.edu/~jmcauley/",
        "dblp": "295/8249;;299/9534;;28/4326-7;32/1593-1;;202/2351;29/3483",
        "google_scholar": "_iKeQFwAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;;sTVqEUMAAAAJ;https://scholar.google.com/citations?hl=en;;2yaiWZ8AAAAJ;icbo4M0AAAAJ",
        "orcid": ";;;;;0000-0002-5991-2050;;;0000-0003-0955-7588",
        "linkedin": ";;;jennyshen56/;;tong-yu-42790744;;rajammanabrolu/;",
        "or_profile": "~Junda_Wu1;~Rohan_Surana1;~Zhouhang_Xie1;~Yiran_Shen2;~Yu_Xia9;~Tong_Yu3;~Ryan_A._Rossi2;~Prithviraj_Ammanabrolu1;~Julian_McAuley1",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;Adobe Research;;University of California, San Diego;University of California, San Diego, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;adobe.com;;ucsd.edu;eng.ucsd.edu",
        "position": "PhD student;MS student;PhD student;PhD student;PhD student;Senior Research Scientist;;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nwu2025incontext,\ntitle={In-context Ranking Preference Optimization},\nauthor={Junda Wu and Rohan Surana and Zhouhang Xie and Yiran Shen and Yu Xia and Tong Yu and Ryan A. Rossi and Prithviraj Ammanabrolu and Julian McAuley},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=L2NPhLAKEd}\n}",
        "github": "",
        "project": "",
        "reviewers": "P8DX;Fvxe;5e1t;SMDr",
        "site": "https://openreview.net/forum?id=L2NPhLAKEd",
        "pdf_size": 0,
        "rating": "6;6;7;9",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4714045207910316
    },
    {
        "id": "L4HHkCDz2x",
        "title": "AIOS: LLM Agent Operating System",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. \nFurthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control)  for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to $2.1\\times$ faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.",
        "keywords": "Large Language Model;LLM Agent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kai Mei;Xi Zhu;Wujiang Xu;Mingyu Jin;Wenyue Hua;Zelong Li;Shuyuan Xu;Ruosong Ye;Yingqiang Ge;Yongfeng Zhang",
        "authorids": "~Kai_Mei1;~Xi_Zhu2;~Wujiang_Xu1;~Mingyu_Jin1;~Wenyue_Hua1;~Zelong_Li1;~Shuyuan_Xu1;~Ruosong_Ye1;~Yingqiang_Ge1;~Yongfeng_Zhang1",
        "gender": ";M;M;M;F;M;;M;M;",
        "homepage": ";https://xizhu1022.github.io;https://wujiangxu.github.io/;https://mingyuj666.github.io/;;https://lzl65825.github.io/;;;https://yingqiangge.github.io/;",
        "dblp": "224/4831;15/6877-4;283/0798;;278/7993;196/1897;239/2804;;240/9457;",
        "google_scholar": "8slSsa8AAAAJ;https://scholar.google.com/citations?hl=en;cucjW6wAAAAJ;aYfy924AAAAJ;Yqw8P-QAAAAJ;QuSh1ioAAAAJ;rzhysE4AAAAJ;;5BrSdkUAAAAJ;",
        "orcid": ";0000-0003-3621-8493;;0009-0007-6990-7355;0009-0008-2043-2704;0000-0002-3110-4481;;;;",
        "linkedin": ";xi-zhu-jerry/;;%E6%98%8E%E5%AE%87-%E9%87%91-961159288/?locale=en_US;wenyue-hua-094b6b176/;;;ruosong-ye-a0507724b/;yingqiang-ge-29478b152/;",
        "or_profile": "~Kai_Mei1;~Xi_Zhu2;~Wujiang_Xu1;~Mingyu_Jin1;~Wenyue_Hua1;~Zelong_Li1;~Shuyuan_Xu1;~Ruosong_Ye1;~Yingqiang_Ge1;~Yongfeng_Zhang1",
        "aff": "Rutgers University, New Brunswick;Rutgers University;Rutgers University;Rutgers University;University of California, Santa Barbara;Amazon;Rutgers University;Rutgers University;Amazon;",
        "aff_domain": "rutgers.edu;rutgers.edu;cs.rutgers.edu;cs.rutgers.edu;ucsb.edu;amazon.com;rutgers.edu;rutgers.edu;amazon.com;",
        "position": "PhD student;PhD student;PhD student;PhD student;Postdoc;Researcher;PhD student;PhD student;Researcher;",
        "bibtex": "@inproceedings{\nmei2025aios,\ntitle={{AIOS}: {LLM} Agent Operating System},\nauthor={Kai Mei and Xi Zhu and Wujiang Xu and Mingyu Jin and Wenyue Hua and Zelong Li and Shuyuan Xu and Ruosong Ye and Yingqiang Ge and Yongfeng Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=L4HHkCDz2x}\n}",
        "github": "",
        "project": "",
        "reviewers": "ZN9n;4vNH;UuGH;FGa7",
        "site": "https://openreview.net/forum?id=L4HHkCDz2x",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "3;2;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "L7jS3peM3w",
        "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. We incorporate the two-stream SlowFast mechanism into a streamlined training pipeline, and perform joint video-image training on a carefully curated data mixture of only publicly available datasets. Our primary focus is on highly efficient model scales (1B and 3B), demonstrating that even relatively small Video LLMs can achieve state-of-the-art performance on video understanding, meeting the demand for mobile-friendly models. Experimental results demonstrate that SF-LLaVA-1.5 achieves superior performance on a wide range of video and image tasks, with robust results at all model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales across various video benchmarks.",
        "keywords": "Multimodal LLM;Video Understanding;Video Question Answering;Long-Form Video Understanding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mingze Xu;Mingfei Gao;Shiyu Li;Jiasen Lu;Zhe Gan;Zhengfeng Lai;Meng Cao;Kai Kang;Yinfei Yang;Afshin Dehghan",
        "authorids": "~Mingze_Xu2;~Mingfei_Gao1;~Shiyu_Li2;~Jiasen_Lu2;~Zhe_Gan1;~Zhengfeng_Lai1;~Meng_Cao2;~Kai_Kang2;~Yinfei_Yang1;~Afshin_Dehghan5",
        "gender": ";;M;;M;M;M;M;;",
        "homepage": "https://xumingze0308.github.io/;https://fly6464.github.io;;;http://zhegan27.github.io/;https://zjujefflai.github.io/;https://www.linkedin.com/in/caomeng/;http://kangk.ai;;",
        "dblp": ";67/6825;;;41/7845;272/6220;;;117/4082;",
        "google_scholar": "KNcECJQAAAAJ;kMe-G5AAAAAJ;;;E64XWyMAAAAJ;https://scholar.google.com/citations?view_op=list_works;;brEBMIkAAAAJ;kvDbu90AAAAJ;",
        "orcid": ";;;;;0000-0002-2984-7913;;0000-0002-6707-4616;;",
        "linkedin": ";;shiyu-li-a43360a0/;;zhe-gan-a2229a78/;zhengfeng-jeff-lai-b3a214160/;caomeng/;;;",
        "or_profile": "~Mingze_Xu2;~Mingfei_Gao1;~Shiyu_Li2;~Jiasen_Lu2;~Zhe_Gan1;~Zhengfeng_Lai1;~Meng_Cao2;~Kai_Kang2;~Yinfei_Yang1;~Afshin_Dehghan5",
        "aff": "Apple;Apple;Apple;;Apple;Apple;Apple;Apple;Apple;",
        "aff_domain": "apple.com;apple.com;apple.com;;apple.com;apple.com;apple.com;apple.com;apple.com;",
        "position": "Researcher;Researcher;Researcher;;Principal Researcher;Researcher;Researcher;Researcher;Researcher;",
        "bibtex": "@inproceedings{\nxu2025slowfastllava,\ntitle={SlowFast-{LL}a{VA}-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding},\nauthor={Mingze Xu and Mingfei Gao and Shiyu Li and Jiasen Lu and Zhe Gan and Zhengfeng Lai and Meng Cao and Kai Kang and Yinfei Yang and Afshin Dehghan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=L7jS3peM3w}\n}",
        "github": "",
        "project": "",
        "reviewers": "edor;9Koa;2u6V;sRib",
        "site": "https://openreview.net/forum?id=L7jS3peM3w",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "LH2ZKviJoI",
        "title": "Data-Centric Human Preference with Rationales for Direct Preference Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Aligning language models with human preferences through  reinforcement learning from human feedback is crucial for their safe and effective deployment. The human preference is typically represented through comparison where one response is chosen over another for a given prompt. However, standard preference datasets often lack explicit information on why a particular choice was made, presenting an ambiguity that can hinder efficient learning and robust alignment, especially given the high cost of acquiring extensive human annotations. While many studies focus on algorithmic improvements, this work adopts a data-centric perspective, exploring how to enhance learning from existing preference data. We propose augmenting standard preference pairs with rationales that explain the reasoning behind the human preference. Specifically, we introduce a simple and principled framework that leverages machine-generated rationales to enrich preference data for preference optimization algorithms. Our comprehensive analysis demonstrates that incorporating rationales improves learning efficiency. Extensive experiments reveal some advantages: rationale-augmented learning accelerates convergence and can achieve higher final model performance. Furthermore, this approach is versatile and compatible with various direct preference optimization algorithms. Our findings showcase the potential of thoughtful data design in preference learning, demonstrating that enriching existing datasets with explanatory rationales can help unlock improvements in model alignment and annotation efficiency.",
        "keywords": "data-centric AI;rationales",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hoang Anh Just;Ming Jin;Anit Kumar Sahu;Huy Phan;Ruoxi Jia",
        "authorids": "~Hoang_Anh_Just1;~Ming_Jin1;~Anit_Kumar_Sahu1;~Huy_Phan2;~Ruoxi_Jia1",
        "gender": ";M;;M;",
        "homepage": "https://justhoanganh.com;www.jinming.tech;;https://pquochuy.github.io/;https://ruoxijia.info/",
        "dblp": "307/2901;;;;147/5355-1",
        "google_scholar": "XcBDQhAAAAAJ;;;RegoACcAAAAJ;JCrug-YAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Hoang_Anh_Just1;~Ming_Jin1;~Anit_Kumar_Sahu1;~Huy_Phan2;~Ruoxi_Jia1",
        "aff": "Virginia Polytechnic Institute and State University;University of California, Berkeley;;Meta;Virginia Tech",
        "aff_domain": "vt.edu;berkeley.edu;;meta.com;vt.edu",
        "position": "PhD student;PhD student;;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\njust2025datacentric,\ntitle={Data-Centric Human Preference with Rationales for Direct Preference Alignment},\nauthor={Hoang Anh Just and Ming Jin and Anit Kumar Sahu and Huy Phan and Ruoxi Jia},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=LH2ZKviJoI}\n}",
        "github": "",
        "project": "",
        "reviewers": "wt4m;8NKj;gDSV;DP63",
        "site": "https://openreview.net/forum?id=LH2ZKviJoI",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "LKINTp7Gdo",
        "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language model (LM) agents are increasingly used as autonomous decision-makers which need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world\u2014key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs\u2019 ability to explore and infer causal relationships, using the well-established Blicket Test paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This \u201cdisjunctive bias\u201d persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not child-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.",
        "keywords": "agents;decision making;exploration;cognitive science;causal inference",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anthony GX-Chen;Dongyan Lin;Mandana Samiei;Doina Precup;Blake Aaron Richards;Rob Fergus;Kenneth Marino",
        "authorids": "~Anthony_GX-Chen1;~Dongyan_Lin1;~Mandana_Samiei1;~Doina_Precup1;~Blake_Aaron_Richards1;~Rob_Fergus1;~Kenneth_Marino1",
        "gender": ";;;F;M;M;M",
        "homepage": ";;;http://cs.mcgill.ca/~dprecup/;http://linclab.org;http://cs.nyu.edu/fergus/;http://kennethmarino.com",
        "dblp": "310/1681.html;;;p/DoinaPrecup;70/10850;77/3763;192/1969",
        "google_scholar": "7jAlFsIAAAAJ;t_Xg9MIAAAAJ;;https://scholar.google.com.tw/citations?user=j54VcVEAAAAJ;https://scholar.google.ca/citations?user=1CPY1LsAAAAJ;https://scholar.google.com.tw/citations?user=GgQ9GEkAAAAJ;",
        "orcid": ";;;;0000-0001-9662-2151;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Anthony_GX-Chen1;~Dongyan_Lin1;~Mandana_Samiei1;~Doina_Precup1;~Blake_Aaron_Richards1;~Rob_Fergus1;~Kenneth_Marino1",
        "aff": "New York University;Meta Facebook;;Google DeepMind+McGill University;Google+McGill University+Mila - Quebec Artificial Intelligence Institute;Meta+New York University+Google;University of Utah+Google",
        "aff_domain": "nyu.edu;meta.com;;google.com+mcgill.ca;google.com+mcgill.ca+mila.quebec;meta.com+nyu.edu+google.com;utah.edu+google.com",
        "position": "PhD student;Postdoc;;Research Team Lead+Associate Professor;Researcher+Associate Professor+Associate Professor;Researcher+Professor+Research scientist;Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\ngx-chen2025language,\ntitle={Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?},\nauthor={Anthony GX-Chen and Dongyan Lin and Mandana Samiei and Doina Precup and Blake Aaron Richards and Rob Fergus and Kenneth Marino},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=LKINTp7Gdo}\n}",
        "github": "",
        "project": "",
        "reviewers": "o9YV;Gqdc;w25T;badZ",
        "site": "https://openreview.net/forum?id=LKINTp7Gdo",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            29,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "LriQ3NY9uL",
        "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses *verifiers* to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: *scaling the number of verifier models*. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. To investigate scaling up the verification compute, we propose to combine multiple Aspect Verifiers (AVs) --- off-the-shelf LLMs prompted to verify different aspects of outputs. AVs are a convenient building block for MAV since they can be easily combined without any additional training. We introduce BoN-MAV as a simple multi-agent verification algorithm that combines best-of-*n* sampling with aspect verifiers, and we show that performance improves as we spend more verification compute at test-time by increasing the number and type of verifiers. Moreover, we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number and type of verifier models as a promising new dimension for improving language model performance at test time.",
        "keywords": "large language models;test-time compute;verification;scaling",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shalev Lifshitz;Sheila A. McIlraith;Yilun Du",
        "authorids": "~Shalev_Lifshitz1;~Sheila_A._McIlraith1;~Yilun_Du1",
        "gender": ";F;",
        "homepage": ";http://www.cs.toronto.edu/~sheila/;https://yilundu.github.io",
        "dblp": ";66/3221;204/4379",
        "google_scholar": "brwVM08AAAAJ;https://scholar.google.com.tw/citations?user=ny2zuvMAAAAJ;",
        "orcid": ";0000-0003-4953-0945;",
        "linkedin": ";sheila-mcilraith-a76aa513/?originalSubdomain=ca;",
        "or_profile": "~Shalev_Lifshitz1;~Sheila_A._McIlraith1;~Yilun_Du1",
        "aff": "ArdaLabs;Department of Computer Science, University of Toronto;Google",
        "aff_domain": "ardalabs.ai;cs.toronto.edu;google.com",
        "position": "Researcher;Full Professor;Researcher",
        "bibtex": "@inproceedings{\nlifshitz2025multiagent,\ntitle={Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers},\nauthor={Shalev Lifshitz and Sheila A. McIlraith and Yilun Du},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=LriQ3NY9uL}\n}",
        "github": "",
        "project": "",
        "reviewers": "tRY4;6Jrp;G6Pq;D7wR",
        "site": "https://openreview.net/forum?id=LriQ3NY9uL",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "M7cl4Ldw61",
        "title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. *Falsifying* hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Yet current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them. We advocate for developing benchmarks that evaluate this inverse capability \u2014 creating counterexamples for subtly incorrect solutions. To demonstrate this approach, we start with the domain of algorithmic problem solving, where counterexamples can be evaluated automatically using code execution. Specifically, we introduce REFUTE, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples. Our analysis finds that the best reasoning agents, even OpenAI o3-mini (high) with code execution feedback, can create counterexamples for only $<9$% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to $48$% of these problems from scratch. We hope our work spurs progress in evaluating and enhancing LMs' ability to falsify incorrect solutions \u2014 a capability that is crucial for both accelerating research and making models self-improve through reliable reflective reasoning.",
        "keywords": "code; self-repair; falsification",
        "primary_area": "",
        "supplementary_material": "/attachment/8be7e705cc31c530663056dbb1c28e95a1c2dd87.zip",
        "author": "Shiven Sinha;Shashwat Goel;Ponnurangam Kumaraguru;Jonas Geiping;Matthias Bethge;Ameya Prabhu",
        "authorids": "~Shiven_Sinha1;~Shashwat_Goel1;~Ponnurangam_Kumaraguru3;~Jonas_Geiping1;~Matthias_Bethge1;~Ameya_Prabhu1",
        "gender": "M;M;;M;M;M",
        "homepage": ";https://shash42.github.io/;https://precog.iiit.ac.in/;https://jonasgeiping.github.io/;https://bethgelab.org;https://drimpossible.github.io/",
        "dblp": "368/3856;300/8333.html;97/5147.html;190/7229;77/3005;181/4512",
        "google_scholar": "WO51AfgAAAAJ;exaNV-0AAAAJ;MfzQyP8AAAAJ;https://scholar.google.de/citations?user=206vNCEAAAAJ;https://scholar.google.com/citations?hl=en;0kK7sSAAAAAJ",
        "orcid": "0009-0000-5259-2683;;;;;",
        "linkedin": "shiven-sinha/;shashwatgoel42/;ponguru/;;;",
        "or_profile": "~Shiven_Sinha1;~Shashwat_Goel1;~Ponnurangam_Kumaraguru3;~Jonas_Geiping1;~Matthias_Bethge1;~Ameya_Prabhu1",
        "aff": "International Institute of Information Technology, Hyderabad+International Institute of Information Technology, Hyderabad;ELLIS, Max Planck Institute;International Institute of Information Technology Hyderabad ;ELLIS Institute T\u00fcbingen+Max Planck Institute for Intelligent Systems, Max-Planck Institute;University of Tuebingen;Eberhard-Karls-Universit\u00e4t T\u00fcbingen",
        "aff_domain": "iiit.ac.in+iiit.ac.in;tuebingen.mpg.de;iiit.ac.in;tue.ellis.eu+tuebingen.mpg.de;uni-tuebingen.de;uni-tuebingen.de",
        "position": "MS student+Undergrad student;PhD student;Full Professor;Principal Researcher+Principal Researcher;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nsinha2025can,\ntitle={Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation},\nauthor={Shiven Sinha and Shashwat Goel and Ponnurangam Kumaraguru and Jonas Geiping and Matthias Bethge and Ameya Prabhu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=M7cl4Ldw61}\n}",
        "github": "",
        "project": "",
        "reviewers": "uUPk;sybC;J67i;LiKZ",
        "site": "https://openreview.net/forum?id=M7cl4Ldw61",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;5;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "MPTlWIVSMU",
        "title": "Have Large Language Models Learned to Reason? A Characterization via 3-SAT",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. In theory, autoregressive LLMs with Chain-of-Thought (CoT) can perform more serial computations to solve complex reasoning tasks. However, recent studies suggest that, despite this capacity, LLMs do not truly learn to reason but instead fit on statistical features. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of state-of-the-art LLMs by varying the inherent hardness of the problem instances. By comparing DeepSeek R1 with other LLMs, our findings reveal two key insights (1) LLM accuracy drops significantly on harder instances, suggesting all current models struggle when statistical shortcuts are unavailable (2) Unlike other LLMs, R1 shows signs of having learned the underlying reasoning. Following a principled experimental protocol, our study moves beyond the benchmark-driven evidence often found in LLM reasoning research. Our findings highlight important gaps and suggest clear directions for future research. Link to our code.",
        "keywords": "Large Language Models;Reasoning;Computational Complexity;Logic;Satisfiability;Phase Transitions",
        "primary_area": "",
        "supplementary_material": "",
        "author": "RISHI HAZRA;Gabriele Venturato;Pedro Zuidberg Dos Martires;Luc De Raedt",
        "authorids": "~RISHI_HAZRA1;~Gabriele_Venturato1;~Pedro_Zuidberg_Dos_Martires1;~Luc_De_Raedt1",
        "gender": "M;;M;M",
        "homepage": "https://rishihazra.github.io/;https://gabventurato.github.io;https://pedrozudo.github.io/;https://people.cs.kuleuven.be/~luc.deraedt/",
        "dblp": "252/5033;301/6407;223/4292;r/LucDeRaedt",
        "google_scholar": "aWN72pEAAAAJ;QX5rfgcAAAAJ;;https://scholar.google.com.tw/citations?user=dgobB6AAAAAJ",
        "orcid": "0000-0003-3422-2085;0000-0002-0048-0898;;0000-0002-6860-6303",
        "linkedin": ";gabrieleventurato/;;",
        "or_profile": "~RISHI_HAZRA1;~Gabriele_Venturato1;~Pedro_Zuidberg_Dos_Martires1;~Luc_De_Raedt1",
        "aff": "Meta Facebook;KU Leuven;\u00d6rebro University;KU Leuven+\u00d6rebro University+KU Leuven, Belgium",
        "aff_domain": "meta.com;kuleuven.be;oru.se;+oru.se+cs.kuleuven.be",
        "position": "Postdoc;PhD student;Assistant Professor;Full Professor+Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nhazra2025have,\ntitle={Have Large Language Models Learned to Reason? A Characterization via 3-{SAT}},\nauthor={RISHI HAZRA and Gabriele Venturato and Pedro Zuidberg Dos Martires and Luc De Raedt},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=MPTlWIVSMU}\n}",
        "github": "",
        "project": "",
        "reviewers": "jBva;r8DN;ApVz;YUcH",
        "site": "https://openreview.net/forum?id=MPTlWIVSMU",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "3;3;1;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            2.75,
            1.0897247358851685
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "MiR3ObcF3C",
        "title": "$\\mu$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model\u2019s internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches\u2014especially window-based autoregressive methods\u2014often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing (\\toolname), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across five benchmarks demonstrate that \\toolname improves edit efficacy by up to 12.33\\% over state-of-the-art methods, and remains robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs.",
        "keywords": "knowledge editing;model editing;large language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zian Su;Ziyang Huang;Kaiyuan Zhang;Xiangyu Zhang",
        "authorids": "~Zian_Su1;~Ziyang_Huang4;~Kaiyuan_Zhang1;~Xiangyu_Zhang3",
        "gender": "M;M;M;M",
        "homepage": ";;https://kaiyuanzhang.com/;https://www.cs.purdue.edu/homes/xyzhang",
        "dblp": ";;147/6644-2;",
        "google_scholar": "gSQzZT0AAAA;QfPKo60AAAAJ;https://scholar.google.com/citations?hl=en;PXbu1wIAAAAJ",
        "orcid": ";0009-0009-2764-6091;0000-0001-6023-363X;",
        "linkedin": ";;kaiyuan-zhang/;",
        "or_profile": "~Zian_Su1;~Ziyang_Huang4;~Kaiyuan_Zhang1;~Xiangyu_Zhang3",
        "aff": "Purdue University;Johns Hopkins University;Purdue University;Purdue University",
        "aff_domain": "purdue.edu;cs.jhu.edu;cs.purdue.edu;cs.purdue.edu",
        "position": "PhD student;MS student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nsu2025muke,\ntitle={\\${\\textbackslash}mu\\${KE}: Matryoshka Unstructured Knowledge Editing of Large Language Models},\nauthor={Zian Su and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=MiR3ObcF3C}\n}",
        "github": "",
        "project": "",
        "reviewers": "LBJL;bMyA;Nt4i;tkzF",
        "site": "https://openreview.net/forum?id=MiR3ObcF3C",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "MsgdEkcLRz",
        "title": "LawFlow: Collecting and Simulating Lawyers\u2019 Thought Processes on Business Formation Case Studies",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce _LawFlow_, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, _LawFlow_ captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice.\nUsing _LawFlow_, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/)",
        "keywords": "pretraining data; synthetic data; reasoning; legal perspectives; legal drafting; LLM; chain of thought",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Debarati Das;Khanh Chi Le;Ritik Sachin Parkar;Karin De Langis;Brendan Madson;Chad M. Berryman;Robin M Willis;Daniel H Moses;Brett McDonnell;Daniel Schwarcz;Dongyeop Kang",
        "authorids": "~Debarati_Das1;~Khanh_Chi_Le1;~Ritik_Sachin_Parkar1;~Karin_De_Langis1;~Brendan_Madson1;~Chad_M._Berryman1;~Robin_M_Willis1;~Daniel_H_Moses1;~Brett_McDonnell1;~Daniel_Schwarcz1;~Dongyeop_Kang2",
        "gender": "F;F;M;F;M;;F;;M;;",
        "homepage": ";;;;;;;;https://law.umn.edu/profiles/brett-mcdonnell;https://www.law.umn.edu/profiles/daniel-schwarcz;",
        "dblp": "331/4274;;;251/5492;;;;;;;",
        "google_scholar": "NpoVjbMAAAAJ;;Lv-UEXQAAAAJ;https://scholar.google.com/scholar?hl=en;;;;;;;",
        "orcid": ";;;;;;;;;;",
        "linkedin": ";khanh-chi-le/;ritikparkar88/;;https://linkedin.com/in/brendanmadson;chad-berryman-716145203/;robinwillisrealtor/;danielmoses99/;;;",
        "or_profile": "~Debarati_Das1;~Khanh_Chi_Le1;~Ritik_Sachin_Parkar1;~Karin_De_Langis1;~Brendan_Madson1;~Chad_M._Berryman1;~Robin_M_Willis1;~Daniel_H_Moses1;~Brett_McDonnell1;~Daniel_Schwarcz1;~Dongyeop_Kang2",
        "aff": "University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;University of Minnesota - Twin Cities;",
        "aff_domain": "umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;umn.edu;",
        "position": "PhD student;Undergrad student;MS student;PhD student;Researcher;Researcher;Researcher;Researcher;Full Professor;Full Professor;",
        "bibtex": "@inproceedings{\ndas2025lawflow,\ntitle={LawFlow: Collecting and Simulating Lawyers{\\textquoteright} Thought Processes on Business Formation Case Studies},\nauthor={Debarati Das and Khanh Chi Le and Ritik Sachin Parkar and Karin De Langis and Brendan Madson and Chad M. Berryman and Robin M Willis and Daniel H Moses and Brett McDonnell and Daniel Schwarcz and Dongyeop Kang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=MsgdEkcLRz}\n}",
        "github": "",
        "project": "",
        "reviewers": "dgKy;ZrPG;PPMy",
        "site": "https://openreview.net/forum?id=MsgdEkcLRz",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "NAcvSI2CRM",
        "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of their respective _meta-inferential_ properties. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
        "keywords": "computational semantics;natural language inference;logic",
        "primary_area": "",
        "supplementary_material": "/attachment/03b068cff59811924a42c4f55e2f14114e1bedfa.zip",
        "author": "Rasmus Blanck;Bill Noble;Stergios Chatzikyriakidis",
        "authorids": "~Rasmus_Blanck1;~Bill_Noble1;~Stergios_Chatzikyriakidis1",
        "gender": "M;;",
        "homepage": "https://rasmusblanck.com/;https://winobes.github.io;",
        "dblp": "198/2387.html;128/3867.html;",
        "google_scholar": "FU0wy-EAAAAJ;;",
        "orcid": "0000-0002-4576-7817;0000-0002-8323-4149;",
        "linkedin": "rasmus-blanck-977841101/;;",
        "or_profile": "~Rasmus_Blanck1;~Bill_Noble1;~Stergios_Chatzikyriakidis1",
        "aff": "G\u00f6teborg University;G\u00f6teborg University+University of Gothenburg;",
        "aff_domain": "gu.se;gu.se+gu.se;",
        "position": "Assistant Professor;Postdoc+PhD student;",
        "bibtex": "@inproceedings{\nblanck2025reverseengineering,\ntitle={Reverse-engineering {NLI}: A study of the meta-inferential properties of Natural Language Inference},\nauthor={Rasmus Blanck and Bill Noble and Stergios Chatzikyriakidis},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=NAcvSI2CRM}\n}",
        "github": "",
        "project": "",
        "reviewers": "UYbC;yGJG;ndSY;NiGL",
        "site": "https://openreview.net/forum?id=NAcvSI2CRM",
        "pdf_size": 0,
        "rating": "2;6;7;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            2.0615528128088303
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.48507125007266594
    },
    {
        "id": "NC6G1KCxlt",
        "title": "Phased Training for LLM-powered Text Retrieval Models Beyond Data Scaling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current efforts in building large language models (LLMs) based general-purpose text retrieval models primarily focus on architectural design and training data scaling. However, significant challenges remain in effectively modeling diverse retrieval tasks and domains, including multi-task conflict, data imbalance, and training efficiency. To address these challenges, we propose a novel phased training framework for text retrieval, featuring:  (1) robust foundation modeling with core relevance data, (2) progressive specialization through modular task adaptation, and (3) knowledge fusion via weight interpolation based model merging. This framework simultaneously optimizes both embedding and reranking models through a unified architecture. We also present an efficient yet scalable data synthesis pipeline to expand training data, based on open-source LLMs. These synthetic data can be efficiently incorporated into the phased training framework, enhancing model performance. We identify five distinct types of retrieval tasks, \\ie basic relevance retrieval, code retrieval, tool retrieval, complex instruction-based retrieval, as well as reasoning-intensive retrieval, conducting extensive experiments. Our method achieves the best performance across MTEB and various retrieval benchmarks of the five task types. Further analysis demonstrates the effectiveness and efficiency of our proposed training framework and data synthesis pipeline.",
        "keywords": "Text Retrieval;Text Embedding;Reranking;LLM-based Embedding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xin Zhang;Yanzhao Zhang;Wen Xie;Dingkun Long;Mingxin Li;Pengjun Xie;Meishan Zhang;Wenjie Li;Min Zhang",
        "authorids": "~Xin_Zhang15;~Yanzhao_Zhang1;~Wen_Xie3;~Dingkun_Long1;~Mingxin_Li2;~Pengjun_Xie2;~Meishan_Zhang1;~Wenjie_Li1;~Min_Zhang9",
        "gender": "M;M;M;M;M;M;M;F;M",
        "homepage": "https://izhx.github.io;;;;;;https://zhangmeishan.github.io/;https://web.comp.polyu.edu.hk/cswjli/;https://zhangmin-nlp-ai.github.io/",
        "dblp": "76/1584-97;244/0823;;190/7094.html;;212/1755.html;127/0273;33/3999-2.html;83/5342-5",
        "google_scholar": "Cn1zs9cAAAAJ;;https://scholar.google.com/citations?hl=zh-CN;;;;https://scholar.google.com/citations?hl=zh-CN;Rx5swD4AAAAJ;https://scholar.google.com/citations?",
        "orcid": "0000-0002-2550-3056;0009-0003-6581-7783;0009-0009-7849-338X;0000-0001-6570-9406;0009-0002-7669-1824;;;0000-0002-7360-8864;0000-0002-3895-5510",
        "linkedin": ";;;;;;;;",
        "or_profile": "~Xin_Zhang15;~Yanzhao_Zhang1;~Wen_Xie3;~Dingkun_Long1;~Mingxin_Li2;~Pengjun_Xie2;~Meishan_Zhang1;~Wenjie_Li1;~Min_Zhang9",
        "aff": "Hong Kong Polytechnic University+Harbin Institute of Technology, Shenzhen;Alibaba Group+Alibaba Group;Harbin Institute of Technology;Alibaba Group;Alibaba Group+Beihang University;Alibaba Group;Harbin Institute of Technology (Shenzhen), China;The Hong Kong Polytechnic University;Harbin Institute of Technology, Shenzhen+Harbin Institute of Technology",
        "aff_domain": "polyu.edu.hk+hit.edu.cn;alibaba-inc.com+alibaba-inc.com;stu.hit.edu.cn;alibaba-inc.com;alibaba-inc.com+act.buaa.edu.cn;alibaba-inc.com;hit.edu.cn;comp.polyu.edu.hk;hit.edu.cn+hit.edu.cn",
        "position": "PhD student+PhD student;Researcher+Researcher;MS student;Researcher;Researcher+MS student;Researcher;Associate Professor;Full Professor;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nzhang2025phased,\ntitle={Phased Training for {LLM}-powered Text Retrieval Models Beyond Data Scaling},\nauthor={Xin Zhang and Yanzhao Zhang and Wen Xie and Dingkun Long and Mingxin Li and Pengjun Xie and Meishan Zhang and Wenjie Li and Min Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=NC6G1KCxlt}\n}",
        "github": "",
        "project": "",
        "reviewers": "G5Ni;n97a;uJyM;5b9A",
        "site": "https://openreview.net/forum?id=NC6G1KCxlt",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "5;4;4;5",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "NMIqKUdDkw",
        "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language model (LLM) unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing *undesired* data-model influences from the pretrained model while preserving its general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel *coreset effect* within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), *e.g.*, as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic  approaches. We explain the coreset effect in LLM unlearning  through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks.",
        "keywords": "Machine Unlearning;Coreset;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Soumyadeep Pal;Changsheng Wang;James Diffenderfer;Bhavya Kailkhura;Sijia Liu",
        "authorids": "~Soumyadeep_Pal1;~Changsheng_Wang1;~James_Diffenderfer1;~Bhavya_Kailkhura1;~Sijia_Liu1",
        "gender": ";M;;M;M",
        "homepage": ";;;https://people.llnl.gov/kailkhura1;https://lsjxjtu.github.io/",
        "dblp": "236/2130.html;;188/4110;132/8938;128/6972-1",
        "google_scholar": "https://scholar.google.ca/citations?user=c2VU-_4AAAAJ;pFYLRCIAAAAJ;nRr24_QAAAAJ;SQpJmOgAAAAJ;C7dO_UgAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";https://www.linkedin.cn/incareer/in/ACoAADkrmB4BYKKCZF_KcQvS1gcFybWGd8bg1dk;;;",
        "or_profile": "~Soumyadeep_Pal1;~Changsheng_Wang1;~James_Diffenderfer1;~Bhavya_Kailkhura1;~Sijia_Liu1",
        "aff": "Michigan State University;Michigan State University;Lawrence Livermore National Labs;Lawrence Livermore National Laboratory;Michigan State University",
        "aff_domain": "msu.edu;msu.edu;llnl.gov;llnl.gov;msu.edu",
        "position": "PhD student;PhD student;Researcher;Research Staff;Assistant Professor",
        "bibtex": "@inproceedings{\npal2025llm,\ntitle={{LLM} Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks},\nauthor={Soumyadeep Pal and Changsheng Wang and James Diffenderfer and Bhavya Kailkhura and Sijia Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=NMIqKUdDkw}\n}",
        "github": "",
        "project": "",
        "reviewers": "sYCW;wji5;xsZY",
        "site": "https://openreview.net/forum?id=NMIqKUdDkw",
        "pdf_size": 0,
        "rating": "3;6;7",
        "confidence": "4;5;3",
        "wc_review": "",
        "rating_avg": [
            5.333333333333333,
            1.699673171197595
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.2401922307076307
    },
    {
        "id": "NRrXHppaBg",
        "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse.\nThis paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary.\nEvaluations across public benchmarks and our newly introduced IC dataset, designed to challenge hallucination control, demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.",
        "keywords": "Self-Improvement;Multimodal Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shijian Deng;Wentian Zhao;Yu-Jhe Li;Kun Wan;Daniel Miranda;Ajinkya Kale;Yapeng Tian",
        "authorids": "~Shijian_Deng1;~Wentian_Zhao3;~Yu-Jhe_Li1;~Kun_Wan1;~Daniel_Miranda1;~Ajinkya_Kale1;~Yapeng_Tian1",
        "gender": ";M;M;;M;;M",
        "homepage": ";https://wentian.zhao.com;https://yujheli.github.io/;;;;http://www.yapengtian.com/",
        "dblp": ";https://www.linkedin.com/feed/;127/3527;;;04/6453;176/4020",
        "google_scholar": ";zqndXGgAAAAJ;https://scholar.google.com.tw/citations?user=MpLiwTIAAAAJ;;;;lxCqdpoAAAAJ",
        "orcid": ";https://www.linkedin.com/feed/;;;;;0000-0003-1423-4513",
        "linkedin": ";https://www.linkedin.com/feed/;;;daniel-miranda-01755742;;",
        "or_profile": "~Shijian_Deng1;~Wentian_Zhao3;~Yu-Jhe_Li1;~Kun_Wan1;~Daniel_Miranda1;~Ajinkya_Kale1;~Yapeng_Tian1",
        "aff": ";Adobe Systems;Adobe Systems;;Adobe Systems;Adobe Systems;University of Texas at Dallas",
        "aff_domain": ";adobe.com;adobe.com;;adobe.com;adobe.com;utdallas.edu",
        "position": ";Researcher;Researcher;;Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\ndeng2025efficient,\ntitle={Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach},\nauthor={Shijian Deng and Wentian Zhao and Yu-Jhe Li and Kun Wan and Daniel Miranda and Ajinkya Kale and Yapeng Tian},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=NRrXHppaBg}\n}",
        "github": "",
        "project": "",
        "reviewers": "17vT;WN6m;LhJg;Kgrp",
        "site": "https://openreview.net/forum?id=NRrXHppaBg",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "NmGSvZoU3K",
        "title": "Analyzing Multilingualism in Large Language Models with Sparse Autoencoders",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite the impressive multilingual capabilities of recent large language models (LLMs), the mechanisms underlying their language-specific processing remain largely unclear. In this paper, we investigate how LLMs handle multilingualism through the lens of sparse autoencoders (SAEs), uncovering distinctive patterns that offer new insights into their internal workings. Specifically, we introduce two novel concepts\u2014task instruction\u2013focused (TF) and heading-focused (HF) SAE features\u2014and use them to reveal intrinsic discrepancies between high- and low-performing languages. Our analysis yields several key findings: (1) SAEs provide concrete evidence that LLMs have a precise understanding of prompt structure; (2) heading keywords (e.g., \u201cQuestion,\u201d \u201cChoices,\u201d and \u201cAnswer\u201d) play a distinct role in LLM processing; and (3) low-performing languages exhibit a relative deficiency in TF features compared to high-performing languages.\n    \nBuilding on these insights, we propose two practical strategies to improve zero-shot multilingual performance: (1) incorporating English heading keywords and (2) amplifying TF features through steering. Our approach improves zero-shot performance in low-performing languages by up to 3.7% on average on ARC-Challenge and MMLU, while also shedding new light on fundamental differences between high- and low-performing languages in LLMs. Our code is available at https://github.com/ihcho2/SAE-ML.",
        "keywords": "Large Language Models;Multilingualism;Sparse Autoencoders",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ikhyun Cho;Julia Hockenmaier",
        "authorids": "~Ikhyun_Cho4;~Julia_Hockenmaier1",
        "gender": ";F",
        "homepage": ";https://cs.illinois.edu/directory/profile/juliahmr",
        "dblp": ";64/2448",
        "google_scholar": ";https://scholar.google.com.tw/citations?user=iIiVrrQAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Ikhyun_Cho4;~Julia_Hockenmaier1",
        "aff": ";University of Illinois, Urbana Champaign+Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen",
        "aff_domain": ";illinois.edu+lmu.de",
        "position": ";Full Professor+Researcher",
        "bibtex": "@inproceedings{\ncho2025analyzing,\ntitle={Analyzing Multilingualism in Large Language Models with Sparse Autoencoders},\nauthor={Ikhyun Cho and Julia Hockenmaier},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=NmGSvZoU3K}\n}",
        "github": "",
        "project": "",
        "reviewers": "Bq9B;ZoVv;BE52;1FdK;mB9c;UmKf",
        "site": "https://openreview.net/forum?id=NmGSvZoU3K",
        "pdf_size": 0,
        "rating": "6;6;6;6;6;7",
        "confidence": "4;2;4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.166666666666667,
            0.37267799624996495
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.7453559924999298
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4000000000000001
    },
    {
        "id": "O6I0Av7683",
        "title": "Reasoning Models Know When They\u2019re Right: Probing Hidden States for Self-Verification",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning models have achieved remarkable performance on tasks like math and logical reasoning thanks to their ability to search during reasoning. However, they still suffer from \\textit{overthinking}, often performing unnecessary reasoning steps even after reaching the correct answer. This raises the question: \\textit{can models evaluate the correctness of their intermediate answers during reasoning?}\nIn this work, we study whether reasoning models  encode information about answer correctness through probing the model's hidden states. The resulting probe can verify intermediate answers with high accuracy and produces highly calibrated scores. Additionally, we find models' hidden states encode correctness of future answers, enabling ealy prediction of the correctness before the intermediate answer is fully formulated.\nWe then use the probe as a verifier to decide whether to exit reasoning at intermediate answers during inference, reducing the number of inference tokens by 24\\% without compromising performance. These findings confirm that reasoning models do encode a notion of correctness yet fail to exploit it, revealing substantial untapped potential to enhance their efficiency.",
        "keywords": "Reasoning models; Chain-of-thought reasoning (CoT); Intermediate answers; Overthinking",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anqi Zhang;Yulin Chen;Jane Pan;Chen Zhao;Aurojit Panda;Jinyang Li;He He",
        "authorids": "~Anqi_Zhang1;~Yulin_Chen1;~Jane_Pan1;~Chen_Zhao2;~Aurojit_Panda1;~Jinyang_Li1;~He_He2",
        "gender": ";F;F;M;;;",
        "homepage": ";;https://janepan9917.github.io/;http://umiacs.umd.edu/~chenz/;;;",
        "dblp": ";;347/8346.html;81/3-9;;;",
        "google_scholar": ";tAiXl18AAAAJ;aDtZe-kAAAAJ;zehsvT8AAAAJ;;;",
        "orcid": ";;;;;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Anqi_Zhang1;~Yulin_Chen1;~Jane_Pan1;~Chen_Zhao2;~Aurojit_Panda1;~Jinyang_Li1;~He_He2",
        "aff": ";New York University;New York University;NYU Shanghai;;;",
        "aff_domain": ";nyu.edu;nyu.edu;nyu.edu;;;",
        "position": ";PhD student;PhD student;Assistant Professor;;;",
        "bibtex": "@inproceedings{\nzhang2025reasoning,\ntitle={Reasoning Models Know When They{\\textquoteright}re Right: Probing Hidden States for Self-Verification},\nauthor={Anqi Zhang and Yulin Chen and Jane Pan and Chen Zhao and Aurojit Panda and Jinyang Li and He He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=O6I0Av7683}\n}",
        "github": "",
        "project": "",
        "reviewers": "5iSN;KxZk;Etou",
        "site": "https://openreview.net/forum?id=O6I0Av7683",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "O7bF6nlSOD",
        "title": "Evaluating the Diversity and Quality of LLM Generated Content",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent work suggests that preference-tuning techniques\u2014such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO\u2014reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity\u2014diversity among outputs that meet quality thresholds\u2014which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models\u2014particularly those trained via RL\u2014often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.",
        "keywords": "Diversity;Alignment;LLMs;Evaluation;Program Synthesis;Code Generation;Creative Writing",
        "primary_area": "",
        "supplementary_material": "/attachment/42f1580fbebf5f89e0d841b74303206a0ec077c6.zip",
        "author": "Alexander Shypula;Shuo Li;Botong Zhang;Vishakh Padmakumar;Kayo Yin;Osbert Bastani",
        "authorids": "~Alexander_Shypula1;~Shuo_Li7;~Botong_Zhang1;~Vishakh_Padmakumar1;~Kayo_Yin1;~Osbert_Bastani1",
        "gender": ";M;;;F;M",
        "homepage": ";;;https://vishakhpk.github.io/;https://kayoyin.github.io/;http://obastani.github.io",
        "dblp": ";;;285/5184;262/3579.html;21/11275",
        "google_scholar": ";-QaDf40AAAAJ;;OeBKZ8AAAAAJ;Wc8oLVwAAAAJ;cxYepGkAAAAJ",
        "orcid": ";;;0000-0002-3396-3589;0000-0001-6831-2981;",
        "linkedin": ";shuo-li-bbb2a11b1/;;;kayoyin/;",
        "or_profile": "~Alexander_Shypula1;~Shuo_Li7;~Botong_Zhang1;~Vishakh_Padmakumar1;~Kayo_Yin1;~Osbert_Bastani1",
        "aff": ";University of Pennsylvania;;New York University;University of California, Berkeley;University of Pennsylvania",
        "aff_domain": ";upenn.edu;;nyu.edu;berkeley.edu;upenn.edu",
        "position": ";PhD student;;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nshypula2025evaluating,\ntitle={Evaluating the Diversity and Quality of {LLM} Generated Content},\nauthor={Alexander Shypula and Shuo Li and Botong Zhang and Vishakh Padmakumar and Kayo Yin and Osbert Bastani},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=O7bF6nlSOD}\n}",
        "github": "",
        "project": "",
        "reviewers": "rZeC;7jf8;p6Pc",
        "site": "https://openreview.net/forum?id=O7bF6nlSOD",
        "pdf_size": 0,
        "rating": "6;6;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "OGwE7LwtcR",
        "title": "G1yphD3c0de: Towards Safer Language Models on Visually Perturbed Texts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Visual text perturbations are increasingly used to bypass content moderation systems, where characters are replaced with visually similar Unicode alternatives that humans can easily recognize but text-only filters fail to detect. While existing research has examined the generation and classification of such evasion techniques, the critical task of restoration remains underexplored. To address this challenge, we present GlyphDecode, a novel framework designed to restore visually perturbed text to its original form. Our framework consists of two key components: (1) GlyphPerturber, which generates visually perturbed text images for training, and (2) GlyphRestorer, which learns to recover the original text through a multimodal transformer architecture. GlyphRestorer is a light-weight and fast module that can be applied in a plug-and-play manner with off-the-shelf LLMs and multimodal LLMs to enhance harmful content detection. To evaluate restoration efficacy in real-world scenarios, we introduce GlyphSynth publicly available, a specialized dataset containing realistic examples of content moderation evasion from diverse sources including DEA(Drug Enforcement Administration) reports and social media platforms. Experimental results demonstrate that our approach significantly outperforms baselines in text restoration, and enabling multimodal language models to better detect harmful content disguised through visual manipulations. Our work bridges an important gap in content moderation systems by addressing not only the detection but also the recovery of manipulated text, contributing to more effective safeguards against increasingly sophisticated evasion tactics.",
        "keywords": "safety;societal implications;multimodality",
        "primary_area": "",
        "supplementary_material": "/attachment/73ae442f313b2bcb950d1ff8c21fbc0d155ca084.zip",
        "author": "Yejinchoi;Yejin Yeo;Yejin Son;Seungju Han;Youngjae Yu",
        "authorids": "~Yejinchoi1;~Yejin_Yeo1;~Yejin_Son3;~Seungju_Han2;~Youngjae_Yu1",
        "gender": "F;;F;M;M",
        "homepage": "https://mirlab.yonsei.ac.kr/people/yejin_choi.html;;https://github.com/ozzaney?tab=repositories;https://seungjuhan.me;https://yj-yu.github.io/home/",
        "dblp": "89/579-4;;359/0753;;188/6210",
        "google_scholar": "https://scholar.google.com/citations?hl=ko;;;g_anRqAAAAAJ;https://scholar.google.co.kr/citations?user=WDO24ZYAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;seungju-han-66b85017a/;",
        "or_profile": "~Yejinchoi1;~Yejin_Yeo1;~Yejin_Son3;~Seungju_Han2;~Youngjae_Yu1",
        "aff": "Yonsei University;;Yonsei University;Computer Science Department, Stanford University+NVIDIA;Yonsei University",
        "aff_domain": "yonsei.ac.kr;;yonsei.ac.kr;cs.stanford.edu+nvidia.com;yonsei.ac.kr",
        "position": "MS student;;MS student;PhD student+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nyejinchoi2025gyphdcde,\ntitle={G1yphD3c0de: Towards Safer Language Models on Visually Perturbed Texts},\nauthor={Yejinchoi and Yejin Yeo and Yejin Son and Seungju Han and Youngjae Yu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=OGwE7LwtcR}\n}",
        "github": "",
        "project": "",
        "reviewers": "hrDV;LeSr;zU4F;zqtB",
        "site": "https://openreview.net/forum?id=OGwE7LwtcR",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;3;2;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "OKvSnV5Ar7",
        "title": "Limitations of refinement methods for weak to strong generalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Standard techniques for aligning large language models (LLMs) utilize human-produced data, which could limit the capability of any aligned LLM to human level. Label refinement and weak training have emerged as promising strategies to address this *superalignment* problem. In this work, we adopt probabilistic assumptions commonly used to study label refinement and analyze whether refinement can be outperformed by alternative approaches, including computationally intractable oracle methods. We show that both weak training and label refinement suffer from irreducible error, leaving a performance gap between label refinement and the oracle. These results motivate future research into developing  alternative methods for weak to strong generalization that synthesize the practicality of label refinement or weak training and the optimality of the oracle procedure.",
        "keywords": "Weak to strong generalization;superalignment;transfer learning",
        "primary_area": "",
        "supplementary_material": "/attachment/abc7fd4434a497d95c9d4ae706bc33726fe44207.zip",
        "author": "Seamus Somerstep;Yaacov Ritov;Mikhail Yurochkin;Subha Maity;Yuekai Sun",
        "authorids": "~Seamus_Somerstep1;~Yaacov_Ritov2;~Mikhail_Yurochkin1;~Subha_Maity1;~Yuekai_Sun1",
        "gender": ";M;M;M;",
        "homepage": "https://somerstep.github.io;http://www-personal.umich.edu/~yritov/jr.html;https://moonfolk.github.io/;https://lsa.umich.edu/stats/people/phd-students/smaity.html;https://yuekai.github.io/",
        "dblp": ";34/4054;191/6719;278/2922;",
        "google_scholar": ";JlNrOwcAAAAJ;QjBF9sUAAAAJ;eD9vCGMAAAAJ;6T1XtW8AAAAJ",
        "orcid": ";0000-0002-6046-8479;;;",
        "linkedin": "seamus-somerstep-a1a217194;;mikhail-yurochkin-a45659114/;;",
        "or_profile": "~Seamus_Somerstep1;~Yaacov_Ritov2;~Mikhail_Yurochkin1;~Subha_Maity1;~Yuekai_Sun1",
        "aff": "University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;Mohamed bin Zayed University of Artificial Intelligence;University of Waterloo;University of Michigan - Ann Arbor",
        "aff_domain": "umich.edu;umich.edu;mbzuai.ac.ae;uwaterloo.ca;umich.edu",
        "position": "PhD student;Full Professor;Principal Researcher;Assistant Professor;Assistant \u2192 Associate Professor of Statistics",
        "bibtex": "@inproceedings{\nsomerstep2025limitations,\ntitle={Limitations of refinement methods for weak to strong generalization},\nauthor={Seamus Somerstep and Yaacov Ritov and Mikhail Yurochkin and Subha Maity and Yuekai Sun},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=OKvSnV5Ar7}\n}",
        "github": "",
        "project": "",
        "reviewers": "BFqD;nFeH;PQE1;PyrR",
        "site": "https://openreview.net/forum?id=OKvSnV5Ar7",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "OeYdS51k8F",
        "title": "LM Agents May Fail to Act on Their Own Risk Knowledge",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language model (LM) agents have demonstrated significant potential for automating real-world tasks, yet they pose a diverse array of potential, severe risks in safety-critical scenarios.  In this work, we identify a significant gap between LM agents' risk awareness and safety execution abilities: while they often answer \"Yes'' to queries like $\\texttt{\"Is executing `sudo rm -rf /*' dangerous?\"}$, they will likely fail to identify such risks in instantiated trajectories or even directly perform these risky actions when acting as agents. To systematically investigate this, we develop a comprehensive evaluation framework to examine agents' safety across three progressive dimensions: 1) their knowledge about potential risks, 2) their ability to identify corresponding risks in execution trajectories, and 3) their actual behaviors to avoid executing these risky actions. Our evaluation reveals two critical performance gaps that resemble the generator-validator gaps observed in LMs: while agents demonstrate near-perfect risk knowledge (>98\\% pass rates), they fail to apply this knowledge when identifying risks in actual scenarios, with performance dropping by >23\\%, and often still execute risky actions (<26\\% pass rates). This trend persists even in specialized reasoning models like DeepSeek-R1, reinforcing the challenge of translating an LM's risk knowledge into safe decision-making. We take advantage of these observed gaps to develop a risk verifier that independently critiques the proposed actions by agents, with an abstractor that converts specific execution trajectories into abstract descriptions where LMs can more effectively identify the risks. Our overall system achieves a significant reduction of risky action execution by 55.3\\% over vanilla-prompted agents.",
        "keywords": "Large Language Models;Language Model Agents;AI Safety;Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuzhi Tang;Tianxiao Li;Elizabeth Li;Chris J. Maddison;Honghua Dong;Yangjun Ruan",
        "authorids": "~Yuzhi_Tang2;~Tianxiao_Li2;~Elizabeth_Li1;~Chris_J._Maddison1;~Honghua_Dong1;~Yangjun_Ruan1",
        "gender": "M;M;F;;M;M",
        "homepage": ";;;;https://dhh1995.github.io/;http://www.cs.toronto.edu/~yjruan/",
        "dblp": ";;;;238/2646;237/3892",
        "google_scholar": ";;;;MrGN4oMAAAAJ;https://scholar.google.com.hk/citations?user=9AdCSywAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "yuzhi-tang/;tonyltx/;elizabethli26;;;",
        "or_profile": "~Yuzhi_Tang2;~Tianxiao_Li2;~Elizabeth_Li1;~Chris_J._Maddison1;~Honghua_Dong1;~Yangjun_Ruan1",
        "aff": "Department of Computer Science, University of Toronto+Boson AI;University of California, Berkeley;University of Toronto;;Department of Computer Science, University of Toronto;Stanford University+University of Toronto",
        "aff_domain": "cs.toronto.edu+boson.ai;berkeley.edu;utoronto.ca;;cs.toronto.edu;stanford.edu+toronto.edu",
        "position": "MS student+Intern;MS student;Researcher;;PhD student;Visiting Scholar+PhD student",
        "bibtex": "@inproceedings{\ntang2025lm,\ntitle={{LM} Agents May Fail to Act on Their Own Risk Knowledge},\nauthor={Yuzhi Tang and Tianxiao Li and Elizabeth Li and Chris J. Maddison and Honghua Dong and Yangjun Ruan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=OeYdS51k8F}\n}",
        "github": "",
        "project": "",
        "reviewers": "MVQF;M9KV;xJF8;deXw",
        "site": "https://openreview.net/forum?id=OeYdS51k8F",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "OgWh4J7bkT",
        "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in post-training methodologies for large language models (LLMs) have highlighted reinforcement learning (RL) as a critical component for enhancing reasoning. However, the substantial computational costs associated with RL-based approaches have led to growing interest in alternative paradigms, such as Direct Preference Optimization (DPO). In this study, we investigate the effectiveness of DPO in facilitating self-improvement for LLMs through iterative preference-based learning. We demonstrate that a single round of DPO with coarse filtering significantly enhances mathematical reasoning performance, particularly for strong base model. Furthermore, we design an iterative enhancement framework for both the generator and the reward model (RM), enabling their mutual improvement through online interaction across multiple rounds of DPO. Finally, with simple verifiable rewards, our model DPO-VP achieves RL-level performance with significantly lower computational overhead. These findings highlight DPO as a scalable and cost-effective alternative to RL, offering a practical solution for enhancing LLM reasoning in resource-constrained situations.",
        "keywords": "LLM Reasoning;Iterative Optimization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Songjun Tu;Jiahao Lin;Xiangyu Tian;Qichao Zhang;Linjing Li;Yuqian Fu;Nan Xu;Wei He;Xiangyuan Lan;Dongmei Jiang;Dongbin Zhao",
        "authorids": "~Songjun_Tu1;~Jiahao_Lin4;~Xiangyu_Tian1;~Qichao_Zhang3;~Linjing_Li1;~Yuqian_Fu3;~Nan_Xu4;~Wei_He14;~Xiangyuan_Lan4;~Dongmei_Jiang2;~Dongbin_Zhao1",
        "gender": ";M;;;M;;;M;;F;M",
        "homepage": ";https://github.com/SunnyYYLin;;;https://people.ucas.edu.cn/~ljli;;;https://hwcoder.top/about;;https://scholar.google.com/citations?user=Awsue7sAAAAJ&hl=en;http://people.ucas.ac.cn/~zhaodongbin?language=en",
        "dblp": ";;;;41/9180;;;;;;40/255",
        "google_scholar": "_5Ir0soAAAAJ;;;;7QO2H6wAAAAJ;;;;;Awsue7sAAAAJ;",
        "orcid": "0009-0008-3302-8188;;;;0000-0002-8737-099X;;;;;0000-0002-6238-8499;0000-0001-8218-9633",
        "linkedin": ";;;;;;;;;;",
        "or_profile": "~Songjun_Tu1;~Jiahao_Lin4;~Xiangyu_Tian1;~Qichao_Zhang3;~Linjing_Li1;~Yuqian_Fu3;~Nan_Xu4;~Wei_He14;~Xiangyuan_Lan4;~Dongmei_Jiang2;~Dongbin_Zhao1",
        "aff": "Institute of Automation Chinese Academy of Sciences;University of Chinese Academy of Sciences;;;Institute of Automation, Chinese Academy of Sciences;;;Fudan University;;Peng Cheng Laboratory;Institute of Automation, Chinese Academy of Sciences",
        "aff_domain": "ia.ac.cn;ucas.ac.cn;;;ia.ac.cn;;;fudan.edu.cn;;pcl.ac.cn;ia.ac.cn",
        "position": "PhD student;Undergrad student;;;Full Professor;;;MS student;;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\ntu2025enhancing,\ntitle={Enhancing {LLM} Reasoning with Iterative {DPO}: A Comprehensive Empirical Investigation},\nauthor={Songjun Tu and Jiahao Lin and Xiangyu Tian and Qichao Zhang and Linjing Li and Yuqian Fu and Nan Xu and Wei He and Xiangyuan Lan and Dongmei Jiang and Dongbin Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=OgWh4J7bkT}\n}",
        "github": "",
        "project": "",
        "reviewers": "1K3W;AnMH;znvZ;V4JU",
        "site": "https://openreview.net/forum?id=OgWh4J7bkT",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            27,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Orvjm9UqH2",
        "title": "Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) increasingly serve as tools for knowledge acquisition, yet users cannot effectively specify how they want information presented. When users request that LLMs \"cite reputable sources,\" \"express appropriate uncertainty,\" or \"include multiple perspectives,\" they discover that current interfaces provide no structured way to articulate these preferences. The result is prompt sharing folklore: community-specific copied prompts passed through trust relationships rather than based on measured efficacy. We propose the Epistemic Alignment Framework, a set of ten challenges in knowledge transmission derived from the philosophical literature of epistemology, concerning issues such as uncertainty expression, evidence quality assessment, and calibration of testimonial reliance. The framework serves as a structured intermediary between user needs and system capabilities, creating a common vocabulary to bridge the gap between what users want and what systems deliver. Through a thematic analysis of custom prompts and personalization strategies shared on online communities where these issues are actively discussed, we find users develop elaborate workarounds to address each of the challenges. We then apply our framework to two prominent model providers, OpenAI and Anthropic, through structured content analysis of their documented policies and product features. Our analysis shows that while these providers have partially addressed the challenges we identified, they fail to establish adequate mechanisms for specifying epistemic preferences, lack transparency about how preferences are implemented, and offer no verification tools to confirm whether preferences were followed. For AI developers, the Epistemic Alignment Framework offers concrete guidance for supporting diverse approaches to knowledge; for users, it works toward information delivery that aligns with their specific needs rather than defaulting to one-size-fits-all approaches.",
        "keywords": "epistemology of AI;language model behavior;human-AI interaction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicholas Clark;Hua Shen;Bill Howe;Tanu Mitra",
        "authorids": "~Nicholas_Clark2;~Hua_Shen1;~Bill_Howe1;~Tanu_Mitra1",
        "gender": ";F;M;F",
        "homepage": ";http://hua-shen.org/;https://faculty.washington.edu/billhowe/;http://tanumitra.com/",
        "dblp": ";;h/BillHowe;38/11520.html",
        "google_scholar": "lRikbuYAAAAJ;zFjlv1sAAAAJ;dQ-x9NQAAAAJ;5q_BkVAAAAAJ",
        "orcid": ";0000-0002-4928-525X;;",
        "linkedin": ";hua-shen/;;",
        "or_profile": "~Nicholas_Clark2;~Hua_Shen1;~Bill_Howe1;~Tanu_Mitra1",
        "aff": "University of Washington;University of Washington;University of Washington;University of Washington",
        "aff_domain": "uw.edu;uw.edu;u.washington.edu;washington.edu",
        "position": "PhD student;Postdoc;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nclark2025epistemic,\ntitle={Epistemic Alignment: A Mediating Framework for User-{LLM} Knowledge Delivery},\nauthor={Nicholas Clark and Hua Shen and Bill Howe and Tanu Mitra},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Orvjm9UqH2}\n}",
        "github": "",
        "project": "",
        "reviewers": "EdtK;Cydn;o8K6;yf1j",
        "site": "https://openreview.net/forum?id=Orvjm9UqH2",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;4;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7071067811865476
    },
    {
        "id": "P61AgRyU7E",
        "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, cross evaluations show that our system generalizes well across diverse health misinformation topics and datasets. And human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.",
        "keywords": "Large Language Model;Multi-agent;Retrieval-Augmented Generation;Health Misinformation;Counterspeech",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anirban Saha Anik;Xiaoying Song;Elliott Wang;Bryan Wang;Bengisu Yarimbas;Lingzi Hong",
        "authorids": "~Anirban_Saha_Anik1;~Xiaoying_Song1;~Elliott_Wang1;~Bryan_Wang3;~Bengisu_Yarimbas1;~Lingzi_Hong1",
        "gender": "M;F;M;M;F;F",
        "homepage": "https://anirbansahaanik.github.io/;;;;;",
        "dblp": ";;;;;144/3339",
        "google_scholar": "yhtMiNoAAAAJ;wn5zGK4AAAAJ;;;;H9ymNRQAAAAJ",
        "orcid": "0000-0002-7824-3702;0000-0001-9390-1155;;;;0000-0001-8412-8180",
        "linkedin": "anirban-saha-anik/;xiaoying-song-0335b5295/;elliott-wang-873223309/;bryan-wang-92a1432b8/;bengisu-y;",
        "or_profile": "~Anirban_Saha_Anik1;~Xiaoying_Song1;~Elliott_Wang1;~Bryan_Wang3;~Bengisu_Yarimbas1;~Lingzi_Hong1",
        "aff": "University of North Texas;University of North Texas;University of North Texas;University of North Texas;University of North Texas;University of North Texas",
        "aff_domain": "unt.edu;unt.edu;unt.edu;unt.edu;unt.edu;unt.edu",
        "position": "MS student;PhD student;Undergrad student;Undergrad student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nanik2025multiagent,\ntitle={Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation},\nauthor={Anirban Saha Anik and Xiaoying Song and Elliott Wang and Bryan Wang and Bengisu Yarimbas and Lingzi Hong},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=P61AgRyU7E}\n}",
        "github": "",
        "project": "",
        "reviewers": "dAts;FM46;7B6M;p6nq",
        "site": "https://openreview.net/forum?id=P61AgRyU7E",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "PYHwlyu2fa",
        "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable performance in various vision-language tasks. However, it is still unclear how accurately LVLMs can perceive visual information in images. In particular, the capability of LVLMs to perceive geometric information, such as shape, angle, and size, remains insufficiently analyzed, although the perception of these properties is crucial for tasks that require a detailed visual understanding. In this work, we introduce VisOnlyQA, a dataset for evaluating the geometric perception of LVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric information in images, while human performance is nearly perfect. VisOnlyQA consists of 12 tasks that directly ask about geometric information in geometric shapes, charts, chemical structures, and 3D shapes. Our experiments highlight the following findings: (i) State-of-the-art LVLMs struggle with basic geometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5 Pro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve this issue. Fine-tuning on the training set of VisOnlyQA is not always effective, even for in-distribution tasks. (iii) LLM may be the bottleneck. LVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA, while it does not require complex reasoning, suggesting that the way LVLMs process information from visual encoders is a bottleneck. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.",
        "keywords": "vision-language models",
        "primary_area": "",
        "supplementary_material": "/attachment/47d05e93f5817d673511927ceb3287d1c28ae8dc.zip",
        "author": "Ryo Kamoi;Yusen Zhang;Sarkar Snigdha Sarathi Das;Ranran Haoran Zhang;Rui Zhang",
        "authorids": "~Ryo_Kamoi1;~Yusen_Zhang1;~Sarkar_Snigdha_Sarathi_Das1;~Ranran_Haoran_Zhang2;~Rui_Zhang7",
        "gender": "M;M;M;;M",
        "homepage": "https://ryokamoi.github.io/;https://www.yuszh.com;https://sarathismg.github.io/;;https://ryanzhumich.github.io/",
        "dblp": "254/2890;38/10863-1.html;255/4887;;60/2536-37",
        "google_scholar": "4OWTLKAAAAAJ;FGyMx88AAAAJ;V7lBToMAAAAJ;;nhuB5CEAAAAJ",
        "orcid": "0000-0002-8442-4171;;;;",
        "linkedin": "ryokamoi/;;;;",
        "or_profile": "~Ryo_Kamoi1;~Yusen_Zhang1;~Sarkar_Snigdha_Sarathi_Das1;~Ranran_Haoran_Zhang2;~Rui_Zhang7",
        "aff": "Pennsylvania State University;Pennsylvania State University;Pennsylvania State University;;Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;psu.edu;;psu.edu",
        "position": "PhD student;PhD student;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nkamoi2025visonlyqa,\ntitle={VisOnly{QA}: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information},\nauthor={Ryo Kamoi and Yusen Zhang and Sarkar Snigdha Sarathi Das and Ranran Haoran Zhang and Rui Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=PYHwlyu2fa}\n}",
        "github": "",
        "project": "",
        "reviewers": "PGab;v9QR;EwqX",
        "site": "https://openreview.net/forum?id=PYHwlyu2fa",
        "pdf_size": 0,
        "rating": "4;7;7",
        "confidence": "5;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844387
    },
    {
        "id": "Pbs4i3FgbD",
        "title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A key component of building safe and reliable language models is enabling the models to appropriately refuse to follow certain instructions or answer certain questions. We may want models to output refusal messages for various categories of user queries, for example, ill-posed questions, instructions for committing illegal acts, or queries which require information past the model's knowledge horizon. Engineering models that refuse to answer such questions is complicated by the fact that an individual may want their model to exhibit varying levels of sensitivity for refusing queries of various categories, and different users may want different refusal rates. The current default approach involves training multiple models with varying proportions of refusal messages from each category to achieve the desired refusal rates, which is computationally expensive and may require training a new model to accommodate each user's desired preference over refusal rates. To address these challenges, we propose refusal tokens, one such token for each refusal category or a single refusal token, which are prepended to the model's responses during training. We then show how to increase or decrease the probability of generating the refusal token for each category during inference to steer the model's refusal behavior. Refusal tokens enable controlling a single model's refusal rates without the need of any further fine-tuning, but only by selectively intervening during generation.",
        "keywords": "LLM;Refusals",
        "primary_area": "",
        "supplementary_material": "/attachment/7752bce29aea8f6a0690ad433de3e3048047aa96.zip",
        "author": "Neel Jain;Aditya Shrivastava;Chenyang Zhu;Daben Liu;Alfy Samuel;Ashwinee Panda;Anoop Kumar;Micah Goldblum;Tom Goldstein",
        "authorids": "~Neel_Jain1;~Aditya_Shrivastava1;~Chenyang_Zhu3;~Daben_Liu1;~Alfy_Samuel1;~Ashwinee_Panda1;~Anoop_Kumar1;~Micah_Goldblum1;~Tom_Goldstein1",
        "gender": ";;;M;F;M;M;;M",
        "homepage": ";https://www.aditya-shrivastava.com/;;;;https://kiddyboots216.github.io/;;;https://www.cs.umd.edu/~tomg/",
        "dblp": ";;267/0396;93/2343.html;136/6038.html;270/1582.html;44/6841;241/7231;25/8184",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;https://scholar.google.com/citations?hl=en;;FM7JCgQAAAAJ;NTqD9TAAAAAJ;pGDKzuUAAAAJ;KmSuVtgAAAAJ",
        "orcid": ";;;;;;0009-0007-9124-7541;;",
        "linkedin": "neel-jain-0a6a239/;;chenyangzhu/;daben-liu-9351941/;alfysamuel;https://linkedin.com/in/ashwineepanda;anoop-kumar-293191/;;",
        "or_profile": "~Neel_Jain1;~Aditya_Shrivastava1;~Chenyang_Zhu3;~Daben_Liu1;~Alfy_Samuel1;~Ashwinee_Panda1;~Anoop_Kumar1;~Micah_Goldblum1;~Tom_Goldstein1",
        "aff": "University of Maryland, College Park;Capital One;Capital One;CapitalOne;CapitalOne;University of Maryland, College Park;Amazon;Columbia University;University of Maryland, College Park",
        "aff_domain": "umd.edu;capitalone.com;capitalone.com;capitalone.com;capitalone.com;umd.edu;amazon.com;columbia.edu;umd.edu",
        "position": "PhD student;Researcher;Researcher;Principal Researcher;Researcher;Postdoc;Computer Scientist;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\njain2025refusal,\ntitle={Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models},\nauthor={Neel Jain and Aditya Shrivastava and Chenyang Zhu and Daben Liu and Alfy Samuel and Ashwinee Panda and Anoop Kumar and Micah Goldblum and Tom Goldstein},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Pbs4i3FgbD}\n}",
        "github": "",
        "project": "",
        "reviewers": "cYf6;gWX3;VwAm;Kbim",
        "site": "https://openreview.net/forum?id=Pbs4i3FgbD",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "4;4;3;5",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.42640143271122083
    },
    {
        "id": "Pdyh3USc2A",
        "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates\u2014where one model is given the official answer to defend, and another constructs and defends an alternative answer\u2014adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm's effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination\u2014a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% \u2192 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that \"pretraining on the test set is no longer all you need,\" offering a sustainable path for measuring the genuine reasoning ability of advanced language models.",
        "keywords": "debate-driven evaluation;QA benchmarks;multi-agent debate;language model evaluation;benchmark contamination;model memorization;adversarial evaluation;dynamic assessment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Linbo Cao;Jinman Zhao",
        "authorids": "~Linbo_Cao1;~Jinman_Zhao2",
        "gender": "M;F",
        "homepage": ";https://www.cs.toronto.edu/~jzhao/",
        "dblp": "387/7752.html;",
        "google_scholar": "MvVnENkAAAAJ;",
        "orcid": ";",
        "linkedin": "linbo-cao;",
        "or_profile": "~Linbo_Cao1;~Jinman_Zhao2",
        "aff": "University of Waterloo;Department of Computer Science, University of Toronto",
        "aff_domain": "uwaterloo.ca;cs.toronto.edu",
        "position": "Undergrad student;PhD student",
        "bibtex": "@inproceedings{\ncao2025pretraining,\ntitle={Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to {QA} Benchmarks},\nauthor={Linbo Cao and Jinman Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Pdyh3USc2A}\n}",
        "github": "",
        "project": "",
        "reviewers": "fs1s;5Sib;SpHc;wZxD",
        "site": "https://openreview.net/forum?id=Pdyh3USc2A",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "Pg0PAvbhGv",
        "title": "Rank1: Test-Time Compute for Reranking in Information Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.",
        "keywords": "retrieval;reranking;test-time compute",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Orion Weller;Kathryn Ricci;Eugene Yang;Andrew Yates;Dawn Lawrie;Benjamin Van Durme",
        "authorids": "~Orion_Weller1;~Kathryn_Ricci1;~Eugene_Yang2;~Andrew_Yates2;~Dawn_Lawrie1;~Benjamin_Van_Durme2",
        "gender": "M;;M;M;F;",
        "homepage": "https://orionweller.github.io/;;https://eugene.zone;https://andrewyates.net;https://hltcoe.jhu.edu/researcher/dawn-lawrie/;",
        "dblp": "248/7910;331/1034.html;127/0482;49/7109;l/DawnLawrie.html;",
        "google_scholar": "SYYd4iAAAAAJ;;JVqgs4gAAAAJ;https://scholar.google.de/citations?user=b4ciDMsAAAAJ;Ij9zwyoAAAAJ;",
        "orcid": ";;0000-0002-0051-1535;0000-0002-5970-880X;0000-0001-7347-7086;",
        "linkedin": ";;;;dawnjlawrie/;",
        "or_profile": "~Orion_Weller1;~Kathryn_Ricci1;~Eugene_Yang2;~Andrew_Yates2;~Dawn_Lawrie1;~Benjamin_Van_Durme2",
        "aff": "Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;",
        "aff_domain": "jhu.edu;jhu.edu;jh.edu;jhu.edu;jhu.edu;",
        "position": "PhD student;PhD student;Researcher;Senior Research Scientist;Researcher;",
        "bibtex": "@inproceedings{\nweller2025rank,\ntitle={Rank1: Test-Time Compute for Reranking in Information Retrieval},\nauthor={Orion Weller and Kathryn Ricci and Eugene Yang and Andrew Yates and Dawn Lawrie and Benjamin Van Durme},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Pg0PAvbhGv}\n}",
        "github": "",
        "project": "",
        "reviewers": "Lnne;iKPU;Znd7",
        "site": "https://openreview.net/forum?id=Pg0PAvbhGv",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "PhaE8TSM5j",
        "title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks, while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents are sensitive to the outcome of certain key steps, which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until we identify a step exhibiting positive reward differentials, i.e., rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO method achieves superior performance while requiring much less exploration cost.",
        "keywords": "large language model;agent;reinforcement learning;process reward model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zilong Wang;Jingfeng Yang;Sreyashi Nag;Samarth Varshney;Xianfeng Tang;Haoming Jiang;Jingbo Shang;Sheikh Muhammad Sarwar",
        "authorids": "~Zilong_Wang1;~Jingfeng_Yang2;~Sreyashi_Nag1;~Samarth_Varshney1;~Xianfeng_Tang1;~Haoming_Jiang1;~Jingbo_Shang2;~Sheikh_Muhammad_Sarwar1",
        "gender": "M;M;F;M;M;M;M;M",
        "homepage": "https://zilongwang.me;https://jingfengyang.github.io/;;;https://xta.ng/;https://hmjianggatech.github.io;https://shangjingbo1226.github.io/;https://scholar.google.com/citations?user=LFzh2g8AAAAJ&hl=en",
        "dblp": "42/898-2;;;383/8650.html;33/7694;230/3684;151/3145.html;98/11239.html",
        "google_scholar": "S_wQccsAAAAJ;hysBvrwAAAAJ;https://scholar.google.com/citations?hl=en;nAPDZH0AAAAJ;u1PEv-QAAAAJ;XaFhuG8AAAAJ;0SkFI4MAAAAJ;LFzh2g8AAAAJ",
        "orcid": "0000-0002-1614-0943;;;0000-0002-3841-0799;;;;",
        "linkedin": ";jingfeng-yang-797864172/;sreyashi-nag/;samarth-varshney/;xianfengtang/;;;sheikh-muhammad-sarwar-07403122/",
        "or_profile": "~Zilong_Wang1;~Jingfeng_Yang2;~Sreyashi_Nag1;~Samarth_Varshney1;~Xianfeng_Tang1;~Haoming_Jiang1;~Jingbo_Shang2;~Sheikh_Muhammad_Sarwar1",
        "aff": "University of California, San Diego;Amazon;Amazon;Amazon;Amazon;OpenAI+Amazon;University of California, San Diego;Amazon",
        "aff_domain": "ucsd.edu;amazon.com;amazon.com;amazon.com;amazon.com;openai.com+amazon.com;ucsd.edu;amazon.com",
        "position": "PhD student;Researcher;Applied Scientist;Researcher;Researcher;Researcher+Principal Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nwang2025rro,\ntitle={{RRO}: {LLM} Agent Optimization Through Rising Reward Trajectories},\nauthor={Zilong Wang and Jingfeng Yang and Sreyashi Nag and Samarth Varshney and Xianfeng Tang and Haoming Jiang and Jingbo Shang and Sheikh Muhammad Sarwar},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=PhaE8TSM5j}\n}",
        "github": "",
        "project": "",
        "reviewers": "QfKa;a4Sq;w85m;yiKS",
        "site": "https://openreview.net/forum?id=PhaE8TSM5j",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "Q5pVZCrrKr",
        "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the *Code Abstraction and Reasoning Challenge*, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC",
        "keywords": "Agent;Large Language Model;Reasoning;Code;Program Synthesis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anjiang Wei;Tarun Suresh;Jiannan Cao;Naveen Kannan;Yuheng Wu;Kai Yan;Thiago S. F. X. Teixeira;Ke Wang;Alex Aiken",
        "authorids": "~Anjiang_Wei1;~Tarun_Suresh1;~Jiannan_Cao1;~Naveen_Kannan1;~Yuheng_Wu2;~Kai_Yan1;~Thiago_S._F._X._Teixeira1;~Ke_Wang1;~Alex_Aiken1",
        "gender": "M;M;M;M;M;M;M;;M",
        "homepage": "https://cs.stanford.edu/~anjiang/;https://tarsur909.github.io/;https://scholar.google.com/citations?user=B0EM6JwAAAAJ;;;https://kaiyan289.github.io/;https://thiagotei.github.io/;https://kbwang.bitbucket.io/;https://theory.stanford.edu/~aiken/",
        "dblp": "280/5656;348/7104;361/2118.html;;;;;181/2613-9;a/AAiken",
        "google_scholar": "HoG_nZAAAAAJ;Yxx6B5YAAAAJ;B0EM6JwAAAAJ;;;KElKfgQAAAAJ;https://scholar.google.com/citations?view_op=list_works;KIe98hIAAAAJ;https://scholar.google.com.tw/citations?user=3vKjkoQAAAAJ",
        "orcid": "0000-0003-1654-6027;0000-0002-1426-7633;;;0009-0003-3664-7017;;0000-0002-8031-0652;;0000-0002-3723-9555",
        "linkedin": ";tarun-suresh-802231157/;;naveenkannan0;;%E5%BC%80-%E9%A2%9C-18b7931b1/;;;",
        "or_profile": "~Anjiang_Wei1;~Tarun_Suresh1;~Jiannan_Cao1;~Naveen_Kannan1;~Yuheng_Wu2;~Kai_Yan1;~Thiago_S._F._X._Teixeira1;~Ke_Wang1;~Alex_Aiken1",
        "aff": "Computer Science Department, Stanford University;Department of Computer Science;Massachusetts Institute of Technology;Stanford University;Stanford University;University of Illinois, Urbana Champaign;Intel;Visa Research;NVIDIA+SLAC National Accelerator Laboratory+Stanford University",
        "aff_domain": "cs.stanford.edu;cs.illinois.edu;mit.edu;stanford.edu;stanford.edu;cs.illinois.edu;intel.com;visa.com;nvidia.com+slac.stanford.edu+cs.stanford.edu",
        "position": "PhD student;Undergrad student;MS student;Undergrad student;MS student;PhD student;Researcher;Research Scientist;Visiting Professor+Division Director+Full Professor",
        "bibtex": "@inproceedings{\nwei2025codearc,\ntitle={Code{ARC}: Benchmarking Reasoning Capabilities of {LLM} Agents for Inductive Program Synthesis},\nauthor={Anjiang Wei and Tarun Suresh and Jiannan Cao and Naveen Kannan and Yuheng Wu and Kai Yan and Thiago S. F. X. Teixeira and Ke Wang and Alex Aiken},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Q5pVZCrrKr}\n}",
        "github": "",
        "project": "",
        "reviewers": "w6n7;4Yd9;3DHT;G4M6",
        "site": "https://openreview.net/forum?id=Q5pVZCrrKr",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "5;4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Q6TCkggzQ2",
        "title": "HIPPO-VIDEO : Simulating Watch Histories with Large Language Models for History-Driven Video Highlighting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The exponential growth of video content has made personalized video highlighting an essential task, as user preferences are highly variable and complex. Existing video datasets, however, often lack personalization, relying on isolated videos or simple text queries that fail to capture the intricacies of user behavior.  In this work, we introduce HIPPO-VIDEO, a novel dataset for personalized video highlighting, created using an LLM-based user simulator to generate realistic watch histories reflecting diverse user preferences. The dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400 videos across 170 semantic categories. To validate our dataset, we propose HiPHer, a method that leverages these personalized watch histories to predict preference-conditioned segment-wise saliency scores. \nThrough extensive experiments, we demonstrate that our method outperforms existing generic and query-based approaches, showcasing its potential for highly user-centric video highlighting in real-world scenarios. The code is publicly available at https://anonymous.4open.science/r/HIPPO-4EEE/README.md.",
        "keywords": "video understanding;personalization;highlight detection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jeongeun Lee;Youngjae Yu;Dongha Lee",
        "authorids": "~Jeongeun_Lee3;~Youngjae_Yu1;~Dongha_Lee1",
        "gender": "F;M;M",
        "homepage": "https://github.com/jeongeunnn-e;https://yj-yu.github.io/home/;https://donalee.github.io",
        "dblp": "150/5297;188/6210;12/760-3",
        "google_scholar": ";https://scholar.google.co.kr/citations?user=WDO24ZYAAAAJ;driVwKwAAAAJ",
        "orcid": "0009-0003-8073-4521;;0000-0003-2173-3476",
        "linkedin": "lee-jeongeun/;;",
        "or_profile": "~Jeongeun_Lee3;~Youngjae_Yu1;~Dongha_Lee1",
        "aff": "Yonsei University;Yonsei University;Yonsei University",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "position": "MS student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025hippovideo,\ntitle={{HIPPO}-{VIDEO} : Simulating Watch Histories with Large Language Models for History-Driven Video Highlighting},\nauthor={Jeongeun Lee and Youngjae Yu and Dongha Lee},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Q6TCkggzQ2}\n}",
        "github": "",
        "project": "",
        "reviewers": "6S7Q;xrxV;jWne",
        "site": "https://openreview.net/forum?id=Q6TCkggzQ2",
        "pdf_size": 0,
        "rating": "6;6;8",
        "confidence": "4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "QBmxLlmRYG",
        "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations.  To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70\\% reduction in decoding time and over 90\\% win-ties in utility and safety benchmarks.",
        "keywords": "Large Language Models;LLM Alignment",
        "primary_area": "",
        "supplementary_material": "/attachment/5dc60966cda3252835f4c0db2fb156fc31772b56.zip",
        "author": "Bolian Li;Yifan Wang;Anamika Lochab;Ananth Grama;Ruqi Zhang",
        "authorids": "~Bolian_Li1;~Yifan_Wang14;~Anamika_Lochab1;~Ananth_Grama1;~Ruqi_Zhang1",
        "gender": "M;F;F;M;F",
        "homepage": "https://lblaoke.github.io/;https://cacayaya.github.io/;https://anamikalochab.github.io/AL/;https://www.cs.purdue.edu/homes/ayg/;https://ruqizhang.github.io/",
        "dblp": "304/3220.html;;;g/AnanthGrama.html;",
        "google_scholar": "wNDoepwAAAAJ;hqL5jWYAAAAJ;;https://scholar.google.com.tw/citations?user=bpsZlEQAAAAJ;4ojpmc8AAAAJ",
        "orcid": "0000-0002-1977-0764;;0009-0002-7186-524X;;",
        "linkedin": "bolian-li-554001297/;yifan-wang-66521524b/;;;",
        "or_profile": "~Bolian_Li1;~Yifan_Wang14;~Anamika_Lochab1;~Ananth_Grama1;~Ruqi_Zhang1",
        "aff": "Purdue University;Purdue University;Purdue University;Purdue University;Purdue University",
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu;purdue.edu;purdue.edu",
        "position": "PhD student;PhD student;PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025cascade,\ntitle={Cascade Reward Sampling for Efficient Decoding-Time Alignment},\nauthor={Bolian Li and Yifan Wang and Anamika Lochab and Ananth Grama and Ruqi Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QBmxLlmRYG}\n}",
        "github": "",
        "project": "",
        "reviewers": "ToeH;t2yh;Fnvn;rS2s",
        "site": "https://openreview.net/forum?id=QBmxLlmRYG",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "3;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6324555320336758
    },
    {
        "id": "QByEdZMJdx",
        "title": "HyperINF: Unleashing the HyperPower of Schulz's Method for Data Influence Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Influence functions provide a principled approach to assess individual training samples' contributions to specific targets. However, their high computational costs have limited applications in large-scale models and datasets. While existing approximation methods have reduced computational overhead, they often suffer from inaccurate estimation due to weak convergence guarantees. Hyperpower methods offer rigorous convergence guarantees for matrix inverse approximation, but their matrix multiplication operations typically involve intractable memory and computation costs for large-scale models.\nWe propose HyperINF, an efficient and accurate influence function approximation leveraging the hyperpower HyperINF, specifically Schulz's iterative algorithm. To address computation-intensive matrix multiplication, we incorporate generalized Fisher information (GFIM) as a low-rank Hessian matrix approximation, reducing memory and computation overhead to constant costs.\nThrough comprehensive convergence simulations on matrix inversion, we demonstrate HyperINF's superior accuracy and stability compared to baselines. We further validate its efficacy through extensive real-world data attribution tasks, including mislabeled data detection and data selection for LLM and VLM fine-tuning. On LoRA-tuned models, HyperINF achieves superior downstream performance with minimal memory and computational overhead, while other approaches suffer significant degradation. Our code is available at https://github.com/Blackzxy/HyperINF .",
        "keywords": "language model; data; influence score",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinyu Zhou;Simin Fan;Martin Jaggi",
        "authorids": "~Xinyu_Zhou8;~Simin_Fan1;~Martin_Jaggi1",
        "gender": "M;F;M",
        "homepage": ";https://olivia-fsm.github.io/;https://mlo.epfl.ch",
        "dblp": ";;17/4402",
        "google_scholar": "VJiIQi8AAAAJ;YFJJxpQAAAAJ;https://scholar.google.ch/citations?user=r1TJBr8AAAAJ",
        "orcid": ";0000-0002-1490-9413;0000-0003-1579-5558",
        "linkedin": "xinyu-zhou-8151ab208/;;",
        "or_profile": "~Xinyu_Zhou8;~Simin_Fan1;~Martin_Jaggi1",
        "aff": "Shanghai AI Laboratory;EPFL - EPF Lausanne;EPFL",
        "aff_domain": "shlab.org.cn;epfl.ch;epfl.ch",
        "position": "Intern;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nzhou2025hyperinf,\ntitle={Hyper{INF}: Unleashing the HyperPower of Schulz's Method for Data Influence Estimation},\nauthor={Xinyu Zhou and Simin Fan and Martin Jaggi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QByEdZMJdx}\n}",
        "github": "",
        "project": "",
        "reviewers": "KW2V;fxsH;ZA5B;fWxU",
        "site": "https://openreview.net/forum?id=QByEdZMJdx",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;2;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "QDtORaZt8K",
        "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks in the mid-training phase facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3\\%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6\\% improvement on WebArena and an 5.4\\% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data\u2014previously considered closely aligned with GUI agent tasks and widely utilized for training\u2014has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0\\% on WebArena and 12.2\\% on AndroidWorld.",
        "keywords": "GUI agent;middle training;llm as agent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junlei Zhang;Zichen Ding;Chang Ma;Zijie Chen;Qiushi Sun;Zhenzhong Lan;Junxian He",
        "authorids": "~Junlei_Zhang1;~Zichen_Ding1;~Chang_Ma2;~Zijie_Chen3;~Qiushi_Sun1;~Zhenzhong_Lan2;~Junxian_He1",
        "gender": "M;M;;M;M;;M",
        "homepage": ";https://heroding77.github.io/;;;https://qiushisun.github.io/;;https://jxhe.github.io",
        "dblp": "197/3153.html;245/0972-2;;;247/8469;27/3780;188/6127.html",
        "google_scholar": ";N4NNsRMAAAAJ;;ZoOXqzMAAAAJ;QgMkYFAAAAAJ;tlDABkgAAAAJ;BIFGeoUAAAAJ",
        "orcid": ";0009-0000-1436-0291;;;0000-0002-5207-818X;;",
        "linkedin": ";;;;qiushi-sun/;;",
        "or_profile": "~Junlei_Zhang1;~Zichen_Ding1;~Chang_Ma2;~Zijie_Chen3;~Qiushi_Sun1;~Zhenzhong_Lan2;~Junxian_He1",
        "aff": "Westlake University;East China Normal University;;Westlake University+Zhejiang University;University of Hong Kong;Westlake University;Hong Kong University of Science and Technology",
        "aff_domain": "westlake.edu;ecnu.edu.cn;;westlake.edu+zju.edu.cn;hku.hk;westlake.edu.cn;ust.hk",
        "position": "PhD student;MS student;;PhD student+PhD student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025breaking,\ntitle={Breaking the Data Barrier -- Building {GUI} Agents Through Task Generalization},\nauthor={Junlei Zhang and Zichen Ding and Chang Ma and Zijie Chen and Qiushi Sun and Zhenzhong Lan and Junxian He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QDtORaZt8K}\n}",
        "github": "",
        "project": "",
        "reviewers": "GCJU;L7Ty;yoKR;qdBH",
        "site": "https://openreview.net/forum?id=QDtORaZt8K",
        "pdf_size": 0,
        "rating": "3;6;7;7",
        "confidence": "5;3;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.6393596310755
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6469966392206306
    },
    {
        "id": "QGJ9ttXLTy",
        "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors --- verification, backtracking, subgoal setting, and backward chaining --- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor --- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.",
        "keywords": "Reasoning;RL;self-improvement;backtracking;test-time compute;planning;search",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kanishk Gandhi;Ayush K Chakravarthy;Anikait Singh;Nathan Lile;Noah Goodman",
        "authorids": "~Kanishk_Gandhi1;~Ayush_K_Chakravarthy1;~Anikait_Singh1;~Nathan_Lile1;~Noah_Goodman1",
        "gender": ";;M;M;",
        "homepage": ";;https://asap7772.github.io/;https://nathanlile.com;https://cocolab.stanford.edu/",
        "dblp": ";;302/3876;;96/1216",
        "google_scholar": ";;lPaISmIAAAAJ;y6v6c-4AAAAJ;OUpIbcQAAAAJ",
        "orcid": ";;;0009-0009-2326-0458;",
        "linkedin": ";ayush-chakravarthy/;asap7772/;https://linkedin.com/in/nathanwangerinlile/;",
        "or_profile": "~Kanishk_Gandhi1;~Ayush_K_Chakravarthy1;~Anikait_Singh1;~Nathan_Lile1;~Noah_Goodman1",
        "aff": ";Stanford University;Stanford University;SynthLabs.ai;Stanford University",
        "aff_domain": ";stanford.edu;stanford.edu;synthlabs.ai;stanford.edu",
        "position": ";MS student;PhD student;Researcher;Full Professor",
        "bibtex": "@inproceedings{\ngandhi2025cognitive,\ntitle={Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective {ST}aRs},\nauthor={Kanishk Gandhi and Ayush K Chakravarthy and Anikait Singh and Nathan Lile and Noah Goodman},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QGJ9ttXLTy}\n}",
        "github": "",
        "project": "",
        "reviewers": "UQGC;1iHv;ZG7V;CcUW",
        "site": "https://openreview.net/forum?id=QGJ9ttXLTy",
        "pdf_size": 0,
        "rating": "6;7;7;10",
        "confidence": "2;5;5;5",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.5
        ],
        "confidence_avg": [
            4.25,
            1.299038105676658
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896258
    },
    {
        "id": "QNaHC8njYt",
        "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.",
        "keywords": "large language models;language model application;spatial data integration;spatial reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bin HAN;Robert Wolfe;Anat Caspi;Bill Howe",
        "authorids": "~Bin_HAN1;~Robert_Wolfe1;~Anat_Caspi1;~Bill_Howe1",
        "gender": "M;;;M",
        "homepage": "https://beanham.github.io/;https://wolferobert3.github.io/;;https://faculty.washington.edu/billhowe/",
        "dblp": ";50/689;;h/BillHowe",
        "google_scholar": "z0n3JnQAAAAJ;5FZ12lEAAAAJ;;dQ-x9NQAAAAJ",
        "orcid": ";;;",
        "linkedin": "bin-han-960703/;wolferobertiii/;anat-caspi-phd-8b0a161/;",
        "or_profile": "~Bin_HAN1;~Robert_Wolfe1;~Anat_Caspi1;~Bill_Howe1",
        "aff": "University of Washington;University of Washington;University of Washington;University of Washington",
        "aff_domain": "uw.edu;uw.edu;cs.washington.edu;u.washington.edu",
        "position": "PhD student;PhD student;Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nhan2025can,\ntitle={Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses},\nauthor={Bin HAN and Robert Wolfe and Anat Caspi and Bill Howe},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QNaHC8njYt}\n}",
        "github": "",
        "project": "",
        "reviewers": "ap53;VhgA;xnXe;boFx",
        "site": "https://openreview.net/forum?id=QNaHC8njYt",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "QTrW2HWNXe",
        "title": "Language Model Uncertainty Quantification with Attention Chain",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers.\nWhile most existing research focuses on short, directly answerable questions with closed-form outputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM responses is increasingly important.\nThis added complexity complicates uncertainty quantification (UQ) because the probabilities assigned to answer tokens are conditioned on a vast space of preceding reasoning tokens.\nDirect marginalization is infeasible, and the dependency inflates probability estimates, causing overconfidence in UQ.\nTo address this, we propose UQAC, an efficient method that narrows the reasoning space to a tractable size for marginalization.\nUQAC iteratively constructs an \"attention chain\" of tokens deemed \"semantically crucial to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most influential predecessors, then iterates this process until reaching the input tokens.\nThe resulting chain is further refined with similarity filtering and probability thresholding, which reduce the reasoning space, facilitating the approximation of the marginal answer token probabilities.\nWe validate UQAC on multiple reasoning benchmarks with advanced open-source LLMs, demonstrating that it consistently delivers reliable UQ estimates with high computational efficiency.",
        "keywords": "Uncertainty Estimation;Large Language Model;Attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yinghao Li;Rushi Qiang;Lama Moukheiber;Chao Zhang",
        "authorids": "~Yinghao_Li3;~Rushi_Qiang1;~Lama_Moukheiber2;~Chao_Zhang15",
        "gender": "M;M;;",
        "homepage": "https://yinghao-li.github.io/;https://rushi-q.github.io/;;http://chaozhang.org/",
        "dblp": "15/1534;372/2703;;94/3019-14",
        "google_scholar": "2WSooDIAAAAJ;https://scholar.google.com/citations?view_op=list_works;5RB9Pg8AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0000-0002-7188-4136;;;0000-0003-3009-598X",
        "linkedin": "yinghao-li-903412473/;;;",
        "or_profile": "~Yinghao_Li3;~Rushi_Qiang1;~Lama_Moukheiber2;~Chao_Zhang15",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025language,\ntitle={Language Model Uncertainty Quantification with Attention Chain},\nauthor={Yinghao Li and Rushi Qiang and Lama Moukheiber and Chao Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QTrW2HWNXe}\n}",
        "github": "",
        "project": "",
        "reviewers": "ZMZs;4Zzx;5EYn;tYEK",
        "site": "https://openreview.net/forum?id=QTrW2HWNXe",
        "pdf_size": 0,
        "rating": "4;6;7;8",
        "confidence": "4;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.479019945774904
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8783100656536799
    },
    {
        "id": "QbLbXz8Idp",
        "title": "Reinforcement Learning Enhanced Full-Duplex Spoken Dialogue Language Models for Conversational Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mainstream spoken dialogue language models (SDLMs) primarily handle turn-based interactions by alternating between processing user speech and generating responses. Recently emerging full-duplex SDLMs have showcased more natural and engaging conversational performance by simultaneously listening and speaking. However, the complex dynamics of human conversation introduce unique challenges to full-duplex SDLMs: Beyond generating reasonable responses, these models must exhibit diverse and prompt conversational behaviors in real-time interactions with the user. In this work, we present an efficient full-duplex SDLM optimized by Online Reinforcement with Interactive Speech Evaluation (ORISE). In ORISE, we design a customized reward function derived from automated annotations of online generated speech to guide the model toward well-formed and speech-text aligned responses. Experimental results show that ORISE effectively improves robustness and accuracy in handling conversational dynamics, including turn-taking, user barge-in, and backchanneling. Furthermore, ORISE enables the model to adapt to unseen noise conditions without relying on any labeled data, demonstrating the generalization of ORISE in real-world scenarios.",
        "keywords": "Full-Duplex model;Spoken Dialogue Models;Speech-to-Speech model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chen Chen;Ke Hu;Chao-Han Huck Yang;Ankita Pasad;Edresson Casanova;Weiqing Wang;Szu-Wei Fu;Jason Li;Zhehuai Chen;Jagadeesh Balam;Boris Ginsburg",
        "authorids": "~Chen_Chen56;~Ke_Hu12;~Chao-Han_Huck_Yang1;~Ankita_Pasad1;~Edresson_Casanova1;~Weiqing_Wang4;~Szu-Wei_Fu1;~Jason_Li1;~Zhehuai_Chen1;~Jagadeesh_Balam1;~Boris_Ginsburg1",
        "gender": ";;M;F;M;M;;M;M;M;",
        "homepage": ";;https://huckiyang.github.io/;;;;https://jasonswfu.github.io/JasonFu.github.io/;;https://www.linkedin.com/in/chenzhehuai/;;",
        "dblp": ";;230/4012;;;;160/0591;;173/6484;35/1863.html;",
        "google_scholar": ";;TT3XJW8AAAAJ;rsBBj3IAAAAJ;QvS6LVwAAAAJ;wKuKV6AAAAAJ;eSGkKm4AAAAJ;https://scholar.google.ca/citations?user=V28bxDwAAAAJ;;;",
        "orcid": ";;0000-0003-2879-8811;;;;;;;;",
        "linkedin": ";;;ankita-pasad-5b12a017/;edresson;weiqing-wang-846aa3191/;https://www.linkedin.com/mwlite/in/szu-wei-fu-78b47817a;;;;",
        "or_profile": "~Chen_Chen56;~Ke_Hu12;~Chao-Han_Huck_Yang1;~Ankita_Pasad1;~Edresson_Casanova1;~Weiqing_Wang4;~Szu-Wei_Fu1;~Jason_Li1;~Zhehuai_Chen1;~Jagadeesh_Balam1;~Boris_Ginsburg1",
        "aff": ";;NVIDIA Research;NVIDIA;NVIDIA;NVIDIA;NVIDIA+Microsoft;NVIDIA;NVIDIA;NVIDIA;",
        "aff_domain": ";;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com+microsoft.com;nvidia.com;nvidia.com;nvidia.com;",
        "position": ";;Researcher;Researcher;Researcher;Researcher;Researcher+Researcher;Researcher;Principal Researcher;Principal Researcher;",
        "bibtex": "@inproceedings{\nchen2025reinforcement,\ntitle={Reinforcement Learning Enhanced Full-Duplex Spoken Dialogue Language Models for Conversational Interactions},\nauthor={Chen Chen and Ke Hu and Chao-Han Huck Yang and Ankita Pasad and Edresson Casanova and Weiqing Wang and Szu-Wei Fu and Jason Li and Zhehuai Chen and Jagadeesh Balam and Boris Ginsburg},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QbLbXz8Idp}\n}",
        "github": "",
        "project": "",
        "reviewers": "mDwK;nQWA;W4Zs;PVVb",
        "site": "https://openreview.net/forum?id=QbLbXz8Idp",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "5;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "QsQatTzATT",
        "title": "Humans overrely on overconfident language models, across languages",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Prior work shows that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., 'I think it's') differs sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate LLM safety in a global context. Our work finds that overreliance risks are high across languages.\nWe first analyze the distribution of LLM-generated epistemic markers and observe that LLMs are overconfident across languages, frequently generating strengtheners even as part of incorrect responses. Model generations are, however, sensitive to documented cross-linguistic variation in usage: for example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. Next, we measure human reliance rates across languages, finding that reliance behaviors differ cross-linguistically: for example, participants are significantly more likely to discount expressions of uncertainty in Japanese than in English\n(i.e., ignore their 'hedging' function and rely on generations that contain them). Taken together, these results indicate a high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations.",
        "keywords": "multilingual language models;uncertainty",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Neil Rathi;Dan Jurafsky;Kaitlyn Zhou",
        "authorids": "~Neil_Rathi1;~Dan_Jurafsky1;~Kaitlyn_Zhou1",
        "gender": "M;M;F",
        "homepage": "https://nlp.stanford.edu/~rathi/;http://web.stanford.edu/~jurafsky/;https://cs.stanford.edu/~katezhou/",
        "dblp": ";31/985;179/4603",
        "google_scholar": "9r4-IukAAAAJ;uZg9l58AAAAJ;SQAK2mwAAAAJ",
        "orcid": ";;0000-0001-8804-8161",
        "linkedin": ";;",
        "or_profile": "~Neil_Rathi1;~Dan_Jurafsky1;~Kaitlyn_Zhou1",
        "aff": "Stanford University;Stanford University;Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "position": "Undergrad student;Full Professor;PhD student",
        "bibtex": "@inproceedings{\nrathi2025humans,\ntitle={Humans overrely on overconfident language models, across languages},\nauthor={Neil Rathi and Dan Jurafsky and Kaitlyn Zhou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QsQatTzATT}\n}",
        "github": "",
        "project": "",
        "reviewers": "EaEa;R4P6;NTZ4;TxkE",
        "site": "https://openreview.net/forum?id=QsQatTzATT",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "Qu0znWWckM",
        "title": "Do Language Models Agree with Human Perceptions of Suspense in Stories?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Suspense is an affective response to narrative text that is believed to involve complex cognitive processes in humans. Several psychological models have been developed to describe this phenomenon and the circumstances under which text might trigger it. We replicate four seminal psychological studies of human perceptions of suspense, substituting human responses with those of different open-weight and closed-source LMs. We conclude that while LMs can distinguish whether a text is intended to induce suspense in people, LMs cannot accurately estimate the relative amount of suspense within a text sequence as compared to human judgments, nor can LMs properly capture the human perception for the rise and fall of suspense across multiple text segments. We probe the abilities of LM suspense understanding by adversarially permuting the story text to identify what cause human and LM perceptions of suspense to diverge. We conclude that, while LMs can superficially identify and track certain facets of suspense, they do not process suspense in the same way as human readers.",
        "keywords": "Language Models (LMs);Cognitive Science;Psycholinguistics;Human Alignment;Theory of Mind",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Glenn Matlin;Devin Zhang;Rodrigo Barroso Loza;Diana M. Popescu;Joni Isbell;Chandreyi Chakraborty;Mark Riedl",
        "authorids": "~Glenn_Matlin1;~Devin_Zhang1;~Rodrigo_Barroso_Loza1;~Diana_M._Popescu1;~Joni_Isbell1;~Chandreyi_Chakraborty1;~Mark_Riedl1",
        "gender": "M;M;M;F;F;F;",
        "homepage": "https://glennmatlin.doctor;;;;;;http://eilab.gatech.edu/mark-riedl.html",
        "dblp": ";;;;;;",
        "google_scholar": "https://scholar.google.com/scholar?oi=bibs;;;KzxOphcAAAAJ;;;Yg_QjxcAAAAJ",
        "orcid": ";;;0009-0000-9040-5631;;;",
        "linkedin": "glennmatlin;devinzhang415;rodrigoloza/;diana-popescu25/;joniisbell/;chandreyi-zini-chakraborty/;",
        "or_profile": "~Glenn_Matlin1;~Devin_Zhang1;~Rodrigo_Barroso_Loza1;~Diana_M._Popescu1;~Joni_Isbell1;~Chandreyi_Chakraborty1;~Mark_Riedl1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;Undergrad student;MS student;Undergrad student;Undergrad student;Undergrad student;Full Professor",
        "bibtex": "@inproceedings{\nmatlin2025do,\ntitle={Do Language Models Agree with Human Perceptions of Suspense in Stories?},\nauthor={Glenn Matlin and Devin Zhang and Rodrigo Barroso Loza and Diana M. Popescu and Joni Isbell and Chandreyi Chakraborty and Mark Riedl},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Qu0znWWckM}\n}",
        "github": "",
        "project": "",
        "reviewers": "q6MK;9aGN;3sVh;aCDc",
        "site": "https://openreview.net/forum?id=Qu0znWWckM",
        "pdf_size": 0,
        "rating": "7;7;7;9",
        "confidence": "4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "QzJRtz8HNx",
        "title": "Can LLMs Handle WebShell Detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "WebShell attacks, where malicious scripts are injected into web servers, pose a significant cybersecurity threat. Traditional machine learning and deep learning methods are often hampered by challenges such as the need for extensive training data, catastrophic forgetting, and poor generalization. Recently, Large Language Models (LLMs) have emerged as a powerful alternative for code-related tasks, but their potential in WebShell detection remains underexplored. In this paper, we make two major contributions: (1) a comprehensive evaluation of seven LLMs, including GPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against traditional sequence- and graph-based methods using a dataset of 26.59K PHP scripts, and (2) the Behavioral Function-Aware Detection (BFAD) framework, designed to address the specific challenges of applying LLMs to this domain. Our framework integrates three components: a Critical Function Filter that isolates malicious PHP function calls, a Context-Aware Code Extraction strategy that captures the most behaviorally indicative code segments, and Weighted Behavioral Function Profiling (WBFP) that enhances in-context learning by prioritizing the most relevant demonstrations based on discriminative function-level profiles. Our results show that, stemming from their distinct analytical strategies, larger LLMs achieve near-perfect precision but lower recall, while smaller models exhibit the opposite trade-off. However, all baseline models lag behind previous State-Of-The-Art (SOTA) methods. With the application of BFAD, the performance of all LLMs improves significantly, yielding an average F1 score increase of 13.82%. Notably, larger models like GPT-4, LLaMA-3.1-70B, and Qwen-2.5-Coder-14B now outperform SOTA benchmarks, while smaller models such as Qwen-2.5-Coder-3B achieve performance competitive with traditional methods. This work is the first to explore the feasibility and limitations of LLMs for WebShell detection and provides solutions to address the challenges in this task.",
        "keywords": "WebShell detection;Large Language Models;Code Analysis;Cybersecurity;In-Context Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Feijiang Han;Jiaming Zhang;Chuyi Deng;Jianheng Tang;Yunhuai Liu",
        "authorids": "~Feijiang_Han1;~Jiaming_Zhang17;~Chuyi_Deng1;~Jianheng_Tang2;~Yunhuai_Liu1",
        "gender": "M;;M;M;M",
        "homepage": ";;;;http://www.yunhuai.net",
        "dblp": "338/6702;;;;38/559",
        "google_scholar": ";;;;fsaThecAAAAJ",
        "orcid": ";;;0000-0002-4762-5943;0000-0002-1180-8078",
        "linkedin": "feijianghan;;dcy-2144a1297/;;",
        "or_profile": "~Feijiang_Han1;~Jiaming_Zhang17;~Chuyi_Deng1;~Jianheng_Tang2;~Yunhuai_Liu1",
        "aff": "University of Pennsylvania;;Central South University;Peking University;Peking University",
        "aff_domain": "seas.upenn.edu;;csu.edu;stu.pku.edu.cn;pku.edu.cn",
        "position": "MS student;;Undergrad student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nhan2025can,\ntitle={Can {LLM}s Handle WebShell Detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework},\nauthor={Feijiang Han and Jiaming Zhang and Chuyi Deng and Jianheng Tang and Yunhuai Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=QzJRtz8HNx}\n}",
        "github": "",
        "project": "",
        "reviewers": "swDn;1h5H;EuGo",
        "site": "https://openreview.net/forum?id=QzJRtz8HNx",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "R135tO3SJJ",
        "title": "Impact of LLM Alignment on Impression Formation in Social Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Impression formation plays a crucial role in shaping social life, influencing behaviors, attitudes, and interactions across different contexts. Affect Control Theory (ACT) offers a well-established, empirically grounded model of how people form impressions and evaluate social interactions. We investigate whether Large Language Models (LLMs) exhibit patterns of impression formation that align with ACT's predictions. As a case study, we focus on gendered social interactions\u2014how an LLM perceives gender in a prototypic social interaction. We compare several preference-tuned derivatives of LLaMA-3 model family (including LLaMA-Instruct, Tulu-3, and DeepSeek-R1-Distill) with GPT-4 as a baseline, examining the extent to which alignment or preference tuning influences the models' tendencies in forming gender impressions. We find that LLMs form impressions quite differently than ACT. Notably, LLMs are insensitive to situational context: the impression of an interaction is overwhelmingly driven by the identity of the actor, regardless of the actor\u2019s actions or the recipient of those actions. This stands in contrast to ACT\u2019s interaction-based reasoning, which accounts for the interplay of identities, behaviors, and recipients. We further find that preference tuning often amplifies or skews certain impressions in unpredicted ways. Our corpus offers a benchmark for assessing LLMs' social intelligence; we encourage further research using ACT-like frameworks to explore how tuning influences impression formation across diverse social dimensions.",
        "keywords": "Large Language Models;Impression Formation;Alignment;Preference-tuning;Affect Control Theory",
        "primary_area": "",
        "supplementary_material": "/attachment/5b714302ae223c96227996ba6cd5ec1442f0154d.zip",
        "author": "Ala N. Tak;Anahita Bolourani;Daniel B. Shank;Jonathan Gratch",
        "authorids": "~Ala_N._Tak1;~Anahita_Bolourani1;~Daniel_B._Shank1;~Jonathan_Gratch1",
        "gender": "M;F;M;M",
        "homepage": ";;https://sites.mst.edu/shankd/;https://people.ict.usc.edu/~gratch/",
        "dblp": ";;;71/3911.html",
        "google_scholar": "y1Nx_boAAAAJ;GSaTidkAAAAJ;;HF448PMAAAAJ",
        "orcid": ";;;0000-0002-5959-809X",
        "linkedin": "ala-n-tak-abb575133/;;;",
        "or_profile": "~Ala_N._Tak1;~Anahita_Bolourani1;~Daniel_B._Shank1;~Jonathan_Gratch1",
        "aff": "University of Southern California;University of California, Los Angeles;Missouri University of Science and Technology;University of Southern California",
        "aff_domain": "usc.edu;ucla.edu;mst.edu;usc.edu",
        "position": "PhD student;PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\ntak2025impact,\ntitle={Impact of {LLM} Alignment on Impression Formation in Social Interactions},\nauthor={Ala N. Tak and Anahita Bolourani and Daniel B. Shank and Jonathan Gratch},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=R135tO3SJJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "tUeP;FtGm;7PD1;SonJ",
        "site": "https://openreview.net/forum?id=R135tO3SJJ",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "2;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "R1NWMExESj",
        "title": "Knowledge Graph Retrieval-Augmented Generation via GNN-Guided Prompting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in open-domain question answering (QA), but their reliance on knowledge learned during pretraining limits their ability to provide accurate and up-to-date information. Knowledge Graph Retrieval-Augmented Generation (KG-RAG) enhances LLMs by incorporating structured knowledge from knowledge graphs (KGs). A common approach in KG-RAG is to retrieve relevant knowledge paths starting from entities in the input question and expanding along KG edges by LLM reasoning. However, existing KG-RAG methods suffer from the challenge that retrieval is performed step by step greedily using only local graph context, which can lead to retrieval errors that prematurely discard essential paths. To address the issue and perform more accurate retrieval, we propose GGR (GNN-Guided Retrieval for LLM Reasoning), a novel GNN-enhanced KG-RAG framework that integrates graph-based relevance scoring into the retrieval process. Our approach computes global importance scores across a contextualized subgraph, ensuring that key reasoning knowledge paths are preserved, even if their local relevance appears weak. Additionally, we introduce local semantic alignment by incorporating query-relation semantic similarity, refining the relation selection of LLM. Extensive experiments on Question-Answering tasks demonstrate that our method significantly improves retrieval accuracy and answer quality, demonstrating the effectiveness of combining graph-based reasoning and LLM-driven retrieval for structured knowledge integration.",
        "keywords": "Knowledge Graph;Retrieval-Augmented Generation;Question Answering;Large Language Model;Prompt",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haochen Liu;Song Wang;Jundong Li",
        "authorids": "~Haochen_Liu3;~Song_Wang6;~Jundong_Li2",
        "gender": "M;M;M",
        "homepage": "https://haochenliu2000.github.io/;https://songw-sw.github.io/;https://jundongli.github.io/",
        "dblp": ";;144/7997.html",
        "google_scholar": ";;uY6ek7sAAAAJ",
        "orcid": ";0000-0003-1273-7694;",
        "linkedin": ";;",
        "or_profile": "~Haochen_Liu3;~Song_Wang6;~Jundong_Li2",
        "aff": "University of Virginia, Charlottesville;University of Virginia;University of Virginia",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu",
        "position": "PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025knowledge,\ntitle={Knowledge Graph Retrieval-Augmented Generation via {GNN}-Guided Prompting},\nauthor={Haochen Liu and Song Wang and Jundong Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=R1NWMExESj}\n}",
        "github": "",
        "project": "",
        "reviewers": "w5S6;4Aeo;YGPG",
        "site": "https://openreview.net/forum?id=R1NWMExESj",
        "pdf_size": 0,
        "rating": "6;6;8",
        "confidence": "5;5;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.666666666666667,
            0.4714045207910317
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9999999999999997
    },
    {
        "id": "R7qRUFHGTx",
        "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to $8\\times$ the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification.",
        "keywords": "test-time scaling;self-consistency;generative reward models;compute-matched analysis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nishad Singhi;Hritik Bansal;Arian Hosseini;Aditya Grover;Kai-Wei Chang;Marcus Rohrbach;Anna Rohrbach",
        "authorids": "~Nishad_Singhi1;~Hritik_Bansal2;~Arian_Hosseini1;~Aditya_Grover1;~Kai-Wei_Chang1;~Marcus_Rohrbach1;~Anna_Rohrbach1",
        "gender": "Not Specified;M;M;M;M;M;F",
        "homepage": "https://nishadsinghi.github.io;https://sites.google.com/view/hbansal;;https://aditya-grover.github.io;http://kwchang.net;https://rohrbach.vision/;https://anna-rohrbach.net/",
        "dblp": ";239/5922;218/5690;162/5052;18/2428;http://dblp.uni-trier.de/pers/hd/r/Rohrbach:Marcus;152/5114",
        "google_scholar": "L1b6JqsAAAAJ;gAKTYtoAAAAJ;https://scholar.google.com/citations?hl=en;oOhnPUgAAAAJ;fqDBtzYAAAAJ;3kDtybgAAAAJ;https://scholar.google.de/citations?user=GHpxNQIAAAAJ",
        "orcid": ";;;;0000-0001-5365-0072;;0000-0003-1161-6006",
        "linkedin": "https://linkedin.com/nishadsinghi;hritik-bansal/;;;kai-wei-chang-41239040;;",
        "or_profile": "~Nishad_Singhi1;~Hritik_Bansal2;~Arian_Hosseini1;~Aditya_Grover1;~Kai-Wei_Chang1;~Marcus_Rohrbach1;~Anna_Rohrbach1",
        "aff": "Technische Universit\u00e4t Darmstadt;University of California, Los Angeles;Google+University of Montreal+Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal;University of California, Los Angeles;University of California, Los Angeles+Amazon;Technische Universit\u00e4t Darmstadt;Technische Universit\u00e4t Darmstadt",
        "aff_domain": "tu-darmstadt.de;ucla.edu;deepmind.com+umontreal.ca+mila.umontreal.ca;ucla.edu;ucla.edu+amazon.com;tu-darmstadt.de;tu-darmstadt.de",
        "position": "PhD student;PhD student;Researcher+PhD student+PhD student;Assistant Professor;Associate Professor+Researcher;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nsinghi2025when,\ntitle={When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for {LLM} Reasoning},\nauthor={Nishad Singhi and Hritik Bansal and Arian Hosseini and Aditya Grover and Kai-Wei Chang and Marcus Rohrbach and Anna Rohrbach},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=R7qRUFHGTx}\n}",
        "github": "",
        "project": "",
        "reviewers": "hnKb;CSkR;Kdvi",
        "site": "https://openreview.net/forum?id=R7qRUFHGTx",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "5;4;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "R94bCTckhV",
        "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.",
        "keywords": "User-Centric Summarization;Long-form Opinions;Retrieval-Augmented Generation (RAG);Reference-free Verification;Dataset Construction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mir Tafseer Nayeem;Davood Rafiei",
        "authorids": "~Mir_Tafseer_Nayeem1;~Davood_Rafiei2",
        "gender": "M;M",
        "homepage": "https://tafseer-nayeem.github.io/;https://webdocs.cs.ualberta.ca/~drafiei/",
        "dblp": "125/2693;r/DRafiei",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=lNxSDIwAAAAJ",
        "orcid": ";",
        "linkedin": "mtnayeem/;",
        "or_profile": "~Mir_Tafseer_Nayeem1;~Davood_Rafiei2",
        "aff": "University of Alberta;University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\nnayeem2025opiniorag,\ntitle={Opinio{RAG}: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews},\nauthor={Mir Tafseer Nayeem and Davood Rafiei},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=R94bCTckhV}\n}",
        "github": "",
        "project": "",
        "reviewers": "Pz2B;T7ko;3mxc",
        "site": "https://openreview.net/forum?id=R94bCTckhV",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "ROtDZDUgvw",
        "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "There is growing interest in leveraging LLMs to aid in astronomy and other scientific research, but benchmarks for LLM evaluation in general have not kept pace with the increasingly diverse ways that real people evaluate and use these models. In this study, we seek to improve evaluation procedures by building an understanding of how users evaluate LLMs. We focus on a particular use case: an LLM-powered retrieval-augmented generation bot for engaging with astronomical literature, which we deployed via Slack. Our inductive coding of 368 queries to the bot over four weeks and our follow-up interviews with 11 astronomers reveal how humans evaluated this system, including the types of questions asked and the criteria for judging responses. We synthesize our findings into concrete recommendations for building better benchmarks, which we then employ in constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our work offers ways to improve LLM evaluation and ultimately usability, particularly for use in scientific research.",
        "keywords": "Human-centered computing;AI for Science;LM Evaluation;LLM for Astronomy",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alina Hyk;Kiera McCormick;Mian Zhong;Ioana Ciuc\u0103;Sanjib Sharma;John F Wu;J. E. G. Peek;Kartheik G. Iyer;Ziang Xiao;Anjalie Field",
        "authorids": "~Alina_Hyk1;~Kiera_McCormick1;~Mian_Zhong1;~Ioana_Ciuc\u01031;~Sanjib_Sharma1;~John_F_Wu1;~J._E._G._Peek1;~Kartheik_G._Iyer1;~Ziang_Xiao1;~Anjalie_Field2",
        "gender": "F;;;;M;M;M;M;;",
        "homepage": ";https://kieramccormick.github.io/;;;;https://jwuphysics.github.io;http://jegpeek.space;https://kartheikiyer.com/;;",
        "dblp": ";;;;;;;;196;",
        "google_scholar": ";;;;d67_2YcAAAAJ;bhxxaPwAAAAJ;;YcYe9bcAAAAJ;MjkODLEAAAAJ;",
        "orcid": "0009-0009-2008-0638;;;;0000-0002-0920-809X;0000-0002-5077-881X;0000-0003-4797-7030;0000-0001-9298-3523;;",
        "linkedin": ";kiera-mccormick/;;;;jwuphysics/;;;;",
        "or_profile": "~Alina_Hyk1;~Kiera_McCormick1;~Mian_Zhong1;~Ioana_Ciuc\u01031;~Sanjib_Sharma1;~John_F_Wu1;~J._E._G._Peek1;~Kartheik_G._Iyer1;~Ziang_Xiao1;~Anjalie_Field2",
        "aff": "Oregon State University;Johns Hopkins University+Loyola University Maryland;;;Space Telescope Science Institute;Space Telescope Science Institute;Johns Hopkins University;Columbia University;Department of Computer Science, Whiting School of Engineering;",
        "aff_domain": "oregonstate.edu;johnshopkins.edu+loyola.edu;;;stsci.edu;stsci.edu;jhu.edu;columbia.edu;cs.jhu.edu;",
        "position": "Undergrad student;Grad student+Undergrad student;;;Associate Professor;Researcher;Researcher;Postdoc;Assistant Professor;",
        "bibtex": "@inproceedings{\nhyk2025from,\ntitle={From Queries to Criteria: Understanding How Astronomers Evaluate {LLM}s},\nauthor={Alina Hyk and Kiera McCormick and Mian Zhong and Ioana Ciuc{\\u{a}} and Sanjib Sharma and John F Wu and J. E. G. Peek and Kartheik G. Iyer and Ziang Xiao and Anjalie Field},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ROtDZDUgvw}\n}",
        "github": "",
        "project": "",
        "reviewers": "6qMG;g8n8;aurw;Sraf",
        "site": "https://openreview.net/forum?id=ROtDZDUgvw",
        "pdf_size": 0,
        "rating": "3;6;7;8",
        "confidence": "5;3;5;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.8708286933869707
        ],
        "confidence_avg": [
            4.0,
            1.0
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5345224838248488
    },
    {
        "id": "RUAoV3j6tM",
        "title": "Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While Large Language Models (LLMs) are often used as virtual tutors in computer science (CS) education, this approach can foster passive learning and over-reliance. This paper presents a novel pedagogical paradigm that inverts this model: students act as instructors who must teach an LLM to solve problems. To facilitate this, we developed strategies for designing questions with engineered knowledge gaps that only a student can bridge, and we introduce Socrates, a system for deploying this method with minimal overhead. We evaluated our approach in an undergraduate course and found that this active-learning method led to statistically significant improvements in student performance compared to historical cohorts. Our work demonstrates a practical, cost-effective framework for using LLMs to deepen student engagement and mastery.",
        "keywords": "Large Language Model;Computer Science Education;Human-AI Collaboration;Role Reversal",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinming Yang;Haasil Pujara;Jun Li",
        "authorids": "~Xinming_Yang1;~Haasil_Pujara1;~Jun_Li66",
        "gender": "F;M;M",
        "homepage": ";;https://www.junli.io/",
        "dblp": ";;",
        "google_scholar": ";;My5KJhoAAAAJ",
        "orcid": ";;",
        "linkedin": "xinming-ellie-yang-0a1866202/;haasil-pujara;",
        "or_profile": "~Xinming_Yang1;~Haasil_Pujara1;~Jun_Li66",
        "aff": "City University of New York;CUNY Hunter College;City University of New York",
        "aff_domain": "graduate.cuny.edu;hunter.cuny.edu;cuny.edu",
        "position": "PhD student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025learning,\ntitle={Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education},\nauthor={Xinming Yang and Haasil Pujara and Jun Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=RUAoV3j6tM}\n}",
        "github": "",
        "project": "",
        "reviewers": "eJrt;VpCm;4NdV;q5Y1",
        "site": "https://openreview.net/forum?id=RUAoV3j6tM",
        "pdf_size": 0,
        "rating": "5;5;6;6",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "RsnxggqW4l",
        "title": "Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Dense retrieval systems have been widely used in various NLP applications. However, their vulnerabilities to potential attacks have been underexplored.\nThis paper investigates a novel attack scenario where the attackers aim to mislead the retrieval system into retrieving the attacker-specified contents. Those contents, injected into the retrieval corpus by attackers, can include harmful text like hate speech or spam. \nUnlike prior methods that rely on model weights and generate conspicuous, unnatural outputs, we propose a covert backdoor attack triggered by grammar errors. Our approach ensures that the attacked models can function normally for standard queries while covertly triggering the retrieval of the attacker's contents in response to minor linguistic mistakes.\nSpecifically, dense retrievers are trained with contrastive loss and hard negative sampling. Surprisingly, our findings demonstrate that contrastive loss is notably sensitive to grammatical errors, and hard negative sampling can exacerbate susceptibility to backdoor attacks. \nOur proposed method achieves a high attack success rate with a minimal corpus poisoning rate of only 0.048\\%, while preserving normal retrieval performance. This indicates that the method has negligible impact on user experience for error-free queries.\nFurthermore, evaluations across three real-world defense strategies reveal that the malicious passages embedded within the corpus remain highly resistant to detection and filtering, underscoring the robustness and subtlety of the proposed attack.",
        "keywords": "backdoor attack;dense retrieval",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Quanyu Long;Yue Deng;Leilei Gan;Wenya Wang;Sinno Jialin Pan",
        "authorids": "~Quanyu_Long1;~Yue_Deng3;~Leilei_Gan1;~Wenya_Wang1;~Sinno_Jialin_Pan1",
        "gender": "M;M;;F;",
        "homepage": "http://quanyulong.net;https://ntudy.github.io/;;https://personal.ntu.edu.sg/wangwy/;",
        "dblp": "259/0397;35/8109-10;;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;https://scholar.google.com.sg/citations?user=eOKISncAAAAJ;",
        "orcid": ";0009-0006-3682-8047;;0000-0001-5612-7818;",
        "linkedin": ";yue0068/;;;",
        "or_profile": "~Quanyu_Long1;~Yue_Deng3;~Leilei_Gan1;~Wenya_Wang1;~Sinno_Jialin_Pan1",
        "aff": "Nanyang Technological University;School of Computer Science and  Engineering, Nanyang Technological University;;Nanyang Technological University;",
        "aff_domain": "ntu.edu.sg;scse.ntu.edu.sg;;ntu.edu.sg;",
        "position": "PhD student;PhD student;;Assistant Professor;",
        "bibtex": "@inproceedings{\nlong2025backdoor,\ntitle={Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers},\nauthor={Quanyu Long and Yue Deng and Leilei Gan and Wenya Wang and Sinno Jialin Pan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=RsnxggqW4l}\n}",
        "github": "",
        "project": "",
        "reviewers": "dmtx;tAj4;uAKb",
        "site": "https://openreview.net/forum?id=RsnxggqW4l",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "5;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9999999999999998
    },
    {
        "id": "Rwhi91ideu",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). \nPrompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. \nThis paper introduces \\Ours, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval.\n\\Ours optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function.\nExperiments on seven question-answering datasets show that \\Ours improves performance by 41\\% (Qwen2.5-7B) and 20\\% (Qwen2.5-3B) over RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning.",
        "keywords": "reasoning;retrieval;reinforcement learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bowen Jin;Hansi Zeng;Zhenrui Yue;Jinsung Yoon;Sercan O Arik;Dong Wang;Hamed Zamani;Jiawei Han",
        "authorids": "~Bowen_Jin1;~Hansi_Zeng1;~Zhenrui_Yue1;~Jinsung_Yoon1;~Sercan_O_Arik1;~Dong_Wang21;~Hamed_Zamani1;~Jiawei_Han1",
        "gender": "M;;M;M;M;M;M;M",
        "homepage": "https://peterjin.me/;https://hansizeng.github.io/;http://yueeeeeeee.github.io/;https://sites.google.com/corp/view/jinsungyoon;https://www.sercanarik.com/;https://www.wangdong.org/;https://groups.cs.umass.edu/zamani/;http://hanj.cs.illinois.edu/",
        "dblp": "235/8066;;279/8978;173/5409.html;;40/3934-2;150/5324;h/JiaweiHan.html",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;;9Iy_KmsAAAAJ;kiFd6A8AAAAJ;;-NfMhb0AAAAJ;d2uzDIAAAAAJ;https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ",
        "orcid": "0000-0003-1295-2829;;0000-0002-0309-2065;;0000-0001-6333-1729;0000-0002-9599-8023;;0000-0002-3629-2696",
        "linkedin": "bowen-peter-jin/;;zhenrui-yue/;jinsung-yoon-bb7751b8;;;;",
        "or_profile": "~Bowen_Jin1;~Hansi_Zeng1;~Zhenrui_Yue1;~Jinsung_Yoon1;~Sercan_O_Arik1;~Dong_Wang21;~Hamed_Zamani1;~Jiawei_Han1",
        "aff": "University of Illinois, Urbana Champaign;University of Massachusetts at Amherst;University of Illinois, Urbana Champaign;Google;Google;University of Illinois, Urbana Champaign;University of Massachusetts at Amherst;University of Illinois at Urbana-Champaign (UIUC)",
        "aff_domain": "illinois.edu;umass.edu;illinois.edu;google.com;google.com;illinois.edu;umass.edu;illinois.edu",
        "position": "PhD student;PhD student;PhD student;Research Scientist;Research Scientist;Associate Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\njin2025searchr,\ntitle={Search-R1: Training {LLM}s to Reason and Leverage Search Engines with Reinforcement Learning},\nauthor={Bowen Jin and Hansi Zeng and Zhenrui Yue and Jinsung Yoon and Sercan O Arik and Dong Wang and Hamed Zamani and Jiawei Han},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Rwhi91ideu}\n}",
        "github": "",
        "project": "",
        "reviewers": "cUvm;yiKE;udKF;atAs",
        "site": "https://openreview.net/forum?id=Rwhi91ideu",
        "pdf_size": 0,
        "rating": "6;7;7;9",
        "confidence": "4;4;3;5",
        "wc_review": "",
        "rating_avg": [
            7.25,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6488856845230502
    },
    {
        "id": "S2IKxulLT1",
        "title": "Weight ensembling improves reasoning in language models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate a pitfall during the training of reasoning models where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, Pass@1 reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we note that WiSE-FT provides complementary gains across performance metrics that is not achievable by diversity-inducing decoding strategies alone, like temperature scaling. We formalize a \\emph{bias-variance tradeoff} of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling and possibly other decoding strategies face an inherent tradeoff between decreasing variance with increasing bias.",
        "keywords": "test-time scaling;RL;reasoning;diversity;decoding",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xingyu Dang;Christina Baek;Kaiyue Wen;J Zico Kolter;Aditi Raghunathan",
        "authorids": "~Xingyu_Dang2;~Christina_Baek2;~Kaiyue_Wen1;~J_Zico_Kolter1;~Aditi_Raghunathan1",
        "gender": "M;;M;;F",
        "homepage": "https://dangxingyu.github.io/;https://kebaek.github.io;https://whenwen.github.io/;;https://www.cs.cmu.edu/~aditirag/",
        "dblp": ";202/7238;322/0395;;166/1409",
        "google_scholar": "_qG6rGkAAAAJ;;;;Ch9iRwQAAAAJ",
        "orcid": ";;0000-0002-3128-868X;;",
        "linkedin": "xingyu-dang-87b315267/;;kaiyue-wen-a3a336192/;;",
        "or_profile": "~Xingyu_Dang2;~Christina_Baek2;~Kaiyue_Wen1;~J_Zico_Kolter1;~Aditi_Raghunathan1",
        "aff": "Princeton University+Institute for Interdisciplinary Information Sciences, Tsinghua University;Carnegie Mellon University;Stanford University;;Carnegie Mellon University",
        "aff_domain": "princeton.edu+mails.tsinghua.edu.cn;cmu.edu;stanford.edu;;cmu.edu",
        "position": "PhD student+Undergrad student;PhD student;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\ndang2025weight,\ntitle={Weight ensembling improves reasoning in language models},\nauthor={Xingyu Dang and Christina Baek and Kaiyue Wen and J Zico Kolter and Aditi Raghunathan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=S2IKxulLT1}\n}",
        "github": "",
        "project": "",
        "reviewers": "CtBR;Mseo;yBuj;jb4c",
        "site": "https://openreview.net/forum?id=S2IKxulLT1",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "S4nTXotasR",
        "title": "Bootstrapping Visual Assistant Modeling with Situated Interaction Simulation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Visual assistants that can guide humans through complex tasks in physical environments have significant potential, yet their development is hindered by the high cost of human-in-the-loop data collection. We present BASIS (Bootstrapping Assistant modeling with Situated Interaction Simulation), a novel framework that fundamentally rethinks how visual assistants are developed and evaluated. Rather than relying on expensive human data collection, BASIS leverages simulation to bootstrap capable assistants through three interconnected stages: (1) Situated Interaction Simulation generates high-quality synthetic data through interactions between oracle assistants and simulated users; (2) Autonomous Model Development trains and continuously evaluates assistant models using this synthetic data; and (3) Real-User Validation verifies effectiveness with human users. We implement BASIS in Alexa Arena and demonstrate that our best model\u2014despite being fine-tuned solely on synthetic data and operating under realistic perception conditions\u2014enables real human users to achieve a 72.9% success rate, approaching the 88.6% performance of an oracle assistant with access to privileged information of perfect perception. Through detailed error analysis, we identify object identification as the primary bottleneck for current visual assistants. Our approach successfully bridges the gap between simulation and reality, establishing a scalable pipeline for developing assistants that can effectively guide users through complex tasks. Project website: https://colm-basis.github.io/",
        "keywords": "visual assistant;embodied;simulation;multimodal;LLM agent;situated dialogue",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yichi Zhang;Run Peng;Yinpei Dai;Lingyun Wu;Xuweiyi Chen;Qiaozi Gao;Joyce Chai",
        "authorids": "~Yichi_Zhang1;~Run_Peng1;~Yinpei_Dai1;~Lingyun_Wu2;~Xuweiyi_Chen1;~Qiaozi_Gao1;~Joyce_Chai2",
        "gender": "M;M;M;M;M;M;",
        "homepage": "https://594zyc.github.io/;https://roihn.github.io/;https://yinpeidai.github.io/;;https://xuweiyichen.github.io/;;",
        "dblp": "86/7054-1;354/3815;209/9564;;;173/1986;",
        "google_scholar": "xkBBhY8AAAAJ;dqTJFVcAAAAJ;EzAk5DUAAAAJ;;QgoY8GEAAAAJ;Ub3LlsgAAAAJ;",
        "orcid": "0000-0003-3214-1070;;;;;;",
        "linkedin": "yichi-zhang-354a83128/;;;dravenwu;;;",
        "or_profile": "~Yichi_Zhang1;~Run_Peng1;~Yinpei_Dai1;~Lingyun_Wu2;~Xuweiyi_Chen1;~Qiaozi_Gao1;~Joyce_Chai2",
        "aff": "University of Michigan;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Virginia, Charlottesville;Amazon;",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;cs.virginia.edu;amazon.com;",
        "position": "PhD student;PhD student;PhD student;Researcher;PhD student;Scientist;",
        "bibtex": "@inproceedings{\nzhang2025bootstrapping,\ntitle={Bootstrapping Visual Assistant Modeling with Situated Interaction Simulation},\nauthor={Yichi Zhang and Run Peng and Yinpei Dai and Lingyun Wu and Xuweiyi Chen and Qiaozi Gao and Joyce Chai},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=S4nTXotasR}\n}",
        "github": "",
        "project": "",
        "reviewers": "3Fih;DrY1;PSAt;EikH",
        "site": "https://openreview.net/forum?id=S4nTXotasR",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "SHB0sLrZrh",
        "title": "MegaMath: Pushing the Limits of Open Math Corpora",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mathematical reasoning represents a cornerstone of human intelligence, driving problem-solving and innovation, and thus serves as a key indicator of the advanced capabilities of large language models(LLMs). However, the research community still lacks an open, adequate-scaled, high-quality mathematical corpus to match the data requirements of top-grade LLMs. We present MegaMath, an open dataset curated from diverse, mathematics-focused sources, designed to enhance LLMs' proficiency in mathematical reasoning. Specifically, MegaMath is curated via following practices: (1) Revisiting web data: We re-extract all mathematical documents with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all aimed at acquiring higher-quality data specifically for the mathematical domain on the Internet. (2)Recalling Math-related code data: We identify high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We conduct various data synthesis practices, resulting in a massive dataset including both synthetic text such as QA-style data, and code. By integrating these strategies and validating their practicality via extensive ablations, MegaMath delivers 371B tokens with largest quantity and top quality among existing open math pre-training datasets.",
        "keywords": "Pre-training Data;Mathematical Reasoning;Synthetic Data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fan Zhou;Zengzhi Wang;Nikhil Ranjan;Zhoujun Cheng;Liping Tang;Guowei He;Zhengzhong Liu;Eric P. Xing",
        "authorids": "~Fan_Zhou6;~Zengzhi_Wang1;~Nikhil_Ranjan2;~Zhoujun_Cheng1;~Liping_Tang2;~Guowei_He1;~Zhengzhong_Liu1;~Eric_Xing1",
        "gender": "M;M;M;M;F;;M;M",
        "homepage": "https://koalazf99.github.io/;https://sinclaircoder.github.io/;https://nikhilranjan7.github.io/;http://blankcheng.github.io;;https://www.mbzuai.ac.ae;https://hunterhector.github.io/;http://www.cs.cmu.edu/~epxing/",
        "dblp": ";34/133;;;;;166/0352;36/3855",
        "google_scholar": "qi8UzmkAAAAJ;https://scholar.google.com/citations?hl=zh-CN;;t41vrrQAAAAJ;;;S9E-hMwAAAAJ;https://scholar.google.com.tw/citations?user=5pKTRxEAAAAJ",
        "orcid": ";0000-0002-6146-6248;;;;;;",
        "linkedin": ";;nikhilranjan7;;%E4%B8%BD%E5%B9%B3-%E5%94%90-51972419a;;hunterhector/;",
        "or_profile": "~Fan_Zhou6;~Zengzhi_Wang1;~Nikhil_Ranjan2;~Zhoujun_Cheng1;~Liping_Tang2;~Guowei_He1;~Zhengzhong_Liu1;~Eric_Xing1",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;Mohamed bin Zayed University of Artificial Intelligence;UC San Diego;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed Univeristy of AI+School of Computer Science, Carnegie Mellon University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;mbzuai.ac.ae;ucsd.edu;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae+cs.cmu.edu",
        "position": "MS student;PhD student;Researcher;PhD student;NLP Engineer;Researcher;Researcher;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nzhou2025megamath,\ntitle={MegaMath: Pushing the Limits of Open Math Corpora},\nauthor={Fan Zhou and Zengzhi Wang and Nikhil Ranjan and Zhoujun Cheng and Liping Tang and Guowei He and Zhengzhong Liu and Eric P. Xing},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=SHB0sLrZrh}\n}",
        "github": "",
        "project": "",
        "reviewers": "CT6F;J7PA;PQwQ",
        "site": "https://openreview.net/forum?id=SHB0sLrZrh",
        "pdf_size": 0,
        "rating": "6;6;8",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.49999999999999983
    },
    {
        "id": "SlRtFwBdzP",
        "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel \"superficial reflection bias\" where phrases mimicking reasoning (e.g., \"wait, let me think...\") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\\% in preference alignment datasets and 14\\% in fact-related datasets, in-context learning that provides up to 27\\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\\% in preference datasets and 16\\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges. Our code is available at \\url{https://github.com/Persdre/LRM-bias-evaluation}.",
        "keywords": "Large Reasoning Models;LLM Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qian Wang;Zhanzhi Lou;Zhenheng Tang;Nuo Chen;Xuandong Zhao;Wenxuan Zhang;Dawn Song;Bingsheng He",
        "authorids": "~Qian_Wang25;~Zhanzhi_Lou1;~Zhenheng_Tang2;~Nuo_Chen4;~Xuandong_Zhao1;~Wenxuan_Zhang1;~Dawn_Song1;~Bingsheng_He1",
        "gender": ";M;;;M;;F;M",
        "homepage": ";https://zzzlou.github.io/;;https://nuojohnchen.github.io/;https://xuandongzhao.github.io/;https://isakzhang.github.io/;;http://www.comp.nus.edu.sg/~hebs/",
        "dblp": ";;;135/5622-2.html;244/8033;85/1177-1.html;s/DXSong;h/BingshengHe.html",
        "google_scholar": "KAGrBdoAAAAJ;CigEkagAAAAJ;;https://scholar.google.com/citations?hl=zh-CN;CxeH4uoAAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=RogYLKYAAAAJ",
        "orcid": ";;;0000-0001-6563-1215;;;;0000-0001-8618-4581",
        "linkedin": ";;;;xuandong-zhao-a3270610b/;wenxuan-zhang-608b88153/;;bingsheng-he-7734b131",
        "or_profile": "~Qian_Wang25;~Zhanzhi_Lou1;~Zhenheng_Tang2;~Nuo_Chen4;~Xuandong_Zhao1;~Wenxuan_Zhang1;~Dawn_Song1;~Bingsheng_He1",
        "aff": "National University of Singapore;National University of Singapore;;National University of Singapore+The Chinese University of Hong Kong, Shenzhen;UC Berkeley;Singapore University of Technology and Design+Alibaba Group;University of California, Berkeley;National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg;;u.nus.edu+cuhk.edu.cn;berkeley.edu;sutd.edu.sg+alibaba-inc.com;berkeley.edu;nus.edu.sg",
        "position": "PhD student;Undergrad student;;PhD student+Researcher;Postdoc;Assistant Professor+Researcher;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwang2025assessing,\ntitle={Assessing Judging Bias in Large Reasoning Models: An Empirical Study},\nauthor={Qian Wang and Zhanzhi Lou and Zhenheng Tang and Nuo Chen and Xuandong Zhao and Wenxuan Zhang and Dawn Song and Bingsheng He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=SlRtFwBdzP}\n}",
        "github": "",
        "project": "",
        "reviewers": "aQcV;gZAk;DpbF;sZDC",
        "site": "https://openreview.net/forum?id=SlRtFwBdzP",
        "pdf_size": 0,
        "rating": "6;7;8;9",
        "confidence": "4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            54,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.2581988897471611
    },
    {
        "id": "SoEmgM1ioC",
        "title": "Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose **CollabUIAgents**, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. \nEmpirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems. Our work is available at https://github.com/THUNLP-MT/CollabUIAgents.",
        "keywords": "Large language model; Multi-agent learning; Reinforcement learning; UI agent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhitao He;Zijun Liu;Peng Li;Yi R. Fung;Ming Yan;Ji Zhang;Fei Huang;Yang Liu",
        "authorids": "~Zhitao_He1;~Zijun_Liu2;~Peng_Li2;~Yi_R._Fung1;~Ming_Yan2;~Ji_Zhang3;~Fei_Huang2;~Yang_Liu19",
        "gender": "M;M;M;;M;;M;M",
        "homepage": ";;http://www.lpeng.net/;;;;https://sites.google.com/view/fei-huang;http://nlp.csai.tsinghua.edu.cn/~ly/",
        "dblp": ";;83/6353-30;;51/5332-4.html;86/1953-11;h/FeiHuang.html;51/3710-5",
        "google_scholar": "ULvoYXgAAAAJ;vXsVhPcAAAAJ;hgYzkOQAAAAJ;;uIUfGxYAAAAJ;cgnuJDUAAAAJ;9r98PpoAAAAJ;https://scholar.google.com.hk/citations?user=lVhoKNcAAAAJ",
        "orcid": "0009-0003-3317-1260;;0000-0003-1374-5979;;0000-0003-4959-8878;;;0000-0002-3087-242X",
        "linkedin": ";%E5%AD%90%E5%90%9B-%E5%88%98-164596263/;;;;;fei-huang-cas-cmu;",
        "or_profile": "~Zhitao_He1;~Zijun_Liu2;~Peng_Li2;~Yi_R._Fung1;~Ming_Yan2;~Ji_Zhang3;~Fei_Huang2;~Yang_Liu19",
        "aff": "Hong Kong University of Science and Technology;Department of Computer Science and Technology, Tsinghua University;Tsinghua University;;Alibaba Group;Alibaba Group;Alibaba Group US;Tsinghua University",
        "aff_domain": "connect.ust.hk;cs.tsinghua.edu.cn;tsinghua.edu.cn;;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;tsinghua.edu.cn",
        "position": "PhD student;PhD student;Associate Professor;;Instructor;Senior Staff Engineer;Senior Research Director;Professor",
        "bibtex": "@inproceedings{\nhe2025advancing,\ntitle={Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization},\nauthor={Zhitao He and Zijun Liu and Peng Li and Yi R. Fung and Ming Yan and Ji Zhang and Fei Huang and Yang Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=SoEmgM1ioC}\n}",
        "github": "",
        "project": "",
        "reviewers": "sVoJ;Mpaq;wGPK;9rgY",
        "site": "https://openreview.net/forum?id=SoEmgM1ioC",
        "pdf_size": 0,
        "rating": "4;6;6;6",
        "confidence": "4;4;3;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            34,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "SrKdi4MsUW",
        "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. In this paper, we introduce $\\textit{LongPerceptualThoughts}$, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, we propose a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then distills simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, we demonstrate notable improvements over existing visual reasoning data-generation methods. Our model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the challenging text reasoning benchmark, MMLU-Pro, by +2 points.",
        "keywords": "multimodal reasoning;vision-language models;chain-of-thought",
        "primary_area": "",
        "supplementary_material": "/attachment/ba91a96a582fba071ab27d740f8f0a19b0eb11d4.zip",
        "author": "Yuan-Hong Liao;Sven Elflein;Liu He;Laura Leal-Taix\u00e9;Yejin Choi;Sanja Fidler;David Acuna",
        "authorids": "~Yuan-Hong_Liao2;~Sven_Elflein1;~Liu_He2;~Laura_Leal-Taix\u00e91;~Yejin_Choi1;~Sanja_Fidler1;~David_Acuna1",
        "gender": "M;M;M;F;F;F;M",
        "homepage": "https://andrewliao11.github.io;https://selflein.github.io/;https://arking1995.github.io/;https://dvl.in.tum.de/team/lealtaixe/;https://yejinc.github.io/;http://www.cs.toronto.edu/~fidler/;http://www.cs.toronto.edu/~davidj/",
        "dblp": "17/2625;;;47/8483;89/579-1;08/6607;217/2906",
        "google_scholar": "cL05XGsAAAAJ;https://scholar.google.de/citations?hl=en;wfeqp2oAAAAJ;tT2TC-UAAAAJ;vhP-tlcAAAAJ;CUlqK5EAAAAJ;https://scholar.google.ca/citations?user=9aFd9dEAAAAJ",
        "orcid": ";;0000-0001-9715-2606;;;;",
        "linkedin": "andrewliao11/;;liu-he-1a7239197/;;;sanja-fidler-2846a1a?trk=hp-identity-name;",
        "or_profile": "~Yuan-Hong_Liao2;~Sven_Elflein1;~Liu_He2;~Laura_Leal-Taix\u00e91;~Yejin_Choi1;~Sanja_Fidler1;~David_Acuna1",
        "aff": "University of Toronto;University of Toronto+Department of Computer Science, University of Toronto+NVIDIA;Purdue University;NVIDIA+Technical University Munich;Computer Science Department, Stanford University+NVIDIA;Department of Computer Science, University of Toronto;NVIDIA",
        "aff_domain": "toronto.edu;utoronto.ca+cs.toronto.edu+nvidia.com;purdue.edu;nvidia.com+tum.de;cs.stanford.edu+nvidia.com;cs.toronto.edu;nvidia.com",
        "position": "PhD student;PhD student+PhD student+Intern;PhD student;Principal Researcher+Assistant Professor;Full Professor+Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nliao2025longperceptualthoughts,\ntitle={LongPerceptualThoughts: Distilling System-2 Reasoning  for System-1 Perception},\nauthor={Yuan-Hong Liao and Sven Elflein and Liu He and Laura Leal-Taix{\\'e} and Yejin Choi and Sanja Fidler and David Acuna},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=SrKdi4MsUW}\n}",
        "github": "",
        "project": "",
        "reviewers": "V9Qt;WtLJ;DSu2;EHkH",
        "site": "https://openreview.net/forum?id=SrKdi4MsUW",
        "pdf_size": 0,
        "rating": "4;4;6;7",
        "confidence": "2;4;4;5",
        "wc_review": "",
        "rating_avg": [
            5.25,
            1.299038105676658
        ],
        "confidence_avg": [
            3.75,
            1.0897247358851685
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7505683356701914
    },
    {
        "id": "Sz3ZU6oeVJ",
        "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions, and \\textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \\textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.",
        "keywords": "Large Language Models;Direct Preference Optimization;Preference Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/e375627504f8c6a7d8f8af6f540ac91b443b2c1b.zip",
        "author": "Bingxiang He;Wenbin Zhang;Jiaxi Song;Cheng Qian;Zixuan Fu;Bowen Sun;Ning Ding;Haiwen Hong;Longtao Huang;Hui Xue;Ganqu Cui;Wanxiang Che;Zhiyuan Liu;Maosong Sun",
        "authorids": "~Bingxiang_He1;~Wenbin_Zhang3;~Jiaxi_Song1;~Cheng_Qian4;~Zixuan_Fu2;~Bowen_Sun2;~Ning_Ding5;~Haiwen_Hong1;~Longtao_Huang2;~Hui_Xue5;~Ganqu_Cui1;~Wanxiang_Che1;~Zhiyuan_Liu1;~Maosong_Sun1",
        "gender": "M;;M;;F;M;M;M;M;;M;M;M;M",
        "homepage": "https://hbx-hbx.github.io/;;https://github.com/Jacky-777;;;;https://www.stingning.cn/;;http://people.ucas.edu.cn/~huanglongtao?language=en;;https://cgq15.github.io/;http://ir.hit.edu.cn/~car/;http://nlp.csai.tsinghua.edu.cn/~lzy;https://www.cs.tsinghua.edu.cn/csen/info/1312/4394.htm",
        "dblp": "322/5932;;;;;;;297/4419;76/10119;;232/3064;https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang;53/3245-1;95/3291-1",
        "google_scholar": "mb36VikAAAAJ;;;;https://scholar.google.com/citations?view_op=list_works;;uZXQuYAAAAAJ;;EQDfV9cAAAAJ;;3IVSzZgAAAAJ;SVlQ6IEAAAAJ;dT0v5u0AAAAJ;https://scholar.google.com.tw/citations?user=zIgT0HMAAAAJ",
        "orcid": ";;;;;0009-0007-4596-0047;;;;;;;0000-0002-7709-2543;",
        "linkedin": ";https://www.linkedin.cn/incareer/in/ACoAADOMbHEBBHe39PK2eK0gTam8cPUxm6TyYXk;;;;;;%E6%B5%B7%E6%96%87-%E6%B4%AA-b690721b6/;;;;;;",
        "or_profile": "~Bingxiang_He1;~Wenbin_Zhang3;~Jiaxi_Song1;~Cheng_Qian4;~Zixuan_Fu2;~Bowen_Sun2;~Ning_Ding5;~Haiwen_Hong1;~Longtao_Huang2;~Hui_Xue5;~Ganqu_Cui1;~Wanxiang_Che1;~Zhiyuan_Liu1;~Maosong_Sun1",
        "aff": "Tsinghua University;Harbin Institute of Technology;Tsinghua University;;Tsinghua University+Soochow University;Tsinghua University;Tsinghua University+Tsinghua University;Alibaba Group;Alibaba Group;;Shanghai AI Laboratory;Harbin Institute of Technology;Tsinghua University;Tsinghua University",
        "aff_domain": "tsinghua.edu.cn;hit.edu.cn;tsinghua.edu.cn;;mail.tsinghua.edu.cn+stu.suda.edu.cn;tsinghua.edu.cn;mail.tsinghua.edu.cn+mail.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com;;pjlab.org.cn;hit.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;PhD student;Undergrad student;;MS student+Undergrad student;Undergrad student;Assistant Professor+Postdoc;Researcher;Researcher;;Researcher;Full Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nhe2025air,\ntitle={{AIR}: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset},\nauthor={Bingxiang He and Wenbin Zhang and Jiaxi Song and Cheng Qian and Zixuan Fu and Bowen Sun and Ning Ding and Haiwen Hong and Longtao Huang and Hui Xue and Ganqu Cui and Wanxiang Che and Zhiyuan Liu and Maosong Sun},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Sz3ZU6oeVJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "E4jA;EMwy;xkuT",
        "site": "https://openreview.net/forum?id=Sz3ZU6oeVJ",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "T2TZ0RY4Zk",
        "title": "LIMO: Less is More for Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. \nSpecifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3% accuracy on AIME24 and 95.6% on MATH500, surpassing previous fine-tuned models (6.5% on AIME24, 59.2% on MATH500) while using only 1% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8% absolute improvement across diverse benchmarks, outperforming models trained on 100\u00d7 more data.\nSynthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as \u201ccognitive templates\u201d that guide reasoning.",
        "keywords": "Large language models;Mathematical reasoning;Data efficiency;Supervised fine-tuning;Inference-time computation;Reasoning chains",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yixin Ye;Zhen Huang;Yang Xiao;Ethan Chern;Shijie Xia;Pengfei Liu",
        "authorids": "~Yixin_Ye1;~Zhen_Huang9;~Yang_Xiao6;~Ethan_Chern1;~Shijie_Xia2;~Pengfei_Liu1",
        "gender": "F;M;M;;M;M",
        "homepage": "https://bleaves.github.io/;https://huangzhen02.github.io/;https://xiaoyang66.github.io/;;https://shijie-xia.github.io/;http://pfliu.com/",
        "dblp": ";;;;375/1162;34/3381-3",
        "google_scholar": "YOL-RtEAAAAJ;;rLqDPtQAAAAJ;;https://scholar.google.com/citations?hl=zh-CN;oIz_CYEAAAAJ",
        "orcid": ";;0009-0009-6191-1522;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Yixin_Ye1;~Zhen_Huang9;~Yang_Xiao6;~Ethan_Chern1;~Shijie_Xia2;~Pengfei_Liu1",
        "aff": "Shanghai Jiaotong University;Soochow University;Hong Kong Polytechnic University;;Shanghai Jiaotong University;Shanghai Jiaotong University",
        "aff_domain": "sjtu.edu.cn;suda.edu.cn;polyu.edu.hk;;sjtu.edu.cn;sjtu.edu",
        "position": "Undergrad student;Undergrad student;PhD student;;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nye2025limo,\ntitle={{LIMO}: Less is More for Reasoning},\nauthor={Yixin Ye and Zhen Huang and Yang Xiao and Ethan Chern and Shijie Xia and Pengfei Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=T2TZ0RY4Zk}\n}",
        "github": "",
        "project": "",
        "reviewers": "vreW;EDcw;QUFL;SdiK",
        "site": "https://openreview.net/forum?id=T2TZ0RY4Zk",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;5;2;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            1.0897247358851685
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.13245323570650439
    },
    {
        "id": "TA6azZKWJq",
        "title": "Self-Evolving Critique Abilities in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans. In such scenarios, leveraging the *critique* ability of LLMs themselves\u2014identifying and correcting flaws\u2014shows considerable promise. This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities *without* external supervision *unresolved*. We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities. We find that naive data generation approaches often produce superficial critiques of low quality. To address this limitation, we propose a contrastive-critic approach that uses reference solutions to enhance the understanding of LLMs for relevant concepts and incorporates a self-validation scheme to further improve data quality. Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements: a 10.0\\% relative gain in critique-correction accuracy and a 19.0\\% relative improvement in error identification F1-score across various benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.",
        "keywords": "Large Language Models;Critique Model;Synthetic Data;Mathematical Reasoning;Scalable Oversight",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhengyang Tang;Ziniu Li;Zhenyang Xiao;Tian Ding;Ruoyu Sun;Benyou Wang;Dayiheng Liu;Fei Huang;Tianyu Liu;Bowen Yu;Junyang Lin",
        "authorids": "~Zhengyang_Tang1;~Ziniu_Li1;~Zhenyang_Xiao1;~Tian_Ding1;~Ruoyu_Sun1;~Benyou_Wang2;~Dayiheng_Liu1;~Fei_Huang3;~Tianyu_Liu3;~Bowen_Yu3;~Junyang_Lin1",
        "gender": "M;M;M;M;;M;M;M;M;M;M",
        "homepage": ";http://www.liziniu.org/;;;https://ruoyus.github.io/;https://wabyking.github.io/old.html;https://dayihengliu.github.io/;;;https://yubowen-ph.github.io/;",
        "dblp": "247/3097;254/0986;351/3467;;30/9879-1;169/1793;https://dblp.uni-trier.de/pers/hd/l/Liu:Dayiheng;h/FeiHuang-5;134/1099-1;95/10266-2.html;215/3823",
        "google_scholar": "2RRV0PQAAAAJ;80UnKQQAAAAJ;ObcXlIEAAAAJ;https://scholar.google.com.hk/citations?user=lVkDF-YAAAAJ;PsfzbCMAAAAJ;Jk4vJU8AAAAJ;pPLQrX4AAAAJ;https://scholar.google.com.hk/citations?user=7udAEzMAAAAJ;https://scholar.google.com.hk/citations?user=6hHbBwwAAAAJ;oHoEp34AAAAJ;qp6IwtgAAAAJ",
        "orcid": ";;;0000-0002-9383-8405;;0000-0002-1501-9914;0000-0002-8755-8941;;;0000-0002-6804-1859;",
        "linkedin": ";;;;;;;;;;",
        "or_profile": "~Zhengyang_Tang1;~Ziniu_Li1;~Zhenyang_Xiao1;~Tian_Ding1;~Ruoyu_Sun1;~Benyou_Wang2;~Dayiheng_Liu1;~Fei_Huang3;~Tianyu_Liu3;~Bowen_Yu3;~Junyang_Lin1",
        "aff": "The Chinese University of Hong Kong, Shenzhen;The Chinese University of Hong Kong, Shenzhen;Peking University;Shenzhen Research Institute of Big Data;The Chinese University of Hong Kong;The Chinese University of Hong Kong, Shenzhen;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group;Alibaba Group",
        "aff_domain": "cuhk.edu.cn;cuhk.edu.cn;stu.pku.edu.cn;sribd.cn;cuhk.edu.cn;cuhk.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "position": "PhD student;PhD student;MS student;Researcher;Associate Professor;Assistant Professor;Researcher;Researcher;Staff Engineer;Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\ntang2025selfevolving,\ntitle={Self-Evolving Critique Abilities in Large Language Models},\nauthor={Zhengyang Tang and Ziniu Li and Zhenyang Xiao and Tian Ding and Ruoyu Sun and Benyou Wang and Dayiheng Liu and Fei Huang and Tianyu Liu and Bowen Yu and Junyang Lin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=TA6azZKWJq}\n}",
        "github": "",
        "project": "",
        "reviewers": "DGVo;jYD3;whUs",
        "site": "https://openreview.net/forum?id=TA6azZKWJq",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "TMB9SKqit9",
        "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.",
        "keywords": "persuasion;large language models;safety;evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Minqian Liu;Zhiyang Xu;Xinyi Zhang;Heajun An;Sarvech Qadir;Qi Zhang;Pamela J. Wisniewski;Jin-Hee Cho;Sang Won Lee;Ruoxi Jia;Lifu Huang",
        "authorids": "~Minqian_Liu2;~Zhiyang_Xu1;~Xinyi_Zhang26;~Heajun_An1;~Sarvech_Qadir1;~Qi_Zhang41;~Pamela_J._Wisniewski1;~Jin-Hee_Cho1;~Sang_Won_Lee1;~Ruoxi_Jia1;~Lifu_Huang1",
        "gender": "M;M;F;M;M;F;F;F;M;;M",
        "homepage": "https://mqianliu.github.io/;;;;https://scholar.google.com/citations?user=3yR3pAIAAAAJ&hl=en;;https://stirlab.org;https://people.cs.vt.edu/~jicho/;https://www.sangwonlee;https://ruoxijia.info/;https://wilburone.github.io/",
        "dblp": "193/2086;267/2280;;401/8222;;;k/PamelaKarr;71/2230;01/4601-2.html;147/5355-1;127/0072",
        "google_scholar": "xCR8nrwAAAAJ;11zbVUAAAAAJ;https://scholar.google.com/citations?hl=en;;3yR3pAIAAAAJ;;;wToVkEUAAAAJ;II2NGWIAAAAJ;JCrug-YAAAAJ;76IEGtYAAAAJ",
        "orcid": ";;0009-0003-4188-0104;0009-0007-6124-3750;0009-0006-7962-5792;0000-0002-3607-3258;0000-0002-6223-1029;;0000-0002-1026-315X;;",
        "linkedin": ";;xinyi-zhang-8bb150366/;anheajun/;;qi-zhang-19458a190/;pamjwis/;;;;",
        "or_profile": "~Minqian_Liu2;~Zhiyang_Xu1;~Xinyi_Zhang26;~Heajun_An1;~Sarvech_Qadir1;~Qi_Zhang41;~Pamela_J._Wisniewski1;~Jin-Hee_Cho1;~Sang_Won_Lee1;~Ruoxi_Jia1;~Lifu_Huang1",
        "aff": "Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University;Vanderbilt University;Virginia Polytechnic Institute and State University;Socio-Technical Interaction Research Lab ;Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University+Virginia Polytechnic Institute and State University;Virginia Tech;University of California, Davis",
        "aff_domain": "vt.edu;vt.edu;vt.edu;vt.edu;vanderbilt.edu;vt.edu;stirlab.org;vt.edu;vt.edu+vt.edu;vt.edu;ucdavis.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;PhD student;PhD student;Researcher;Associate Professor;Associate Professor+Assistant Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025llm,\ntitle={{LLM} Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models},\nauthor={Minqian Liu and Zhiyang Xu and Xinyi Zhang and Heajun An and Sarvech Qadir and Qi Zhang and Pamela J. Wisniewski and Jin-Hee Cho and Sang Won Lee and Ruoxi Jia and Lifu Huang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=TMB9SKqit9}\n}",
        "github": "",
        "project": "",
        "reviewers": "8jnH;DLYk;UYEj;jeKk",
        "site": "https://openreview.net/forum?id=TMB9SKqit9",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "5;3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.42640143271122083
    },
    {
        "id": "Tahpc3iAnO",
        "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length.\nRecent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. \nIn this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths.\nIn this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation.\nDPE significantly surpasses well-known baselines such as YaRN and Self-Extend.\nDPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2.\nIn addition to its impressive extrapolation capability,\n DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER.\nWhen compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K.",
        "keywords": "Long Context;Extrapolation;Training-Free Framework",
        "primary_area": "",
        "supplementary_material": "/attachment/0101b7de0eaaa5eccaca6af9016daed25bb78a2b.zip",
        "author": "Yi Lu;Wanxu Zhao;Xin Zhou;Chenxin An;Chenglong Wang;Shuo Li;Yuming Yang;Jun Zhao;Tao Ji;Tao Gui;Qi Zhang;Xuanjing Huang",
        "authorids": "~Yi_Lu7;~Wanxu_Zhao2;~Xin_Zhou6;~Chenxin_An1;~Chenglong_Wang6;~Shuo_Li12;~Yuming_Yang1;~Jun_Zhao5;~Tao_Ji1;~Tao_Gui1;~Qi_Zhang8;~Xuanjing_Huang1",
        "gender": ";M;;M;M;;M;M;M;M;M;F",
        "homepage": ";https://cs.fudan.edu.cn/;;https://chenxinan-fdu.github.io/;https://wangclnlp.github.io/wangchenglong.github.io/;;;;http://taoji.site/;;http://qizhang.info;https://xuanjing-huang.github.io/",
        "dblp": ";;05/3403-12;289/7002;;;222/1970;47/2026-19.html;;135/6973;52/323-1;05/6735-1",
        "google_scholar": ";;8AWfEb0AAAAJ;fY69CxIAAAAJ;_pu31FgAAAAJ;;T6_-tTgAAAAJ;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.com/citations?hl=zh-CN;;XfqR3yYAAAAJ;RGsMgZA4H78C",
        "orcid": ";;;;;;0009-0003-9518-7372;;0000-0002-9957-6661;;;0000-0001-9197-9426",
        "linkedin": ";;;;;;yuming-yang-649889171/;;;;;",
        "or_profile": "~Yi_Lu7;~Wanxu_Zhao2;~Xin_Zhou6;~Chenxin_An1;~Chenglong_Wang6;~Shuo_Li12;~Yuming_Yang1;~Jun_Zhao5;~Tao_Ji1;~Tao_Gui1;~Qi_Zhang8;~Xuanjing_Huang1",
        "aff": ";Fudan University;Fudan University;University of Hong Kong;Northeastern University;;Fudan University;Fudan University;Fudan University;Fudan University;Fudan University;Fudan University",
        "aff_domain": ";fudan.edu.cn;fudan.edu.cn;hku.hk;neu.edu.cn;;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "position": ";MS student;PhD student;PhD student;PhD student;;PhD student;PhD student;Postdoc;Assistant Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nlu2025effective,\ntitle={Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation},\nauthor={Yi Lu and Wanxu Zhao and Xin Zhou and Chenxin An and Chenglong Wang and Shuo Li and Yuming Yang and Jun Zhao and Tao Ji and Tao Gui and Qi Zhang and Xuanjing Huang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Tahpc3iAnO}\n}",
        "github": "",
        "project": "",
        "reviewers": "X83u;HAao;DyvB;Cdu5",
        "site": "https://openreview.net/forum?id=Tahpc3iAnO",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;2",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8703882797784891
    },
    {
        "id": "TiRiDMkTmG",
        "title": "Out-of-Distribution Detection using Synthetic Data Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin.",
        "keywords": "out-of-distribution detection;out-of-distribution generalization;synthetic data",
        "primary_area": "",
        "supplementary_material": "/attachment/47161fc624b3315b25b57a1de37783c4cbf8c017.zip",
        "author": "Momin Abbas;Muneeza Azmat;Raya Horesh;Mikhail Yurochkin",
        "authorids": "~Momin_Abbas1;~Muneeza_Azmat1;~Raya_Horesh1;~Mikhail_Yurochkin1",
        "gender": "M;F;F;M",
        "homepage": "https://mominabbas.github.io/;https://muneezaazmat.wordpress.com/;;https://moonfolk.github.io/",
        "dblp": "322/1186;;;191/6719",
        "google_scholar": "ASb7hfoAAAAJ;;jAmSI-cAAAAJ;QjBF9sUAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;mikhail-yurochkin-a45659114/",
        "or_profile": "~Momin_Abbas1;~Muneeza_Azmat1;~Raya_Horesh1;~Mikhail_Yurochkin1",
        "aff": "International Business Machines;International Business Machines;International Business Machines;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "ibm.com;ibm.com;ibm.com;mbzuai.ac.ae",
        "position": "Research Scientist;Researcher;Research Staff member;Principal Researcher",
        "bibtex": "@inproceedings{\nabbas2025outofdistribution,\ntitle={Out-of-Distribution Detection using Synthetic Data Generation},\nauthor={Momin Abbas and Muneeza Azmat and Raya Horesh and Mikhail Yurochkin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=TiRiDMkTmG}\n}",
        "github": "",
        "project": "",
        "reviewers": "Hpyt;tPhL;E79n",
        "site": "https://openreview.net/forum?id=TiRiDMkTmG",
        "pdf_size": 0,
        "rating": "4;6;7",
        "confidence": "3;4;2",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            1.247219128924647
        ],
        "confidence_avg": [
            3.0,
            0.816496580927726
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3273268353539886
    },
    {
        "id": "TiTk6VDz2H",
        "title": "The Blessing and Curse of Dimensionality in Safety Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The focus on safety alignment in large language models (LLMs) has increased significantly due to their widespread adoption across different domains. The scale of LLMs play a contributing role in their success, and the growth in parameter count follows larger hidden dimensions. In this paper, we hypothesize that while the increase in dimensions has been a key advantage, it may lead to emergent problems as well. These problems emerge as the linear structures in the activation space can be exploited, in the form of activation engineering, to circumvent its safety alignment. Through detailed visualizations of linear subspaces associated with different concepts, such as safety, across various model scales, we show that the curse of high-dimensional representations uniquely impacts LLMs. Further substantiating our claim, we demonstrate that projecting the representations of the model onto a lower dimensional subspace can preserve sufficient information for alignment while avoiding those linear structures. Empirical results confirm that such dimensional reduction significantly reduces susceptibility to jailbreaking through representation engineering. Building on our empirical validations, we provide theoretical insights into these linear jailbreaking methods relative to a model's hidden dimensions. Broadly speaking, our work posits that the high dimensions of a model's internal representations can be both a blessing and a curse in safety alignment.",
        "keywords": "safety alignment;large language models;jailbreak;activation engineering;linear separation hypothesis",
        "primary_area": "",
        "supplementary_material": "/attachment/72d52ac6f2cbf593a830158aaec0ea435826d90c.zip",
        "author": "Rachel S.Y. Teo;Laziz Abdullaev;Tan Minh Nguyen",
        "authorids": "~Rachel_S.Y._Teo1;~Laziz_Abdullaev1;~Tan_Minh_Nguyen1",
        "gender": ";M;M",
        "homepage": ";;https://tanmnguyen89.github.io/",
        "dblp": ";380/3830;255/4725",
        "google_scholar": ";IwZ0wCYAAAAJ;OizOh88AAAAJ",
        "orcid": ";;",
        "linkedin": ";laziz-abdullaev/;",
        "or_profile": "~Rachel_S.Y._Teo1;~Laziz_Abdullaev1;~Tan_Minh_Nguyen1",
        "aff": ";National University of Singapore;National University of Singapore",
        "aff_domain": ";u.nus.edu;nus.edu.sg",
        "position": ";PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nteo2025the,\ntitle={The Blessing and Curse of Dimensionality in Safety Alignment},\nauthor={Rachel S.Y. Teo and Laziz Abdullaev and Tan Minh Nguyen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=TiTk6VDz2H}\n}",
        "github": "",
        "project": "",
        "reviewers": "6omJ;NYCF;ii6K;bcLY",
        "site": "https://openreview.net/forum?id=TiTk6VDz2H",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            27,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "Tqj3fYqhwS",
        "title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Spoken language understanding (SLU) is indispensable for half of all living languages that lack a formal writing system, since these languages cannot pair automatic speech recognition (ASR) with language models to benefit from language technology. Even if low-resource languages possess a writing system, ASR for these languages remains unreliable due to limited bimodal speech and text training data. However, the evaluation of multilingual SLU remains limited to shallow tasks such as intent classification or language identification. To address this, we present Fleurs-SLU, a multilingual SLU benchmark that encompasses (i) 692 hours of speech for topical utterance classification in 102 languages and (ii) multiple-choice question answering through listening comprehension spanning 944 hours of speech across 92 languages. We extensively evaluate both end-to-end speech classification models and cascaded systems that combine speech-to-text transcription with subsequent classification by large language models on Fleurs-SLU. Our results show that cascaded systems exhibit greater robustness in multilingual SLU tasks, though speech encoders can achieve competitive performance in topical speech classification when appropriately pre-trained. We further find a strong correlation between robust multilingual ASR, effective speech-to-text translation, and strong multilingual SLU, highlighting the mutual benefits between acoustic and semantic speech representations.",
        "keywords": "spoken language understanding;multilingual benchmarks;multilingual evaluation",
        "primary_area": "",
        "supplementary_material": "/attachment/e9114f54a52339dacc131c52fec8c4f2fafe8cf5.zip",
        "author": "Fabian David Schmidt;Ivan Vuli\u0107;Goran Glava\u0161;David Ifeoluwa Adelani",
        "authorids": "~Fabian_David_Schmidt1;~Ivan_Vuli\u01071;~Goran_Glava\u01611;~David_Ifeoluwa_Adelani1",
        "gender": "M;M;M;M",
        "homepage": "https://fdschmidt93.github.io/;https://sites.google.com/site/ivanvulic/;https://sites.google.com/view/goranglavas;https://dadelani.github.io/",
        "dblp": "254/9181;77/9768;50/11059;230/6973",
        "google_scholar": "U_ukcNYAAAAJ;ZX8js60AAAAJ;Ym0myOwAAAAJ;https://scholar.google.ca/citations?user=W9sTkS0AAAAJ",
        "orcid": ";;;0000-0002-0193-2083",
        "linkedin": ";ivan-vuli%C4%87-286b4a81/;goran-glava\u0161-8484b420;david-adelani-7557b337/",
        "or_profile": "~Fabian_David_Schmidt1;~Ivan_Vuli\u01071;~Goran_Glava\u01611;~David_Ifeoluwa_Adelani1",
        "aff": "Bayerische Julius-Maximilians-Universit\u00e4t W\u00fcrzburg;Google DeepMind+University of Cambridge;Julius-Maximilians-Universit\u00e4t W\u00fcrzburg;McGill University",
        "aff_domain": "uni-wuerzburg.de;google.com+cam.ac.uk;uni-wuerzburg.de;mcgill.ca",
        "position": "PhD student;Researcher+Principal Research Associate;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nschmidt2025fleursslu,\ntitle={Fleurs-{SLU}: A Massively Multilingual Benchmark for Spoken Language Understanding},\nauthor={Fabian David Schmidt and Ivan Vuli{\\'c} and Goran Glava{\\v{s}} and David Ifeoluwa Adelani},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Tqj3fYqhwS}\n}",
        "github": "",
        "project": "",
        "reviewers": "VcwH;t3tg;TY1K;KQ4z",
        "site": "https://openreview.net/forum?id=Tqj3fYqhwS",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "TyXf9dwpZP",
        "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have shown impressive versatility as general purpose models.\nHowever, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass.\nIn domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation\u2013reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance.\nOur results show that AdaptiVocab reduces token usage by over 25% without compromising performance.",
        "keywords": "Domain Adaptation;Efficiency;Tokenization;Vocabulary Adaptation;Efficient LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Itay Nakash;Nitay Calderon;Eyal Ben-David;Elad Hoffer;Roi Reichart",
        "authorids": "~Itay_Nakash1;~Nitay_Calderon1;~Eyal_Ben-David1;~Elad_Hoffer1;~Roi_Reichart1",
        "gender": "Not Specified;M;M;M;M",
        "homepage": "https://itay-nakash.github.io/;https://nitaytech.github.io/;https://eyalbd2.github.io/;http://www.deeplearning.co.il;https://roireichart.com/",
        "dblp": "391/3360;305/0373;234/9089;156/0135;96/5429",
        "google_scholar": "0XIy_1AAAAAJ;_1qnAh4AAAAJ;ArqbkI4AAAAJ;https://scholar.google.co.il/citations?user=iEfTH7AAAAAJ;https://scholar.google.co.il/citations?user=xXJIsh4AAAAJ",
        "orcid": ";;;;",
        "linkedin": "itay-nakash/;nitay-calderon-b628a6185;eyal-bd/;;roi-reichart-ba2a8a7/",
        "or_profile": "~Itay_Nakash1;~Nitay_Calderon1;~Eyal_Ben-David1;~Elad_Hoffer1;~Roi_Reichart1",
        "aff": "International Business Machines;Technion - Israel Institute of Technology;Technion - Israel Institute of Technology, Technion;NVIDIA;Technion, Israel Institute of Technology",
        "aff_domain": "ibm.com;campus.technion.ac.il;technion.ac.il;nvidia.com;technion.ac.il",
        "position": "Researcher;PhD student;PhD student;Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nnakash2025adaptivocab,\ntitle={AdaptiVocab: Enhancing {LLM} Efficiency in Focused Domains through Lightweight Vocabulary Adaptation},\nauthor={Itay Nakash and Nitay Calderon and Eyal Ben-David and Elad Hoffer and Roi Reichart},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=TyXf9dwpZP}\n}",
        "github": "",
        "project": "",
        "reviewers": "6fu8;Rk2K;j7wR;CyPJ",
        "site": "https://openreview.net/forum?id=TyXf9dwpZP",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "U2ihVSREUb",
        "title": "Bayesian scaling laws for in-context learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates.\nPrior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions.\nIn this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL.\nIn experiments with GPT-2 models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities.\nTo illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset.\nIn all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.",
        "keywords": "in-context learning;bayesian inference;scaling laws",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aryaman Arora;Dan Jurafsky;Christopher Potts;Noah Goodman",
        "authorids": "~Aryaman_Arora1;~Dan_Jurafsky1;~Christopher_Potts1;~Noah_Goodman1",
        "gender": "M;M;M;",
        "homepage": "https://aryaman.io/;http://web.stanford.edu/~jurafsky/;http://web.stanford.edu/~cgpotts/;https://cocolab.stanford.edu/",
        "dblp": "263/6933;31/985;13/2617;96/1216",
        "google_scholar": "0-4GKw8AAAAJ;uZg9l58AAAAJ;3j08YoAAAAAJ;OUpIbcQAAAAJ",
        "orcid": "0000-0002-4977-8206;;0000-0002-7978-6055;",
        "linkedin": "aryaman-arora2020/;;;",
        "or_profile": "~Aryaman_Arora1;~Dan_Jurafsky1;~Christopher_Potts1;~Noah_Goodman1",
        "aff": "Stanford University;Stanford University;Stanford University;Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "position": "PhD student;Full Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\narora2025bayesian,\ntitle={Bayesian scaling laws for in-context learning},\nauthor={Aryaman Arora and Dan Jurafsky and Christopher Potts and Noah Goodman},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=U2ihVSREUb}\n}",
        "github": "",
        "project": "",
        "reviewers": "hrBS;yniy;cur5",
        "site": "https://openreview.net/forum?id=U2ihVSREUb",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "U6C7odo5SX",
        "title": "Rerouting LLM Routers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLM routers balance quality and cost of responding to queries by routing them to a cheaper or more expensive LLM depending on the query's estimated complexity.  Routers are a type of what we call ``LLM control planes,'' i.e., systems that orchestrate multiple LLMs.\n\nIn this paper, we investigate adversarial robustness of LLM control planes using routers as a concrete example.  We formulate LLM control-plane integrity as a distinct problem in AI safety, where the adversary's goal is to control the order or selection of LLMs employed to process users' queries.  We then demonstrate that it is possible to generate query-independent ``gadget'' strings that, when added to any query, cause routers to send this query to a strong LLM.  In contrast to conventional adversarial inputs, gadgets change the control flow but preserve or even improve the quality of outputs generated in response to adversarially modified queries.\n\nWe show that this attack is successful both in white-box and black-box settings against several open-source and commercial routers.  We also show that perplexity-based defenses fail, and investigate alternatives.",
        "keywords": "LLMs;Routers;Adversarial Machine Learning;ML Security",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Avital Shafran;Roei Schuster;Tom Ristenpart;Vitaly Shmatikov",
        "authorids": "~Avital_Shafran1;~Roei_Schuster1;~Tom_Ristenpart1;~Vitaly_Shmatikov1",
        "gender": ";M;M;",
        "homepage": ";http://www.roeis.com;https://rist.tech.cornell.edu;",
        "dblp": "254/2733;180/8190.html;;",
        "google_scholar": "h58d7XQAAAAJ;https://scholar.google.ca/citations?user=Bgoc0bAAAAAJ;MGVrVSIAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Avital_Shafran1;~Roei_Schuster1;~Tom_Ristenpart1;~Vitaly_Shmatikov1",
        "aff": "Hebrew University of Jerusalem;Wild Moose;Cornell University;",
        "aff_domain": "huji.ac.il;wildmoose.ai;cornell.edu;",
        "position": "PhD student;Researcher;Full Professor;",
        "bibtex": "@inproceedings{\nshafran2025rerouting,\ntitle={Rerouting {LLM} Routers},\nauthor={Avital Shafran and Roei Schuster and Tom Ristenpart and Vitaly Shmatikov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=U6C7odo5SX}\n}",
        "github": "",
        "project": "",
        "reviewers": "2R6h;Zdu3;52ek;ZVFv;WMXC;xwTG",
        "site": "https://openreview.net/forum?id=U6C7odo5SX",
        "pdf_size": 0,
        "rating": "4;5;6;6;7;8",
        "confidence": "4;5;3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.2909944487358056
        ],
        "confidence_avg": [
            3.8333333333333335,
            0.6871842709362768
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.18786728732554484
    },
    {
        "id": "ULYqB2JORB",
        "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Accurately forecasting the performance of Large Language Models (LLMs) before extensive fine-tuning or merging can substantially reduce both computational expense and development time. Although prior approaches like scaling laws account for global factors such as parameter size or training tokens, they often overlook explicit lineage relationships\u2014i.e., which models are derived or merged from which parents. In this work, we propose a novel Lineage-Regularized Matrix Factorization (LRMF) framework that encodes ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging multi-hop parent--child connections, LRMF consistently outperforms conventional matrix factorization and collaborative filtering methods in both instance-level and benchmark-level performance prediction. Our large-scale study includes 2,934 publicly available Hugging Face models and 21,000+ instances across 6 major benchmarks, \nshowing that the introduction of lineage constraints yields up to 0.15\u20130.30 higher correlation coefficients with actual performance compared to baseline methods.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate estimates for newly derived or merged models even with minimal data. This lineage-guided strategy thus offers a resource-efficient way to inform hyperparameter tuning, data selection, and model combination in modern LLM development.",
        "keywords": "LLM;Performance Estimation;Matrix Factorization;Neural Collaborative Filtering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Takuya Tamura;Taro Yano;Masafumi Enomoto;Masafumi Oyamada",
        "authorids": "~Takuya_Tamura1;~Taro_Yano1;~Masafumi_Enomoto1;~Masafumi_Oyamada1",
        "gender": "M;M;M;M",
        "homepage": ";;;https://mooz.github.io/",
        "dblp": ";382/0105;292/1437;28/11004",
        "google_scholar": "https://scholar.google.com/citations?hl=ja;https://scholar.google.es/citations?hl=ja;https://scholar.google.co.jp/citations?user=FxLZ0rUAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0002-4045-7350",
        "linkedin": ";;;",
        "or_profile": "~Takuya_Tamura1;~Taro_Yano1;~Masafumi_Enomoto1;~Masafumi_Oyamada1",
        "aff": "NEC;NEC;NEC;NEC",
        "aff_domain": "nec.com;nec.com;nec.com;nec.com",
        "position": "Researcher;Researcher;Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\ntamura2025can,\ntitle={Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance},\nauthor={Takuya Tamura and Taro Yano and Masafumi Enomoto and Masafumi Oyamada},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ULYqB2JORB}\n}",
        "github": "",
        "project": "",
        "reviewers": "WqPU;adW9;zxPT;vHug",
        "site": "https://openreview.net/forum?id=ULYqB2JORB",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "4;3;2;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Uic3ojVhXh",
        "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity.\nWe introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data.\nTo preserve the ability to recall famous quotations, we additionally develop a variant of ParaPO that uses system prompts to control whether LMs should reduce regurgitation. \nOn Llama3.1-8B, ParaPO consistently reduces regurgitation across all datasets we evaluated (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9).\nOn the instruction-tuned model Tulu3-8B, ParaPO with system prompts achieve a 27.5\\% reduction in regurgitation (from 8.7 to 6.3) in creative writing, while preserving similar accuracy in requesting famous quotations. In contrast, the base Tulu model with inference-time system prompts achieves only a 3.5\\% reduction (from 8.7 to 8.4).",
        "keywords": "security and privacy;fine-tuning;ethical considerations in NLP applications",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tong Chen;Faeze Brahman;Jiacheng Liu;Niloofar Mireshghallah;Weijia Shi;Pang Wei Koh;Luke Zettlemoyer;Hannaneh Hajishirzi",
        "authorids": "~Tong_Chen3;~Faeze_Brahman1;~Jiacheng_Liu2;~Niloofar_Mireshghallah1;~Weijia_Shi1;~Pang_Wei_Koh1;~Luke_Zettlemoyer1;~Hannaneh_Hajishirzi1",
        "gender": "M;F;M;;;M;M;F",
        "homepage": ";https://fabrahman.github.io;https://github.com/liujch1998;;https://weijiashi.notion.site/;http://cs.stanford.edu/~pangwei;https://www.cs.washington.edu/people/faculty/lsz/;https://homes.cs.washington.edu/~hannaneh/",
        "dblp": "22/1512;276/6005;289/6273;;132/80601;10/10453;21/6793;52/1296",
        "google_scholar": "fOcXofAAAAAJ;viCG2ikAAAAJ;GJfoBZAAAAAJ;;https://scholar.google.com/citations?hl=en;Nn990CkAAAAJ;https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ;LOV6_WIAAAAJ",
        "orcid": ";;0000-0003-3308-2869;;0000-3200-0000-0011;;;",
        "linkedin": ";;liujch1998/;;weijia-shi-773768112;;luke-zettlemoyer-a0109b226/;",
        "or_profile": "~Tong_Chen3;~Faeze_Brahman1;~Jiacheng_Liu2;~Niloofar_Mireshghallah1;~Weijia_Shi1;~Pang_Wei_Koh1;~Luke_Zettlemoyer1;~Hannaneh_Hajishirzi1",
        "aff": "University of Washington, Seattle;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence+Paul G. Allen School of Computer Science and Engineering, University of Washington;;University of Washington, Seattle;Allen Institute for Artificial Intelligence+University of Washington;University of Washington+Meta Facebook+Meta;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "cs.washington.edu;allenai.org;allenai.org+cs.washington.edu;;uw.edu;allenai.org+cs.washington.edu;cs.washington.edu+fb.com+meta.com;allenai.org+uw.edu",
        "position": "PhD student;Researcher;Intern+PhD student;;PhD student;Visiting Research Scientist+Assistant Professor;Full Professor+Researcher+Researcher;senior director+Associate Professor",
        "bibtex": "@inproceedings{\nchen2025parapo,\ntitle={Para{PO}: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data},\nauthor={Tong Chen and Faeze Brahman and Jiacheng Liu and Niloofar Mireshghallah and Weijia Shi and Pang Wei Koh and Luke Zettlemoyer and Hannaneh Hajishirzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Uic3ojVhXh}\n}",
        "github": "",
        "project": "",
        "reviewers": "nEDD;6ZUs;Jibe",
        "site": "https://openreview.net/forum?id=Uic3ojVhXh",
        "pdf_size": 0,
        "rating": "5;6;6",
        "confidence": "3;3;4",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "UmUXPXHtdl",
        "title": "Scaling Laws of Synthetic Data for Language Model",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks, driven by high-quality web data used in pre-training. However, recent studies indicate web data is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our experiments with SynthLLM on math domain include: (1) SynthLLM generates synthetic data that reliably adheres to rectified scaling law across various model sizes; (2) Performance gains gradually diminish near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to raw pre-training data, offering a viable path toward continued improvement in model performance.",
        "keywords": "Synthetic Data; Scaling Laws",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zeyu Qin;Qingxiu Dong;Xingxing Zhang;Li Dong;Xiaolong Huang;Ziyi Yang;MAHMOUD KHADEMI;Dongdong Zhang;Hany Hassan Awadalla;Yi R. Fung;Weizhu Chen;Minhao Cheng;Furu Wei",
        "authorids": "~Zeyu_Qin1;~Qingxiu_Dong1;~Xingxing_Zhang1;~Li_Dong1;~Xiaolong_Huang1;~Ziyi_Yang1;~MAHMOUD_KHADEMI2;~Dongdong_Zhang4;~Hany_Hassan_Awadalla1;~Yi_R._Fung1;~Weizhu_Chen1;~Minhao_Cheng1;~Furu_Wei1",
        "gender": "M;F;M;M;M;M;M;M;;;M;M;M",
        "homepage": "https://alan-qin.github.io/;https://dqxiu.github.io/;https://xingxingzhang.github.io/;http://dong.li;;;https://www.microsoft.com/en-us/research/people/mkhademi/;https://www.microsoft.com/en-us/research/people/dozhang/;;;https://www.microsoft.com/en-us/research/people/wzchen/;https://cmhcbb.github.io/;https://www.microsoft.com/en-us/research/people/fuwei/",
        "dblp": "271/5778;284/0673;59/9985-2.html;85/5090-4;;;;02/621-1.html;;;79/2536;174/1717;72/5870",
        "google_scholar": "3LXI4-MAAAAJ;ibcR7VkAAAAJ;5yX53usAAAAJ;wEfQgPgAAAAJ;;JkyLIM0AAAAJ;x7Ddt3oAAAAJ;w2qu71oAAAAJ;;;LG_E-4EAAAAJ;_LkC1yoAAAAJ;G-V1VpwAAAAJ",
        "orcid": "0000-0003-1733-7892;;;;;;;;;;;0000-0003-3965-4215;",
        "linkedin": "zeyu-qin-546398179/;qingxiu-dong-a3758a199/;;;xiaolong-huang-446182b4/;ziyi-yang;;;;;;;",
        "or_profile": "~Zeyu_Qin1;~Qingxiu_Dong1;~Xingxing_Zhang1;~Li_Dong1;~Xiaolong_Huang1;~Ziyi_Yang1;~MAHMOUD_KHADEMI2;~Dongdong_Zhang4;~Hany_Hassan_Awadalla1;~Yi_R._Fung1;~Weizhu_Chen1;~Minhao_Cheng1;~Furu_Wei1",
        "aff": "Hong Kong University of Science and Technology;Peking University;Microsoft Research Asia;Microsoft Research;Microsoft;Databricks;Microsoft;Microsoft Research Asia;;;Microsoft GenAI;Pennsylvania State University;Microsoft Research",
        "aff_domain": "ust.hk;pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;databricks.com;microsoft.com;microsoft.com;;;microsoft.com;psu.edu;microsoft.com",
        "position": "PhD student;PhD student;Researcher;Principal Researcher;Researcher;Principal Researcher;Researcher;Researcher;;;Vice President;Assistant Professor;Distinguished Scientist",
        "bibtex": "@inproceedings{\nqin2025scaling,\ntitle={Scaling Laws of Synthetic Data for Language Model},\nauthor={Zeyu Qin and Qingxiu Dong and Xingxing Zhang and Li Dong and Xiaolong Huang and Ziyi Yang and MAHMOUD KHADEMI and Dongdong Zhang and Hany Hassan Awadalla and Yi R. Fung and Weizhu Chen and Minhao Cheng and Furu Wei},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=UmUXPXHtdl}\n}",
        "github": "",
        "project": "",
        "reviewers": "Pj9b;xJ12;oLip;nd1f",
        "site": "https://openreview.net/forum?id=UmUXPXHtdl",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            27,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "VGw1viYliK",
        "title": "Steering Large Language Model Activations in Sparse Spaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A key challenge in AI alignment is guiding large language models (LLMs) to follow desired behaviors at test time. Activation steering, which modifies internal model activations during inference, offers a promising solution. However, prior work in dense activation spaces struggles with $\\textit{superposition}$, where multiple features become entangled, limiting interpretability and precise control. In contrast, sparse representations offer an untapped opportunity for more interpretable behavior modulation. In this work, we introduce $\\textit{Sparse Activation Steering}$ (SAS), a novel method for steering LLM behavior in $\\textit{sparse spaces}$. By isolating behavior-specific features (i.e., latent dimensions) through a contrastive prompt-pairing approach, we define a set of features that can selectively reinforce or suppress behaviors. Experiments on Gemma 2 LLMs show that SAS vectors enable steering on par with its dense counterpart while offering interpretability advantages such as easier compositionality of features in these spaces. Furthermore, our scaling studies on sparse latents reveal a trend toward greater sparsity in SAS vectors, approaching ideal $\\textit{monosemanticity}$.",
        "keywords": "AI alignment;Activation steering;Sparse representations;Sparse autoencoders (SAEs);Large language models (LLMs);Interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Reza Bayat;Ali Rahimi-Kalahroudi;Mohammad Pezeshki;Sarath Chandar;Pascal Vincent",
        "authorids": "~Reza_Bayat1;~Ali_Rahimi-Kalahroudi1;~Mohammad_Pezeshki1;~Sarath_Chandar1;~Pascal_Vincent1",
        "gender": "M;M;M;M;M",
        "homepage": ";https://alirahkay.github.io;https://mohammadpz.github.io/;http://sarathchandar.in/;http://www.iro.umontreal.ca/~vincentp",
        "dblp": ";319/3332;139/0888;45/8542;43/861",
        "google_scholar": ";;HT85tXsAAAAJ;https://scholar.google.co.in/citations?user=yxWtZLAAAAAJ;WBCKQMsAAAAJ",
        "orcid": ";;;;",
        "linkedin": "reza-bayat-186282138/;;;;",
        "or_profile": "~Reza_Bayat1;~Ali_Rahimi-Kalahroudi1;~Mohammad_Pezeshki1;~Sarath_Chandar1;~Pascal_Vincent1",
        "aff": "Mila - Quebec Artificial Intelligence Institute;Mila - Quebec Artificial Intelligence Institute;Meta Facebook;\u00c9cole Polytechnique de Montr\u00e9al;University of Montreal+Facebook A.I. Research",
        "aff_domain": "mila.quebec;mila.quebec;meta.com;polymtl.ca;umontreal.ca+fb.com",
        "position": "Researcher;Researcher;Researcher;Associate Professor;Adjunct Professor+Research Scientist",
        "bibtex": "@inproceedings{\nbayat2025steering,\ntitle={Steering Large Language Model Activations in Sparse Spaces},\nauthor={Reza Bayat and Ali Rahimi-Kalahroudi and Mohammad Pezeshki and Sarath Chandar and Pascal Vincent},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VGw1viYliK}\n}",
        "github": "",
        "project": "",
        "reviewers": "3o12;siLA;QAAq;9Lrh",
        "site": "https://openreview.net/forum?id=VGw1viYliK",
        "pdf_size": 0,
        "rating": "4;6;7;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.479019945774904
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.29277002188455997
    },
    {
        "id": "VSwRuGtB5n",
        "title": "MapIQ: Evaluating Multimodal Large Language Models for Map Question Answering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types\u2014choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.",
        "keywords": "Large Language Models;Visual Question Answering;Maps;Geospatial Analysis;Visual Analytics;Benchmark Dataset",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Varun Srivastava;Fan Lei;Srija Mukhopadhyay;Vivek Gupta;Ross Maciejewski",
        "authorids": "~Varun_Srivastava3;~Fan_Lei1;~Srija_Mukhopadhyay1;~Vivek_Gupta2;~Ross_Maciejewski1",
        "gender": "M;M;F;M;M",
        "homepage": ";https://sanmisanfan.github.io/;;https://vgupta123.github.io;http://rmaciejewski.faculty.asu.edu/",
        "dblp": ";;371/0904;71/5332-1;81/5349.html",
        "google_scholar": ";FE6I5jIAAAAJ;;https://scholar.google.co.in/citations?user=Bs5H0S4AAAAJ;https://scholar.google.com.tw/citations?user=nChgOjEAAAAJ",
        "orcid": "0009-0002-7736-3574;;;;",
        "linkedin": ";;ajirs/;keviv9/;",
        "or_profile": "~Varun_Srivastava3;~Fan_Lei1;~Srija_Mukhopadhyay1;~Vivek_Gupta2;~Ross_Maciejewski1",
        "aff": "Arizona State University;Arizona State University;International Institute of Information Technology Hyderabad+International Institute of Information Technology Hyderabad;Arizona State University;Arizona State University",
        "aff_domain": "asu.edu;asu.edu;iiit.ac.in+iiit.ac.in;asu.edu;asu.edu",
        "position": "PhD student;PhD student;MS student+Undergrad student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nsrivastava2025mapiq,\ntitle={Map{IQ}: Evaluating Multimodal Large Language Models for Map Question Answering},\nauthor={Varun Srivastava and Fan Lei and Srija Mukhopadhyay and Vivek Gupta and Ross Maciejewski},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VSwRuGtB5n}\n}",
        "github": "",
        "project": "",
        "reviewers": "inaB;5c5R;Kk1H;2KAj",
        "site": "https://openreview.net/forum?id=VSwRuGtB5n",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "VYdbeSoXWD",
        "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have recently demonstrated impressive action sequence prediction capabilities but often struggle with dynamic, long-horizon tasks such as real-time strategic games. In a game such as StarCraft II (SC2), agents need to manage resource constraints and adapt to evolving battlefield situations in a partially observable environment. This often overwhelms exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a meta-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coherent, structured multistep action sequences. The SP then orchestrates these proposals into a single, environmentally adaptive plan that ensures local decisions aligning with long-term strategies. We call this HIMA (Hierarchical Imitation Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that encompasses all race match combinations in SC2. Our empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with meta-level orchestration to develop more robust, general-purpose AI agents.",
        "keywords": "multi agent;real-time simulation;strategic reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daechul Ahn;San Kim;Jonghyun Choi",
        "authorids": "~Daechul_Ahn4;~San_Kim2;~Jonghyun_Choi1",
        "gender": "M;M;M",
        "homepage": "https://dcahn12.github.io;https://mounkim.github.io/;https://ppolon.github.io/",
        "dblp": "303/4574;;21/11103",
        "google_scholar": "https://scholar.google.com/citations?hl=ko;;uiGWnm4AAAAJ",
        "orcid": "0000-0002-8689-3107;;0000-0002-7934-8434",
        "linkedin": "https://linkedin.com/in/daechul-ahn-a1b958145;san-kim-1b4495316?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;jonghyun-choi-459bb615/",
        "or_profile": "~Daechul_Ahn4;~San_Kim2;~Jonghyun_Choi1",
        "aff": "Seoul National University;Seoul National University;Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "position": "PhD student;MS student;Associate Professor",
        "bibtex": "@inproceedings{\nahn2025society,\ntitle={Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning},\nauthor={Daechul Ahn and San Kim and Jonghyun Choi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VYdbeSoXWD}\n}",
        "github": "",
        "project": "",
        "reviewers": "RWbE;W96F;crGH;bnMe",
        "site": "https://openreview.net/forum?id=VYdbeSoXWD",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Vi5cIfIslX",
        "title": "Sample Efficient Preference Alignment in LLMs via Active Exploration",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Preference-based feedback is important for many applications in machine learning where evaluation of a reward function is not feasible. Notable recent examples arise in preference alignment for large language models, including in reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). For many applications of preference alignment, the cost of acquiring human feedback can be substantial. In this work, we take advantage of the fact that one can often choose contexts at which to obtain human feedback to most efficiently identify a good policy, and formalize the setting as an \\emph{active contextual dueling bandit} problem. We propose an active exploration algorithm to efficiently select the data and provide theoretical proof that it has a polynomial worst-case regret bound. We extend the setting and methodology for practical use in preference alignment of large language models. We provide two extensions, an online and an offline approach. Our method outperforms the baselines with limited samples of human preferences on several language models and four real-world datasets including two new datasets that we contribute to the literature.",
        "keywords": "Sample Efficient;DPO;RLHF;Alignment;Active learning;LLMs",
        "primary_area": "",
        "supplementary_material": "/attachment/76e6987c2fa36c3a4f40a8581f3f410848ba4ab8.zip",
        "author": "Viraj Mehta;Syrine Belakaria;Vikramjeet Das;Ojash Neopane;Yijia Dai;Ilija Bogunovic;Barbara E Engelhardt;Stefano Ermon;Jeff Schneider;Willie Neiswanger",
        "authorids": "~Viraj_Mehta1;~Syrine_Belakaria1;~Vikramjeet_Das1;~Ojash_Neopane1;~Yijia_Dai1;~Ilija_Bogunovic2;~Barbara_Engelhardt1;~Stefano_Ermon1;~Jeff_Schneider1;~Willie_Neiswanger2",
        "gender": "M;F;M;M;F;;F;M;;M",
        "homepage": "http://virajm.com;https://www.sbelakaria.com/;;https://oneopane.github.io/;https://daiyijia02.github.io;;https://beehive.stanford.edu;http://cs.stanford.edu/~ermon/;https://www.cs.cmu.edu/~schneide;https://willieneis.github.io/",
        "dblp": "https://dblp.org/pers/m/Mehta:Viraj.html;200/8277;323/0004;176/5399.html;;;27/2355;47/8135;38/247;120/7593.html",
        "google_scholar": "4pHjHBkAAAAJ;9NNlVb8AAAAJ;https://scholar.google.com/citations?hl=en;lmAQ1l8AAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=VEGtG7YAAAAJ;;3bSbb20AAAAJ;QwKHApEAAAAJ",
        "orcid": "0000-0002-2021-9718;;0000-0001-8292-6752;;;;;;0000-0002-5080-9073;",
        "linkedin": "virajrmehta/;;linkedin.com/in/vikramjeetd;;yijia-dai/;;;;jeff-schneider-1593b322/;",
        "or_profile": "~Viraj_Mehta1;~Syrine_Belakaria1;~Vikramjeet_Das1;~Ojash_Neopane1;~Yijia_Dai1;~Ilija_Bogunovic2;~Barbara_Engelhardt1;~Stefano_Ermon1;~Jeff_Schneider1;~Willie_Neiswanger2",
        "aff": "Carnegie Mellon University;Stanford University;Carnegie Mellon University;Carnegie Mellon University;Department of Computer Science, Cornell University;;Stanford University;Stanford University;Carnegie Mellon University;University of Southern California",
        "aff_domain": "cmu.edu;stanford.edu;andrew.cmu.edu;cmu.edu;cs.cornell.edu;;stanford.edu;stanford.edu;cs.cmu.edu;usc.edu",
        "position": "PhD student;Postdoc;MS student;PhD student;PhD student;;Full Professor;Associate Professor;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nmehta2025sample,\ntitle={Sample Efficient Preference Alignment in {LLM}s via Active Exploration},\nauthor={Viraj Mehta and Syrine Belakaria and Vikramjeet Das and Ojash Neopane and Yijia Dai and Ilija Bogunovic and Barbara E Engelhardt and Stefano Ermon and Jeff Schneider and Willie Neiswanger},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Vi5cIfIslX}\n}",
        "github": "",
        "project": "",
        "reviewers": "up2F;JaHM;1Thk",
        "site": "https://openreview.net/forum?id=Vi5cIfIslX",
        "pdf_size": 0,
        "rating": "5;6;8",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            1.247219128924647
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Vnw9c1YLhV",
        "title": "A Critical Look At Tokenwise Reward-Guided Text Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) can be improved by aligning with human preferences through fine-tuning -- the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their ability to bypass LLM fine-tuning, prediction-time tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during decoding in a bid to steer the generation towards sequences with high rewards. However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this, we propose to train a Bradley-Terry reward model on partial sequences explicitly, and autoregressively sample from the implied tokenwise policy during decoding. We study the properties of this reward model and the resulting policy: we show that this policy is proportional to the ratio of two distinct RLHF policies. Our simple approach outperforms previous RGTG methods and performs similarly to strong offline baselines without large-scale LLM fine-tuning.",
        "keywords": "LLM;RLHF;Alignment;Model Efficiency;Reward Models;Sampling;Test-time",
        "primary_area": "",
        "supplementary_material": "/attachment/318baf88ec5cccfdcfb67114ab9ef513205be824.zip",
        "author": "Ahmad Rashid;Ruotian Wu;Julia Grosse;Agustinus Kristiadi;Pascal Poupart",
        "authorids": "~Ahmad_Rashid1;~Ruotian_Wu1;~Julia_Grosse1;~Agustinus_Kristiadi1;~Pascal_Poupart2",
        "gender": ";M;;;M",
        "homepage": "https://ahmadrash.github.io/;https://wrttz.github.io/;;https://agustinus.kristia.de;https://cs.uwaterloo.ca/~ppoupart",
        "dblp": "239/5145;;;215/3954;26/2122",
        "google_scholar": "https://scholar.google.ca/citations?user=YPbaQkQAAAAJ;https://scholar.google.ca/citations?user=DOJe2iQAAAAJ;;_1qe2mYAAAAJ;https://scholar.google.ca/citations?user=KhAJWroAAAAJ",
        "orcid": ";;;0000-0003-1615-1121;",
        "linkedin": ";ruotian-wu-8ba1b5248/;;agustinus-kristiadi/;",
        "or_profile": "~Ahmad_Rashid1;~Ruotian_Wu1;~Julia_Grosse1;~Agustinus_Kristiadi1;~Pascal_Poupart2",
        "aff": "University of Waterloo;University of Waterloo;;University of Western Ontario+Vector Institute;University of Waterloo",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;;uwo.ca+vectorinstitute.ai;uwaterloo.ca",
        "position": "PhD student;MS student;;Assistant Professor+Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nrashid2025a,\ntitle={A Critical Look At Tokenwise Reward-Guided Text Generation},\nauthor={Ahmad Rashid and Ruotian Wu and Julia Grosse and Agustinus Kristiadi and Pascal Poupart},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Vnw9c1YLhV}\n}",
        "github": "",
        "project": "",
        "reviewers": "Z2q2;2dhF;rFy9;JW6d",
        "site": "https://openreview.net/forum?id=Vnw9c1YLhV",
        "pdf_size": 0,
        "rating": "6;6;6;8",
        "confidence": "4;5;3;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5222329678670935
    },
    {
        "id": "VrEPiN5WhM",
        "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers \u2013 short, irrelevant text that, when appended to math problems, systematically misleads models to output incorrect answers without altering the problem\u2019s semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a faster, less expensive proxy target model (DeepSeek V3) and successfully transferring them to slower, expensive, and more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distill-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending Interesting fact: cats sleep most of their lives to any math problem leads to more than doubling the chances of a model getting the answer wrong. Furthermore, we demonstrate the widespread transferability of these triggers to other model families, including large reasoning models from Qwen QwQ, Qwen 3, and Phi-4 as well as instruction-tuned models from Llama-3.1 and Mistral. These tests showed that the models were affected by error rates that increased by up to 500% for reasoning models and by 700% for instruction-tuned models. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. CatAttack triggers dataset with model responses is available at https://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers",
        "keywords": "Adversarial attacks;Query-agnostic adversarial triggers;Reasoning Models;Automatic Iterative Attack;Math-based triggers;Security;Redteaming",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Meghana Arakkal Rajeev;Rajkumar Ramamurthy;Prapti Trivedi;Vikas Yadav;Oluwanifemi Bamgbose;Sathwik Tejaswi Madhusudhan;James Zou;Nazneen Rajani",
        "authorids": "~Meghana_Arakkal_Rajeev1;~Rajkumar_Ramamurthy1;~Prapti_Trivedi2;~Vikas_Yadav2;~Oluwanifemi_Bamgbose1;~Sathwik_Tejaswi_Madhusudhan2;~James_Zou1;~Nazneen_Rajani1",
        "gender": "F;;F;M;M;M;;",
        "homepage": ";;;;;https://www.linkedin.com/in/sat95;;",
        "dblp": ";199/2181;;;;;;",
        "google_scholar": ";vVzcztcAAAAJ;-5C9NjIAAAAJ;FyS1eswAAAAJ;;wwVSfRsAAAAJ;23ZXZvEAAAAJ;",
        "orcid": ";;0009-0004-7573-206X;;;;;",
        "linkedin": "meghana-a-rajeev/;;prapti-trivedi/;vyf95/;nifemi-bam;sat95;;",
        "or_profile": "~Meghana_Arakkal_Rajeev1;~Rajkumar_Ramamurthy1;~Prapti_Trivedi2;~Vikas_Yadav2;~Oluwanifemi_Bamgbose1;~Sathwik_Tejaswi_Madhusudhan2;~James_Zou1;~Nazneen_Rajani1",
        "aff": "Collinear AI;Fraunhofer Institute IAIS, Fraunhofer IAIS;Collinear AI;ServiceNow Inc;ServiceNow Inc;ServiceNow Inc;Stanford University;",
        "aff_domain": "collinear.ai;iais.fraunhofer.de;other.su;servicenow.com;servicenow.com;servicenow.com;stanford.edu;",
        "position": "Researcher;Researcher;Researcher;Researcher;Applied Research Scientist;Researcher;Assistant Professor;",
        "bibtex": "@inproceedings{\nrajeev2025cats,\ntitle={Cats Confuse Reasoning {LLM}: Query Agnostic Adversarial Triggers for Reasoning Models},\nauthor={Meghana Arakkal Rajeev and Rajkumar Ramamurthy and Prapti Trivedi and Vikas Yadav and Oluwanifemi Bamgbose and Sathwik Tejaswi Madhusudhan and James Zou and Nazneen Rajani},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VrEPiN5WhM}\n}",
        "github": "",
        "project": "",
        "reviewers": "nuP7;tXkG;fHU7;fivw",
        "site": "https://openreview.net/forum?id=VrEPiN5WhM",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "5;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8703882797784891
    },
    {
        "id": "VvSWiNIuPL",
        "title": "On Mechanistic Circuits for Extractive Question-Answering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent studies have extracted circuits from the computational graphs of language models for simple language tasks such as entity tracking or indirect object identification. In our paper, we scale up circuit extraction to a real-world language modeling task: context-augmented language modeling for question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution. We extract circuits as a function of internal model components (e.g., attention heads, attention layers, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the language model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass! Using this insight, we then introduce AttnAttrib, a fast data attribution algorithm. Through a range of empirical experiments across different extractive QA benchmarks, we show that performing data attribution with AttnAttrib obtains state-of-the-art attribution results across different language models. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by (i) using the attribution from our extracted attention head as an additional signal during the forward pass and (ii) scaling the output of a small set of attention heads. Beyond mechanistic understanding, our paper provides tangible applications of mechanistic circuits in the form of reliable data attribution and model steering.",
        "keywords": "mechanistic circuits;interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Samyadeep Basu;Vlad I Morariu;Ryan A. Rossi;Nanxuan Zhao;Zichao Wang;Soheil Feizi;Varun Manjunatha",
        "authorids": "~Samyadeep_Basu1;~Vlad_I_Morariu1;~Ryan_A._Rossi2;~Nanxuan_Zhao1;~Zichao_Wang1;~Soheil_Feizi2;~Varun_Manjunatha1",
        "gender": "M;M;;F;Not Specified;M;M",
        "homepage": "https://samyadeepbasu.github.io/;https://research.adobe.com/person/vlad-morariu/;;http://nxzhao.com;https://zichaow.github.io;https://www.cs.umd.edu/~sfeizi/;https://research.adobe.com/person/varun-manjunatha/",
        "dblp": "250/9138;27/6671;;224/0709;188/0340.html;57/2132;https://dblp.org/pers/m/Manjunatha:Varun.html",
        "google_scholar": "6aRwDecAAAAJ;oyWpVa8AAAAJ;;;IbCALKcAAAAJ;lptAmrMAAAAJ;nO-We6sAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Samyadeep_Basu1;~Vlad_I_Morariu1;~Ryan_A._Rossi2;~Nanxuan_Zhao1;~Zichao_Wang1;~Soheil_Feizi2;~Varun_Manjunatha1",
        "aff": "University of Maryland, College Park;Adobe;;Adobe Research;Adobe Research;University of Maryland, College Park;Adobe Systems",
        "aff_domain": "umd.edu;adobe.com;;adobe.com;adobe.com;umd.edu;adobe.com",
        "position": "PhD student;Senior Research Scientist;;Researcher;Research Scientist;Associate Professor;Research Scientist",
        "bibtex": "@inproceedings{\nbasu2025on,\ntitle={On Mechanistic Circuits for Extractive Question-Answering},\nauthor={Samyadeep Basu and Vlad I Morariu and Ryan A. Rossi and Nanxuan Zhao and Zichao Wang and Soheil Feizi and Varun Manjunatha},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VvSWiNIuPL}\n}",
        "github": "",
        "project": "",
        "reviewers": "z9as;F4x8;mTR6",
        "site": "https://openreview.net/forum?id=VvSWiNIuPL",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9999999999999997
    },
    {
        "id": "VzXpFjKgJg",
        "title": "Model-Agnostic Policy Explanations with Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based *only* on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior. Importantly, we show that participants are unable to detect hallucinations in explanations, underscoring the need for explainability methods that minimize hallucinations by design.",
        "keywords": "Explainability;Model-Agnostic Explanations;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhang Xi-Jia;Yue Guo;Shufei Chen;Simon Stepputtis;Matthew Craig Gombolay;Katia P. Sycara;Joseph Campbell",
        "authorids": "~Zhang_Xi-Jia1;~Yue_Guo7;~Shufei_Chen1;~Simon_Stepputtis1;~Matthew_Craig_Gombolay1;~Katia_P._Sycara1;~Joseph_Campbell1",
        "gender": ";F;F;;;F;",
        "homepage": ";http://www.sophieyueguo.com;;https://simonstepputtis.com/;;;",
        "dblp": ";73/735-3.html;;192/7092;;s/KatiaPSycara;179/2732",
        "google_scholar": ";1OXzO1gAAAAJ;;WUQgzsAAAAAJ;;VWv6a9kAAAAJ;1NmM6OUAAAAJ",
        "orcid": ";0009-0001-9970-4803;;0009-0003-0519-3454;;;",
        "linkedin": ";;https://linkedin.com/in/csf77;simon-stepputtis/;;;",
        "or_profile": "~Zhang_Xi-Jia1;~Yue_Guo7;~Shufei_Chen1;~Simon_Stepputtis1;~Matthew_Craig_Gombolay1;~Katia_P._Sycara1;~Joseph_Campbell1",
        "aff": ";TikTok Inc.;Georgia Institute of Technology;Virginia Polytechnic Institute and State University+Carnegie Mellon University;;Carnegie Mellon University;Purdue University",
        "aff_domain": ";bytedance.com;gatech.edu;vt.edu+cmu.edu;;cmu.edu;purdue.edu",
        "position": ";Researcher;MS student;Assistant Professor+Postdoc;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxi-jia2025modelagnostic,\ntitle={Model-Agnostic Policy Explanations with Large Language Models},\nauthor={Zhang Xi-Jia and Yue Guo and Shufei Chen and Simon Stepputtis and Matthew Craig Gombolay and Katia P. Sycara and Joseph Campbell},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=VzXpFjKgJg}\n}",
        "github": "",
        "project": "",
        "reviewers": "d9mA;QBcL;i7vC",
        "site": "https://openreview.net/forum?id=VzXpFjKgJg",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "WARZwyDf17",
        "title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As language models improve and grow capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets harder, and human-annotated test sets\u2014which are expensive to create\u2014saturate more quickly. A compelling alternative is to design reliable strategies to automate the creation of test data and evaluation, but previous attempts either rely on pre-existing data, or focus solely on individual tasks. We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality benchmarks for any task by leveraging language models for both synthetic test data creation and evaluation. ZSB is simple and flexible: it requires only the creation of a prompt for data generation and one for evaluation; it is scalable to tasks and languages where collecting real-world data is costly or impractical; it is model-agnostic, allowing the creation of increasingly challenging benchmarks as models improve. To assess the effectiveness of our framework, we create benchmarks for five text-only tasks and a multi-modal one: general capabilities in four languages (English, Chinese, French, and Korean), translation, and general vision-language capabilities in English. We then rank a broad range of open and closed systems on our benchmarks. ZSB rankings consistently correlate strongly with human rankings, outperforming widely-adopted standard benchmarks. Through ablations, we find that strong benchmarks can be created with open models, and that judge model size and dataset variety are crucial drivers of performance. We release all our benchmarks, and code to reproduce our experiments and to produce new benchmarks.",
        "keywords": "automatic evaluation;large language models;vision language models;multilinguality;llm-as-a-judge",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jos\u00e9 Pombal;Nuno M Guerreiro;Ricardo Rei;Andre Martins",
        "authorids": "~Jos\u00e9_Pombal1;~Nuno_M_Guerreiro1;~Ricardo_Rei1;~Andre_Martins1",
        "gender": ";;;M",
        "homepage": ";https://nunonmg.github.io/;;https://andre-martins.github.io/",
        "dblp": ";267/0265;;m/AndreFTMartins",
        "google_scholar": "Tt_-ImgAAAAJ;268m9FgAAAAJ;;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ",
        "orcid": ";;;",
        "linkedin": "jos%C3%A9-maria-prc-pombal/;;;",
        "or_profile": "~Jos\u00e9_Pombal1;~Nuno_M_Guerreiro1;~Ricardo_Rei1;~Andre_Martins1",
        "aff": "Instituto Superior T\u00e9cnico+Unbabel;Unbabel+Instituto Superior T\u00e9cnico;;Instituto Superior T\u00e9cnico+Unbabel",
        "aff_domain": "tecnico.ulisboa.pt+unbabel.com;unbabel.com+tecnico.ulisboa.pt;;tecnico.ulisboa.pt+unbabel.com",
        "position": "PhD student+Researcher;Researcher+PhD student;;Associate Professor+Research Scientist",
        "bibtex": "@inproceedings{\npombal2025zeroshot,\ntitle={Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models},\nauthor={Jos{\\'e} Pombal and Nuno M Guerreiro and Ricardo Rei and Andre Martins},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=WARZwyDf17}\n}",
        "github": "",
        "project": "",
        "reviewers": "B5mp;D9Dp;hRX1;PEQ1",
        "site": "https://openreview.net/forum?id=WARZwyDf17",
        "pdf_size": 0,
        "rating": "4;5;6;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7745966692414834
    },
    {
        "id": "WIfns41MAb",
        "title": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent studies have discovered that large language models (LLM) may be ``fooled'' to output private information, including training data, system prompts, and personally identifiable information, under carefully crafted adversarial prompts. Existing red-teaming approaches for privacy leakage either rely on manual efforts or focus solely on system prompt extraction, making them ineffective for severe risks of training data leakage.\nWe propose LeakAgent, a novel black-box red-teaming framework for LLM privacy leakage. Our framework trains an open-source LLM through reinforcement learning as the attack agent to generate adversarial prompts for both training data extraction and system prompt extraction.\nTo achieve this, we propose a novel reward function to provide effective and fine-grained rewards and design novel mechanisms to balance exploration and exploitation during learning and enhance the diversity of adversarial prompts. Through extensive evaluations, we first show that LeakAgent significantly outperforms existing rule-based approaches in training data extraction and automated methods in system prompt leakage. We also demonstrate the effectiveness of LeakAgent in extracting system prompts from real-world applications in OpenAI's GPT Store. We further demonstrate LeakAgent's effectiveness in evading the existing guardrail defense and its helpfulness in enabling better safety alignment.\nFinally, we validate our customized designs through a detailed ablation study.\nWe release our code here \\url{https://github.com/rucnyz/LeakAgent}.",
        "keywords": "Privacy Leakage;LLM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuzhou Nie;Zhun Wang;Ye Yu;Xian Wu;Xuandong Zhao;Nathaniel D. Bastian;Wenbo Guo;Dawn Song",
        "authorids": "~Yuzhou_Nie1;~Zhun_Wang1;~Ye_Yu5;~Xian_Wu8;~Xuandong_Zhao1;~Nathaniel_D._Bastian1;~Wenbo_Guo1;~Dawn_Song1",
        "gender": "M;;M;M;M;M;M;F",
        "homepage": "https://rucnyz.github.io/;;https://github.com/mouryounohako;https://nuwuxian.github.io/;https://xuandongzhao.github.io/;https://cyber.army.mil/About-Us/ACI-Research-Team/Bastian/;https://henrygwb.github.io/;",
        "dblp": ";;;03/5595-7.html;244/8033;132/5837.html;144/1238-2.html;s/DXSong",
        "google_scholar": ";;;ptWUm0EAAAAJ;CxeH4uoAAAAJ;M2aMMxQAAAAJ;KyPheRMAAAAJ;",
        "orcid": "0000-0002-8352-3303;;;;;0000-0001-9957-2778;;",
        "linkedin": ";;;;xuandong-zhao-a3270610b/;nathanielbastian/;;",
        "or_profile": "~Yuzhou_Nie1;~Zhun_Wang1;~Ye_Yu5;~Xian_Wu8;~Xuandong_Zhao1;~Nathaniel_D._Bastian1;~Wenbo_Guo1;~Dawn_Song1",
        "aff": "University of California, Santa Barbara+University of Maryland, College Park;;Peking University;Meta Facebook;UC Berkeley;United States Military Academy;University of California, Santa Barbara;University of California, Berkeley",
        "aff_domain": "ucsb.edu+umd.edu;;stu.pku.edu.cn;meta.com;berkeley.edu;westpoint.edu;ucsb.edu;berkeley.edu",
        "position": "PhD student+Intern;;Undergrad student;Researcher;Postdoc;Principal Researcher;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nnie2025leakagent,\ntitle={LeakAgent: {RL}-based Red-teaming Agent for {LLM} Privacy Leakage},\nauthor={Yuzhou Nie and Zhun Wang and Ye Yu and Xian Wu and Xuandong Zhao and Nathaniel D. Bastian and Wenbo Guo and Dawn Song},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=WIfns41MAb}\n}",
        "github": "",
        "project": "",
        "reviewers": "vXdG;1rTe;visk;CRuy",
        "site": "https://openreview.net/forum?id=WIfns41MAb",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "WLgfeRhuA0",
        "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the \\textit{Explicit Knowledge Boundary Modeling} (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility.",
        "keywords": "reliability;knowledge boundary;self-awareness",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hang Zheng;Hongshen Xu;Yuncong Liu;Shuai Fan;Lu Chen;Pascale Fung;Kai Yu",
        "authorids": "~Hang_Zheng3;~Hongshen_Xu1;~Yuncong_Liu1;~Shuai_Fan1;~Lu_Chen3;~Pascale_Fung1;~Kai_Yu3",
        "gender": "M;M;M;M;M;F;M",
        "homepage": ";https://speechlab.sjtu.edu.cn/members/hongshen-xu;;;https://coai-sjtu.github.io;http://pascale.home.ece.ust.hk/;https://x-lance.sjtu.edu.cn/~kaiyu/",
        "dblp": ";314/8140;275/9143;283/9278-5.html;69/157-2;29/4187;197/1322-4",
        "google_scholar": ";;yAqxFPgAAAAJ;;https://scholar.google.ca/citations?user=Fb3jWaYAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0003-7157-5995;0000-0002-6770-6564;;;;;0000-0002-7102-9826",
        "linkedin": ";;;;;;",
        "or_profile": "~Hang_Zheng3;~Hongshen_Xu1;~Yuncong_Liu1;~Shuai_Fan1;~Lu_Chen3;~Pascale_Fung1;~Kai_Yu3",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;AISpeech Ltd;Shanghai Jiaotong University;HKUST;Shanghai Jiaotong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;aispeech.com;sjtu.edu.cn;ece.ust.hk;sjtu.edu.cn",
        "position": "MS student;PhD student;PhD student;Researcher;Associate Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nzheng2025enhancing,\ntitle={Enhancing {LLM} Reliability via Explicit Knowledge Boundary Modeling},\nauthor={Hang Zheng and Hongshen Xu and Yuncong Liu and Shuai Fan and Lu Chen and Pascale Fung and Kai Yu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=WLgfeRhuA0}\n}",
        "github": "",
        "project": "",
        "reviewers": "GHeu;gFAY;xXcS;MJZ5",
        "site": "https://openreview.net/forum?id=WLgfeRhuA0",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;4;5;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "WnZjdQOWiY",
        "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Code LLMs have shown promising results with converting tasks in natural language to programs that can be executed by service robots. We are interested in finetuning small, specialized LLMs for this purpose, but collecting datasets of task-program pairs specific to each robot is time-consuming and expensive. While approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of generating novel tasks given a few examples, they are unable to provide the corresponding programs that correctly abide by physical-world and robot-constraints using the provided programming interface. Using a simulator is a natural potential solution to checking for such constraints, but building simulation environments that can handle arbitrary tasks and their necessary objects and locations, is challenging. To address these challenges, we introduce ROBO-INSTRUCT, which synthesizes task-specific simulation environments on the fly during program execution, by opportunistically inferring entity properties and enforcing corresponding constraints based on how the entities are used in the task program. Additionally, ROBO-INSTRUCT integrates an LLM-aided post-processing procedure to refine instructions for better alignment with robot programs. We demonstrate the effectiveness of ROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models outperform all baseline methods and even match or surpass the performance of several larger and proprietary models.",
        "keywords": "Self-Instruct;Fine-tuning Code LLMs for service robot tasks;Domain-Specific Program Generation;Code LLMs for Robotics",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zichao Hu;Junyi Jessy Li;Arjun Guha;Joydeep Biswas",
        "authorids": "~Zichao_Hu1;~Junyi_Jessy_Li2;~Arjun_Guha3;~Joydeep_Biswas1",
        "gender": "M;F;Not Specified;M",
        "homepage": ";https://jessyli.com;https://khoury.northeastern.edu/~arjunguha;https://www.joydeepb.com/",
        "dblp": ";148/9553;15/2016;84/73",
        "google_scholar": "Qk-v-okAAAAJ;tJGm3-YAAAAJ;yMU0f9EAAAAJ;https://scholar.google.com.tw/citations?user=f28F1YUAAAAJ",
        "orcid": "0009-0007-6433-8878;;0000-0002-7493-3271;0000-0002-1211-1731",
        "linkedin": ";;;",
        "or_profile": "~Zichao_Hu1;~Junyi_Jessy_Li2;~Arjun_Guha3;~Joydeep_Biswas1",
        "aff": "University of Texas at Austin;University of Texas at Austin;Northeastern University;NVIDIA+The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;neu.edu;nvidia.com+cs.utexas.edu",
        "position": "PhD student;Associate Professor;Associate Professor;Visiting Professor+Associate Professor",
        "bibtex": "@inproceedings{\nhu2025roboinstruct,\ntitle={Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code {LLM}s},\nauthor={Zichao Hu and Junyi Jessy Li and Arjun Guha and Joydeep Biswas},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=WnZjdQOWiY}\n}",
        "github": "",
        "project": "",
        "reviewers": "txhX;LLwZ;PKmn",
        "site": "https://openreview.net/forum?id=WnZjdQOWiY",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "WzGypILLDb",
        "title": "DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated LLMs with Refined Rotation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Rotating the activation and weight matrices to reduce the influence of outliers in large language models (LLMs) has recently attracted significant attention, particularly in the context of model quantization. Prior studies have shown that in low-precision quantization scenarios, such as 4-bit weights and 4-bit activations~(W4A4), randomized Hadamard transforms can achieve significantly higher accuracy than randomized orthogonal transforms. Notably, the reason behind this phenomenon remains unknown. In this paper, we find that these transformations show substantial improvement in eliminating outliers for common tokens and achieve similar quantization error. The primary reason for the accuracy difference lies in the fact that randomized Hadamard transforms can slightly reduce the quantization error for tokens with massive activations while randomized orthogonal transforms increase the quantization error. Due to the extreme rarity of these tokens and their critical impact on model accuracy, we consider this a long-tail optimization problem, and therefore construct a simple yet effective method: a weighted loss function. Additionally, we propose an optimization strategy for the rotation matrix that involves alternating optimization of quantization parameters while employing orthogonal Procrustes transforms to refine the rotation matrix. This makes the distribution of the rotated activation values more conducive to quantization, especially for tokens with massive activations. Our method enhances the Rotated LLMs by achieving dual free, **Outlier-Free** and **Massive Activation-Free**, dubbed as **DFRot**. Extensive experiments demonstrate the effectiveness and efficiency of DFRot. By tuning the rotation matrix using just a single sample, DFRot achieves a perplexity improvement of 0.98 and 0.95 on W4A4KV4 and W4A4KV16, respectively, for LLaMA3-70B, a model known for its quantization challenges. Code is available at https://github.com/JingyangXiang/DFRot.",
        "keywords": "W4A4 quantization;randomized hadamard transforms;randomized orthogonal transforms;outlier;masssive activation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jingyang Xiang;Sai Qian Zhang",
        "authorids": "~Jingyang_Xiang2;~Sai_Qian_Zhang1",
        "gender": ";",
        "homepage": ";https://saiqianzhang.com/",
        "dblp": ";164/7945",
        "google_scholar": ";",
        "orcid": ";0000-0002-4815-9235",
        "linkedin": ";",
        "or_profile": "~Jingyang_Xiang2;~Sai_Qian_Zhang1",
        "aff": ";New York University",
        "aff_domain": ";nyu.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nxiang2025dfrot,\ntitle={{DFR}ot: Achieving Outlier-Free and Massive Activation-Free for Rotated {LLM}s with Refined Rotation},\nauthor={Jingyang Xiang and Sai Qian Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=WzGypILLDb}\n}",
        "github": "",
        "project": "",
        "reviewers": "JUny;gFVi;ZCwi",
        "site": "https://openreview.net/forum?id=WzGypILLDb",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "2;4;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.0,
            0.816496580927726
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5
    },
    {
        "id": "X2RXpFA6Vh",
        "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models.\nHowever, current MoE models typically use only few experts due to prohibitive training and inference cost.\nWe propose _**T**est-**T**ime **M**odel **M**erging_ (TTMM) which scales the MoE paradigm to orders of magnitude more experts and uses model merging to avoid almost any test-time overhead.\nWe show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt.\nTTT has recently been shown to significantly improve language models, but is computationally expensive.\nWe find that performance of TTMM improves with more experts and approaches the performance of TTT.\nMoreover, we find that with a 1B parameter base model, _TTMM is more than $100\\times$ faster than TTT_ at test-time by amortizing the cost of TTT at train-time.\nThus, TTMM offers a promising cost-effective approach to scale test-time training.",
        "keywords": "test-time training;model merging;mixture of experts;language modeling;local learning;transductive learning",
        "primary_area": "",
        "supplementary_material": "/attachment/7454da207edc9f4504d90fe9d3fa15d8e2804df7.zip",
        "author": "Ryo Bertolissi;Jonas H\u00fcbotter;Ido Hakimi;Andreas Krause",
        "authorids": "~Ryo_Bertolissi1;~Jonas_H\u00fcbotter1;~Ido_Hakimi1;~Andreas_Krause1",
        "gender": "M;M;;M",
        "homepage": ";https://jonhue.github.io;;https://las.inf.ethz.ch/krausea",
        "dblp": ";300/4583;245/9227;87/1831-1.html",
        "google_scholar": ";pxi_RkwAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.ch/citations?user=eDHv58AAAAAJ",
        "orcid": ";;;0000-0001-7260-9673",
        "linkedin": "ryo-bertolissi-3497ab291;jonhue/;idohakimi/;krausea/",
        "or_profile": "~Ryo_Bertolissi1;~Jonas_H\u00fcbotter1;~Ido_Hakimi1;~Andreas_Krause1",
        "aff": "ETHZ - ETH Zurich;ETH Zurich;ETHZ - ETH Zurich;ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "position": "Undergrad student;PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nbertolissi2025local,\ntitle={Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging},\nauthor={Ryo Bertolissi and Jonas H{\\\"u}botter and Ido Hakimi and Andreas Krause},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=X2RXpFA6Vh}\n}",
        "github": "",
        "project": "",
        "reviewers": "tuLa;7eGv;Ltqr",
        "site": "https://openreview.net/forum?id=X2RXpFA6Vh",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "X39dK0SX9W",
        "title": "Agents Are All You Need for LLM Unlearning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. In this work we show that \n\\textit{agents might be all we need for effective and practical LLM unlearning}. We present the first agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and \\texttt{ALU} seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that \\texttt{ALU} consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring time cost that remains effectively constant regardless of the number of unlearning targets. We further highlight \\texttt{ALU}'s superior performance compared to existing methods when evaluated at scale. Specifically, \\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.",
        "keywords": "LLM Agents;unlearning;Safety in AI",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Debdeep Sanyal;Murari Mandal",
        "authorids": "~Debdeep_Sanyal1;~Murari_Mandal1",
        "gender": "M;M",
        "homepage": ";https://murarimandal.github.io/",
        "dblp": ";175/8628",
        "google_scholar": ";https://scholar.google.co.in/citations?user=U8AyzLIAAAAJ",
        "orcid": ";0000-0002-0157-0967",
        "linkedin": "debdeep-sanyal-18264a211/;https://linkedin.com/in/murari-mandal",
        "or_profile": "~Debdeep_Sanyal1;~Murari_Mandal1",
        "aff": "Kalinga Institute of Industrial Technology (KIIT);Kalinga Institute of Industrial Technology (KIIT) Bhubaneswar India",
        "aff_domain": "kiit.ac.in;kiit.ac.in",
        "position": "Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nsanyal2025agents,\ntitle={Agents Are All You Need for {LLM} Unlearning},\nauthor={Debdeep Sanyal and Murari Mandal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=X39dK0SX9W}\n}",
        "github": "",
        "project": "",
        "reviewers": "kKUh;vUGi;dgMs;vLYx",
        "site": "https://openreview.net/forum?id=X39dK0SX9W",
        "pdf_size": 0,
        "rating": "3;6;6;8",
        "confidence": "5;3;4;5",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.7853571071357126
        ],
        "confidence_avg": [
            4.25,
            0.82915619758885
        ],
        "replies_avg": [
            34,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.1266600992762247
    },
    {
        "id": "X5vFauyVWr",
        "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Model editing aims to efficiently update a pre-trained model\u2019s knowledge without the need for time-consuming full retraining. While existing pioneering editing methods achieve promising results, they primarily focus on editing single-modal language models (LLMs). However, for vision-language models (VLMs), which involve multiple modalities, the role and impact of each modality on editing performance remain largely unexplored. To address this gap, we explore the impact of textual and visual modalities on model editing and find that: (1) textual and visual representations reach peak sensitivity at different layers, reflecting their varying importance; and (2) editing both modalities can efficiently update knowledge, but this comes at the cost of compromising the model\u2019s original capabilities. Based on our findings, we propose DualEdit, an editor that modifies both textual and visual modalities at their respective key layers. Additionally, we introduce a gating module within the more sensitive textual modality, allowing DualEdit to efficiently update new knowledge while preserving the model\u2019s original information. We evaluate DualEdit across multiple VLM backbones and benchmark datasets, demonstrating its superiority over state-of-the-art VLM editing baselines as well as adapted LLM editing methods on different evaluation metrics. Codes are available at https://github.com/zhiyiscs/DualEdit.",
        "keywords": "Model Editing;Multimodal Learning;VLM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhiyi Shi;Binjie Wang;Chongjie Si;Yichen Wu;Junsik Kim;Hanspeter Pfister",
        "authorids": "~Zhiyi_Shi1;~Binjie_Wang1;~Chongjie_Si1;~Yichen_Wu2;~Junsik_Kim1;~Hanspeter_Pfister1",
        "gender": "M;M;M;M;M;M",
        "homepage": ";;https://chongjiesi.site;https://wuyichen-97.github.io/;https://sites.google.com/site/jskimcv/;https://vcg.seas.harvard.edu",
        "dblp": ";;324/2201;;89/6937-1;p/HanspeterPfister",
        "google_scholar": "OkwdBmgAAAAJ;;wXc2EtsAAAAJ;https://scholar.google.com/citations?hl=zh-CN;https://scholar.google.co.kr/citations?user=p5tuyxwAAAAJ;tvBEoaMAAAAJ",
        "orcid": ";;;0000-0003-2859-3285;0000-0003-2555-5232;0000-0002-3620-2582",
        "linkedin": ";binjie-wang-91514b25a/;chongjiesi;;;hpfister/",
        "or_profile": "~Zhiyi_Shi1;~Binjie_Wang1;~Chongjie_Si1;~Yichen_Wu2;~Junsik_Kim1;~Hanspeter_Pfister1",
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign+School of Engineering and Applied Sciences, Harvard University;Fudan University+Harvard University;Shanghai Jiaotong University;Harvard University+Harvard University+City University of Hong Kong;Harvard University;Harvard University",
        "aff_domain": "cs.illinois.edu+seas.harvard.edu;fudan.edu.cn+harvard.edu;sjtu.edu.cn;harvard.edu+seas.harvard.edu+cityu.edu.hk;harvard.edu;harvard.edu",
        "position": "PhD student+Researcher;Undergrad student+Visiting undergraduate student;PhD student;Postdoc+Researcher+PhD student;Postdoctoral fellow;Full Professor",
        "bibtex": "@inproceedings{\nshi2025dualedit,\ntitle={DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models},\nauthor={Zhiyi Shi and Binjie Wang and Chongjie Si and Yichen Wu and Junsik Kim and Hanspeter Pfister},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=X5vFauyVWr}\n}",
        "github": "",
        "project": "",
        "reviewers": "CGGe;DZuA;Ykrx",
        "site": "https://openreview.net/forum?id=X5vFauyVWr",
        "pdf_size": 0,
        "rating": "6;6;6",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "XNQHMYsUHf",
        "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in large language models (LLMs) have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test time-compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling\u2014especially under a fixed compute budget\u2014remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models intro suboptimal strategies, and (2) explicit CoT supervision can discourage \u2018implicit\u2018 (non verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm.",
        "keywords": "LLM Reasoning;Backtracking;Test-time Computate",
        "primary_area": "",
        "supplementary_material": "/attachment/0a403995ea0dfbbe8a68a5305859fe4ebe0618c1.zip",
        "author": "Tian Qin;David Alvarez-Melis;Samy Jelassi;Eran Malach",
        "authorids": "~Tian_Qin3;~David_Alvarez-Melis1;~Samy_Jelassi1;~Eran_Malach3",
        "gender": "F;M;M;",
        "homepage": "https://sunnytqin.github.io/;https://dmelis.github.io/;https://sjelassi.github.io/;",
        "dblp": ";168/8255;222/3149;",
        "google_scholar": ";XsxZrYYAAAAJ;;",
        "orcid": ";0000-0002-9591-8986;;",
        "linkedin": "sunny-qin-b70567203/;;;",
        "or_profile": "~Tian_Qin3;~David_Alvarez-Melis1;~Samy_Jelassi1;~Eran_Malach3",
        "aff": "Harvard University;School of Engineering and Applied Sciences, Harvard University+Microsoft;Harvard University;",
        "aff_domain": "g.harvard.edu;seas.harvard.edu+microsoft.com;harvard.edu;",
        "position": "PhD student;Assistant Professor+Senior Researcher;Postdoc;",
        "bibtex": "@inproceedings{\nqin2025to,\ntitle={To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning},\nauthor={Tian Qin and David Alvarez-Melis and Samy Jelassi and Eran Malach},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=XNQHMYsUHf}\n}",
        "github": "",
        "project": "",
        "reviewers": "VtTK;GNL9;z8oL;EL6J",
        "site": "https://openreview.net/forum?id=XNQHMYsUHf",
        "pdf_size": 0,
        "rating": "6;6;8;9",
        "confidence": "4;3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.25,
            1.299038105676658
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5555555555555555
    },
    {
        "id": "XZm1ekzERf",
        "title": "NoveltyBench: Evaluating Creativity and Diversity in Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from *mode collapse*, the inability to generate diverse and novel outputs.\nOur work introduces **NoveltyBench**, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs.\nNoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers.\nNotably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility.\nWhile prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize creativity alongside quality.",
        "keywords": "generation;diversity;evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yiming Zhang;Harshita Diddee;Susan Holm;Hanchen Liu;Xinyue Liu;Vinay Samuel;Barry Wang;Daphne Ippolito",
        "authorids": "~Yiming_Zhang5;~Harshita_Diddee1;~Susan_Holm1;~Hanchen_Liu1;~Xinyue_Liu11;~Vinay_Samuel1;~Barry_Wang1;~Daphne_Ippolito1",
        "gender": "M;F;F;M;F;M;M;F",
        "homepage": "http://y0mingzhang.github.io/;https://harshitadd.netlify.app/;https://www.lti.cs.cmu.edu/people/15509/susan-e-holm;;https://cauchy221.github.io/;;https://Barry.Wang;http://www.daphnei.com",
        "dblp": "76/5416-22;280/8888;;;;357/5144;320/5158;192/2031.html",
        "google_scholar": ";https://scholar.google.com/citations?mauthors=Harshita+Diddee;;;vSvPaUoAAAAJ;Ab_Gl3cAAAAJ;2S7N5bQAAAAJ;",
        "orcid": ";0000-0002-0852-7371;;;;;0009-0008-3810-8494;",
        "linkedin": ";harshita-diddee/;;kevinliu040914/;;vinaysamuel2003/;barry-w/;",
        "or_profile": "~Yiming_Zhang5;~Harshita_Diddee1;~Susan_Holm1;~Hanchen_Liu1;~Xinyue_Liu11;~Vinay_Samuel1;~Barry_Wang1;~Daphne_Ippolito1",
        "aff": "School of Computer Science, Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;University of Maryland, College Park+Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cmu.edu;cmu.edu;cmu.edu;andrew.cmu.edu;umd.edu+cmu.edu;cmu.edu;cmu.edu",
        "position": "PhD student;PhD student;Researcher;Undergrad student;MS student;MS student+Undergrad student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025noveltybench,\ntitle={NoveltyBench: Evaluating Creativity and Diversity in Language Models},\nauthor={Yiming Zhang and Harshita Diddee and Susan Holm and Hanchen Liu and Xinyue Liu and Vinay Samuel and Barry Wang and Daphne Ippolito},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=XZm1ekzERf}\n}",
        "github": "",
        "project": "",
        "reviewers": "KaJt;SHky;3g5a",
        "site": "https://openreview.net/forum?id=XZm1ekzERf",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9999999999999997
    },
    {
        "id": "XhdNFeMclS",
        "title": "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\\ell_0$. In this paper, we reveal a theoretical link that the $\\ell_2$-norm of the sparse feature vector can be approximated with the $\\ell_2$-norm of the dense vector with a closed-form error, which allows sparse autoencoders to be trained without the need to manually determine $\\ell_0$. Specifically, we validate two applications of our theoretical findings. First, we introduce a new methodology that can assess the feature activations of pre-trained SAEs by computing the theoretically expected value from the input embedding, which has been overlooked by existing SAE evaluation methods and loss functions. Second, we introduce a novel activation function, top-AFA, which builds upon our formulation of approximate feature activation (AFA). This function enables top-$k$ style activation without requiring a constant hyperparameter $k$ to be tuned, dynamically determining the number of activated features for each input. By training SAEs on three intermediate layers to reconstruct GPT2 hidden embeddings for over 80 million tokens from the OpenWebText dataset, we demonstrate the empirical merits of this approach and compare it with current state-of-the-art $k$-sparse autoencoders.\nOur code is available at: https://github.com/SewoongLee/top-afa-sae.",
        "keywords": "mechanistic interpretability;sparse autoencoder",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sewoong Lee;Adam Davies;Marc E. Canby;Julia Hockenmaier",
        "authorids": "~Sewoong_Lee2;~Adam_Davies2;~Marc_E._Canby1;~Julia_Hockenmaier1",
        "gender": ";Non-Binary;;F",
        "homepage": ";https://ahdavies6.github.io/;;https://cs.illinois.edu/directory/profile/juliahmr",
        "dblp": ";;;64/2448",
        "google_scholar": ";vqkOH7gAAAAJ;;https://scholar.google.com.tw/citations?user=iIiVrrQAAAAJ",
        "orcid": ";0000-0002-0610-2732;;",
        "linkedin": ";adamhdavies/;;",
        "or_profile": "~Sewoong_Lee2;~Adam_Davies2;~Marc_E._Canby1;~Julia_Hockenmaier1",
        "aff": ";University of Illinois, Urbana Champaign;;University of Illinois, Urbana Champaign+Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen",
        "aff_domain": ";illinois.edu;;illinois.edu+lmu.de",
        "position": ";PhD student;;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nlee2025evaluating,\ntitle={Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality},\nauthor={Sewoong Lee and Adam Davies and Marc E. Canby and Julia Hockenmaier},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=XhdNFeMclS}\n}",
        "github": "",
        "project": "",
        "reviewers": "QxGv;8rCC;Ji2v;b4e1",
        "site": "https://openreview.net/forum?id=XhdNFeMclS",
        "pdf_size": 0,
        "rating": "4;6;7;7",
        "confidence": "4;4;2;2",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.0,
            1.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8164965809277259
    },
    {
        "id": "XvCBtm5PgF",
        "title": "Self-Steering Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While test-time reasoning enables language models (LMs) to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its *abstract structure*\u2014both how to verify solutions and *how to search* for them. This paper introduces DisCIPL, a method for \u201cself-steering\u201d LMs where a *Planner model* generates a task-specific *inference program* that is executed by a population of *Follower models*. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. Our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.",
        "keywords": "Probabilistic inference;sequential Monte Carlo;code generation;test-time search;constrained generation;reasoning;language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gabriel Grand;Joshua B. Tenenbaum;Vikash Mansinghka;Alexander K. Lew;Jacob Andreas",
        "authorids": "~Gabriel_Grand1;~Joshua_B._Tenenbaum1;~Vikash_Mansinghka1;~Alexander_K._Lew1;~Jacob_Andreas1",
        "gender": "M;;M;M;M",
        "homepage": "https://www.gabegrand.com;;;http://alexlew.net;http://web.mit.edu/jda/www",
        "dblp": "215/3760;t/JoshuaBTenenbaum;23/1731;242/4530;97/8154",
        "google_scholar": "1qmAFhsAAAAJ;;;TiF1WEsAAAAJ;dnZ8udEAAAAJ",
        "orcid": "0000-0003-1920-0021;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Gabriel_Grand1;~Joshua_B._Tenenbaum1;~Vikash_Mansinghka1;~Alexander_K._Lew1;~Jacob_Andreas1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "position": "PhD student;Professor;Principal Research Scientist;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\ngrand2025selfsteering,\ntitle={Self-Steering Language Models},\nauthor={Gabriel Grand and Joshua B. Tenenbaum and Vikash Mansinghka and Alexander K. Lew and Jacob Andreas},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=XvCBtm5PgF}\n}",
        "github": "",
        "project": "",
        "reviewers": "3m39;WVvh;DTJV;DG1U",
        "site": "https://openreview.net/forum?id=XvCBtm5PgF",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;5;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            30,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Y131N9fUbU",
        "title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively. The code of this paper is available at https://github.com/Kaffaljidhmah2/SpecDec_pp.",
        "keywords": "speculative decoding;MDP theory;large language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaixuan Huang;Xudong Guo;Mengdi Wang",
        "authorids": "~Kaixuan_Huang1;~Xudong_Guo1;~Mengdi_Wang1",
        "gender": "M;M;F",
        "homepage": "https://hackyhuang.github.io/;;http://mwang.princeton.edu",
        "dblp": ";;",
        "google_scholar": "EfxwV6oAAAAJ;;",
        "orcid": ";;",
        "linkedin": ";xudong-guo-835543aa/;",
        "or_profile": "~Kaixuan_Huang1;~Xudong_Guo1;~Mengdi_Wang1",
        "aff": "Princeton University;Tsinghua University;Princeton University",
        "aff_domain": "princeton.edu;mails.tsinghua.edu.cn;princeton.edu",
        "position": "PhD student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nhuang2025specdec,\ntitle={SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths},\nauthor={Kaixuan Huang and Xudong Guo and Mengdi Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Y131N9fUbU}\n}",
        "github": "",
        "project": "",
        "reviewers": "3xMF;HC5A;jQ11",
        "site": "https://openreview.net/forum?id=Y131N9fUbU",
        "pdf_size": 0,
        "rating": "5;7;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            1.247219128924647
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "YLze3CETYP",
        "title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.",
        "keywords": "code generation and understanding;benchmarking;NLP datasets;evaluation methodologies;automatic evaluation of datasets;evaluation;metrics;reproducibility;statistical testing for evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aleksander Ficek;Somshubra Majumdar;Vahid Noroozi;Boris Ginsburg",
        "authorids": "~Aleksander_Ficek1;~Somshubra_Majumdar1;~Vahid_Noroozi2;~Boris_Ginsburg1",
        "gender": "M;M;;",
        "homepage": ";http://titu1994.github.io/;;",
        "dblp": ";206/6501;;",
        "google_scholar": "tIry9oUAAAAJ;h-qsXdwAAAAJ;;",
        "orcid": ";0000-0001-5635-4893;;",
        "linkedin": "aleksanderficek/;;;",
        "or_profile": "~Aleksander_Ficek1;~Somshubra_Majumdar1;~Vahid_Noroozi2;~Boris_Ginsburg1",
        "aff": "NVIDIA;NVIDIA;;",
        "aff_domain": "nvidia.com;nvidia.com;;",
        "position": "Researcher;Researcher;;",
        "bibtex": "@inproceedings{\nficek2025scoring,\ntitle={Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning},\nauthor={Aleksander Ficek and Somshubra Majumdar and Vahid Noroozi and Boris Ginsburg},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=YLze3CETYP}\n}",
        "github": "",
        "project": "",
        "reviewers": "3nRV;FFwf;49g2;WWNV",
        "site": "https://openreview.net/forum?id=YLze3CETYP",
        "pdf_size": 0,
        "rating": "4;4;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "YgwQ7sXPXU",
        "title": "Learning Adaptive Parallel Reasoning with Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have\nsignificant limitations: serialized chain-of-thought approaches generate\noverly long outputs, leading to increased latency and exhausted context\nwindows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited\nperformance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables\nlanguage models to orchestrate both serialized and parallel computations\nend-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key\ninnovation is our end-to-end reinforcement learning strategy, optimizing\nboth parent and child inference threads to enhance task success rate without\nrequiring predefined reasoning structures. Experiments on the Countdown\nreasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context);\n(2) superior scalability with increased computation (80.1% vs. 66.6% at 20k\ntotal tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3%\nat approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
        "keywords": "large language models;reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiayi Pan;Xiuyu Li;Long Lian;Charlie Victor Snell;Yifei Zhou;Adam Yala;Trevor Darrell;Kurt Keutzer;Alane Suhr",
        "authorids": "~Jiayi_Pan1;~Xiuyu_Li1;~Long_Lian1;~Charlie_Victor_Snell1;~Yifei_Zhou1;~Adam_Yala1;~Trevor_Darrell2;~Kurt_Keutzer1;~Alane_Suhr1",
        "gender": "M;Not Specified;M;M;M;M;;M;Not Specified",
        "homepage": "https://www.jiayipan.me/;https://xiuyuli.com/;https://github.com/TonyLianLong;https://sea-snell.github.io;https://yifeizhou02.github.io/;http://adamyala.csail.mit.edu/;;https://people.eecs.berkeley.edu/~keutzer/;http://www.alanesuhr.com",
        "dblp": "39/6476-2;279/5847;276/0012;;50/7699;177/9396;;k/KurtKeutzer.html;203/9306",
        "google_scholar": "n9Y_sQEAAAAJ;https://scholar.google.com/citations?;eOLxyqUAAAAJ;dD7EpwQAAAAJ;;a4unsk4AAAAJ;;ID9QePIAAAAJ;daslsUkAAAAJ",
        "orcid": "0000-0003-0817-4083;;0000-0001-6098-189X;;;0000-0001-9576-2590;;0000-0003-3868-8501;",
        "linkedin": ";;longlian/;;yifei-zhou-57aa9b222/;;;kurtkeutzer/;",
        "or_profile": "~Jiayi_Pan1;~Xiuyu_Li1;~Long_Lian1;~Charlie_Victor_Snell1;~Yifei_Zhou1;~Adam_Yala1;~Trevor_Darrell2;~Kurt_Keutzer1;~Alane_Suhr1",
        "aff": "University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley+University of California, San Francisco;;University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu+ucsf.edu;;berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;PhD student;Assistant Professor+Assistant Professor;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\npan2025learning,\ntitle={Learning Adaptive Parallel Reasoning with Language Models},\nauthor={Jiayi Pan and Xiuyu Li and Long Lian and Charlie Victor Snell and Yifei Zhou and Adam Yala and Trevor Darrell and Kurt Keutzer and Alane Suhr},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=YgwQ7sXPXU}\n}",
        "github": "",
        "project": "",
        "reviewers": "oCwp;VxmG;RZt7",
        "site": "https://openreview.net/forum?id=YgwQ7sXPXU",
        "pdf_size": 0,
        "rating": "6;7;9",
        "confidence": "3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            1.247219128924647
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.944911182523068
    },
    {
        "id": "Z2El1U94bq",
        "title": "FormaRL: Enhancing Autoformalization with no Labeled Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Autoformalization is one of the central tasks in formal verification, while its advancement remains hindered due to the data scarcity and the absence efficient methods. In this work we propose **FormaRL**, a simple yet efficient reinforcement learning framework for autoformalization which only requires a small amount of unlabeled data. FormaRL integrates syntax check from Lean compiler and consistency check from large language model to calculate the reward, and adopts GRPO algorithm to update the formalizer. We also curated a proof problem dataset from undergraduate-level math materials, named **uproof**, in the hope to facilitate the exploration of autoformalization and theorem proving in advanced math. Experiments show that FormaRL can increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by 4 $\\sim$ 6x (4.04\\% $\\to$ 26.15\\% on ProofNet and 2.4\\% $\\to$ 9.6\\% on uproof) with merely 859 unlabeled data. And on uproof our method also achieved a strong improvement in out-of-distribution performance compared to existing open-source state-of-the-art autoformalizers on both pass@1 accuracy (6.2\\% $\\to$ 9.6\\%) and pass@16 accuracy (24.4\\% $\\to$ 33.6\\%).  Training code of FormaRL is open-sourced at [https://github.com/THUNLP-MT/FormaRL](https://github.com/THUNLP-MT/FormaRL).",
        "keywords": "Large Language Model;Formal Verification;Autoformalization;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/e15a03408291a966b2ffe7431f1e2b9c894481f7.zip",
        "author": "Yanxing Huang;Xinling Jin;Sijie Liang;Fuwen Luo;Peng Li;Yang Liu",
        "authorids": "~Yanxing_Huang1;~Xinling_Jin1;~Sijie_Liang1;~Fuwen_Luo1;~Peng_Li2;~Yang_Liu19",
        "gender": ";F;F;M;M;M",
        "homepage": ";;;;http://www.lpeng.net/;http://nlp.csai.tsinghua.edu.cn/~ly/",
        "dblp": ";;;317/1971;83/6353-30;51/3710-5",
        "google_scholar": ";;;AIKlZXcAAAAJ;hgYzkOQAAAAJ;https://scholar.google.com.hk/citations?user=lVhoKNcAAAAJ",
        "orcid": ";;0009-0006-9510-0267;0009-0001-9183-9383;0000-0003-1374-5979;0000-0002-3087-242X",
        "linkedin": ";jinxinling;;;;",
        "or_profile": "~Yanxing_Huang1;~Xinling_Jin1;~Sijie_Liang1;~Fuwen_Luo1;~Peng_Li2;~Yang_Liu19",
        "aff": ";Tongji University;Beijing Forestry University;Tsinghua University;Tsinghua University;Tsinghua University",
        "aff_domain": ";tongji.edu.cn;bjfu.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": ";Undergrad student;Undergrad student;PhD student;Associate Professor;Professor",
        "bibtex": "@inproceedings{\nhuang2025formarl,\ntitle={Forma{RL}: Enhancing Autoformalization with no Labeled Data},\nauthor={Yanxing Huang and Xinling Jin and Sijie Liang and Fuwen Luo and Peng Li and Yang Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Z2El1U94bq}\n}",
        "github": "",
        "project": "",
        "reviewers": "iRck;okNw;bE1K;MZ5Q",
        "site": "https://openreview.net/forum?id=Z2El1U94bq",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "5;4;4;2",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            1.0897247358851685
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7608859102526822
    },
    {
        "id": "Z3L35tQTEg",
        "title": "Multi-Token Attention",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This \u201csingle token attention\u201d bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other\u2019s attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector\u2019s capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method\u2019s ability to leverage richer information proves particularly beneficial.",
        "keywords": "Deep learning architectures;Large Language Model (LLM);Transformer;Attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Olga Golovneva;Tianlu Wang;Jason E Weston;Sainbayar Sukhbaatar",
        "authorids": "~Olga_Golovneva1;~Tianlu_Wang1;~Jason_E_Weston1;~Sainbayar_Sukhbaatar1",
        "gender": "F;F;;M",
        "homepage": ";https://tianlu-wang.github.io/;;",
        "dblp": "280/3377;185/5529;;56/10550",
        "google_scholar": ";inzQqX8AAAAJ;;ri1sE34AAAAJ",
        "orcid": ";;;",
        "linkedin": "olgagolovneva/;;;",
        "or_profile": "~Olga_Golovneva1;~Tianlu_Wang1;~Jason_E_Weston1;~Sainbayar_Sukhbaatar1",
        "aff": "Meta Facebook;Meta;;Meta AI",
        "aff_domain": "fb.com;meta.com;;meta.com",
        "position": "Researcher;Researcher;;Research Scientist",
        "bibtex": "@inproceedings{\ngolovneva2025multitoken,\ntitle={Multi-Token Attention},\nauthor={Olga Golovneva and Tianlu Wang and Jason E Weston and Sainbayar Sukhbaatar},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Z3L35tQTEg}\n}",
        "github": "",
        "project": "",
        "reviewers": "ryV8;3NEq;AZNg;maCc",
        "site": "https://openreview.net/forum?id=Z3L35tQTEg",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Z8vtD1egtI",
        "title": "ADAPT: Actively Discovering and Adapting to Preferences for any Task",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Assistive agents should be able to perform under-specified long-horizon tasks while respecting user preferences. We introduce Actively Discovering and Adapting to Preferences for any Task (ADAPT) \u2013 a benchmark designed to evaluate agents\u2019 ability to adhere to user preferences across various household tasks through active questioning. Next, we propose Reflection-DPO, a novel training approach for adapting large language models (LLMs) to the task of active questioning. Reflection-DPO finetunes a \u2018student\u2019 LLM to follow the actions of a privileged \u2018teacher\u2019 LLM, and optionally ask a question to gather necessary information to better predict the teacher action. We find that prior approaches that use state-of-the-art LLMs fail to sufficiently follow user preferences in ADAPT due to insufficient questioning and poor adherence to elicited preferences. In contrast, Reflection-DPO achieves a higher rate of satisfying user preferences, outperforming a zero-shot chain-of-thought baseline by 6.1% on unseen users.",
        "keywords": "Task Oriented Agents;Interactive Learning;Active Dialog;Personalization;Task Planning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Maithili Patel;Xavier Puig;Ruta Desai;Roozbeh Mottaghi;Sonia Chernova;Joanne Truong;Akshara Rai",
        "authorids": "~Maithili_Patel1;~Xavier_Puig1;~Ruta_Desai1;~Roozbeh_Mottaghi1;~Sonia_Chernova2;~Joanne_Truong1;~Akshara_Rai1",
        "gender": "F;M;;;F;;",
        "homepage": "https://maithili.github.io;https://people.csail.mit.edu/xavierpuig/;;http://roozbehm.info;https://www.cc.gatech.edu/~chernova/;;https://ai.facebook.com/people/akshara-rai",
        "dblp": "334/4404;50/8429;;36/633;27/1140;;",
        "google_scholar": ";;;CCV58dgAAAAJ;EYo_WkEAAAAJ;;",
        "orcid": "0000-0001-8730-9198;;;;0000-0001-6320-0825;;",
        "linkedin": "maithili/;;;roozbeh-mottaghi-63397aa0;;;",
        "or_profile": "~Maithili_Patel1;~Xavier_Puig1;~Ruta_Desai1;~Roozbeh_Mottaghi1;~Sonia_Chernova2;~Joanne_Truong1;~Akshara_Rai1",
        "aff": "Georgia Institute of Technology;Meta;;Meta+University of Washington;Georgia Institute of Technology;;FAIR, Meta AI",
        "aff_domain": "gatech.edu;fb.com;;meta.com+cs.washington.edu;gatech.edu;;meta.com",
        "position": "PhD student;Researcher;;Research Scientist Manager+Affiliate Professor ;Associate Professor;;Researcher",
        "bibtex": "@inproceedings{\npatel2025adapt,\ntitle={{ADAPT}: Actively Discovering and Adapting to Preferences for any Task},\nauthor={Maithili Patel and Xavier Puig and Ruta Desai and Roozbeh Mottaghi and Sonia Chernova and Joanne Truong and Akshara Rai},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Z8vtD1egtI}\n}",
        "github": "",
        "project": "",
        "reviewers": "bZvY;ojsc;tqzp",
        "site": "https://openreview.net/forum?id=Z8vtD1egtI",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "ZSMnX3LBva",
        "title": "In-Context Occam\u2019s Razor: How Transformers Prefer Simpler Hypotheses on the Fly",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-context learning (ICL) enables transformers to adapt to new tasks through contextual examples without parameter updates. While existing research has typically studied ICL in fixed-complexity setups, real-world language models encounter tasks of diverse complexity levels. This paper investigates how transformers navigate hierarchical task structures where higher-complexity categories can perfectly represent any pattern generated by simpler ones. \nWe design testbeds based on Markov chains and linear regression that reveal transformers not only identify the correct complexity level for each task but also accurately infer the corresponding parameters\u2014even when the in-context examples fit multiple complexity hypotheses. Notably, when presented with data generated by simpler processes, transformers consistently favor the least complex sufficient explanation. We theoretically explain this behavior through a Bayesian framework, demonstrating that transformers effectively implement an in-context Bayesian Occam's razor by balancing model fit against complexity penalties.",
        "keywords": "In-context learning;transformers;linear regression;Markov chains;Bayesian Occam's razor",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Puneesh Deora;Bhavya Vasudeva;Tina Behnia;Christos Thrampoulidis",
        "authorids": "~Puneesh_Deora1;~Bhavya_Vasudeva1;~Tina_Behnia1;~Christos_Thrampoulidis1",
        "gender": ";;F;",
        "homepage": "https://puneesh00.github.io;https://estija.github.io;;https://sites.google.com/view/cthrampo/home",
        "dblp": "250/9324;250/9545;323/4405;127/6532",
        "google_scholar": "https://scholar.google.co.in/citations?user=cn1wdTUAAAAJ;https://scholar.google.co.in/citations?user=ZCSsIokAAAAJ;https://scholar.google.com/citations?hl=en;",
        "orcid": ";;;",
        "linkedin": "puneesh-deora-82b101128/?originalSubdomain=in;;;",
        "or_profile": "~Puneesh_Deora1;~Bhavya_Vasudeva1;~Tina_Behnia1;~Christos_Thrampoulidis1",
        "aff": "University of British Columbia;University of Southern California;University of British Columbia;University of British Columbia",
        "aff_domain": "ubc.ca;usc.edu;ubc.ca;ubc.ca",
        "position": "PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\ndeora2025incontext,\ntitle={In-Context Occam{\\textquoteright}s Razor: How Transformers Prefer Simpler Hypotheses on the Fly},\nauthor={Puneesh Deora and Bhavya Vasudeva and Tina Behnia and Christos Thrampoulidis},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ZSMnX3LBva}\n}",
        "github": "",
        "project": "",
        "reviewers": "neZS;peHZ;Qdck;BYfu",
        "site": "https://openreview.net/forum?id=ZSMnX3LBva",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "ZYVAtUUNbH",
        "title": "Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge Expansion for Dense Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing dense retrieval models struggle with reasoning-intensive retrieval task as they fail to capture implicit relevance that requires reasoning beyond surface-level semantic information.\nTo address these challenges, we propose Scenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval framework that explicitly indexes implicit relevance by decomposing documents into scenario-based retrieval units. \nSPIKE organizes documents into scenario, which encapsulates the reasoning process necessary to uncover implicit relationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large language model (LLM), then distills these reasoning capabilities into a smaller, efficient scenario generator. \nDuring inference, SPIKE incorporates scenario-level relevance alongside document-level relevance, enabling reasoning-aware retrieval. \nExtensive experiments demonstrate that SPIKE consistently enhances retrieval performance across various query types and dense retrievers. \nIt also enhances the retrieval experience for users through scenario and offers valuable contextual information for LLMs in retrieval-augmented generation (RAG).",
        "keywords": "Information Retrieval;Reasoning Intensive Retrieval;Dense Retrieval;Reasoning;LLM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sangam Lee;Ryang Heo;SeongKu Kang;Dongha Lee",
        "authorids": "~Sangam_Lee1;~Ryang_Heo1;~SeongKu_Kang1;~Dongha_Lee1",
        "gender": ";;M;M",
        "homepage": "https://github.com/augustinLib;;https://seongku-kang.github.io/;https://donalee.github.io",
        "dblp": "388/0381;;251/9613.html;12/760-3",
        "google_scholar": ";;fB0K-fMAAAAJ;driVwKwAAAAJ",
        "orcid": "0009-0000-2479-7606;;0000-0001-5528-1426;0000-0003-2173-3476",
        "linkedin": ";;;",
        "or_profile": "~Sangam_Lee1;~Ryang_Heo1;~SeongKu_Kang1;~Dongha_Lee1",
        "aff": "Yonsei University;;Korea University+University of Illinois Urbana-Champaign;Yonsei University",
        "aff_domain": "yonsei.ac.kr;;korea.ac.kr+cs.illinois.edu;yonsei.ac.kr",
        "position": "PhD student;;Assistant Professor+Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025imagine,\ntitle={Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge Expansion for Dense Retrieval},\nauthor={Sangam Lee and Ryang Heo and SeongKu Kang and Dongha Lee},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ZYVAtUUNbH}\n}",
        "github": "",
        "project": "",
        "reviewers": "ZcZG;93yk;3oWX;8hp9",
        "site": "https://openreview.net/forum?id=ZYVAtUUNbH",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "ZZ4tcxJvux",
        "title": "E$^2$-RAG: Towards Editable Efficient RAG by Editing Compressed KV Caches",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval-Augmented Generation (RAG) demonstrates remarkable capabilities for enhancing the performance of Large Language Models (LLMs) by integrating external knowledge.\nStandard RAG introduces additional computations due to the extra retrieved context.\nTo improve efficiency, recent studies propose compressing chunk tokens into compact forms, such as key-value (KV) caches.\nHowever, maintaining these compressed KV caches in an updated state presents a significant challenge, undermining the primary goal of RAG: acquiring up-to-date knowledge.\nIn this work, we propose **E$^{2}$-RAG**, the first **E**ditable **E**fficient-**RAG** method designed to efficiently edit compressed KV caches for knowledge updates.\nE$^2$-RAG features an encoder-decoder architecture similar to efficient RAG methods, along with an additional editor.\nThe encoder-decoder compresses chunk tokens into KV caches and generates responses.\nThe editor takes old KV caches and new knowledge tokens as inputs, enabling efficient updates to the KV caches.\nTo formalize knowledge updating, we define three operations: INSERT, DELETE, and UPDATE.\nWe create three sets of datasets for each operation.\nThrough extensive experiments, E$^2$-RAG achieves nearly **40x faster** editing compared to recomputing KV caches while maintaining **3x faster** generation efficiency than standard RAG, with a performance downgrade of 1%-5%.\nWe also conduct various ablation studies, including multi-turn editing, multi-chunk capability, and knowledge conflicts, to explore the capabilities of E$^2$-RAG.",
        "keywords": "Retrieval Augmented Generation",
        "primary_area": "",
        "supplementary_material": "/attachment/cb6b8a5009cad693d953c6721496aa806e2f5e1b.zip",
        "author": "Tongxu Luo;Wenyu Du;HanWen Hao;Min Zhang;Hao Yang;Benyou Wang",
        "authorids": "~Tongxu_Luo1;~Wenyu_Du1;~HanWen_Hao1;~Min_Zhang10;~Hao_Yang7;~Benyou_Wang2",
        "gender": "M;;M;M;M;M",
        "homepage": "https://tongxuluo.github.io;;;;https://github.com/yanghaocsg;https://wabyking.github.io/old.html",
        "dblp": ";38/10657;;83/5342-10.html;54/4089-7;169/1793",
        "google_scholar": "aJEGhzkAAAAJ;;https://scholar.google.com.hk/citations?user=iFIvfeAAAAAJ;oVvAyCUAAAAJ;lOsjM5sAAAAJ;Jk4vJU8AAAAJ",
        "orcid": ";;;0000-0002-9624-6851;0000-0001-8861-7010;0000-0002-1501-9914",
        "linkedin": ";;;;;",
        "or_profile": "~Tongxu_Luo1;~Wenyu_Du1;~HanWen_Hao1;~Min_Zhang10;~Hao_Yang7;~Benyou_Wang2",
        "aff": "University of Science and Technology Beijing;the University of Hong Kong, University of Hong Kong;The Insititute of Advanced Computing Technology, Beijing University of Aeronautics and Astronautics;Huawei Technologies Ltd.;Huawei Technologies Ltd.;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "ustb.edu.cn;cs.hku.hk;act.buaa.edu.cn;huawei.com;huawei.com;cuhk.edu.cn",
        "position": "Undergrad student;PhD student;MS student;Researcher;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nluo2025erag,\ntitle={E\\${\\textasciicircum}2\\$-{RAG}: Towards Editable Efficient {RAG} by Editing Compressed {KV} Caches},\nauthor={Tongxu Luo and Wenyu Du and HanWen Hao and Min Zhang and Hao Yang and Benyou Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ZZ4tcxJvux}\n}",
        "github": "",
        "project": "",
        "reviewers": "rSVu;ZyFk;qvW3;TEz1",
        "site": "https://openreview.net/forum?id=ZZ4tcxJvux",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;3;3;5",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "Zfa9jCYGCz",
        "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more \u201cpragmatic\u201d approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets.",
        "keywords": "llm-based metric;machine translation;evaluation;question generation;question answering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Patrick Fernandes;Sweta Agrawal;Emmanouil Zaranis;Andre Martins;Graham Neubig",
        "authorids": "~Patrick_Fernandes1;~Sweta_Agrawal1;~Emmanouil_Zaranis1;~Andre_Martins1;~Graham_Neubig1",
        "gender": ";F;M;M;M",
        "homepage": "https://coderpat.github.io;https://sweta20.github.io/;https://manzar96.github.io;https://andre-martins.github.io/;http://phontron.com",
        "dblp": "207/6964.html;210/7863.html;305/3517;m/AndreFTMartins;03/8155",
        "google_scholar": ";Avsw9IkAAAAJ;9g9PyQsAAAAJ;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ;wlosgkoAAAAJ",
        "orcid": ";;0000-0002-7084-9590;;",
        "linkedin": ";;emmanouil-zaranis-a84924149/;;",
        "or_profile": "~Patrick_Fernandes1;~Sweta_Agrawal1;~Emmanouil_Zaranis1;~Andre_Martins1;~Graham_Neubig1",
        "aff": "School of Computer Science, Carnegie Mellon University+Instituto Superior T\u00e9cnico;Google;Instituto Superior T\u00e9cnico;Instituto Superior T\u00e9cnico+Unbabel;Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu+tecnico.ulisboa.pt;google.com;tecnico.ulisboa.pt;tecnico.ulisboa.pt+unbabel.com;cmu.edu",
        "position": "PhD student+PhD student;Researcher;PhD student;Associate Professor+Research Scientist;Associate Professor",
        "bibtex": "@inproceedings{\nfernandes2025do,\ntitle={Do {LLM}s Understand Your Translations? Evaluating Paragraph-level {MT} with Question Answering},\nauthor={Patrick Fernandes and Sweta Agrawal and Emmanouil Zaranis and Andre Martins and Graham Neubig},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Zfa9jCYGCz}\n}",
        "github": "",
        "project": "",
        "reviewers": "oatu;zwtQ;unEd;qzD2",
        "site": "https://openreview.net/forum?id=Zfa9jCYGCz",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "Zk224WPT42",
        "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As language model agents are applied to real world problems of increasing complexity, they will be expected to formulate plans across large search spaces. If those plans fail for reasons beyond their control, how well do language agents search for alternative ways to achieve their goals? We devise a specialized agentic planning benchmark to study this question. Each planning problem is solved via combinations of function calls. The agent searches for relevant functions from a set of over four thousand possibilities, and observes environmental feedback in the form of function outputs or error messages. Our benchmark confronts the agent with external failures in its workflow, such as functions that suddenly become unavailable. At the same time, even with the introduction of these failures, we guarantee that the task remains solvable. Ideally, an agent\u2019s performance on the planning task should not be affected by the presence of external failures. Overall, we find that language agents struggle to formulate and execute backup plans in response to environment feedback. While state-of-the-art models are often able to identify the correct function to use in the right context, they struggle to adapt to feedback from the environment and often fail to pursue alternate courses of action, even when the search space is artificially restricted. We provide a systematic analysis of the failures of both open-source and commercial models, examining the effects of search space size, as well as the benefits of scaling model size in our setting. Our analysis identifies key challenges for current generative models as well as promising directions for future work.",
        "keywords": "planning;tool-use",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andrew Wang;Sophia Hager;Adi Asija;Daniel Khashabi;Nicholas Andrews",
        "authorids": "~Andrew_Wang3;~Sophia_Hager1;~Adi_Asija1;~Daniel_Khashabi2;~Nicholas_Andrews2",
        "gender": "M;F;M;M;",
        "homepage": "https://ajwang34.github.io/;;https://www.github.com/adiasija2011;http://danielkhashabi.com/;",
        "dblp": ";366/0470;;71/10515;",
        "google_scholar": "clTRNPsAAAAJ;Ft07WhUAAAAJ;;pK2kQvgAAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";sophia-hager-096983169/;adi-asija;;",
        "or_profile": "~Andrew_Wang3;~Sophia_Hager1;~Adi_Asija1;~Daniel_Khashabi2;~Nicholas_Andrews2",
        "aff": "Department of Computer Science, Whiting School of Engineering;Johns Hopkins University;Johns Hopkins University;Johns Hopkins University;",
        "aff_domain": "cs.jhu.edu;jh.edu;jhu.edu;jhu.edu;",
        "position": "PhD student;PhD student;MS student;Assistant Professor;",
        "bibtex": "@inproceedings{\nwang2025hell,\ntitle={Hell or High Water: Evaluating Agentic Recovery from External Failures},\nauthor={Andrew Wang and Sophia Hager and Adi Asija and Daniel Khashabi and Nicholas Andrews},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=Zk224WPT42}\n}",
        "github": "",
        "project": "",
        "reviewers": "3TTA;swx9;DGtT",
        "site": "https://openreview.net/forum?id=Zk224WPT42",
        "pdf_size": 0,
        "rating": "5;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "ZnOoEA2nDn",
        "title": "Rethinking Safety in LLM Fine-tuning: An Optimization Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fine-tuning language models is commonly believed to inevitably harm their safety, i.e., refusing to respond to harmful user requests, even when using harmless datasets, thus requiring additional safety measures.\nWe challenge this belief through systematic testing, showing that poor optimization choices, rather than inherent trade-offs, often cause safety problems, measured as harmful responses to adversarial prompts.\nBy properly selecting key training hyper-parameters, e.g., learning rate, batch size, and gradient steps, we reduce unsafe model responses from 16\\% to approximately 5\\%, as measured by keyword matching, while maintaining utility performance.\nBased on this observation, we propose a simple exponential moving average (EMA) momentum technique in parameter space that preserves safety performance by creating a stable optimization path and retains the original pre-trained model's safety properties.\nOur experiments on the Llama families across multiple datasets (Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can largely be avoided without specialized interventions, outperforming existing approaches that require additional safety data while offering practical guidelines for maintaining both model performance and safety during adaptation.",
        "keywords": "Finetuning LLM;Safety alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Minseon Kim;Jin Myung Kwak;Lama Alssum;Bernard Ghanem;Philip Torr;David Krueger;Fazl Barez;Adel Bibi",
        "authorids": "~Minseon_Kim1;~Jin_Myung_Kwak1;~Lama_Alssum1;~Bernard_Ghanem1;~Philip_Torr1;~David_Krueger1;~Fazl_Barez1;~Adel_Bibi1",
        "gender": ";F;F;M;;M;;M",
        "homepage": "https://kim-minseon.github.io/;;https://cemse.kaust.edu.sa/cs/people/person/lama-alssum;https://ivul.kaust.edu.sa;http://www.robots.ox.ac.uk/~tvg/;https://mila.umontreal.ca/en/person/david-scott-krueger/;;http://adelbibi.com",
        "dblp": "247/5952;218/3507;;37/2516;;142/2741.html;;176/0964",
        "google_scholar": "ZwObZNwAAAAJ;;ONDV2k4AAAAJ;rVsGTeEAAAAJ;;https://scholar.google.ca/citations?user=5Uz70IoAAAAJ;;Q4j2laYAAAAJ",
        "orcid": ";;;0000-0002-5534-587X;;;;0000-0002-6169-3918",
        "linkedin": "minseon-kim-707a84174;jin-myung-kwak-46936595/;;bernardghanem/;;;;adel-bibi-ba3671ab/",
        "or_profile": "~Minseon_Kim1;~Jin_Myung_Kwak1;~Lama_Alssum1;~Bernard_Ghanem1;~Philip_Torr1;~David_Krueger1;~Fazl_Barez1;~Adel_Bibi1",
        "aff": "Microsoft;Korea Advanced Institute of Science & Technology;King Abdullah University of Science and Technology+University of Oxford;King Abdullah University of Science and Technology;University of Oxford;Montreal Institute for Learning Algorithms, University of Montreal, Universit\u00e9 de Montr\u00e9al;;University of Oxford",
        "aff_domain": "microsoft.com;kaist.ac.kr;kaust.edu.sa+ox.ac.uk;kaust.edu.sa;ox.ac.uk;mila.umontreal.ca;;ox.ac.uk",
        "position": "Postdoc;PhD student;PhD student+Intern;Full Professor;Full Professor;Assistant Professor;;Senior Researcher",
        "bibtex": "@inproceedings{\nkim2025rethinking,\ntitle={Rethinking Safety in {LLM} Fine-tuning: An Optimization Perspective},\nauthor={Minseon Kim and Jin Myung Kwak and Lama Alssum and Bernard Ghanem and Philip Torr and David Krueger and Fazl Barez and Adel Bibi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ZnOoEA2nDn}\n}",
        "github": "",
        "project": "",
        "reviewers": "bwwZ;4gg9;cGdT",
        "site": "https://openreview.net/forum?id=ZnOoEA2nDn",
        "pdf_size": 0,
        "rating": "5;6;8",
        "confidence": "3;4;5",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            1.247219128924647
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9819805060619659
    },
    {
        "id": "a201nfn3xX",
        "title": "Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "\u200b\u200bPost-training quantization reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. Existing methods mitigate these drops by keeping some important weights in higher precision; we develop a new mixed-precision approach, Task-Circuit Quantization (TCQ), that directly conditions the quantization process on specific circuits -- which we define as sets of weights associated with downstream task performance. TCQ draws parallels to automated circuit discovery, introducing a novel method to identify a small number of key weights that are particularly important to task performance; these weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TCQ-based quantization to existing mixed-precision quantization methods and GPTQ when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, text-to-SQL tasks and for both Llama-3 and Qwen2.5 models, we find that TCQ outperforms baselines like SPQR and Slim-LLM using the same calibration data and a lower weight budget, achieving major improvements in the 2- and 3-bit regime. With only 3.1 bits we are able to recover 97% of the model's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. Furthermore, we observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, Slim-LLM. Code: [https://github.com/The-Inscrutable-X/TACQ](https://github.com/The-Inscrutable-X/TACQ)",
        "keywords": "Quantization;Mixed Precision;Interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hanqi Xiao;Yi-Lin Sung;Elias Stengel-Eskin;Mohit Bansal",
        "authorids": "~Hanqi_Xiao1;~Yi-Lin_Sung1;~Elias_Stengel-Eskin1;~Mohit_Bansal2",
        "gender": "M;;M;M",
        "homepage": ";;https://esteng.github.io;https://www.cs.unc.edu/~mbansal/",
        "dblp": ";;212/6138;32/5243.html",
        "google_scholar": ";;gr_ZVSQAAAAJ;DN8QtscAAAAJ",
        "orcid": ";;0000-0002-6689-505X;",
        "linkedin": "hanqi-xiao-a4410a229/;;;",
        "or_profile": "~Hanqi_Xiao1;~Yi-Lin_Sung1;~Elias_Stengel-Eskin1;~Mohit_Bansal2",
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill;;University of North Carolina at Chapel Hill;University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;;cs.unc.edu;unc.edu",
        "position": "Undergrad student;;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nxiao2025taskcircuit,\ntitle={Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression},\nauthor={Hanqi Xiao and Yi-Lin Sung and Elias Stengel-Eskin and Mohit Bansal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=a201nfn3xX}\n}",
        "github": "",
        "project": "",
        "reviewers": "zLzX;FNLA;WCni;DkhF",
        "site": "https://openreview.net/forum?id=a201nfn3xX",
        "pdf_size": 0,
        "rating": "5;7;7;9",
        "confidence": "4;3;4;5",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5
    },
    {
        "id": "a6QsOjr3wo",
        "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. In this work, we conceptualize this effect as the $\\textit{data compliance gap} (DCG)$, which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. We measure the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pertaining). Our experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training.\nOur study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions.",
        "keywords": "Responsible AI;AI and Fair Use;Robots.txt Opt-out;LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dongyang Fan;Vinko Sabol\u010dec;Matin Ansaripour;Ayush Kumar Tarun;Martin Jaggi;Antoine Bosselut;Imanol Schlag",
        "authorids": "~Dongyang_Fan2;~Vinko_Sabol\u010dec1;~Matin_Ansaripour1;~Ayush_Kumar_Tarun1;~Martin_Jaggi1;~Antoine_Bosselut1;~Imanol_Schlag3",
        "gender": "F;Not Specified;M;;M;M;M",
        "homepage": ";https://people.epfl.ch/vinko.sabolcec?lang=en;;https://ayushkumartarun.github.io/;https://mlo.epfl.ch;https://atcbosselut.github.io/;",
        "dblp": ";400/2190;;306/7616;17/4402;184/3742;213/4144",
        "google_scholar": ";;;QYJGgtsAAAAJ;https://scholar.google.ch/citations?user=r1TJBr8AAAAJ;XD9hkJwAAAAJ;https://scholar.google.ch/citations?user=nFQJEskAAAAJ",
        "orcid": ";;;;0000-0003-1579-5558;;",
        "linkedin": "fannnndy/;;matin-ansaripour-b2b7a81a3;ayush-kumar-83228b1a2/;;;",
        "or_profile": "~Dongyang_Fan2;~Vinko_Sabol\u010dec1;~Matin_Ansaripour1;~Ayush_Kumar_Tarun1;~Martin_Jaggi1;~Antoine_Bosselut1;~Imanol_Schlag3",
        "aff": "EPFL - EPF Lausanne;EPFL - EPF Lausanne;EPFL - EPF Lausanne;EPFL - EPF Lausanne;EPFL;EPFL;ETHZ - ETH Zurich",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch;ethz.ch",
        "position": "PhD student;PhD student;MS student;PhD student;Associate Professor;Assistant Professor;Postdoc",
        "bibtex": "@inproceedings{\nfan2025can,\ntitle={Can Performant {LLM}s Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs},\nauthor={Dongyang Fan and Vinko Sabol{\\v{c}}ec and Matin Ansaripour and Ayush Kumar Tarun and Martin Jaggi and Antoine Bosselut and Imanol Schlag},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=a6QsOjr3wo}\n}",
        "github": "",
        "project": "",
        "reviewers": "mnmF;Fr1t;QefG",
        "site": "https://openreview.net/forum?id=a6QsOjr3wo",
        "pdf_size": 0,
        "rating": "7;7;10",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            8.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "a6xzTqMUFQ",
        "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of the training and alignment pipeline for large language models (LLMs). Recent advances, such as direct preference optimization (DPO), have simplified the preference learning step. However, collecting preference data remains a challenging and costly process, often requiring expert annotation. This cost can be mitigated by carefully selecting the data points presented for annotation. In this work, we propose an active learning approach to efficiently select prompt and preference pairs using a risk assessment strategy based on the Sharpe Ratio. \nTo address the challenge of unknown preferences prior to annotation, our method evaluates the gradients of all potential preference annotations to assess their impact on model updates. These gradient-based evaluations enable risk assessment of data points regardless of the annotation outcome. By leveraging the DPO loss derivations, we derive a \\emph{closed-form expression} for computing these Sharpe ratios on a per-tuple basis, ensuring our approach remains both \\emph{tractable} and \\emph{computationally efficient}.  We also introduce two variants of our method, each making different assumptions about prior information. Experimental results demonstrate that our method outperforms the baseline by up to 5\\% in win rates against the chosen completion with limited human preference data across several language models and real-world datasets.",
        "keywords": "active learning;RLHF;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Syrine Belakaria;Joshua Kazdan;Charles Marx;Chris Cundy;Willie Neiswanger;Sanmi Koyejo;Barbara E Engelhardt;Stefano Ermon",
        "authorids": "~Syrine_Belakaria1;~Joshua_Kazdan1;~Charles_Marx1;~Chris_Cundy1;~Willie_Neiswanger2;~Sanmi_Koyejo1;~Barbara_Engelhardt1;~Stefano_Ermon1",
        "gender": "F;M;;M;M;;F;M",
        "homepage": "https://www.sbelakaria.com/;https://profiles.stanford.edu/joshua-kazdan?releaseVersion=10.8.0;;http://cundy.me;https://willieneis.github.io/;;https://beehive.stanford.edu;http://cs.stanford.edu/~ermon/",
        "dblp": "200/8277;;;206/7233;120/7593.html;;27/2355;47/8135",
        "google_scholar": "9NNlVb8AAAAJ;;;https://scholar.google.com/citations?view_op=list_works;QwKHApEAAAAJ;;https://scholar.google.com.tw/citations?user=VEGtG7YAAAAJ;",
        "orcid": ";;;;;;;",
        "linkedin": ";;;chrisjcundy/;;;;",
        "or_profile": "~Syrine_Belakaria1;~Joshua_Kazdan1;~Charles_Marx1;~Chris_Cundy1;~Willie_Neiswanger2;~Sanmi_Koyejo1;~Barbara_Engelhardt1;~Stefano_Ermon1",
        "aff": "Stanford University;Stanford University;;FAR AI;University of Southern California;;Stanford University;Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;;far.ai;usc.edu;;stanford.edu;stanford.edu",
        "position": "Postdoc;PhD student;;Researcher;Assistant Professor;;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nbelakaria2025sharpe,\ntitle={Sharpe Ratio-Guided Active Learning for Preference Optimization in {RLHF}},\nauthor={Syrine Belakaria and Joshua Kazdan and Charles Marx and Chris Cundy and Willie Neiswanger and Sanmi Koyejo and Barbara E Engelhardt and Stefano Ermon},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=a6xzTqMUFQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "C2ud;MhkP;MNhU;ZGMy",
        "site": "https://openreview.net/forum?id=a6xzTqMUFQ",
        "pdf_size": 0,
        "rating": "4;6;6;6",
        "confidence": "4;4;2;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "aJDykpJAYF",
        "title": "Shared Global and Local Geometry of Language Model Embeddings",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Researchers have recently suggested that models share common representations. In our work, we find numerous geometric similarities across the token embeddings of large language models. First, we find \u201cglobal\u201d similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each embedding. Both characterizations allow us to find local similarities across token embeddings. Additionally, our intrinsic dimension demonstrates that embeddings lie on a lower dimensional manifold, and that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Based on our findings, we introduce EMB2EMB, a simple application to linearly transform steering vectors from one language model to another, despite the two models having different dimensions.",
        "keywords": "Embeddings;Alignment;Interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andrew Lee;Melanie Weber;Fernanda Vi\u00e9gas;Martin Wattenberg",
        "authorids": "~Andrew_Lee2;~Melanie_Weber1;~Fernanda_Vi\u00e9gas1;~Martin_Wattenberg1",
        "gender": ";;;M",
        "homepage": ";;;http://www.bewitched.com",
        "dblp": ";;;w/MartinWattenberg",
        "google_scholar": "oQiCjnwAAAAJ;;;pv54dqMAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Andrew_Lee2;~Melanie_Weber1;~Fernanda_Vi\u00e9gas1;~Martin_Wattenberg1",
        "aff": "School of Engineering and Applied Sciences, Harvard University;;;Harvard University+Google",
        "aff_domain": "seas.harvard.edu;;;harvard.edu+google.com",
        "position": "Postdoc;;;Full Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nlee2025shared,\ntitle={Shared Global and Local Geometry of Language Model Embeddings},\nauthor={Andrew Lee and Melanie Weber and Fernanda Vi{\\'e}gas and Martin Wattenberg},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=aJDykpJAYF}\n}",
        "github": "",
        "project": "",
        "reviewers": "tRQC;xUSm;PyR3",
        "site": "https://openreview.net/forum?id=aJDykpJAYF",
        "pdf_size": 0,
        "rating": "7;7;8",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9999999999999997
    },
    {
        "id": "aV2hQN9vkp",
        "title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for language model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also introduce a weakness profiling method EvalTree. EvalTree constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we provide an interface that allows practitioners to interactively explore the capability trees built by EvalTree.",
        "keywords": "Evaluation;Interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhiyuan Zeng;Yizhong Wang;Hannaneh Hajishirzi;Pang Wei Koh",
        "authorids": "~Zhiyuan_Zeng3;~Yizhong_Wang2;~Hannaneh_Hajishirzi1;~Pang_Wei_Koh1",
        "gender": "M;M;F;M",
        "homepage": "https://zhiyuan-zeng.github.io/;https://yizhong-wang.com;https://homes.cs.washington.edu/~hannaneh/;http://cs.stanford.edu/~pangwei",
        "dblp": ";79/3601;52/1296;10/10453",
        "google_scholar": "qLJqCqsAAAAJ;y5zpqdAAAAAJ;LOV6_WIAAAAJ;Nn990CkAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Zhiyuan_Zeng3;~Yizhong_Wang2;~Hannaneh_Hajishirzi1;~Pang_Wei_Koh1",
        "aff": "University of Washington;Department of Computer Science, University of Washington;Allen Institute for Artificial Intelligence+University of Washington;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;allenai.org+uw.edu;allenai.org+cs.washington.edu",
        "position": "PhD student;PhD student;senior director+Associate Professor;Visiting Research Scientist+Assistant Professor",
        "bibtex": "@inproceedings{\nzeng2025evaltree,\ntitle={EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees},\nauthor={Zhiyuan Zeng and Yizhong Wang and Hannaneh Hajishirzi and Pang Wei Koh},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=aV2hQN9vkp}\n}",
        "github": "",
        "project": "",
        "reviewers": "n857;DECS;SQGa;pMeM",
        "site": "https://openreview.net/forum?id=aV2hQN9vkp",
        "pdf_size": 0,
        "rating": "8;8;9;10",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            8.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "akHq1QcqeZ",
        "title": "CLIPPER: Compression enables long-context synthetic data generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification\u2014a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we synthesize a dataset of 19K claims paired with source books and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).",
        "keywords": "synthetic data;fine-tuning;instruction-tuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chau Minh Pham;Yapei Chang;Mohit Iyyer",
        "authorids": "~Chau_Minh_Pham1;~Yapei_Chang1;~Mohit_Iyyer1",
        "gender": ";F;M",
        "homepage": ";https://lilakk.github.io/;http://cs.umass.edu/~miyyer",
        "dblp": ";316/9933;148/9178",
        "google_scholar": ";qCjnm-UAAAAJ;rBVA5tcAAAAJ",
        "orcid": ";;",
        "linkedin": ";ella-yapei-chang/;",
        "or_profile": "~Chau_Minh_Pham1;~Yapei_Chang1;~Mohit_Iyyer1",
        "aff": ";University of Maryland, College Park+University of Massachusetts at Amherst;University of Maryland, College Park",
        "aff_domain": ";umd.edu+umass.edu;umd.edu",
        "position": ";PhD student+PhD student;Associate Professor",
        "bibtex": "@inproceedings{\npham2025clipper,\ntitle={{CLIPPER}: Compression enables long-context synthetic data generation},\nauthor={Chau Minh Pham and Yapei Chang and Mohit Iyyer},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=akHq1QcqeZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "WmVp;yM7W;hqHf;jD3k",
        "site": "https://openreview.net/forum?id=akHq1QcqeZ",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "am6p8VFm9l",
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from an established stigmatization framework, our analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.",
        "keywords": "Mental Health;Network Analysis;Stigmatization;Emergent Bias;Toxicity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rijul Magu;Arka Dutta;Sean Kim;Ashiqur R. KhudaBukhsh;Munmun De Choudhury",
        "authorids": "~Rijul_Magu1;~Arka_Dutta2;~Sean_Kim2;~Ashiqur_R._KhudaBukhsh1;~Munmun_De_Choudhury1",
        "gender": ";M;M;M;F",
        "homepage": ";https://www.duttaarka.com/;;https://www.cs.cmu.edu/~akhudabu/;https://www.munmund.net",
        "dblp": ";149/5213.html;;29/7442;76/3034.html",
        "google_scholar": "YHkIE44AAAAJ;ZCxV0PgAAAAJ;;mWyMp38AAAAJ;Z9Pfp6UAAAAJ",
        "orcid": ";;;;0000-0002-8939-264X",
        "linkedin": ";arka-dutta-868993192;seanhahjeankim/;;",
        "or_profile": "~Rijul_Magu1;~Arka_Dutta2;~Sean_Kim2;~Ashiqur_R._KhudaBukhsh1;~Munmun_De_Choudhury1",
        "aff": "Georgia Institute of Technology;Rochester Institute of Technology;Georgia Institute of Technology;Rochester Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;rit.edu;gatech.edu;rit.edu;gatech.edu",
        "position": "PhD student;PhD student;Undergrad student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nmagu2025navigating,\ntitle={Navigating the Rabbit Hole: Emergent Biases in {LLM}-Generated Attack Narratives Targeting Mental Health Groups},\nauthor={Rijul Magu and Arka Dutta and Sean Kim and Ashiqur R. KhudaBukhsh and Munmun De Choudhury},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=am6p8VFm9l}\n}",
        "github": "",
        "project": "",
        "reviewers": "qMkz;C287;hnA4",
        "site": "https://openreview.net/forum?id=am6p8VFm9l",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "ayB1PACN5j",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture with constant memory usage and constant inference time per token. Despite being trained on dramatically fewer tokens than other top models, our 2.9 billion parameter language model achieves a new 3B SoTA on multilingual tasks and matches the current 3B SoTA on English language downstream performance. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to $\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset. To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM; all under the Apache 2.0 License.",
        "keywords": "Goose;LLM;RWKV;RWKV-7;RWKV7;Linear;Linear Attention;SSM;subquadratic",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bo Peng;Ruichong Zhang;Daniel Goldstein;Eric Alcaide;Xingjian Du;Haowen Hou;Jiaju Lin;Jiaxing Liu;Janna Lu;William Merrill;Guangyu Song;Kaifeng Tan;Saiteja Utpala;Nathan Wilce;Johan S. Wind;Tianyi Wu;Daniel Wuttke;Christian Zhou-Zheng",
        "authorids": "~Bo_Peng21;~Ruichong_Zhang1;~Daniel_Goldstein2;~Eric_Alcaide2;~Xingjian_Du1;~Haowen_Hou2;~Jiaju_Lin1;~Jiaxing_Liu2;~Janna_Lu1;~William_Merrill1;~Guangyu_Song1;~Kaifeng_Tan1;~Saiteja_Utpala1;~Nathan_Wilce1;~Johan_S._Wind2;~Tianyi_Wu11;~Daniel_Wuttke1;~Christian_Zhou-Zheng1",
        "gender": "M;;M;;Non-Binary;M;Non-Binary;M;;M;;M;M;M;;;M;M",
        "homepage": "https://www.rwkv.com;https://openreview.net/profile?id=~Ruichong_Zhang1;;;;https://howard-hou.github.io/;https://jiaju-lin-97.github.io/;https://github.com/p81sunshine;;http://lambdaviking.com;;https://github.com/Jellyfish042;;;;;http://wuttke.tech;",
        "dblp": ";;;;;248/9287;;;;19/3512;;;;;;;;402/3936",
        "google_scholar": ";;;;UqBl_VMAAAAJ;P6pDyoYAAAAJ;JCAH3OoAAAAJ;;CuD3RzUAAAAJ;CyjChJQAAAAJ;;;;;;;;",
        "orcid": ";;;;;0000-0003-3850-3722;;;0009-0009-1031-7742;;;;;;;0009-0005-0648-5226;;0009-0004-0244-9853",
        "linkedin": ";;smerkyg/;;;haowen-hou/;;;jannalu;william-merrill-15ab0743/;;;saiteja-utpala/;nathan-w-5603b3141/;;;https://linkedin.com/in/hevok;christian-zhou-zheng-989604263/",
        "or_profile": "~Bo_Peng21;~Ruichong_Zhang1;~Daniel_Goldstein2;~Eric_Alcaide2;~Xingjian_Du1;~Haowen_Hou2;~Jiaju_Lin1;~Jiaxing_Liu2;~Janna_Lu1;~William_Merrill1;~Guangyu_Song1;~Kaifeng_Tan1;~Saiteja_Utpala1;~Nathan_Wilce1;~Johan_S._Wind2;~Tianyi_Wu11;~Daniel_Wuttke1;~Christian_Zhou-Zheng1",
        "aff": "University of Hong Kong;Tsinghua University;Recursal AI, Inc.;;University of Rochester;Shenzhen University;Pennsylvania State University;Zhejiang University;George Mason University;New York University;;Shenzhen University+JiangXi University of Science and Technology;Microsoft;Recursal AI;;Beijing Normal University;Denigma;The Pingry School",
        "aff_domain": "hku.hk;tsinghua.edu.cn;recursal.ai;;rochester.edu;gml.ac.cn;psu.edu;zju.edu.cn;gmu.edu;nyu.edu;;szu.edu.cn+jxust.edu.cn;microsoft.com;recursal.ai;;bnu.edu.cn;denigma.org;pingry.org",
        "position": "Undergrad student;PhD student;Researcher;;PhD student;Assistant Professor;PhD student;Undergrad student;PhD student;Graduate student;;MS student+Undergrad student;Researcher;Researcher;;PhD student;Researcher;High school student",
        "bibtex": "@inproceedings{\npeng2025rwkv,\ntitle={{RWKV}-7 ''Goose'' with Expressive Dynamic State Evolution},\nauthor={Bo Peng and Ruichong Zhang and Daniel Goldstein and Eric Alcaide and Xingjian Du and Haowen Hou and Jiaju Lin and Jiaxing Liu and Janna Lu and William Merrill and Guangyu Song and Kaifeng Tan and Saiteja Utpala and Nathan Wilce and Johan S. Wind and Tianyi Wu and Daniel Wuttke and Christian Zhou-Zheng},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ayB1PACN5j}\n}",
        "github": "",
        "project": "",
        "reviewers": "UGzf;QvXt;GjhW;bdD1",
        "site": "https://openreview.net/forum?id=ayB1PACN5j",
        "pdf_size": 0,
        "rating": "6;8;8;8",
        "confidence": "3;4;4;5",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            18,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "ayi7qezU87",
        "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusing on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques, achieving up to a 20.5 absolute accuracy improvement on TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms competing methods in maintaining long-context comprehension in LLMs; notably, retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve 100% Acc. performance, matching that of a full KV cache.",
        "keywords": "KV Cache Compression;Inference Acceleration",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zefan Cai;Yichi Zhang;Bofei Gao;Yuliang Liu;Yucheng Li;Tianyu Liu;Keming Lu;Wayne Xiong;Yue Dong;Junjie Hu;Wen Xiao",
        "authorids": "~Zefan_Cai1;~Yichi_Zhang16;~Bofei_Gao1;~Yuliang_Liu5;~Yucheng_Li5;~Tianyu_Liu3;~Keming_Lu1;~Wayne_Xiong1;~Yue_Dong2;~Junjie_Hu2;~Wen_Xiao2",
        "gender": ";;M;M;;M;M;M;F;;F",
        "homepage": ";;https://kbsdjames.github.io;;;;;;https://yuedong.us/;;https://www.cs.ubc.ca/~xiaowen3/index.html",
        "dblp": ";;330/2755;;;134/1099-1;65/6898.html;194/1022;84/486;;",
        "google_scholar": ";;;rmxvSgQAAAAJ;;https://scholar.google.com.hk/citations?user=6hHbBwwAAAAJ;WuD2op4AAAAJ;Ls0e7IEAAAAJ;https://scholar.google.ca/citations?user=WYkn4loAAAAJ;;",
        "orcid": ";;;0000-0001-7165-4341;;;;;;;",
        "linkedin": ";;;;;;;wayne-xiong/;;;",
        "or_profile": "~Zefan_Cai1;~Yichi_Zhang16;~Bofei_Gao1;~Yuliang_Liu5;~Yucheng_Li5;~Tianyu_Liu3;~Keming_Lu1;~Wayne_Xiong1;~Yue_Dong2;~Junjie_Hu2;~Wen_Xiao2",
        "aff": ";;Peking University;Nanjing University;;Alibaba Group;Alibaba Group;Microsoft Research;University of California, Riverside+McGill University;;Microsoft",
        "aff_domain": ";;pku.edu.cn;nju.edu.cn;;alibaba-inc.com;alibaba-inc.com;research.microsoft.com;ucr.edu+mcgill.ca;;microsoft.com",
        "position": ";;MS student;PhD student;;Staff Engineer;Researcher;Principal Researcher;Assistant Professor+PhD student;;Researcher",
        "bibtex": "@inproceedings{\ncai2025pyramidkv,\ntitle={Pyramid{KV}: Dynamic {KV} Cache Compression based on Pyramidal Information Funneling},\nauthor={Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Yucheng Li and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Junjie Hu and Wen Xiao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ayi7qezU87}\n}",
        "github": "",
        "project": "",
        "reviewers": "KRVU;DCU1;dYNL",
        "site": "https://openreview.net/forum?id=ayi7qezU87",
        "pdf_size": 0,
        "rating": "6;8;9",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.666666666666667,
            1.247219128924647
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7559289460184545
    },
    {
        "id": "aykM7KUVJZ",
        "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models.",
        "keywords": "Code Generation;Code Reasoning;Large Language Model;LiveCodeBench",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wasi Uddin Ahmad;Sean Narenthiran;Somshubra Majumdar;Aleksander Ficek;Siddhartha Jain;Jocelyn Huang;Vahid Noroozi;Boris Ginsburg",
        "authorids": "~Wasi_Uddin_Ahmad1;~Sean_Narenthiran1;~Somshubra_Majumdar1;~Aleksander_Ficek1;~Siddhartha_Jain1;~Jocelyn_Huang1;~Vahid_Noroozi2;~Boris_Ginsburg1",
        "gender": "M;M;M;M;M;F;;",
        "homepage": "http://wasiahmad.github.io/;https://github.com/SeanNaren;http://titu1994.github.io/;;https://tmfs10.github.io/;;;",
        "dblp": "183/0576;;206/6501;;81/8212;;;",
        "google_scholar": "YCHJZOMAAAAJ;;h-qsXdwAAAAJ;tIry9oUAAAAJ;mBJIa8cAAAAJ;;;",
        "orcid": ";;0000-0001-5635-4893;;;;;",
        "linkedin": "ahmadwasi/;;;aleksanderficek/;;huangjocelyn/;;",
        "or_profile": "~Wasi_Uddin_Ahmad1;~Sean_Narenthiran1;~Somshubra_Majumdar1;~Aleksander_Ficek1;~Siddhartha_Jain1;~Jocelyn_Huang1;~Vahid_Noroozi2;~Boris_Ginsburg1",
        "aff": "NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;;",
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;;",
        "position": "Senior Research Scientist;Researcher;Researcher;Researcher;Researcher;Researcher;;",
        "bibtex": "@inproceedings{\nahmad2025opencodereasoning,\ntitle={OpenCodeReasoning: Advancing Data Distillation for Competitive Coding},\nauthor={Wasi Uddin Ahmad and Sean Narenthiran and Somshubra Majumdar and Aleksander Ficek and Siddhartha Jain and Jocelyn Huang and Vahid Noroozi and Boris Ginsburg},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=aykM7KUVJZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "2zW4;xjD7;dr9w",
        "site": "https://openreview.net/forum?id=aykM7KUVJZ",
        "pdf_size": 0,
        "rating": "5;7;7",
        "confidence": "5;5;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.666666666666667,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "b8cW86QcOD",
        "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. \nExtensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95\\% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\nCode is available at: https://github.com/juzhengz/LoRI.",
        "keywords": "Large Language Models;Parameter-Efficient Fine-Tuning;Model Merging;Sparsity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juzheng Zhang;Jiacheng You;Ashwinee Panda;Tom Goldstein",
        "authorids": "~Juzheng_Zhang2;~Jiacheng_You1;~Ashwinee_Panda1;~Tom_Goldstein1",
        "gender": "M;M;M;M",
        "homepage": "https://juzhengz.github.io/;https://github.com/YouJiacheng;https://kiddyboots216.github.io/;https://www.cs.umd.edu/~tomg/",
        "dblp": "133/2742;;270/1582.html;25/8184",
        "google_scholar": "d8lJm7MAAAAJ;;FM7JCgQAAAAJ;KmSuVtgAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;https://linkedin.com/in/ashwineepanda;",
        "or_profile": "~Juzheng_Zhang2;~Jiacheng_You1;~Ashwinee_Panda1;~Tom_Goldstein1",
        "aff": "University of Maryland, College Park;Tsinghua University;University of Maryland, College Park;University of Maryland, College Park",
        "aff_domain": "umd.edu;tsinghua.edu.cn;umd.edu;umd.edu",
        "position": "PhD student;PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nzhang2025lori,\ntitle={Lo{RI}: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation},\nauthor={Juzheng Zhang and Jiacheng You and Ashwinee Panda and Tom Goldstein},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=b8cW86QcOD}\n}",
        "github": "",
        "project": "",
        "reviewers": "5RGh;cZht;nmP5;nvBE;wCSw",
        "site": "https://openreview.net/forum?id=b8cW86QcOD",
        "pdf_size": 0,
        "rating": "4;6;6;7;7",
        "confidence": "4;4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.0954451150103321
        ],
        "confidence_avg": [
            3.6,
            0.4898979485566356
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.372677996249965
    },
    {
        "id": "bJ9aARjtBu",
        "title": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) solely trained on next-token prediction learn to solve a wide range of problems involving mathematical reasoning. How does this ability evolve during training? We show the first analysis of how mathematical reasoning abilities of several open-weight LLMs develop during pre-training and post-training. To this end, we construct MathCAMPS, a synthetic dataset of novel mathematical reasoning problems grounded in 44 fine-grained skills taken from the Common Core curriculum from K to 8th grades. In one experiment, we show that mathematical skills are learned during pre-training in an order that measurably correlates with the human-designed curriculum, even though training data are randomly ordered. We also show a detailed analysis of which mathematical abilities benefit from instruction-tuning, a widely used post-training method and, in contrast, which skills suffer. Our work paves the way for an empirical understanding of LLM training dynamics in relation to reasoning.",
        "keywords": "math reasoning;training dynamics;reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shubhra Mishra;Gabriel Poesia;Noah Goodman",
        "authorids": "~Shubhra_Mishra1;~Gabriel_Poesia1;~Noah_Goodman1",
        "gender": "F;M;",
        "homepage": ";https://gpoesia.com;https://cocolab.stanford.edu/",
        "dblp": ";150/2695.html;96/1216",
        "google_scholar": "XJcJak4AAAAJ;as5iYn4AAAAJ;OUpIbcQAAAAJ",
        "orcid": ";;",
        "linkedin": "shubhra-mishra/;;",
        "or_profile": "~Shubhra_Mishra1;~Gabriel_Poesia1;~Noah_Goodman1",
        "aff": "Stanford University+Stanford University;Stanford University;Stanford University",
        "aff_domain": "stanford.edu+stanford.edu;stanford.edu;stanford.edu",
        "position": "MS student+Undergrad student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nmishra2025from,\ntitle={From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models},\nauthor={Shubhra Mishra and Gabriel Poesia and Noah Goodman},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bJ9aARjtBu}\n}",
        "github": "",
        "project": "",
        "reviewers": "Y1Xb;XbsE;Mhg7",
        "site": "https://openreview.net/forum?id=bJ9aARjtBu",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "bJCQMKwPVq",
        "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles\u2014a task requiring multimodal adherence to semantic constraints from $\\textbf{text-based clues}$ and intersectional constraints from $\\textbf{visual grid structures}$. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats ($\\textit{text}$ and $\\textit{image}$), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings highlight limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.",
        "keywords": "LLMs;LVLMs;Evaluation;Benchmark",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jixuan Leng;Chengsong Huang;Langlin Huang;Bill Yuchen Lin;William W. Cohen;Haohan Wang;Jiaxin Huang",
        "authorids": "~Jixuan_Leng1;~Chengsong_Huang1;~Langlin_Huang1;~Bill_Yuchen_Lin1;~William_W._Cohen2;~Haohan_Wang1;~Jiaxin_Huang1",
        "gender": "M;M;Not Specified;M;M;M;F",
        "homepage": "https://jixuanleng.com/;https://chengsong-huang.github.io/;https://shrango.github.io/;http://yuchenlin.xyz/;https://wwcohen.github.io/;http://cs.cmu.edu/~haohanw;https://teapot123.github.io/",
        "dblp": "261/6970.html;211/1188;349/8478;190/4518;c/WWCohen.html;132/4066;187/2874-1",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;Mt9xdjYAAAAJ;https://scholar.google.com/citations?hl=en;8ys-38kAAAAJ;nZxJGeUAAAAJ;DnxrVXgAAAAJ",
        "orcid": ";;0000-0001-9631-0334;;;;",
        "linkedin": "jixuan-leng-2862b2227/;;;;;haohanwang/;",
        "or_profile": "~Jixuan_Leng1;~Chengsong_Huang1;~Langlin_Huang1;~Bill_Yuchen_Lin1;~William_W._Cohen2;~Haohan_Wang1;~Jiaxin_Huang1",
        "aff": "Carnegie Mellon University;Washington University, Saint Louis+Tencent AI Lab;Washington University, Saint Louis;xAI+University of Washington;Carnegie Mellon University+Google DeepMind;University of Illinois, Urbana Champaign;Washington University, Saint Louis",
        "aff_domain": "cmu.edu;wustl.edu+tencent.com;wustl.edu;x.ai+uw.edu;cmu.edu+google.com;illinois.edu;wustl.edu",
        "position": "MS student;PhD student+Intern;PhD student;Researcher+Assistant Professor;Full Professor+Principle Scientist;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nleng2025crosswordbench,\ntitle={CrossWordBench: Evaluating the Reasoning Capabilities of {LLM}s and {LVLM}s with Controllable Puzzle Generation},\nauthor={Jixuan Leng and Chengsong Huang and Langlin Huang and Bill Yuchen Lin and William W. Cohen and Haohan Wang and Jiaxin Huang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bJCQMKwPVq}\n}",
        "github": "",
        "project": "",
        "reviewers": "XYjy;7ztd;kmED;wJrT;TLs7;KRbZ",
        "site": "https://openreview.net/forum?id=bJCQMKwPVq",
        "pdf_size": 0,
        "rating": "5;6;6;7;7;7",
        "confidence": "4;4;4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.7453559924999298
        ],
        "confidence_avg": [
            3.8333333333333335,
            0.3726779962499649
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4000000000000001
    },
    {
        "id": "bNTrKqqnG9",
        "title": "The Dual-Route Model of Induction",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Prior work on in-context copying has shown the existence of *induction heads*, which attend to and promote individual tokens during copying. In this work we discover a new type of induction head: *concept-level* induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim (like copying nonsense tokens). These two \"routes\" operate independently: we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. By patching concept induction head outputs, we find that they contain language-independent word representations that mediate natural language translation, suggesting that LLMs represent abstract word meanings independent of language or form.",
        "keywords": "interpretability;induction heads;in-context learning;ICL;detokenization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sheridan Feucht;Eric Todd;Byron C Wallace;David Bau",
        "authorids": "~Sheridan_Feucht1;~Eric_Todd1;~Byron_C_Wallace1;~David_Bau1",
        "gender": "Non-Binary;M;M;M",
        "homepage": "https://sfeucht.github.io/;https://ericwtodd.github.io/;http://www.byronwallace.com/;https://baulab.info/",
        "dblp": "306/1619;162/6042;00/8247;47/3614",
        "google_scholar": ";o12WPZEAAAAJ;KTzRHmwAAAAJ;CYI6cKgAAAAJ",
        "orcid": ";0009-0008-7858-4823;;0000-0003-1744-6765",
        "linkedin": "sheridan-feucht/;eric-w-todd/;;david-bau-4b8130/",
        "or_profile": "~Sheridan_Feucht1;~Eric_Todd1;~Byron_C_Wallace1;~David_Bau1",
        "aff": "Northeastern University;Northeastern University;Northeastern University+Brown University+Northeastern University;Northeastern University",
        "aff_domain": "northeastern.edu;northeastern.edu;+brown.edu+northeastern.edu;northeastern.edu",
        "position": "PhD student;PhD student;Assistant Professor++Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nfeucht2025the,\ntitle={The Dual-Route Model of Induction},\nauthor={Sheridan Feucht and Eric Todd and Byron C Wallace and David Bau},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bNTrKqqnG9}\n}",
        "github": "",
        "project": "",
        "reviewers": "NwaB;sPEp;3CWa;ZnwZ",
        "site": "https://openreview.net/forum?id=bNTrKqqnG9",
        "pdf_size": 0,
        "rating": "5;5;8;9",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.7853571071357126
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "bYu4DOqRY8",
        "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Personalizing large language models (LLMs) to accommodate diverse user preferences is essential for enhancing alignment and user satisfaction. Traditional reinforcement learning from human feedback (RLHF) approaches often rely on monolithic value representations, limiting their ability to adapt to individual preferences. We introduce a novel framework that leverages low-rank preference modeling to efficiently learn and generalize user-specific reward functions. By representing reward functions in a low-dimensional subspace and modeling individual preferences as weighted combinations of shared basis functions, our approach avoids rigid user categorization while enabling scalability and few-shot adaptation. We validate our method on multiple preference datasets, demonstrating superior generalization to unseen users and improved accuracy in preference prediction tasks.",
        "keywords": "preference learning;personalization;reward modeling;plurality",
        "primary_area": "",
        "supplementary_material": "/attachment/378b641524b06586290a0a9e85ffb9de08e61924.zip",
        "author": "Avinandan Bose;Zhihan Xiong;Yuejie Chi;Simon Shaolei Du;Lin Xiao;Maryam Fazel",
        "authorids": "~Avinandan_Bose1;~Zhihan_Xiong1;~Yuejie_Chi1;~Simon_Shaolei_Du1;~Lin_Xiao1;~Maryam_Fazel1",
        "gender": "M;M;;M;;F",
        "homepage": "https://avinandan22.github.io/;https://homes.cs.washington.edu/~zhihanx/;;http://simonshaoleidu.com;;",
        "dblp": "305/7490;255/6096;;176/5602;;10/2309",
        "google_scholar": "https://scholar.google.com/citations?pli=1;OsSiEMEAAAAJ;;OttawxUAAAAJ;vK0-CDcAAAAJ;vlN_kRoAAAAJ",
        "orcid": ";;;;0000-0002-9759-3898;",
        "linkedin": ";zhihan-xiong/;;;;",
        "or_profile": "~Avinandan_Bose1;~Zhihan_Xiong1;~Yuejie_Chi1;~Simon_Shaolei_Du1;~Lin_Xiao1;~Maryam_Fazel1",
        "aff": "Department of Computer Science;University of Washington;;University of Washington;Meta Facebook;University of Washington, Seattle",
        "aff_domain": "cs.washington.edu;washington.edu;;washington.edu;meta.com;uw.edu",
        "position": "PhD student;PhD student;;Assistant Professor;Research Scientist;Full Professor",
        "bibtex": "@inproceedings{\nbose2025lore,\ntitle={LoRe: Personalizing {LLM}s via Low-Rank Reward Modeling},\nauthor={Avinandan Bose and Zhihan Xiong and Yuejie Chi and Simon Shaolei Du and Lin Xiao and Maryam Fazel},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bYu4DOqRY8}\n}",
        "github": "",
        "project": "",
        "reviewers": "YesT;q8WV;Vjsj;yyA8",
        "site": "https://openreview.net/forum?id=bYu4DOqRY8",
        "pdf_size": 0,
        "rating": "5;7;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "bdCWK4NkK7",
        "title": "Hawkeye: Model Collaboration for Efficient Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to excessive intermediate reasoning tokens, which introduce both semantic redundancy and unnecessarily detailed reasoning steps. Moreover, the computational expense and latency remain high, as the cost is determined by the number of output tokens, which encompasses these intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for high-quality responses. Inspired by this, we propose Hawkeye, a novel post-training and inference framework where a large model produce concise CoT instructions to guide a smaller model in response generation. Hawkeye quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, Hawkeye is able to expand responses while reducing token usage and computational cost significantly. Our evaluation results show that Hawkeye can achieve comparable response quality using only 35\\% of the complete CoTs while improving clarity, coherence, and conciseness by approximately 10\\%. Furthermore, Hawkeye can accelerate end-to-end reasoning by up to 3.4\u00d7 on complex math tasks while saving up tp 60\\% inference cost. Hawkeye will be open-sourced and the models will be available soon.",
        "keywords": "reinforcement learning (with human feedback);fine-tuning;compression;decoding algorithms;reasoning algorithms",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jianshu She;Zhuohao Li;Zhemin Huang;Qi Li;Peiran Xu;Haonan Li;Qirong Ho",
        "authorids": "~Jianshu_She1;~Zhuohao_Li3;~Zhemin_Huang1;~Qi_Li39;~Peiran_Xu2;~Haonan_Li2;~Qirong_Ho1",
        "gender": "M;M;;M;;M;",
        "homepage": ";;https://github.com/xtommy-1;https://muqi1029.github.io/;;https://haonan-li.github.io/;",
        "dblp": ";;;;;218/7270.html;13/7590",
        "google_scholar": "e7KrLSEAAAAJ;hZrMIzMAAAAJ;;;;IqfgexsAAAAJ;tR3AZbwAAAAJ",
        "orcid": ";;;;;0000-0001-6623-5089;",
        "linkedin": ";;;;;haonan-li-809709b9/;",
        "or_profile": "~Jianshu_She1;~Zhuohao_Li3;~Zhemin_Huang1;~Qi_Li39;~Peiran_Xu2;~Haonan_Li2;~Qirong_Ho1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence;University of California, Los Angeles;Stanford University;East China Normal University;;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence+Petuum, Inc.",
        "aff_domain": "mbzuai.ac.ae;ucla.edu;stanford.edu;ecnu.edu.cn;;mbzuai.ac.ae;mbzuai.ac.ae+petuum.com",
        "position": "PhD student;PhD student;MS student;Undergrad student;;Postdoc;Assistant Professor+CTO",
        "bibtex": "@inproceedings{\nshe2025hawkeye,\ntitle={Hawkeye: Model Collaboration for Efficient Reasoning},\nauthor={Jianshu She and Zhuohao Li and Zhemin Huang and Qi Li and Peiran Xu and Haonan Li and Qirong Ho},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bdCWK4NkK7}\n}",
        "github": "",
        "project": "",
        "reviewers": "9sDf;zrRu;fxiX;rwTc;kmZC",
        "site": "https://openreview.net/forum?id=bdCWK4NkK7",
        "pdf_size": 0,
        "rating": "6;6;6;6;7",
        "confidence": "3;4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.2,
            0.39999999999999997
        ],
        "confidence_avg": [
            3.8,
            0.39999999999999997
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.2500000000000001
    },
    {
        "id": "bkWERVKzuP",
        "title": "Yourbench: Dynamic Evaluation Set Generation with LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have rapidly outpaced traditional evaluation methodologies, with static benchmarks suffering from saturation, contamination, and domain-specificity limitations while human evaluation remains prohibitively expensive. We present YourBench, an open-source framework that transforms this evaluation paradigm by enabling automated generation of reliable, contamination-free benchmarks directly from user-provided documents without human annotation. To validate our approach, we successfully reproduce the challenging MMLU-Pro benchmark across 86 models spanning 400M to 405B parameters, achieving remarkable Pearson correlations of 0.91-0.99 while generating entirely novel questions for under $15 per model. This demonstrates that dynamically generated evaluations can match the discriminative power of expert-curated benchmarks while eliminating contamination risks. YourBench enables researchers to create domain-specific benchmarks in minutes rather than months. We demonstrate applications in agriculture, personalized education, and RAG training that were previously infeasible. By releasing the YourBench library, Tempora-0325 dataset, 150K+ generated QA pairs, and all evaluation traces, we provide the community with a practical solution to the challenge of keeping pace with rapidly evolving model capabilities.",
        "keywords": "benchmarking;contemporary dataset;dataset;reference-free;automated;llm",
        "primary_area": "",
        "supplementary_material": "/attachment/6f599491b12fd956502386d694a034247fe66463.zip",
        "author": "Sumuk Shashidhar;Cl\u00e9mentine Fourrier;Alina Lozovskaya;Thomas Wolf;Gokhan Tur;Dilek Hakkani-T\u00fcr",
        "authorids": "~Sumuk_Shashidhar1;~Cl\u00e9mentine_Fourrier1;~Alina_Lozovskaya1;~Thomas_Wolf1;~Gokhan_Tur2;~Dilek_Hakkani-T\u00fcr1",
        "gender": "M;;;M;;",
        "homepage": "https://sumuk.org;;;https://thomwolf.io;;",
        "dblp": "358/8879.html;;;;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;D2H5EFEAAAAJ;;",
        "orcid": "0000-0002-8265-9946;;;;;",
        "linkedin": "https://linkedin.com/in/sumuks;;;;;",
        "or_profile": "~Sumuk_Shashidhar1;~Cl\u00e9mentine_Fourrier1;~Alina_Lozovskaya1;~Thomas_Wolf1;~Gokhan_Tur2;~Dilek_Hakkani-T\u00fcr1",
        "aff": "University of Illinois, Urbana-Champaign;;;Hugging Face;;",
        "aff_domain": "cs.illinois.edu;;;huggingface.co;;",
        "position": "Undergrad student;;;Researcher;;",
        "bibtex": "@inproceedings{\nshashidhar2025yourbench,\ntitle={Yourbench: Dynamic Evaluation Set Generation with {LLM}s},\nauthor={Sumuk Shashidhar and Cl{\\'e}mentine Fourrier and Alina Lozovskaya and Thomas Wolf and Gokhan Tur and Dilek Hakkani-T{\\\"u}r},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=bkWERVKzuP}\n}",
        "github": "",
        "project": "",
        "reviewers": "pLCc;ERbz;SnWz;i1Fe",
        "site": "https://openreview.net/forum?id=bkWERVKzuP",
        "pdf_size": 0,
        "rating": "2;6;7;7",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            2.0615528128088303
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.14002800840280097
    },
    {
        "id": "c05qIG1Z2B",
        "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a ``thinking'' phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves >70% win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.",
        "keywords": "diffusion;latent diffusion;language generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Justin Lovelace;Christian K Belardi;Sofian Zalouk;Adhitya Polavaram;Srivatsa R Kundurthy;Kilian Q Weinberger",
        "authorids": "~Justin_Lovelace1;~Christian_K_Belardi1;~Sofian_Zalouk1;~Adhitya_Polavaram1;~Srivatsa_R_Kundurthy1;~Kilian_Q_Weinberger1",
        "gender": "M;M;M;;M;M",
        "homepage": "https://justinlovelace.github.io/;;https://github.com/szalouk;;https://github.com/srivatsa-kundurthy;http://www.cs.cornell.edu/~kilian/",
        "dblp": "251/9496;;;;;88/4801",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;mU5D8d4AAAAJ;;;jsxk8vsAAAAJ",
        "orcid": ";0000-0002-4782-3587;;;;0009-0008-9313-7239",
        "linkedin": ";christian-belardi-bb07a2138/;;adhitya-polavaram/;;",
        "or_profile": "~Justin_Lovelace1;~Christian_K_Belardi1;~Sofian_Zalouk1;~Adhitya_Polavaram1;~Srivatsa_R_Kundurthy1;~Kilian_Q_Weinberger1",
        "aff": "Cornell University;Cornell University;Department of Computer Science, Cornell University;Cornell University;Department of Computer Science, Cornell University+Scale AI;Cornell University+ASAPP Inc.",
        "aff_domain": "cornell.edu;cornell.edu;cs.cornell.edu;cornell.edu;cs.cornell.edu+scale.com;cornell.edu+asapp.com",
        "position": "PhD student;PhD student;PhD student;Undergrad student;Undergrad student+Intern;Professor+Principal Researcher",
        "bibtex": "@inproceedings{\nlovelace2025stopthinkautoregress,\ntitle={Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning},\nauthor={Justin Lovelace and Christian K Belardi and Sofian Zalouk and Adhitya Polavaram and Srivatsa R Kundurthy and Kilian Q Weinberger},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=c05qIG1Z2B}\n}",
        "github": "",
        "project": "",
        "reviewers": "iRQd;fPq9;2MBE;UtyY",
        "site": "https://openreview.net/forum?id=c05qIG1Z2B",
        "pdf_size": 0,
        "rating": "1;6;6;7",
        "confidence": "5;4;3;3",
        "wc_review": "",
        "rating_avg": [
            5.0,
            2.345207879911715
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.899954085146515
    },
    {
        "id": "c0RsezY2D1",
        "title": "LLMs Are In-Context Bandit Reinforcement Learners",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.",
        "keywords": "In-context reinforcement learning;in-context learning;contextual bandits;online learning;large language models",
        "primary_area": "",
        "supplementary_material": "/attachment/856e277943bbf727353282951700d1741ae0833b.zip",
        "author": "Giovanni Monea;Antoine Bosselut;Kiant\u00e9 Brantley;Yoav Artzi",
        "authorids": "~Giovanni_Monea1;~Antoine_Bosselut1;~Kiant\u00e9_Brantley2;~Yoav_Artzi1",
        "gender": ";M;;",
        "homepage": "https://giovannimonea.com;https://atcbosselut.github.io/;;",
        "dblp": ";184/3742;;",
        "google_scholar": "WTJIcf4AAAAJ;XD9hkJwAAAAJ;;",
        "orcid": ";;;",
        "linkedin": "giovanni-monea/;;;",
        "or_profile": "~Giovanni_Monea1;~Antoine_Bosselut1;~Kiant\u00e9_Brantley2;~Yoav_Artzi1",
        "aff": "Cornell University;EPFL;;",
        "aff_domain": "cornell.edu;epfl.ch;;",
        "position": "PhD student;Assistant Professor;;",
        "bibtex": "@inproceedings{\nmonea2025llms,\ntitle={{LLM}s Are In-Context Bandit Reinforcement Learners},\nauthor={Giovanni Monea and Antoine Bosselut and Kiant{\\'e} Brantley and Yoav Artzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=c0RsezY2D1}\n}",
        "github": "",
        "project": "",
        "reviewers": "dGPZ;cGRh;WrPe",
        "site": "https://openreview.net/forum?id=c0RsezY2D1",
        "pdf_size": 0,
        "rating": "4;6;7",
        "confidence": "3;3;5",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            1.247219128924647
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.9428090415820634
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7559289460184546
    },
    {
        "id": "cAFxSuXQvT",
        "title": "DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Speculative Decoding (SD) is a widely used approach to accelerate the inference of large language models (LLMs) without reducing generation quality. It operates by first using a compact model to draft multiple tokens efficiently, followed by parallel verification using the target LLM. This approach leads to faster inference compared to auto-regressive decoding. While there are multiple approaches to create a draft model, one promising approach is to use early-exit methods. These methods draft candidate tokens by using a subset of layers of the primary model and applying the remaining layers for verification, allowing a single model to handle both drafting and verification. While this technique reduces memory usage and computational cost, its performance relies on the choice of the exit layer for drafting and the number of tokens drafted (speculation length) in each SD round. Prior works use hyperparameter exploration to statically select these values. However, our evaluations show that these hyperparameter values are task-specific, and even within a task they are dependent on the current sequence context. We introduce DEL (Dynamic Exit Layer), a plug-and-play method that adaptively selects the exit layer and speculation length during inference. DEL dynamically tracks the token acceptance rate if the tokens are drafted at each layer of an LLM and uses that knowledge to heuristically select the optimal exit layer and speculation length. Our experiments across a broad range of models and downstream tasks show that DEL achieves overall speedups of $2.16\\times$$\\sim$$2.62\\times$ over vanilla auto-regressive decoding and improves upon state-of-the-art SD methods, which peak at $2.43\\times$, by up to $0.19\\times$. The code is available at https://github.com/hoenza/DEL.",
        "keywords": "Speculative Decoding;Efficient Large Language Model;Inference Acceleration",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hossein Entezari Zarch;Lei Gao;Chaoyi Jiang;Murali Annavaram",
        "authorids": "~Hossein_Entezari_Zarch1;~Lei_Gao3;~Chaoyi_Jiang1;~Murali_Annavaram1",
        "gender": "M;M;;M",
        "homepage": "https://hoenza.github.io/;;;http://annavar.am",
        "dblp": "304/2599.html;;;02/5812",
        "google_scholar": "xhVKvhIAAAAJ;TxzNHuIAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0002-4633-6867",
        "linkedin": "hossein-entezari/;;;",
        "or_profile": "~Hossein_Entezari_Zarch1;~Lei_Gao3;~Chaoyi_Jiang1;~Murali_Annavaram1",
        "aff": "University of Southern California;University of Southern California;;University of Southern California",
        "aff_domain": "usc.edu;usc.edu;;usc.edu",
        "position": "PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nzarch2025del,\ntitle={{DEL}: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding},\nauthor={Hossein Entezari Zarch and Lei Gao and Chaoyi Jiang and Murali Annavaram},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cAFxSuXQvT}\n}",
        "github": "",
        "project": "",
        "reviewers": "GrNu;TsAY;wiH3;SrG8",
        "site": "https://openreview.net/forum?id=cAFxSuXQvT",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "cCYWeCzAv0",
        "title": "MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "State-space models (SSMs) have recently attention as an efficient alternative to computationally expensive attention-based models for sequence modeling. They rely on linear recurrences to integrate information over time, enabling fast inference, parallelizable training, and control over recurrence stability.  However, traditional SSMs often suffer from limited effective memory, requiring larger state sizes for improved recall. Moreover, existing SSMs struggle to capture multi-scale dependencies, which are essential for modeling complex structures in time series, images, and natural language. This paper introduces a multi-scale SSM framework that addresses these limitations by representing sequence dynamics across multiple resolution and processing each resolution with specialized state-space dynamics.  By capturing both fine-grained, high-frequency patterns and coarse, global trends, MS-SSM enhances memory efficiency and long-range modeling. We further introduce an input-dependent scale-mixer, enabling dynamic information fusion across resolutions.\nThe proposed approach significantly improves sequence modeling, particularly in long-range and hierarchical tasks, while maintaining computational efficiency. Extensive experiments on benchmarks, including Long Range Arena, hierarchical reasoning, time series classification, and image recognition, demonstrate that MS-SSM consistently outperforms prior SSM-based models, highlighting the benefits of multi-resolution processing in state-space architectures.",
        "keywords": "MS-SSM: Sequence Models;Language models;State Space Model;Multi-Scale;Multi-Resolution",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mahdi Karami;Ali Behrouz;Peilin Zhong;Razvan Pascanu;Vahab Mirrokni",
        "authorids": "~Mahdi_Karami2;~Ali_Behrouz1;~Peilin_Zhong1;~Razvan_Pascanu1;~Vahab_Mirrokni2",
        "gender": "M;M;M;M;M",
        "homepage": "https://karami-m.github.io/;https://Abehrouz.github.io;http://www.cs.columbia.edu/~peilin/;https://razp.info;https://people.csail.mit.edu/mirrokni/Welcome.html",
        "dblp": "90/394.html;220/4163;148/9632;65/8368.html;m/VahabSMirrokni",
        "google_scholar": "https://scholar.google.com/citations?hl=en;UbwVuqIAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=eSPY8LwAAAAJ;opbZfw0AAAAJ",
        "orcid": ";;;;",
        "linkedin": "mahdi-karami-2957412a/;ali-behrouz-506aa2127;;;",
        "or_profile": "~Mahdi_Karami2;~Ali_Behrouz1;~Peilin_Zhong1;~Razvan_Pascanu1;~Vahab_Mirrokni2",
        "aff": "Research, Google+University of Waterloo;Google+Cornell University;Google;Mila - Quebec Artificial Intelligence Institute+Google DeepMind;Google Research",
        "aff_domain": "research.google.com+cs.uwaterloo.ca;google.com+cornell.edu;google.com;mila.quebec+google.com;google.com",
        "position": "Researcher+Researcher;Intern+PhD student;Researcher;Affiliate Member+Research Scientist;VP, Google Fellow",
        "bibtex": "@inproceedings{\nkarami2025msssm,\ntitle={{MS}-{SSM}: A Multi-Scale State Space Model for Efficient Sequence Modeling},\nauthor={Mahdi Karami and Ali Behrouz and Peilin Zhong and Razvan Pascanu and Vahab Mirrokni},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cCYWeCzAv0}\n}",
        "github": "",
        "project": "",
        "reviewers": "x51h;Rtcz;FfTf;xBk6",
        "site": "https://openreview.net/forum?id=cCYWeCzAv0",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "cOlHP5E3qF",
        "title": "Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Supervised fine-tuning (SFT) has emerged as a crucial method for aligning large language models (LLMs) with human-annotated demonstrations. However, SFT, being an off-policy approach similar to behavior cloning, often struggles with overfitting and poor out-of-domain generalization, especially in limited-data scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel fine-tuning method that leverages on-policy techniques to enhance generalization performance. Our approach combines the strengths of SFT and proximal policy optimization (PPO) to achieve more effective alignment from demonstration data. At its core is a reward function designed as the log policy ratio between the SFT model and the pretrained base model. This function serves as an implicit reward signal, using the pretrained policy as a baseline and the SFT policy as a target. By doing so, it enables on-policy fine-tuning without relying on human preference annotations. The integration of this self-rewarding mechanism with PPO addresses key limitations of SFT, improving generalization, data efficiency, and robustness. Our empirical evaluation across a range of natural language processing tasks demonstrates that Self-Rewarding PPO consistently outperforms traditional SFT methods. The results highlight the effectiveness of our approach in aligning LLMs using demonstration data, particularly in scenarios where high-quality annotated data is scarce.",
        "keywords": "Alignment with Demonstration;Self-Rewarding PPO;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Qingru Zhang;Liang Qiu;Ilgee Hong;Zhenghao Xu;Tianyi Liu;Shiyang Li;Rongzhi Zhang;Zheng Li;Lihong Li;Bing Yin;Chao Zhang;Jianshu Chen;Haoming Jiang;Tuo Zhao",
        "authorids": "~Qingru_Zhang2;~Liang_Qiu2;~Ilgee_Hong1;~Zhenghao_Xu1;~Tianyi_Liu2;~Shiyang_Li1;~Rongzhi_Zhang2;~Zheng_Li9;~Lihong_Li1;~Bing_Yin1;~Chao_Zhang15;~Jianshu_Chen1;~Haoming_Jiang1;~Tuo_Zhao2",
        "gender": "M;M;M;M;M;;M;;;M;;M;M;",
        "homepage": "https://qingruzhang.github.io/;https://www.lqiu.info/;https://ilgeehong.github.io/;https://www.isye.gatech.edu/users/zhenghao-xu;https://sites.google.com/view/tianyiliu/home;;https://rongzhizhang.org/;;https://lihongli.github.io;;http://chaozhang.org/;https://chenjianshu.github.io/;https://hmjianggatech.github.io;",
        "dblp": "228/6749;01/1198-1;;357/5585;;;130/7337;;l/LihongLi.html;;94/3019-14;11/3124;230/3684;",
        "google_scholar": "7YM-faYAAAAJ;mr1VxDwAAAAJ;;FRegzp4AAAAJ;;;https://scholar.google.com/citations?hl=en;;Rqy5KDEAAAAJ;qSOxydEAAAAJ;https://scholar.google.com/citations?hl=en;jQeFWdoAAAAJ;XaFhuG8AAAAJ;",
        "orcid": ";0000-0001-9904-2953;;0000-0001-8076-5166;;;;;;0000-0002-5890-0031;0000-0003-3009-598X;;;",
        "linkedin": "qingru-zhang-4b789a187;liangqiu/;;;;;;;lihong-li-9620164;bingyin;;;;",
        "or_profile": "~Qingru_Zhang2;~Liang_Qiu2;~Ilgee_Hong1;~Zhenghao_Xu1;~Tianyi_Liu2;~Shiyang_Li1;~Rongzhi_Zhang2;~Zheng_Li9;~Lihong_Li1;~Bing_Yin1;~Chao_Zhang15;~Jianshu_Chen1;~Haoming_Jiang1;~Tuo_Zhao2",
        "aff": "Georgia Institute of Technology;Amazon;Georgia Institute of Technology+Amazon;Georgia Institute of Technology;Amazon;;Georgia Institute of Technology+Zhejiang University;;Amazon;Amazon;Georgia Institute of Technology;Amazon;OpenAI+Amazon;",
        "aff_domain": "gatech.edu;amazon.com;gatech.edu+amazon.com;gatech.edu;amazon.com;;gatech.edu+zju.edu.cn;;amazon.com;amazon.com;gatech.edu;amazon.com;openai.com+amazon.com;",
        "position": "PhD student;Applied Scientist;PhD student+Intern;PhD student;Researcher;;PhD student+Undergrad student;;Senior Principal Scientist;Senior Science Manager;Assistant Professor;Principal Scientist;Researcher+Principal Researcher;",
        "bibtex": "@inproceedings{\nzhang2025selfrewarding,\ntitle={Self-Rewarding {PPO}: Aligning Large Language Models with Demonstrations Only},\nauthor={Qingru Zhang and Liang Qiu and Ilgee Hong and Zhenghao Xu and Tianyi Liu and Shiyang Li and Rongzhi Zhang and Zheng Li and Lihong Li and Bing Yin and Chao Zhang and Jianshu Chen and Haoming Jiang and Tuo Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cOlHP5E3qF}\n}",
        "github": "",
        "project": "",
        "reviewers": "mRvM;zG4C;vgGq;m2aH;fYjb",
        "site": "https://openreview.net/forum?id=cOlHP5E3qF",
        "pdf_size": 0,
        "rating": "4;5;6;6;6",
        "confidence": "4;3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            5.4,
            0.7999999999999999
        ],
        "confidence_avg": [
            3.2,
            0.39999999999999997
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8750000000000003
    },
    {
        "id": "cQechnXCQt",
        "title": "Approximating Language Model Training Data from Weights",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern language models often have open weights but closed training data. We formalize the problem of data recovery from model weights and propose several baselines and metrics. We develop a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering data given only weights of the original and finetuned models. The training subset pinpointed by our method in a large corpus can be used to train another model to comparable performance. Even when none of the true training data is available, data selected by our method from publicly available Web documents can be used to train a competent model.",
        "keywords": "inversion;training data reconstruction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "John Xavier Morris;Junjie Oscar Yin;Woojeong Kim;Vitaly Shmatikov;Alexander M Rush",
        "authorids": "~John_Xavier_Morris1;~Junjie_Oscar_Yin1;~Woojeong_Kim1;~Vitaly_Shmatikov1;~Alexander_M_Rush1",
        "gender": "M;;F;;M",
        "homepage": "http://jxmo.io;;https://sites.google.com/view/woojeongkim/;;http://rush.seas.harvard.edu/",
        "dblp": "263/9958.html;;243/0064;;http://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=",
        "google_scholar": "Utsbve4AAAAJ;;fGCEQQgAAAAJ;;LIjnUGgAAAAJ",
        "orcid": ";;;;0000-0002-9900-1606",
        "linkedin": ";;woojeong-kim-072ab4160/;;sasha-rush-a69b6917/",
        "or_profile": "~John_Xavier_Morris1;~Junjie_Oscar_Yin1;~Woojeong_Kim1;~Vitaly_Shmatikov1;~Alexander_M_Rush1",
        "aff": "Cornell University;;Cornell University;;Cornell University+School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "cornell.edu;;cornell.edu;;cornell.edu+seas.harvard.edu",
        "position": "PhD student;;PhD student;;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nmorris2025approximating,\ntitle={Approximating Language Model Training Data from Weights},\nauthor={John Xavier Morris and Junjie Oscar Yin and Woojeong Kim and Vitaly Shmatikov and Alexander M Rush},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cQechnXCQt}\n}",
        "github": "",
        "project": "",
        "reviewers": "T5hs;pFEW;cimo;N6vu",
        "site": "https://openreview.net/forum?id=cQechnXCQt",
        "pdf_size": 0,
        "rating": "6;6;6;8",
        "confidence": "2;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "cRE1XrHf1h",
        "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) often develop learned mechanisms specialized to specific datasets, such as reliance on domain-specific correlations, which yield high-confidence predictions without generalizable reasoning. While beneficial in one setting, these dataset-specific mechanisms typically degrade performance when models encounter novel tasks or distributions. In this work, we introduce a fine-tuning approach designed to enhance generalization by identifying and pruning neurons associated with dataset-specific mechanisms in transformer-based LLMs. Our method employs Integrated Gradients to quantify each neuron\u2019s influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning. Selectively pruning these neurons compels the model to depend on generalizable representations. Evaluated across multiple-choice benchmarks, our pruning-based fine-tuning significantly enhances performance, surpassing prior (non-pruning) adaptation methods.",
        "keywords": "LLMs;spurious correlations;Integrated Gradients;generalization;model adaptation",
        "primary_area": "",
        "supplementary_material": "/attachment/0f250681dff84bc33314f010f39c137641aa6d12.zip",
        "author": "Ameen Ali Ali;Shahar Katz;Lior Wolf;Ivan Titov",
        "authorids": "~Ameen_Ali_Ali1;~Shahar_Katz1;~Lior_Wolf1;~Ivan_Titov1",
        "gender": "M;;M;",
        "homepage": "https://www.github.com/ameenali;;http://www.cs.tau.ac.il/~wolf;http://ivan-titov.org",
        "dblp": "280/0176;;83/4103;08/5391",
        "google_scholar": "p6C3GgoAAAAJ;;UbFrXTsAAAAJ;https://scholar.google.nl/citations?user=FKUc3vsAAAAJ",
        "orcid": ";;0000-0001-5578-8892;",
        "linkedin": "ameen-ali-7232589a/;;;",
        "or_profile": "~Ameen_Ali_Ali1;~Shahar_Katz1;~Lior_Wolf1;~Ivan_Titov1",
        "aff": "Tel Aviv University;;Tel Aviv University+Tel Aviv University+Tel Aviv University;University of Edinburgh+University of Amsterdam",
        "aff_domain": "tau.ac.il;;tau.ac.il+tau.ac.il+tau.ac.il;ed.ac.uk+uva.nl",
        "position": "PhD student;;Associate Professor+Professor+Full Professor;Full Professor+Associate Professor",
        "bibtex": "@inproceedings{\nali2025detecting,\ntitle={Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models},\nauthor={Ameen Ali Ali and Shahar Katz and Lior Wolf and Ivan Titov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cRE1XrHf1h}\n}",
        "github": "",
        "project": "",
        "reviewers": "XAzb;u6ZY;PXUN;XM36",
        "site": "https://openreview.net/forum?id=cRE1XrHf1h",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "2;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7071067811865475
    },
    {
        "id": "cWVpXWARbt",
        "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pretrained vision-language models (VLMs) such as CLIP excel in general multimodal comprehension but often struggle to capture nuanced, context-dependent visual cues. This makes it difficult to distinguish between similar-looking concepts with potentially different cultural meanings. Such deficiencies are mainly due to a limited amount of high-quality cultural data, contextual information, and the lack of negative examples that highlight subtle differences. To mitigate this, we design a data curation pipeline leveraging open-sourced VLMs and text-to-image models to construct CulTwin, a synthetic cultural dataset. This dataset consists of paired concept-caption-image triplets, where concepts visually resemble each other but are culturally different. Then, we fine-tune CLIP on CulTwin to develop CultureCLIP, which aligns cultural concepts with contextually enhanced captions and synthetic images through tailored contrastive learning. Experiments on culture-specific benchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a notable 5.49\\% improvement in fine-grained concept recognition on certain tasks while preserving CLIP's original generalization ability, validating the effectiveness of our data synthesis and VLM backbone training paradigm in capturing subtle cultural distinctions.",
        "keywords": "Vision-Language Models;Cultural Understanding;Fine-Grained Recognition;Contextual Knowledge;Synthetic Data Generation;Contrastive Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuchen Huang;Zhiyuan Fan;Zhitao He;Sandeep Polisetty;Wenyan Li;Yi R. Fung",
        "authorids": "~Yuchen_Huang4;~Zhiyuan_Fan2;~Zhitao_He1;~Sandeep_Polisetty1;~Wenyan_Li1;~Yi_R._Fung1",
        "gender": "M;M;M;M;F;",
        "homepage": "https://lukahhcm.github.io/;https://zhiyuan.fan;;http://sandeep06011991.github.io;https://wenyanli.org/;",
        "dblp": ";210/1532;;274/2983;21/6731-1;",
        "google_scholar": ";;ULvoYXgAAAAJ;9atU6PIAAAAJ;JvcZHCsAAAAJ;",
        "orcid": ";;0009-0003-3317-1260;0009-0001-7167-899X;0000-0001-7143-4453;",
        "linkedin": ";;;sandeep-polisetty-b0737425/;;",
        "or_profile": "~Yuchen_Huang4;~Zhiyuan_Fan2;~Zhitao_He1;~Sandeep_Polisetty1;~Wenyan_Li1;~Yi_R._Fung1",
        "aff": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;University of Massachusetts, Amherst;ETHZ - ETH Zurich+University of Copenhagen;",
        "aff_domain": "hkust.edu;ust.hk;connect.ust.hk;umass.edu;ethz.ch+di.ku;",
        "position": "PhD student;Research Postgraduate;PhD student;PhD student;Researcher+PhD student;",
        "bibtex": "@inproceedings{\nhuang2025cultureclip,\ntitle={Culture{CLIP}: Empowering {CLIP} with Cultural Awareness through Synthetic Images and Contextualized Captions},\nauthor={Yuchen Huang and Zhiyuan Fan and Zhitao He and Sandeep Polisetty and Wenyan Li and Yi R. Fung},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=cWVpXWARbt}\n}",
        "github": "",
        "project": "",
        "reviewers": "n9zq;25A8;9Kid",
        "site": "https://openreview.net/forum?id=cWVpXWARbt",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "d4XXFVAlV7",
        "title": "Teach Old SAEs New Domain Tricks with Boosting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.",
        "keywords": "LLM;Interpretability;SAE",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nikita Koriagin;Yaroslav Aksenov;Daniil Laptev;Gleb Gerasimov;Nikita Balagansky;Daniil Gavrilov",
        "authorids": "~Nikita_Koriagin1;~Yaroslav_Aksenov1;~Daniil_Laptev1;~Gleb_Gerasimov1;~Nikita_Balagansky3;~Daniil_Gavrilov1",
        "gender": ";M;M;M;M;",
        "homepage": ";https://github.com/yaraksen;https://github.com/DaniilLaptev;https://github.com/humdinger-g;;https://kefirski.me",
        "dblp": ";369/4488;399/4792;399/9357;318/0989;234/8563",
        "google_scholar": ";https://scholar.google.ru/citations?user=PEYS8bAAAAAJ;;;https://scholar.google.com/citations?authuser=1;https://scholar.google.ru/citations?user=PAZUwukAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "https://linkedin.com/in/nikoryagin/;;;;;",
        "or_profile": "~Nikita_Koriagin1;~Yaroslav_Aksenov1;~Daniil_Laptev1;~Gleb_Gerasimov1;~Nikita_Balagansky3;~Daniil_Gavrilov1",
        "aff": "T-Bank AI Research;T-Tech;T-Tech+Kuban State University;T-Tech+Higher School of Economics;T-Tech+Moscow Institute of Physics and Technology;T-Bank",
        "aff_domain": "tbank.ru;tbank.ru;tbank.ru+kubsu.ru;tbank.ru+edu.hse.ru;tbank.ru+phystech.edu;tbank.ru",
        "position": "Researcher;Researcher;Researcher+MS student;Researcher+Undergrad student;Researcher+PhD student;Principal Researcher",
        "bibtex": "@inproceedings{\nkoriagin2025teach,\ntitle={Teach Old {SAE}s New Domain Tricks with Boosting},\nauthor={Nikita Koriagin and Yaroslav Aksenov and Daniil Laptev and Gleb Gerasimov and Nikita Balagansky and Daniil Gavrilov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=d4XXFVAlV7}\n}",
        "github": "",
        "project": "",
        "reviewers": "F4JZ;MGrV;Zjaf",
        "site": "https://openreview.net/forum?id=d4XXFVAlV7",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "d9EkgbZZH9",
        "title": "You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The goal of translation, be it by human or by machine, is, given some text in a source language, to produce text in a target language that simultaneously 1) preserves the meaning of the source text and 2) achieves natural expression in the target language. However, researchers in the machine translation community usually assess translations using a single score intended to capture semantic accuracy and the naturalness of the output simultaneously. In this paper, we build on recent advances in information theory to mathematically prove and empirically demonstrate that such single-score summaries *do not and cannot* give the complete picture of a system's true performance. Concretely, we prove that a tradeoff exists between accuracy and naturalness and demonstrate it by evaluating the submissions to the WMT24 shared task. Our findings help explain well-known empirical phenomena, such as the observation that optimizing translation systems for a specific accuracy metric (like BLEU) initially improves the system's naturalness, while \u201coverfitting\" the system to the metric can significantly degrade its naturalness. Thus, we advocate for a change in how translations are evaluated: rather than comparing systems using a single number, they should be compared on an *accuracy-naturalness plane*.",
        "keywords": "translation;accuracy;naturalness;tradeoff;distortion;perception;no-reference metric",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Gergely Flamich;David Vilar;Jan-Thorsten Peter;Markus Freitag",
        "authorids": "~Gergely_Flamich1;~David_Vilar1;~Jan-Thorsten_Peter1;~Markus_Freitag2",
        "gender": "M;M;M;M",
        "homepage": "https://gergely-flamich.github.io/;https://research.google/people/david-vilar/;;",
        "dblp": "187/9709;06/6883;69/6938;57/8503",
        "google_scholar": "4Iw9TH8AAAAJ;2cP6vV4AAAAJ;https://scholar.google.de/citations?user=pUS4FG4AAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0009-9831-7455;;;",
        "linkedin": "gergely-flamich-142773102;david-vilar-torres-a6144417/;jan-thorsten-peter-253382a1/;markus-freitag-7b17b4101/",
        "or_profile": "~Gergely_Flamich1;~David_Vilar1;~Jan-Thorsten_Peter1;~Markus_Freitag2",
        "aff": "Imperial College London+University of Cambridge;Google;Google;Google",
        "aff_domain": "imperial.ac.uk+cam.ac.uk;google.com;google.com;google.com",
        "position": "Postdoc+PhD student;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nflamich2025you,\ntitle={You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation},\nauthor={Gergely Flamich and David Vilar and Jan-Thorsten Peter and Markus Freitag},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=d9EkgbZZH9}\n}",
        "github": "",
        "project": "",
        "reviewers": "AhdH;Z5DR;LQ9z;z3nj",
        "site": "https://openreview.net/forum?id=d9EkgbZZH9",
        "pdf_size": 0,
        "rating": "6;7;8;9",
        "confidence": "4;3;4;5",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6324555320336758
    },
    {
        "id": "dNW3RGW0gi",
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. \nHowever, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator $-$ the LLM $-$ through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.",
        "keywords": "Large Language Models;Reinforcement Learning;Evolutionary Search;Algorithm Discovery;Self-Improvement;AI for Math",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anja \u0160urina;Amin Mansouri;Lars C.P.M. Quaedvlieg;Amal Seddas;Maryna Viazovska;Emmanuel Abbe;Caglar Gulcehre",
        "authorids": "~Anja_\u0160urina1;~Amin_Mansouri1;~Lars_C.P.M._Quaedvlieg1;~Amal_Seddas1;~Maryna_Viazovska1;~Emmanuel_Abbe1;~Caglar_Gulcehre1",
        "gender": "F;Not Specified;M;F;F;;M",
        "homepage": ";https://mila.quebec/en/person/amin-mansouri/;https://lars-quaedvlieg.github.io/;https://people.epfl.ch/amal.seddas/?lang=en;;;http://caglarg.com",
        "dblp": "328/8656;340/7700;;;;84/5016;125/2132",
        "google_scholar": "https://scholar.google.com/citations?hl=en;9aNdCL4AAAAJ;f_-rgVcAAAAJ;;;;https://scholar.google.ca/citations?user=7hwJ2ckAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": "anja-%C5%A1urina-423054173/;amansouri3476?original_referer=;lars-quaedvlieg/;;;;",
        "or_profile": "~Anja_\u0160urina1;~Amin_Mansouri1;~Lars_C.P.M._Quaedvlieg1;~Amal_Seddas1;~Maryna_Viazovska1;~Emmanuel_Abbe1;~Caglar_Gulcehre1",
        "aff": "EPFL - EPF Lausanne;School of Computer and Communication Sciences, EPFL - EPF Lausanne;EPFL - EPF Lausanne;EPFL - EPF Lausanne;EPFL - EPF Lausanne;Swiss Federal Institute of Technology Lausanne;Microsoft+EPFL - EPF Lausanne",
        "aff_domain": "epfl.ch;ic.epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch;microsoft.com+epfl.ch",
        "position": "PhD student;PhD student;MS student;MS student;Full Professor;Full Professor;Principal Researcher+EPFL",
        "bibtex": "@inproceedings{\nsurina2025algorithm,\ntitle={Algorithm Discovery With {LLM}s: Evolutionary Search Meets Reinforcement Learning},\nauthor={Anja {\\v{S}}urina and Amin Mansouri and Lars C.P.M. Quaedvlieg and Amal Seddas and Maryna Viazovska and Emmanuel Abbe and Caglar Gulcehre},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dNW3RGW0gi}\n}",
        "github": "",
        "project": "",
        "reviewers": "rxQm;YtJU;e6jZ",
        "site": "https://openreview.net/forum?id=dNW3RGW0gi",
        "pdf_size": 0,
        "rating": "7;8;8",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "dU4Y2sNfJ2",
        "title": "Cutting the Root of Hallucination: Structural Trimming for Vulnerability Mitigation in Code LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce a structural perspective on hallucinations in code-generating language models, framing them as causality anchors in syntax graphs that trigger cascading semantic errors and latent security flaws. This work is the first to systematically connect code hallucinations with vulnerability risks, offering a unified conceptual and practical framework to address them. At the heart of our approach is the notion of hallucination anchors, localized subtrees in the abstract syntax tree (AST) that serve as root causes of defective logic. We propose Structural Trimming (ST), a targeted mitigation method that removes these anchors while preserving functional semantics. To anticipate the effect of trimming, we introduce the Compositional Structural Hallucination Score (CSHS), which quantifies the likelihood that pruning will improve robustness. By grounding error reduction in the syntax graph itself, our method reframes hallucination mitigation as a structured intervention process interpretable, generalizable, and actionable.",
        "keywords": "LLM hallucinations;code generation;program repair;vulnerability mitigation;structural pruning;abstract syntax tree;hallucination detection;CSHS;model-agnostic risk estimation;generative code safety",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yage Zhang",
        "authorids": "~Yage_Zhang2",
        "gender": "F",
        "homepage": "https://laqeey.github.io/",
        "dblp": "",
        "google_scholar": "pv-wG0cAAAAJ",
        "orcid": "",
        "linkedin": "yage-zhang-396ba02a7/?locale=de_DE",
        "or_profile": "~Yage_Zhang2",
        "aff": "CISPA Helmholtz Center for Information Security+Universit\u00e4t des Saarlandes",
        "aff_domain": "cispa.de+uni-saarland.de",
        "position": "PhD student+MS student",
        "bibtex": "@inproceedings{\nzhang2025cutting,\ntitle={Cutting the Root of Hallucination: Structural Trimming for Vulnerability Mitigation in Code {LLM}s},\nauthor={Yage Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dU4Y2sNfJ2}\n}",
        "github": "",
        "project": "",
        "reviewers": "VG3Y;7AVp;DXHf;ZgYG",
        "site": "https://openreview.net/forum?id=dU4Y2sNfJ2",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "5;2;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            1,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.2581988897471611
    },
    {
        "id": "dVqZBagXF3",
        "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought'' Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship'\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector",
        "keywords": "censorship;activation steering;representation engineering;reasoning LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hannah Cyberey;David Evans",
        "authorids": "~Hannah_Cyberey1;~David_Evans1",
        "gender": ";Not Specified",
        "homepage": ";https://www.cs.virginia.edu/evans/",
        "dblp": ";https://dblp.uni-trier.de/pid/e/DavidEvans",
        "google_scholar": ";DsR4PucAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Hannah_Cyberey1;~David_Evans1",
        "aff": ";University of Virginia",
        "aff_domain": ";virginia.edu",
        "position": ";Professor",
        "bibtex": "@inproceedings{\ncyberey2025steering,\ntitle={Steering the CensorShip: Uncovering Representation Vectors for {LLM} ''Thought'' Control},\nauthor={Hannah Cyberey and David Evans},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dVqZBagXF3}\n}",
        "github": "",
        "project": "",
        "reviewers": "hXbe;f3Kh;oUJy;LdFB",
        "site": "https://openreview.net/forum?id=dVqZBagXF3",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "dZRzInscvA",
        "title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their presence in the system summary, has been widely adopted. However, it suffers from a lack of systematicity in the definition and granularity of the sub-units. We address these problems by proposing QAPyramid, which decomposes each reference summary into finer-grained question-answer (QA) pairs according to the QA-SRL framework. We collect QA-SRL annotations for reference summaries from CNN/DM and evaluate 10 summarization systems, resulting in 8.9K QA-level annotations. We show that, compared to Pyramid, QAPyramid provides more systematic and fine-grained content selection evaluation while maintaining high inter-annotator agreement without needing expert annotations. Furthermore, we propose metrics that automate the evaluation pipeline and achieve higher correlations with QAPyra- mid than other widely adopted metrics, allowing future work to accurately and efficiently benchmark summarization systems.",
        "keywords": "Evaluation;Summarization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shiyue Zhang;David Wan;Arie Cattan;Ayal Klein;Ido Dagan;Mohit Bansal",
        "authorids": "~Shiyue_Zhang1;~David_Wan1;~Arie_Cattan1;~Ayal_Klein1;~Ido_Dagan1;~Mohit_Bansal2",
        "gender": "F;M;M;M;M;M",
        "homepage": "https://zhangshiyue.github.io/;;https://ariecattan.github.io/;https://kleinay.github.io/;http://u.cs.biu.ac.il/~dagan/;https://www.cs.unc.edu/~mbansal/",
        "dblp": "186/8393-1.html;17/4695.html;275/3285;252/5395;95/284;32/5243.html",
        "google_scholar": "co9KUGQAAAAJ;oHznAAYAAAAJ;ueEL9eEAAAAJ;RIsnmGMAAAAJ;https://scholar.google.com.tw/citations?user=YzGAGtoAAAAJ;DN8QtscAAAAJ",
        "orcid": ";;;0000-0003-2665-7898;;",
        "linkedin": ";;arie-cattan-8a6907a3/;ayalklein33298a61/;;",
        "or_profile": "~Shiyue_Zhang1;~David_Wan1;~Arie_Cattan1;~Ayal_Klein1;~Ido_Dagan1;~Mohit_Bansal2",
        "aff": "Bloomberg;Department of Computer Science, University of North Carolina at Chapel Hill;Bar Ilan University;Ariel University Center of Samaria+Queen Mary, University of London;Bar-Ilan University;University of North Carolina at Chapel Hill",
        "aff_domain": "bloomberg.net;cs.unc.edu;biu.ac.il;ariel.ac.il+qmul.ac.uk;biu.ac.il;unc.edu",
        "position": "Researcher;PhD student;PhD student;Lecturer+Postdoc;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nzhang2025qapyramid,\ntitle={{QAP}yramid: Fine-grained Evaluation of Content Selection for Text Summarization},\nauthor={Shiyue Zhang and David Wan and Arie Cattan and Ayal Klein and Ido Dagan and Mohit Bansal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dZRzInscvA}\n}",
        "github": "",
        "project": "",
        "reviewers": "u8Yo;YNdU;tvdb;1niJ",
        "site": "https://openreview.net/forum?id=dZRzInscvA",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;3;2",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "dkE5rveDuh",
        "title": "Evaluating LLMs on Chinese Idiom Translation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28\\% of cases. \nWe also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.",
        "keywords": "Large Language Model Evaluation;Chinese Idioms;Meta Analysis;Multilingual Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Cai Yang;Yao Dou;David Heineman;Xiaofeng Wu;Wei Xu",
        "authorids": "~Cai_Yang1;~Yao_Dou1;~David_Heineman1;~Xiaofeng_Wu5;~Wei_Xu5",
        "gender": "M;M;M;M;F",
        "homepage": ";https://yao-dou.github.io/;https://davidheineman.com;;https://cocoxu.github.io/",
        "dblp": ";262/0556;336/4616;;32/1213-4.html",
        "google_scholar": "https://scholar.google.com.au/citations?user=yto0kf8AAAAJ;6_aoS74AAAAJ;JO2Q6CUAAAAJ;;BfOdG-oAAAAJ",
        "orcid": "0000-0001-9923-3530;;;;",
        "linkedin": ";;;xiaofengwugt/;",
        "or_profile": "~Cai_Yang1;~Yao_Dou1;~David_Heineman1;~Xiaofeng_Wu5;~Wei_Xu5",
        "aff": "MPI-SWS;Georgia Institute of Technology;Allen Institute for Artificial Intelligence;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "mpi-sws.org;gatech.edu;allenai.org;gatech.edu;gatech.edu",
        "position": "Intern;PhD student;Researcher;MS student;Associate Professor",
        "bibtex": "@inproceedings{\nyang2025evaluating,\ntitle={Evaluating {LLM}s on Chinese Idiom Translation},\nauthor={Cai Yang and Yao Dou and David Heineman and Xiaofeng Wu and Wei Xu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dkE5rveDuh}\n}",
        "github": "",
        "project": "",
        "reviewers": "j3Fh;9war;vzbR;u3jM",
        "site": "https://openreview.net/forum?id=dkE5rveDuh",
        "pdf_size": 0,
        "rating": "6;6;6;9",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.299038105676658
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "dp4KWuSDzj",
        "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.",
        "keywords": "reinforcement learning;language models;post-training;ppo;pretraining;reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rosie Zhao;Alexandru Meterez;Sham M. Kakade;Cengiz Pehlevan;Samy Jelassi;Eran Malach",
        "authorids": "~Rosie_Zhao1;~Alexandru_Meterez1;~Sham_M._Kakade1;~Cengiz_Pehlevan2;~Samy_Jelassi1;~Eran_Malach3",
        "gender": "F;M;M;;M;",
        "homepage": "https://rosieyzh.github.io/;https://alexandrumeterez.github.io/;https://shamulent.github.io;https://pehlevan.seas.harvard.edu/;https://sjelassi.github.io/;",
        "dblp": "277/9223;;s/SMKakade;145/3480;222/3149;",
        "google_scholar": "rgwbR6wAAAAJ;wSrCMa4AAAAJ;https://scholar.google.com.tw/citations?user=wb-DKCIAAAAJ;veDLTPEAAAAJ;;",
        "orcid": ";;;0000-0001-9767-6063;;",
        "linkedin": "https://linkedin.com/in/rosieyzh;;;;;",
        "or_profile": "~Rosie_Zhao1;~Alexandru_Meterez1;~Sham_M._Kakade1;~Cengiz_Pehlevan2;~Samy_Jelassi1;~Eran_Malach3",
        "aff": "Harvard University;School of Engineering and Applied Sciences, Harvard University;Harvard University;School of Engineering and Applied Sciences, Harvard University;Harvard University;",
        "aff_domain": "g.harvard.edu;seas.harvard.edu;harvard.edu;seas.harvard.edu;harvard.edu;",
        "position": "PhD student;PhD student;Full Professor;Assistant Professor;Postdoc;",
        "bibtex": "@inproceedings{\nzhao2025echo,\ntitle={Echo Chamber: {RL} Post-training Amplifies Behaviors Learned in Pretraining},\nauthor={Rosie Zhao and Alexandru Meterez and Sham M. Kakade and Cengiz Pehlevan and Samy Jelassi and Eran Malach},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dp4KWuSDzj}\n}",
        "github": "",
        "project": "",
        "reviewers": "va6E;9LBw;cgzq",
        "site": "https://openreview.net/forum?id=dp4KWuSDzj",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "5;4;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "dr3eg5ehR2",
        "title": "Learning to Reason for Long-Form Story Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Generating high-quality stories spanning thousands of tokens requires competency across a variety of skills, from tracking plot and character arcs to keeping a consistent and engaging style. Due to the difficulty of sourcing labeled datasets and precise quality measurements, most work using large language models (LLMs) for long-form story generation uses combinations of hand-designed prompting techniques to elicit author-like behavior. This is a manual process that is highly dependent on the specific story-generation task. Motivated by the recent success of applying RL with Verifiable Rewards to domains like math and coding, we propose a general story-generation task (Next-Chapter Prediction) and a reward formulation (Verified Rewards via Completion Likelihood Improvement) that allows us to use an unlabeled book dataset as a learning signal for reasoning. We learn to reason over a story's condensed information and generate a detailed plan for the next chapter. Our reasoning is evaluated via the chapters it helps a story-generator create, and compared against non-trained and supervised finetuning (SFT) baselines. Pairwise human judgments reveal the chapters our learned reasoning produces are preferred across almost all metrics, and the effect is more pronounced in Scifi and Fantasy genres.",
        "keywords": "story generation;reasoning;reinforcement learning;long-context generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexander Gurung;Mirella Lapata",
        "authorids": "~Alexander_Gurung1;~Mirella_Lapata1",
        "gender": "M;F",
        "homepage": "https://alexgurung.me/;https://homepages.inf.ed.ac.uk/mlap/",
        "dblp": ";59/6701",
        "google_scholar": "0ev4wagAAAAJ;j67B9Q4AAAAJ",
        "orcid": ";",
        "linkedin": "alexandergurung/;",
        "or_profile": "~Alexander_Gurung1;~Mirella_Lapata1",
        "aff": "Edinburgh University, University of Edinburgh;Edinburgh University, University of Edinburgh",
        "aff_domain": "inf.ed.ac.uk;inf.ed.ac.uk",
        "position": "PhD student;Full Professor",
        "bibtex": "@inproceedings{\ngurung2025learning,\ntitle={Learning to Reason for Long-Form Story Generation},\nauthor={Alexander Gurung and Mirella Lapata},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dr3eg5ehR2}\n}",
        "github": "",
        "project": "",
        "reviewers": "uAHM;jQ4x;RQpG;oa9L",
        "site": "https://openreview.net/forum?id=dr3eg5ehR2",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "drdrFhKYjP",
        "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification, the ability to recognize who is speaking based on dialogue context. We argue that any meaningful judgment of role-playing quality (how well a character is played) fundamentally depends on first correctly attributing words and actions to the correct persona (who is speaking). We present PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles. PersonaEval uses human-authored dialogues from novels, scripts, and video transcripts, challenging models to determine the correct persona according to the conversation context. Our experiments, including a human study, show that even the best-performing LLMs reach only around 69% accuracy, well below the level needed for reliable evaluation. In contrast, human participants perform near ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still not human enough to effectively judge role-play scenarios. To better understand this gap, we examine training-time adaptation and test-time compute, suggesting that reliable evaluation requires more than task-specific tuning, but depends on strong, human-like reasoning abilities in LLM evaluators. We release our benchmark at https://github.com/maple-zhou/PersonaEval.",
        "keywords": "Role-play;evaluating LLM evaluators;benchmark",
        "primary_area": "",
        "supplementary_material": "/attachment/6a25cb35829c49d55a1670963857bef598b95c9e.zip",
        "author": "Lingfeng Zhou;Jialing Zhang;Jin Gao;Mohan Jiang;Dequan Wang",
        "authorids": "~Lingfeng_Zhou1;~Jialing_Zhang1;~Jin_Gao3;~Mohan_Jiang1;~Dequan_Wang1",
        "gender": "M;F;M;M;",
        "homepage": "https://github.com/maple-zhou;https://dqwang.group/authors/jialing-zhang/;https://jingao.online;https://mhjiang.site/;",
        "dblp": ";;;;",
        "google_scholar": "Io1CisUAAAAJ;;HkVy8voAAAAJ;vkp6-osAAAAJ;",
        "orcid": ";;0009-0002-1129-8490;0009-0000-6184-2605;",
        "linkedin": ";;;;",
        "or_profile": "~Lingfeng_Zhou1;~Jialing_Zhang1;~Jin_Gao3;~Mohan_Jiang1;~Dequan_Wang1",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;",
        "position": "PhD student;MS student;PhD student;Undergrad student;",
        "bibtex": "@inproceedings{\nzhou2025personaeval,\ntitle={PersonaEval: Are {LLM} Evaluators Human Enough to Judge Role-Play?},\nauthor={Lingfeng Zhou and Jialing Zhang and Jin Gao and Mohan Jiang and Dequan Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=drdrFhKYjP}\n}",
        "github": "",
        "project": "",
        "reviewers": "ECv8;kR5i;m7sw;meJF",
        "site": "https://openreview.net/forum?id=drdrFhKYjP",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "dujG4nGClA",
        "title": "URANIA: Differentially Private Insights into AI Use",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce _Urania_, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, _Urania_ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private method inspired by _Clio_  (Tamkin et al. 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.",
        "keywords": "Differential Privacy;Clustering;Summarization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Daogao Liu;Edith Cohen;Badih Ghazi;Peter Kairouz;Pritish Kamath;Alexander Knop;Ravi Kumar;Pasin Manurangsi;Adam Sealfon;Da Yu;Chiyuan Zhang",
        "authorids": "~Daogao_Liu1;~Edith_Cohen1;~Badih_Ghazi1;~Peter_Kairouz1;~Pritish_Kamath2;~Alexander_Knop1;~Ravi_Kumar1;~Pasin_Manurangsi2;~Adam_Sealfon1;~Da_Yu1;~Chiyuan_Zhang1",
        "gender": "M;F;;M;M;;M;M;;M;M",
        "homepage": "https://daogaoliu.github.io/;http://www.cohenwang.com/edith/;https://sites.google.com/view/badihghazi/home;https://kairouzp.github.io/;https://pritishkamath.github.io/;;https://sites.google.com/site/ravik53/;https://pasin30055.github.io/;https://asealfon.github.io/;;http://pluskid.org",
        "dblp": "245/4078;40/1039;125/2134;129/1254;https://dblp.org/pers/k/Kamath:Pritish.html;;k/RaviKumar.html;133/2059;150/6253;48/8545;21/8315",
        "google_scholar": "auA3AaQAAAAJ;O-TV6OgAAAAJ;GBJLTN8AAAAJ;m8NUgw0AAAAJ;1JFARhUAAAAJ;;J_XhIsgAAAAJ;35hM-PkAAAAJ;nrlhJMcAAAAJ;FcRGdiwAAAAJ;l_G2vr0AAAAJ",
        "orcid": ";0000-0002-3926-8237;;;;;0000-0002-2203-2586;;;;",
        "linkedin": ";;badih-ghazi-608379132/;kayrouzp;;;ravi-kumar-a3a9631;;;;",
        "or_profile": "~Daogao_Liu1;~Edith_Cohen1;~Badih_Ghazi1;~Peter_Kairouz1;~Pritish_Kamath2;~Alexander_Knop1;~Ravi_Kumar1;~Pasin_Manurangsi2;~Adam_Sealfon1;~Da_Yu1;~Chiyuan_Zhang1",
        "aff": "Google;Tel Aviv University+Google;Google;Google;Google Research;;Google;Google;Google;Google;Google",
        "aff_domain": "google.com;tau.ac.il+google.com;google.com;google.com;google.com;;google.com;google.com;google.com;google.com;google.com",
        "position": "Postdoc;Full Professor+Research Scientist;Researcher;Research Scientist;Research Scientist;;Research Scientist;Research Scientist;Researcher;Researcher;Research Scientist",
        "bibtex": "@inproceedings{\nliu2025urania,\ntitle={{URANIA}: Differentially Private Insights into {AI} Use},\nauthor={Daogao Liu and Edith Cohen and Badih Ghazi and Peter Kairouz and Pritish Kamath and Alexander Knop and Ravi Kumar and Pasin Manurangsi and Adam Sealfon and Da Yu and Chiyuan Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=dujG4nGClA}\n}",
        "github": "",
        "project": "",
        "reviewers": "hkoW;qwBB;yuSm",
        "site": "https://openreview.net/forum?id=dujG4nGClA",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "e112iu5ssg",
        "title": "Overfill: Two-Stage Models for Efficient Language Model Decoding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose Overfill, which decouples these stages to optimize accuracy-efficiency tradeoffs. Overfill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, Overfill improves generation quality with minimal latency overhead. Our 3B-to-1B Overfill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. Overfill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.",
        "keywords": "deep learning;large language models;inference efficiency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Woojeong Kim;Junxiong Wang;Jing Nathan Yan;Mohamed S. Abdelfattah;Alexander M Rush",
        "authorids": "~Woojeong_Kim1;~Junxiong_Wang1;~Jing_Nathan_Yan1;~Mohamed_S._Abdelfattah1;~Alexander_M_Rush1",
        "gender": "F;;;;M",
        "homepage": "https://sites.google.com/view/woojeongkim/;;https://nathanyanjing.github.io/;;http://rush.seas.harvard.edu/",
        "dblp": "243/0064;;;;http://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=",
        "google_scholar": "fGCEQQgAAAAJ;;;;LIjnUGgAAAAJ",
        "orcid": ";;;;0000-0002-9900-1606",
        "linkedin": "woojeong-kim-072ab4160/;;;;sasha-rush-a69b6917/",
        "or_profile": "~Woojeong_Kim1;~Junxiong_Wang1;~Jing_Nathan_Yan1;~Mohamed_S._Abdelfattah1;~Alexander_M_Rush1",
        "aff": "Cornell University;;Google+Cornell University;;Cornell University+School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "cornell.edu;;google.com+cornell.edu;;cornell.edu+seas.harvard.edu",
        "position": "PhD student;;Researcher+PhD student;;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025overfill,\ntitle={Overfill: Two-Stage Models for Efficient Language Model Decoding},\nauthor={Woojeong Kim and Junxiong Wang and Jing Nathan Yan and Mohamed S. Abdelfattah and Alexander M Rush},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=e112iu5ssg}\n}",
        "github": "",
        "project": "",
        "reviewers": "89kW;wzXx;Y7iV;paUo",
        "site": "https://openreview.net/forum?id=e112iu5ssg",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;2;3;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.816496580927726
    },
    {
        "id": "e5jWdZIX0Q",
        "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Is automated hallucination detection fundamentally possible?\nIn this paper, we introduce a theoretical framework to rigorously study the (im)possibility of automatically detecting hallucinations produced by large language models (LLMs). Our model builds on the classical Gold-Angluin framework of language identification \nand its recent adaptation by Kleinberg and Mullainathan to the language generation setting. Concretely, we investigate whether an algorithm\u2014trained on examples from an unknown target language $K$, chosen from a countable collection of languages $\\mathcal{L}$, and given access to  an LLM\u2014can reliably determine if the LLM\u2019s outputs are correct or constitute hallucinations.\n\nFirst, we establish a strong equivalence between hallucination detection and the classical problem of language identification. Specifically, we prove that any algorithm capable of identifying languages (in the limit) can be efficiently transformed into one that reliably detects hallucinations, and conversely, successful hallucination detection strategy inherently implies language identification. Given the notorious difficulty of language identification, our first result implies that hallucination detection is *impossible* for most collections of languages.  \n\nSecond, we show that once we enrich the detector\u2019s training data, i.e., providing it with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements)\u2014 the conclusion dramatically changes. Under this enriched training regime, we show that automated hallucination detection is *possible* for any countable collection $\\mathcal{L}$.\n\nOur theoretical results, thus, underscore the fundamental importance of expert-labeled feedback in the practical deployment of hallucination detection methods, reinforcing why feedback-based approaches, such as reinforcement learning with human feedback (RLHF), have proven so crucial in improving the reliability and safety of real-world LLMs.",
        "keywords": "hallucinations;theory;RLHF",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amin Karbasi;Omar Montasser;John Sous;Grigoris Velegkas",
        "authorids": "~Amin_Karbasi3;~Omar_Montasser1;~John_Sous1;~Grigoris_Velegkas1",
        "gender": ";M;M;M",
        "homepage": ";https://ttic.uchicago.edu/~omar/;https://www.johnsous.com;",
        "dblp": ";194/3002;;254/1885",
        "google_scholar": ";u455rGAAAAAJ;_PqcB9kAAAAJ;Ty1kgP0AAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Amin_Karbasi3;~Omar_Montasser1;~John_Sous1;~Grigoris_Velegkas1",
        "aff": ";Yale University;Yale University;Yale University",
        "aff_domain": ";yale.edu;yale.edu;yale.edu",
        "position": ";Assistant Professor;Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\nkarbasi2025impossibility,\ntitle={(Im)possibility of Automated Hallucination Detection in Large Language Models},\nauthor={Amin Karbasi and Omar Montasser and John Sous and Grigoris Velegkas},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=e5jWdZIX0Q}\n}",
        "github": "",
        "project": "",
        "reviewers": "zwtc;TJ7A;ypnG;T4oz",
        "site": "https://openreview.net/forum?id=e5jWdZIX0Q",
        "pdf_size": 0,
        "rating": "5;5;6;6",
        "confidence": "4;5;2;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8944271909999159
    },
    {
        "id": "eLWn2XVMHA",
        "title": "Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The deployment of large-scale models, such as large language models (LLMs), incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for model deployment, where choosing efficient inference acceleration schemes become crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a meta-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our meta-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of inference acceleration in decentralized AI systems, offering a path towards more democratic and economically feasible artificial intelligence solutions. Our code and data will be released later.",
        "keywords": "Fast inference;LLMs;meta learning;decentralized systems",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yipeng Du;Zihao Wang;Ahmad Farhan;Claudio Angione;Harry Yang;Fielding Johnston;James P. Buban;Yue Zhao;Yuzhe Yang",
        "authorids": "~Yipeng_Du2;~Zihao_Wang36;~Ahmad_Farhan1;~Claudio_Angione1;~Harry_Yang2;~Fielding_Johnston1;~James_P._Buban1;~Yue_Zhao13;~Yuzhe_Yang1",
        "gender": "M;;M;;;M;;M;M",
        "homepage": ";;https://iafarhan.github.io/;https://sites.google.com/view/angionelab/;http://leehomyc.github.io;https://justfielding.com;;https://viterbi-web.usc.edu/~yzhao010/;https://www.cs.ucla.edu/~yuzhe",
        "dblp": ";;;119/6044.html;;;;48/76-16;213/0962-3.html",
        "google_scholar": "o7_fANUAAAAJ;UhFbFCMAAAAJ;fe1tsJ0AAAAJ;N03p3q0AAAAJ;;;xd-ZcWEAAAAJ;https://scholar.google.ca/citations?user=zoGDYsoAAAAJ;0_bSbIoAAAAJ",
        "orcid": ";;;0000-0002-3140-7909;;;;0000-0003-3401-4921;0000-0002-7634-8295",
        "linkedin": "michael-du-28b035156/;;imafarhan/;claudio-angione/;;;;yzhao062/;yuzhe-yang-6809b2131/",
        "or_profile": "~Yipeng_Du2;~Zihao_Wang36;~Ahmad_Farhan1;~Claudio_Angione1;~Harry_Yang2;~Fielding_Johnston1;~James_P._Buban1;~Yue_Zhao13;~Yuzhe_Yang1",
        "aff": "Georgia Institute of Technology;Hong Kong University of Science and Technology;University of Teesside;University of Teesside+University of Cambridge;Hong Kong University of Science and Technology;nesa;Nesa Labs+University of Illinois at Chicago;University of Southern California;University of California, Los Angeles",
        "aff_domain": "gatech.edu;hkust.edu;tees.ac.uk;tees.ac.uk+cam.ac.uk;ust.hk;nesa.ai;nesa.ai+uic.edu;usc.edu;ucla.edu",
        "position": "MS student;PhD student;MS student;Full Professor+PhD student;Assistant Professor;Researcher;Researcher+Researcher;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ndu2025metalearning,\ntitle={Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments},\nauthor={Yipeng Du and Zihao Wang and Ahmad Farhan and Claudio Angione and Harry Yang and Fielding Johnston and James P. Buban and Yue Zhao and Yuzhe Yang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=eLWn2XVMHA}\n}",
        "github": "",
        "project": "",
        "reviewers": "bZwy;d5hq;pGCu",
        "site": "https://openreview.net/forum?id=eLWn2XVMHA",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "eSAv7GKVFt",
        "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of \u2248 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for\nanalyzing the contributions of textual, vocal, and visual modalities to the classification of content as either sexist or non-sexist; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes\u2014instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.",
        "keywords": "sexism;multimodal;classification;social media;LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Laura De Grazia;Pol Pastells;Mauro V\u00e1zquez Chas;Desmond Elliott;Danae Sanchez Villegas;Mireia Farr\u00fas;Mariona Taul\u00e9 Delor",
        "authorids": "~Laura_De_Grazia1;~Pol_Pastells1;~Mauro_V\u00e1zquez_Chas1;~Desmond_Elliott1;~Danae_Sanchez_Villegas1;~Mireia_Farr\u00fas1;~Mariona_Taul\u00e9_Delor1",
        "gender": "F;M;M;;F;;F",
        "homepage": "https://www.researchgate.net/profile/Laura-De-Grazia;https://pastells.github.io;;;https://danaesavi.github.io/;http://clic.ub.edu/ca/users/mireia-farr%C3%BAs-cabeceran;https://clic.ub.edu/index.php/m_taule",
        "dblp": ";;;46/7536;263/9916;;",
        "google_scholar": "wY7XG74AAAAJ;https://scholar.google.es/citations?user=Isg_PsgAAAAJ;;;https://scholar.google.co.uk/citations?user=jafwsyYAAAAJ;rCvSHQ0AAAAJ;",
        "orcid": ";0000-0003-1302-1372;;;0000-0002-3045-1262;0000-0002-7160-9513;0000-0003-0089-940X",
        "linkedin": "lauradegrazia/;https://linkedin.com/in/pol-pastells/;mauro-v%C3%A1zquez-chas-4437a02b3/;;danae-sanchez-villegas/;mireia-farr%C3%BAs-9b767b7/;",
        "or_profile": "~Laura_De_Grazia1;~Pol_Pastells1;~Mauro_V\u00e1zquez_Chas1;~Desmond_Elliott1;~Danae_Sanchez_Villegas1;~Mireia_Farr\u00fas1;~Mariona_Taul\u00e9_Delor1",
        "aff": "Universitat de Barcelona;Universitat de Barcelona;Universitat de Barcelona+UPC - Barcelona School of Informatics, Universidad Polit\u00e9cnica de Cataluna;Copenhagen University+University of Copenhagen;University of Copenhagen;Universitat de Barcelona;Universitat de Barcelona",
        "aff_domain": "ub.edu.bc;ub.edu;ub.edu+fib.upc.edu;ku.dk+ku.dk;di.ku.dk;ub.edu;ub.edu",
        "position": "PhD student;Researcher;Researcher+MS student;Associate Professor+Assistant Professor;Postdoc;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\ngrazia2025mused,\ntitle={MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos},\nauthor={Laura De Grazia and Pol Pastells and Mauro V{\\'a}zquez Chas and Desmond Elliott and Danae Sanchez Villegas and Mireia Farr{\\'u}s and Mariona Taul{\\'e} Delor},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=eSAv7GKVFt}\n}",
        "github": "",
        "project": "",
        "reviewers": "4cEJ;xWDg;qK5A",
        "site": "https://openreview.net/forum?id=eSAv7GKVFt",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;5",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9999999999999998
    },
    {
        "id": "eqNItk1sWo",
        "title": "Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7\u00d7 more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599\u00d7 and 9\u00d7, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.",
        "keywords": "Large Language Models;Evaluation;Misinformation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Linxin Song;Xuwei Ding;Jieyu Zhang;Taiwei Shi;Ryotaro Shimizu;Rahul Gupta;Yang Liu;Jian Kang;Jieyu Zhao",
        "authorids": "~Linxin_Song1;~Xuwei_Ding1;~Jieyu_Zhang1;~Taiwei_Shi1;~Ryotaro_Shimizu1;~Rahul_Gupta3;~Yang_Liu60;~Jian_Kang1;~Jieyu_Zhao1",
        "gender": "M;M;M;M;M;M;F;M;F",
        "homepage": "https://linxins97.github.io/;;https://jieyuz2.github.io/;https://maksimstw.github.io/;;;;https://jiank2.github.io/;http://jyzhao.net/",
        "dblp": "330/3920.html;;;336/2150;315/2865;;51/3710-4;56/6072-8;59/2379-1",
        "google_scholar": "https://scholar.google.com.hk/citations?user=IjqXzSwAAAAJ;;T_INUHUAAAAJ;yv6nCnMAAAAJ;https://scholar.google.co.jp/citations?user=imbW88cAAAAJ;1CFrm2YAAAAJ;w90wOucAAAAJ;U_jFlOQAAAAJ;9VaGBCQAAAAJ",
        "orcid": "0009-0009-7349-8990;;0000-0002-1846-2436;;0000-0002-4841-1824;;;0000-0003-3902-7131;",
        "linkedin": ";xuwei-ding-506998329/;jieyu-zhang-3baaa8154/;maksimstw/;ryotaro-shimizu-5b60ab202/;;yang-liu-8555143/;jiank2/;",
        "or_profile": "~Linxin_Song1;~Xuwei_Ding1;~Jieyu_Zhang1;~Taiwei_Shi1;~Ryotaro_Shimizu1;~Rahul_Gupta3;~Yang_Liu60;~Jian_Kang1;~Jieyu_Zhao1",
        "aff": "Salesforce Research+University of Southern California;University of Wisconsin - Madison;University of Washington;University of Southern California+Microsoft;University of California, San Diego+ZOZO Research;Amazon;Amazon;Mohamed bin Zayed University of Artificial Intelligence+University of Rochester;University of Southern California",
        "aff_domain": "salesforce.com+usc.edu;wisc.edu;cs.washington.edu;usc.edu+microsoft.com;ucsd.edu+zozo.com;amazon.com;amazon.com;mbzuai.ac.ae+cs.rochester.edu;usc.edu",
        "position": "Intern+PhD student;Undergrad student;PhD student;PhD student+Intern;Researcher+Researcher;Researcher;Principal Researcher;Assistant Professor+Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nsong2025discovering,\ntitle={Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base},\nauthor={Linxin Song and Xuwei Ding and Jieyu Zhang and Taiwei Shi and Ryotaro Shimizu and Rahul Gupta and Yang Liu and Jian Kang and Jieyu Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=eqNItk1sWo}\n}",
        "github": "",
        "project": "",
        "reviewers": "UD1R;cJFu;cTGP;PkVq",
        "site": "https://openreview.net/forum?id=eqNItk1sWo",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "erGpkHCybv",
        "title": "EvalAgents: Discovering Implicit Evaluation Criteria from the Web",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like \"Help me draft an academic talk on coffee intake vs research productivity\", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.",
        "keywords": "evaluation;writing;large language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Manya Wadhwa;Zayne Rea Sprague;Chaitanya Malaviya;Philippe Laban;Junyi Jessy Li;Greg Durrett",
        "authorids": "~Manya_Wadhwa1;~Zayne_Rea_Sprague1;~Chaitanya_Malaviya1;~Philippe_Laban1;~Junyi_Jessy_Li2;~Greg_Durrett1",
        "gender": ";M;M;M;F;M",
        "homepage": "https://manyawadhwa.github.io/;https://zaynesprague.com/;https://chaitanyamalaviya.github.io/;https://people.eecs.berkeley.edu/~phillab/;https://jessyli.com;http://www.cs.utexas.edu/~gdurrett/",
        "dblp": "192/2524.html;311/5080.html;194/3061;220/3590;148/9553;69/7968",
        "google_scholar": "4_uMjzgAAAAJ;https://scholar.google.com/citations?hl=en;s3MzzwwAAAAJ;fR5t200AAAAJ;tJGm3-YAAAAJ;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "manya-wadhwa-9798a79a/;;;;;",
        "or_profile": "~Manya_Wadhwa1;~Zayne_Rea_Sprague1;~Chaitanya_Malaviya1;~Philippe_Laban1;~Junyi_Jessy_Li2;~Greg_Durrett1",
        "aff": "University of Texas at Austin;University of Texas at Austin;University of Pennsylvania;Microsoft;University of Texas at Austin;University of Texas at Austin",
        "aff_domain": "utexas.edu;cs.utexas.edu;upenn.edu;microsoft.com;utexas.edu;utexas.edu",
        "position": "PhD student;PhD student;PhD student;Researcher;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nwadhwa2025evalagents,\ntitle={EvalAgents: Discovering Implicit Evaluation Criteria from the Web},\nauthor={Manya Wadhwa and Zayne Rea Sprague and Chaitanya Malaviya and Philippe Laban and Junyi Jessy Li and Greg Durrett},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=erGpkHCybv}\n}",
        "github": "",
        "project": "",
        "reviewers": "393x;tKpV;1Txy;BmkZ",
        "site": "https://openreview.net/forum?id=erGpkHCybv",
        "pdf_size": 0,
        "rating": "5;5;7;7",
        "confidence": "4;3;5;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.0
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.30151134457776363
    },
    {
        "id": "exW2SFJK4H",
        "title": "The Unlearning Mirage: A Dynamic Framework for Evaluating LLM Unlearning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Unlearning in Large Language Models (LLMs) aims to enhance safety, mitigate biases, and comply with legal mandates, such as the right to be forgotten. However, existing unlearning methods are brittle: minor query modifications, such as multi-hop reasoning and entity aliasing, can recover supposedly forgotten information. As a result, current evaluation metrics often create an illusion of effectiveness, failing to detect these vulnerabilities due to reliance on static, unstructured benchmarks. We propose a dynamic framework that stress tests unlearning robustness using complex structured queries. Our approach first elicits knowledge from the target model (pre-unlearning) and constructs targeted probes, ranging from simple queries to multi-hop chains, allowing precise control over query difficulty. Our experiments show that the framework: (1) shows comparable coverage to existing benchmarks by automatically generating semantically equivalent Q&A probes, (2) aligns with prior evaluations, and (3) uncovers new unlearning failures missed by other benchmarks, particularly in multi-hop settings. Furthermore, activation analyses show that single-hop queries typically follow dominant computation pathways, which are more likely to be disrupted by unlearning methods. In contrast, multi-hop queries tend to use alternative pathways that often remain intact, explaining the brittleness of unlearning techniques in multi-hop settings. Our framework enables practical and scalable evaluation of unlearning methods without the need for manual construction of forget test sets, enabling easier adoption for real-world applications. We release the pip package and the code at https://sites.google.com/view/unlearningmirage/home.",
        "keywords": "Unlearning evaluation;Multi-hop reasoning",
        "primary_area": "",
        "supplementary_material": "/attachment/3effe50b2de608ddbd7719e8b65eb2fd13f7c786.zip",
        "author": "Raj Sanjay Shah;Jing Huang;Keerthiram Murugesan;Nathalie Baracaldo;Diyi Yang",
        "authorids": "~Raj_Sanjay_Shah2;~Jing_Huang2;~Keerthiram_Murugesan1;~Nathalie_Baracaldo1;~Diyi_Yang2",
        "gender": "M;;M;;F",
        "homepage": "https://raj-sanjay-shah.github.io/;https://explanare.github.io/;https://keerthi166.github.io;https://researcher.watson.ibm.com/researcher/view.php?person=us-baracald;https://cs.stanford.edu/~diyiy/",
        "dblp": "262/0806;14/4834-14;178/2877;87/10087;70/11145",
        "google_scholar": "mpGOJ0gAAAAJ;zM_wp_MAAAAJ;-698GEMAAAAJ;3ACndBYAAAAJ;j9jhYqQAAAAJ",
        "orcid": "0000-0002-0847-8426;;0000-0001-6847-522X;;",
        "linkedin": ";;https://linkedin.com/in/keerthiram;;",
        "or_profile": "~Raj_Sanjay_Shah2;~Jing_Huang2;~Keerthiram_Murugesan1;~Nathalie_Baracaldo1;~Diyi_Yang2",
        "aff": "Georgia Institute of Technology;Stanford University;International Business Machines;University of Pittsburgh+IBM, International Business Machines;Stanford University",
        "aff_domain": "gatech.edu;stanford.edu;ibm.com;pitt.edu+us.ibm.com;stanford.edu",
        "position": "PhD student;PhD student;Researcher;PhD student+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nshah2025the,\ntitle={The Unlearning Mirage: A Dynamic Framework for Evaluating {LLM} Unlearning},\nauthor={Raj Sanjay Shah and Jing Huang and Keerthiram Murugesan and Nathalie Baracaldo and Diyi Yang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=exW2SFJK4H}\n}",
        "github": "",
        "project": "",
        "reviewers": "cADi;6xWi;sfKb;zv4e",
        "site": "https://openreview.net/forum?id=exW2SFJK4H",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "f7GG1MbsSM",
        "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model\u2019s observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.",
        "keywords": "LLMs;Knowledge",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zorik Gekhman;Eyal Ben-David;Hadas Orgad;Eran Ofek;Yonatan Belinkov;Idan Szpektor;Jonathan Herzig;Roi Reichart",
        "authorids": "~Zorik_Gekhman1;~Eyal_Ben-David1;~Hadas_Orgad1;~Eran_Ofek1;~Yonatan_Belinkov1;~Idan_Szpektor1;~Jonathan_Herzig2;~Roi_Reichart1",
        "gender": "M;M;F;M;M;;M;M",
        "homepage": ";https://eyalbd2.github.io/;;;https://www.belinkov.com;;https://jonathanherzig.github.io/;https://roireichart.com/",
        "dblp": "275/3280;234/9089;;31/2429;136/8705;15/6513;133/3687.html;96/5429",
        "google_scholar": "c748UcIAAAAJ;ArqbkI4AAAAJ;;9IkHbWUAAAAJ;https://scholar.google.com/citations?authorid=K-6ujU4AAAAJ;XI2CP68AAAAJ;https://scholar.google.co.il/citations?view_op=list_works;https://scholar.google.co.il/citations?user=xXJIsh4AAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": ";eyal-bd/;hadas-orgad/;;;;;roi-reichart-ba2a8a7/",
        "or_profile": "~Zorik_Gekhman1;~Eyal_Ben-David1;~Hadas_Orgad1;~Eran_Ofek1;~Yonatan_Belinkov1;~Idan_Szpektor1;~Jonathan_Herzig2;~Roi_Reichart1",
        "aff": "Technion;Technion - Israel Institute of Technology, Technion;Computer Science Department, Technion - Israel Institute of Technology;Google;Technion;Google;Research, Google;Technion, Israel Institute of Technology",
        "aff_domain": "technion.ac.il;technion.ac.il;cs.technion.ac.il;google.com;technion.ac.il;google.com;research.google.com;technion.ac.il",
        "position": "MS student;PhD student;PhD student;Researcher;Assistant Professor;Researcher;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\ngekhman2025insideout,\ntitle={Inside-Out: Hidden Factual Knowledge in {LLM}s},\nauthor={Zorik Gekhman and Eyal Ben-David and Hadas Orgad and Eran Ofek and Yonatan Belinkov and Idan Szpektor and Jonathan Herzig and Roi Reichart},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=f7GG1MbsSM}\n}",
        "github": "",
        "project": "",
        "reviewers": "6s8b;LjuE;959U;T7HJ",
        "site": "https://openreview.net/forum?id=f7GG1MbsSM",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "4;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            26,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6882472016116854
    },
    {
        "id": "fQcUZMPIvu",
        "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
        "keywords": "Agent;Web Agent;LLM Judge;LLM-as-a-Judge;Digital Agent;Benchmark;Reward Modeling",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xing Han L\u00f9;Amirhossein Kazemnejad;Nicholas Meade;Arkil Patel;Dongchan Shin;Alejandra Zambrano;Karolina Stanczak;Peter Shaw;Christopher Pal;Siva Reddy",
        "authorids": "~Xing_Han_L\u00f91;~Amirhossein_Kazemnejad1;~Nicholas_Meade1;~Arkil_Patel1;~Dongchan_Shin1;~Alejandra_Zambrano1;~Karolina_Stanczak1;~Peter_Shaw1;~Christopher_Pal1;~Siva_Reddy1",
        "gender": ";;;M;;F;F;M;;M",
        "homepage": ";;https://ncmeade.github.io;https://arkilpatel.github.io/;;https://www.linkedin.com/in/alejandra-zambrano-a71092196/;https://karstanczak.github.io;http://www.ptshaw.com;https://scholar.google.ca/citations?user=1ScWJOoAAAAJ&hl=en&oi=ao;http://sivareddy.in",
        "dblp": ";;244/9969;254/5212;;;290/1356;217/1471;45/1217;64/8153",
        "google_scholar": ";;-aLqCbgAAAAJ;-5goVAsAAAAJ;;;4XT8OgEAAAAJ;SmGaQicAAAAJ;https://scholar.google.ca/citations?user=1ScWJOoAAAAJ;",
        "orcid": ";;;;;;0000-0001-7326-9594;;;",
        "linkedin": ";;;arkil-patel;dongchan-shin-2a4890275;;;;;",
        "or_profile": "~Xing_Han_L\u00f91;~Amirhossein_Kazemnejad1;~Nicholas_Meade1;~Arkil_Patel1;~Dongchan_Shin1;~Alejandra_Zambrano1;~Karolina_Stanczak1;~Peter_Shaw1;~Christopher_Pal1;~Siva_Reddy1",
        "aff": ";;McGill University;Mila - Quebec AI Institute+McGill University;Mila - Quebec Artificial Intelligence Institute;Concordia University;ETHZ - ETH Zurich+McGill University+Mila - Quebec Artificial Intelligence Institute;Google DeepMind;Polytechnique Montreal;ServiceNow Inc+Mila, McGill University+Mila, McGill University",
        "aff_domain": ";;mcgill.ca;mila.quebec+mail.mcgill.ca;mila.quebec;mail.concordia.ca;ethz.ch+mail.mcgill.ca+mila.quebec;google.com;polymtl.ca;servicenow.com+cs.mcgill.ca+mila.quebec",
        "position": ";;PhD student;PhD student+PhD student;Intern;MS student;Postdoc+Postdoc+Postdoc;Research Scientist;Full Professor;Researcher+Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nlu2025agentrewardbench,\ntitle={AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories},\nauthor={Xing Han L{\\`u} and Amirhossein Kazemnejad and Nicholas Meade and Arkil Patel and Dongchan Shin and Alejandra Zambrano and Karolina Stanczak and Peter Shaw and Christopher Pal and Siva Reddy},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=fQcUZMPIvu}\n}",
        "github": "",
        "project": "",
        "reviewers": "NzzY;2xan;9MNf;NwGC",
        "site": "https://openreview.net/forum?id=fQcUZMPIvu",
        "pdf_size": 0,
        "rating": "5;7;7;7",
        "confidence": "4;2;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5222329678670935
    },
    {
        "id": "fcRcl1EXc4",
        "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.",
        "keywords": "linguistic reasoning;metalinguistics;LLM evaluation;morphology;linguistics olympiad;interpretability;low resource languages;annotation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mukund Choudhary;KV Aditya Srivatsa;Gaurja Aeron;Antara Raaghavi Bhattacharya;Dang Khoa Dang Dinh;Ikhlasul Akmal Hanif;Daria Kotova;Ekaterina Kochmar;Monojit Choudhury",
        "authorids": "~Mukund_Choudhary1;~KV_Aditya_Srivatsa1;~Gaurja_Aeron1;~Antara_Raaghavi_Bhattacharya1;~Dang_Khoa_Dang_Dinh1;~Ikhlasul_Akmal_Hanif1;~Daria_Kotova1;~Ekaterina_Kochmar2;~Monojit_Choudhury1",
        "gender": "M;M;F;;M;M;F;;M",
        "homepage": "https://mukundc2k.github.io;;;;;;;https://ekochmar.github.io/about/;https://mbzuai.ac.ae/study/faculty/monojit-choudhury/",
        "dblp": "308/2635;372/4550;;;;388/4098;;140/3465.html;29/5841",
        "google_scholar": "29EdHIgAAAAJ;cs5-j9EAAAAJ;;;;;;https://scholar.google.co.uk/citations?user=e2HTYnkAAAAJ;WR1ImCMAAAAJ",
        "orcid": "0000-0001-6526-649X;0000-0002-7583-7580;;0000-0002-4342-4626;;0009-0000-2598-9343;;0000-0003-3328-1374;0000-0001-7473-7839",
        "linkedin": "mukundc2k;kv-aditya-srivatsa/;gaurja-aeron-22659644?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;;khoa-ddd/;ikhlasul-akmal-h/;daria-kotova-191a81240/;ekaterina-kochmar-0a655b14/;monojit-choudhury-54225898/",
        "or_profile": "~Mukund_Choudhary1;~KV_Aditya_Srivatsa1;~Gaurja_Aeron1;~Antara_Raaghavi_Bhattacharya1;~Dang_Khoa_Dang_Dinh1;~Ikhlasul_Akmal_Hanif1;~Daria_Kotova1;~Ekaterina_Kochmar2;~Monojit_Choudhury1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Indian Institute of Technology, Gandhinagar;Harvard University;VinUniversity;Mohamed bin Zayed University of Artificial Intelligence+Universitas Indonesia;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;iitgn.ac.in;harvard.edu;vinuni.edu.vn;mbzuai.ac.ae+ui.ac.id;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae",
        "position": "PhD student;Researcher;MS student;Undergrad student;Undergrad student;PhD student+Undergrad student;MS student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nchoudhary2025unveiling,\ntitle={{UNVEILING}: What Makes Linguistics Olympiad Puzzles Tricky for {LLM}s?},\nauthor={Mukund Choudhary and KV Aditya Srivatsa and Gaurja Aeron and Antara Raaghavi Bhattacharya and Dang Khoa Dang Dinh and Ikhlasul Akmal Hanif and Daria Kotova and Ekaterina Kochmar and Monojit Choudhury},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=fcRcl1EXc4}\n}",
        "github": "",
        "project": "",
        "reviewers": "Qiv2;iDKA;TCNB;D2sE",
        "site": "https://openreview.net/forum?id=fcRcl1EXc4",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "1;3;3;3",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            2.5,
            0.8660254037844386
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8703882797784891
    },
    {
        "id": "fuBrcTH8NM",
        "title": "Efficient Construction of Model Family through Progressive Training Using Model Expansion",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As Large Language Models (LLMs) gain widespread practical applica-\ntion, offering model families with varying parameter sizes has become\nstandard practice to accommodate diverse computational requirements.\nTraditionally, each model in the family is trained independently, incurring\ncomputational costs that scale additively with the number of models. In\nthis work, we propose an efficient method for constructing model families\nvia progressive training, where smaller models are incrementally expanded\nto larger sizes to create a complete model family. Through extensive ex-\nperiments on a model family ranging from 1B to 8B parameters, we show\nthat our approach reduces total computational cost by approximately 25%\nwhile maintaining comparable performance to independently trained mod-\nels. Moreover, by strategically adjusting the maximum learning rate based\non model size, our method outperforms the independent training across\nvarious metrics. Beyond these improvements, our approach also fosters\ngreater consistency in behavior across model sizes.",
        "keywords": "pre-training;model familly;compute efficiency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kazuki Yano;Sho Takase;Sosuke Kobayashi;Shun Kiyono;Jun Suzuki",
        "authorids": "~Kazuki_Yano1;~Sho_Takase2;~Sosuke_Kobayashi1;~Shun_Kiyono1;~Jun_Suzuki1",
        "gender": "M;;M;M;M",
        "homepage": "https://kazuki-ya.github.io/;https://takase.github.io/;https://soskek.github.io/;https://butsugiri.github.io/;https://www.nlp.ecei.tohoku.ac.jp/~jun/",
        "dblp": "97/428;;185/5523;211/7611;78/6923",
        "google_scholar": ";https://scholar.google.co.jp/citations?user=2dvzFDYAAAAJ;VY6PqvsAAAAJ;LS3EdOoAAAAJ;https://scholar.google.co.jp/citations?user=XO5CrIsAAAAJ",
        "orcid": ";;;;0000-0003-2108-1340",
        "linkedin": ";;;;",
        "or_profile": "~Kazuki_Yano1;~Sho_Takase2;~Sosuke_Kobayashi1;~Shun_Kiyono1;~Jun_Suzuki1",
        "aff": "Tohoku University+Tohoku University;LINE Corporation;Tohoku University+Preferred Networks, Inc.;SB Intuitions;Tohoku University",
        "aff_domain": "tohoku.ac.jp+tohoku.ac.jp;linecorp.com;tohoku.ac.jp+preferred.jp;sbintuitions.co.jp;tohoku.ac.jp",
        "position": "PhD student+MS student;Researcher;Researcher+Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nyano2025efficient,\ntitle={Efficient Construction of Model Family through Progressive Training Using Model Expansion},\nauthor={Kazuki Yano and Sho Takase and Sosuke Kobayashi and Shun Kiyono and Jun Suzuki},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=fuBrcTH8NM}\n}",
        "github": "",
        "project": "",
        "reviewers": "hRL8;Az33;D8FB;oPhZ",
        "site": "https://openreview.net/forum?id=fuBrcTH8NM",
        "pdf_size": 0,
        "rating": "3;6;6;8",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.7853571071357126
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.08084520834544431
    },
    {
        "id": "gIqb6zWZoO",
        "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints.\nRecent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks.\nWhile this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood.\nMoreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions.\nIn this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers.\nAdditionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization.\nBased on our enhanced understanding, we introduce KVSink, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. \nExtensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization.\nMoreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.",
        "keywords": "quantization;kv cache;transformer;llm;attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zunhai Su;Kehong Yuan",
        "authorids": "~Zunhai_Su1;~Kehong_Yuan1",
        "gender": "M;M",
        "homepage": "https://github.com/Seaznszhhh;https://www.x-mol.com/university/faculty/61272",
        "dblp": ";31/518.html",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Zunhai_Su1;~Kehong_Yuan1",
        "aff": "Tsinghua University;Tsinghua University",
        "aff_domain": "mail.tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "MS student;Full Professor",
        "bibtex": "@inproceedings{\nsu2025kvsink,\ntitle={{KVS}ink: Understanding and Enhancing the Preservation of Attention Sinks in {KV} Cache Quantization for {LLM}s},\nauthor={Zunhai Su and Kehong Yuan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=gIqb6zWZoO}\n}",
        "github": "",
        "project": "",
        "reviewers": "itTf;G7Pw;vwcZ;Uszr",
        "site": "https://openreview.net/forum?id=gIqb6zWZoO",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "gKdhzBiHay",
        "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism\u2019s outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by $2.17\\times \\sim 2.82\\times$, improves throughput by $2.45\\times \\sim 3.60 \\times$, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.",
        "keywords": "KV cache quantization;LLMs",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hao Wang;Ligong Han;Kai Xu;Akash Srivastava",
        "authorids": "~Hao_Wang22;~Ligong_Han1;~Kai_Xu4;~Akash_Srivastava1",
        "gender": "M;M;M;M",
        "homepage": "https://haowang94.github.io;https://phymhan.github.io;https://xuk.ai;http://akashgit.github.io",
        "dblp": ";187/1675;;24/9528",
        "google_scholar": "A3WtYhAAAAAJ;n2v43R4AAAAJ;https://scholar.google.ca/citations?user=kf3C60wAAAAJ;https://scholar.google.co.uk/citations?user=2h6SZeEAAAAJ",
        "orcid": ";0000-0003-3166-0848;;",
        "linkedin": ";ligongh/;;https://uk.linkedin.com/in/akash-srivastava-aa97361b",
        "or_profile": "~Hao_Wang22;~Ligong_Han1;~Kai_Xu4;~Akash_Srivastava1",
        "aff": "RedHat AI & MIT-IBM Watson AI Lab;International Business Machines;MIT-IBM Watson AI Lab;International Business Machines",
        "aff_domain": "redhat.com;ibm.com;ibm.com;ibm.com",
        "position": "Researcher;Researcher;Research scientist;Principal Researcher",
        "bibtex": "@inproceedings{\nwang2025squat,\ntitle={{SQ}uat: Subspace-orthogonal {KV} Cache Quantization},\nauthor={Hao Wang and Ligong Han and Kai Xu and Akash Srivastava},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=gKdhzBiHay}\n}",
        "github": "",
        "project": "",
        "reviewers": "hftq;fNFy;hLUL",
        "site": "https://openreview.net/forum?id=gKdhzBiHay",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "gKfj7Jb1kj",
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce X-Guard-Train, an open-source multi-turn safety training dataset that's $~20\\times$ larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.",
        "keywords": "Multi-turn Jailbreaks;Adaptive Multi-Agent;Conversational AI Safety;Red-Teaming;Defensive Alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Salman Rahman;Liwei Jiang;James Shiffer;Genglin Liu;Sheriff Issaka;Md Rizwan Parvez;Hamid Palangi;Kai-Wei Chang;Yejin Choi;Saadia Gabriel",
        "authorids": "~Salman_Rahman1;~Liwei_Jiang2;~James_Shiffer1;~Genglin_Liu1;~Sheriff_Issaka1;~Md_Rizwan_Parvez1;~Hamid_Palangi1;~Kai-Wei_Chang1;~Yejin_Choi1;~Saadia_Gabriel1",
        "gender": "M;F;M;M;M;M;M;M;F;F",
        "homepage": "https://salmanrahman.net/;https://liweijiang.me;;https://genglinliu.github.io/;;https://rizwan09.github.io/;https://www.hamidpalangi.com/;http://kwchang.net;https://yejinc.github.io/;https://saadia-gabriel.github.io/",
        "dblp": "278/6004;;;347/9436;;180/3830.html;01/963;18/2428;89/579-1;230/4523",
        "google_scholar": "https://scholar.google.ca/citations?hl=en;lcPsDgUAAAAJ;;xTX3r0IAAAAJ;lD8d_W8AAAAJ;KhC8rtcAAAAJ;https://scholar.google.ca/citations?user=B1lAghgAAAAJ;fqDBtzYAAAAJ;vhP-tlcAAAAJ;VrFFU84AAAAJ",
        "orcid": ";;;;;0000-0002-3708-7803;;0000-0001-5365-0072;;",
        "linkedin": ";;https://linkedin.com/in/jamesnshiffer;genglin-liu-085101190/;sheriff-issaka/;rizwanparvez/;;kai-wei-chang-41239040;;",
        "or_profile": "~Salman_Rahman1;~Liwei_Jiang2;~James_Shiffer1;~Genglin_Liu1;~Sheriff_Issaka1;~Md_Rizwan_Parvez1;~Hamid_Palangi1;~Kai-Wei_Chang1;~Yejin_Choi1;~Saadia_Gabriel1",
        "aff": "UCLA Computer Science Department, University of California, Los Angeles;University of Washington;UCLA Computer Science Department, University of California, Los Angeles;UCLA Computer Science Department, University of California, Los Angeles;University of California, Los Angeles;Qatar Computing Research Institute;Google;University of California, Los Angeles+Amazon;Computer Science Department, Stanford University+NVIDIA;University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;washington.edu;cs.ucla.edu;cs.ucla.edu;ucla.edu;qcri.com;google.com;ucla.edu+amazon.com;cs.stanford.edu+nvidia.com;ucla.edu",
        "position": "PhD student;PhD student;MS student;PhD student;PhD student;Researcher;Staff Research Scientist;Associate Professor+Researcher;Full Professor+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nrahman2025xteaming,\ntitle={X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents},\nauthor={Salman Rahman and Liwei Jiang and James Shiffer and Genglin Liu and Sheriff Issaka and Md Rizwan Parvez and Hamid Palangi and Kai-Wei Chang and Yejin Choi and Saadia Gabriel},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=gKfj7Jb1kj}\n}",
        "github": "",
        "project": "",
        "reviewers": "pyz3;78J2;ktkW;MSYw",
        "site": "https://openreview.net/forum?id=gKfj7Jb1kj",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;3;5;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            28,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "gOKTe1KI8K",
        "title": "StagFormer: Time Staggering Decoder only Transformers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Standard decoding in a Transformer based language model is inherently sequential as we wait for a token\u2019s embedding to pass through all the layers in the network before starting the generation of the next token. In this work, we propose anew architecture StagFormer (Staggered Transformer), which staggered execution along the time axis and thereby enables parallelizing the decoding process along the depth of the model. We achieve this by breaking the dependency of the token representation at time step $i$ in layer $l$ upon the representations of tokens until time step $i$ from layer $l\u22121$. Instead, we stagger the execution and only allow a dependency on token representations until time step $i\u22121$. The later sections of the Transformer still get access to the \u201drich\u201d representations from the prior section but only from those token positions which are one time step behind. StagFormer allows for different sections of the model to be executed in parallel yielding up to 33% speedup in decoding while being quality neutral. We also explore many natural variants of this idea.  We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We show how one can approximate a recurrent model during inference using such weight-sharing. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore demonstrate the scalability of the staggering idea over more than 2 sections of the Transformer.",
        "keywords": "staggered execution;decoder only language models;efficiency;novel architectures;generative models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dylan J Cutler;Arun Kandoor;Nishanth Dikkala;Nikunj Saunshi;Xin Wang;Rina Panigrahy",
        "authorids": "~Dylan_J_Cutler1;~Arun_Kandoor1;~Nishanth_Dikkala1;~Nikunj_Saunshi1;~Xin_Wang30;~Rina_Panigrahy1",
        "gender": "M;M;M;;M;",
        "homepage": "https://github.com/DCtheTall;;http://people.csail.mit.edu/nishanthd/;https://www.nikunjsaunshi.com/;;",
        "dblp": ";299/1391;138/8092;199/2236;;p/RinaPanigrahy",
        "google_scholar": ";068aHyAAAAAJ;CMZoOTIAAAAJ;F24vXggAAAAJ;7BjA8ccAAAAJ;",
        "orcid": ";;;;;",
        "linkedin": ";arun-reddy-kandoor-920aba5/;;;;",
        "or_profile": "~Dylan_J_Cutler1;~Arun_Kandoor1;~Nishanth_Dikkala1;~Nikunj_Saunshi1;~Xin_Wang30;~Rina_Panigrahy1",
        "aff": "Google;Google;Google;Google;Google;Google",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "position": "Researcher;Researcher;Google Research;Researcher;Software Engineer;Research Scientist",
        "bibtex": "@inproceedings{\ncutler2025stagformer,\ntitle={StagFormer: Time Staggering Decoder only Transformers},\nauthor={Dylan J Cutler and Arun Kandoor and Nishanth Dikkala and Nikunj Saunshi and Xin Wang and Rina Panigrahy},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=gOKTe1KI8K}\n}",
        "github": "",
        "project": "",
        "reviewers": "n1xq;TYAW;UGau",
        "site": "https://openreview.net/forum?id=gOKTe1KI8K",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "5;3;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.9428090415820634
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -1.0
    },
    {
        "id": "ghyyHZYORi",
        "title": "Training Plug-and-Play Knowledge Modules with Deep Context Distillation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and retrieval-augmented generation.",
        "keywords": "knowledge extraction;document understanding;modular learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lucas Caccia;Alan Ansell;Edoardo Ponti;Ivan Vuli\u0107;Alessandro Sordoni",
        "authorids": "~Lucas_Caccia1;~Alan_Ansell1;~Edoardo_Ponti1;~Ivan_Vuli\u01071;~Alessandro_Sordoni2",
        "gender": "M;;;M;",
        "homepage": "https://www.cs.mcgill.ca/~lpagec/;;https://ducdauge.github.io/;https://sites.google.com/site/ivanvulic/;",
        "dblp": ";;178/8829;77/9768;",
        "google_scholar": "fuvIITUAAAAJ;;https://scholar.google.ca/citations?user=tklL2q0AAAAJ;ZX8js60AAAAJ;",
        "orcid": ";;0000-0002-6308-1050;;",
        "linkedin": ";;edoardo-maria-ponti/;ivan-vuli%C4%87-286b4a81/;",
        "or_profile": "~Lucas_Caccia1;~Alan_Ansell1;~Edoardo_Ponti1;~Ivan_Vuli\u01071;~Alessandro_Sordoni2",
        "aff": "Microsoft;;University of Edinburgh+NVIDIA;Google DeepMind+University of Cambridge;",
        "aff_domain": "microsoft.com;;ed.ac.uk+nvidia.com;google.com+cam.ac.uk;",
        "position": "Researcher;;Assistant Professor+Researcher;Researcher+Principal Research Associate;",
        "bibtex": "@inproceedings{\ncaccia2025training,\ntitle={Training Plug-and-Play Knowledge Modules with Deep Context Distillation},\nauthor={Lucas Caccia and Alan Ansell and Edoardo Ponti and Ivan Vuli{\\'c} and Alessandro Sordoni},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ghyyHZYORi}\n}",
        "github": "",
        "project": "",
        "reviewers": "1VZd;LZaV;ndgn;HsaC",
        "site": "https://openreview.net/forum?id=ghyyHZYORi",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "h5SRsDax8v",
        "title": "Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language models (LMs) tend to show human-like preferences on a number of syntactic phenomena, but the extent to which these are attributable to direct exposure to the phenomena or more general properties of language is unclear. We explore this with the English dative alternation (DO: \"gave Y the X\" vs. PO: \"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively train small LMs on systematically manipulated input. We focus on two properties that affect the choice of alternant: length and animacy. Both properties are directly present in datives but also reflect more global tendencies for shorter elements to precede longer ones and animates to precede inanimates. First, by manipulating and ablating datives for these biases in the input, we show that direct evidence of length and animacy matters, but easy-first preferences persist even without such evidence. Then, using LMs trained on systematically perturbed datasets to manipulate global length effects (re-linearizing sentences globally while preserving dependency structure), we find that dative preferences can emerge from indirect evidence. We conclude that LMs' emergent syntactic preferences come from a mix of direct and indirect sources.",
        "keywords": "linguistics;dative alternation;indirect evidence;language learning;cognitive science;linguistic constructions",
        "primary_area": "",
        "supplementary_material": "/attachment/95f95f9867e13b8b85bd505fc06b68fc5343fa85.zip",
        "author": "Qing Yao;Kanishka Misra;Leonie Weissweiler;Kyle Mahowald",
        "authorids": "~Qing_Yao1;~Kanishka_Misra1;~Leonie_Weissweiler1;~Kyle_Mahowald1",
        "gender": ";M;;M",
        "homepage": "https://dounick.github.io;https://kanishka.website/;https://www.cis.lmu.de/~weissweiler/;https://mahowak.github.io",
        "dblp": ";46/8138;212/0281;38/11196",
        "google_scholar": "kCe3H3YAAAAJ;-c6SAOMAAAAJ;o4fK4n4AAAAJ;XUmFLVUAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Qing_Yao1;~Kanishka_Misra1;~Leonie_Weissweiler1;~Kyle_Mahowald1",
        "aff": "University of Texas at Austin;Toyota Technological Institute at Chicago;Uppsala University+University of Texas at Austin;The University of Texas at Austin",
        "aff_domain": "utexas.edu;ttic.edu;uu.se+utexas.edu;utexas.edu",
        "position": "PhD student;Assistant Professor;Postdoc+Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nyao2025both,\ntitle={Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models},\nauthor={Qing Yao and Kanishka Misra and Leonie Weissweiler and Kyle Mahowald},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=h5SRsDax8v}\n}",
        "github": "",
        "project": "",
        "reviewers": "tLzp;BPUy;M82d;BYEk",
        "site": "https://openreview.net/forum?id=h5SRsDax8v",
        "pdf_size": 0,
        "rating": "6;8;8;8",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "h99hJlU99U",
        "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies across multiple chunks, since our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-segment relations.  We will release our code.",
        "keywords": "Mamba;Sub-Quadratic Models;Long Context;Long-Range Language Modeling;RNNs",
        "primary_area": "",
        "supplementary_material": "/attachment/d052d7d6c9526dfe93dd56e2282dfe73efb8d2a8.zip",
        "author": "Assaf Ben-Kish;Itamar Zimerman;Muhammad Jehanzeb Mirza;Lior Wolf;James R. Glass;Leonid Karlinsky;Raja Giryes",
        "authorids": "~Assaf_Ben-Kish1;~Itamar_Zimerman1;~Muhammad_Jehanzeb_Mirza1;~Lior_Wolf1;~James_R._Glass1;~Leonid_Karlinsky3;~Raja_Giryes1",
        "gender": "M;M;M;M;;M;M",
        "homepage": "https://assafbk.github.io/website/;;;http://www.cs.tau.ac.il/~wolf;;;https://www.giryes.sites.tau.ac.il/",
        "dblp": ";294/8621;295/9034;83/4103;;05/4463;50/7998",
        "google_scholar": "uHVM7F4AAAAJ;01s_DpwAAAAJ;cES2rkAAAAAJ;UbFrXTsAAAAJ;;https://scholar.google.co.il/citations?user=WbO7tjYAAAAJ;https://scholar.google.co.il/citations?user=9aQUYVQAAAAJ",
        "orcid": ";0000-0001-8321-0609;;0000-0001-5578-8892;;;0000-0002-2830-0297",
        "linkedin": "assaf-ben-kish;;;;;;raja-giryes-0818935/",
        "or_profile": "~Assaf_Ben-Kish1;~Itamar_Zimerman1;~Muhammad_Jehanzeb_Mirza1;~Lior_Wolf1;~James_R._Glass1;~Leonid_Karlinsky3;~Raja_Giryes1",
        "aff": "Massachusetts Institute of Technology+Tel Aviv University;Tel Aviv University;Massachusetts Institute of Technology;Tel Aviv University+Tel Aviv University+Tel Aviv University;;International Business Machines;Tel Aviv University",
        "aff_domain": "mit.edu+tau.ac.il;tau.ac.il;mit.edu;tau.ac.il+tau.ac.il+tau.ac.il;;ibm.com;tauex.tau.ac.il",
        "position": "Intern+PhD student;PhD student;Postdoc;Associate Professor+Professor+Full Professor;;Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nben-kish2025overflow,\ntitle={Overflow Prevention Enhances Long-Context Recurrent {LLM}s},\nauthor={Assaf Ben-Kish and Itamar Zimerman and Muhammad Jehanzeb Mirza and Lior Wolf and James R. Glass and Leonid Karlinsky and Raja Giryes},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=h99hJlU99U}\n}",
        "github": "",
        "project": "",
        "reviewers": "HYQK;g9uQ;jWUi;3P6K",
        "site": "https://openreview.net/forum?id=h99hJlU99U",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "5;4;4;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.5,
            0.5
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "hJkQL9VtWT",
        "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text but has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) The content generated by these models tends to be lengthy and lacks clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a tool augmented multi-task and multi-domain framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool with ChatGPT plugin in https://github.com/GAIR-NLP/factool.",
        "keywords": "factuality;llm;hallucination",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ethan Chern;Steffi Chern;Shiqi Chen;Weizhe Yuan;Kehua Feng;Chunting Zhou;Junxian He;Graham Neubig;Pengfei Liu",
        "authorids": "~Ethan_Chern1;~Steffi_Chern1;~Shiqi_Chen3;~Weizhe_Yuan1;~Kehua_Feng1;~Chunting_Zhou1;~Junxian_He1;~Graham_Neubig1;~Pengfei_Liu1",
        "gender": ";;F;F;M;F;M;M;M",
        "homepage": ";https://steffichern.github.io/;;http://yyy-apple.github.io/;https://weiji-feng.github.io/;https://violet-zct.github.io/;https://jxhe.github.io;http://phontron.com;http://pfliu.com/",
        "dblp": ";;;207/1964;;161/2679;188/6127.html;03/8155;34/3381-3",
        "google_scholar": ";https://scholar.google.com/citations?view_op=list_works;4Tg7zOMAAAAJ;2k5j4eMAAAAJ;PQVboTgAAAAJ;mR5W7EgAAAAJ;BIFGeoUAAAAJ;wlosgkoAAAAJ;oIz_CYEAAAAJ",
        "orcid": ";;;;;;;;",
        "linkedin": ";steffichern/;;weizhey/;;;;;",
        "or_profile": "~Ethan_Chern1;~Steffi_Chern1;~Shiqi_Chen3;~Weizhe_Yuan1;~Kehua_Feng1;~Chunting_Zhou1;~Junxian_He1;~Graham_Neubig1;~Pengfei_Liu1",
        "aff": ";University of Pennsylvania;Hong Kong University of Science and Technology+City University of Hong Kong;New York University;Zhejiang University;Meta AI;Hong Kong University of Science and Technology;Carnegie Mellon University;Shanghai Jiaotong University",
        "aff_domain": ";seas.upenn.edu;hkust.edu.hk+cityu.edu.hk;nyu.edu;zju.edu.cn;meta.com;ust.hk;cmu.edu;sjtu.edu",
        "position": ";PhD student;Intern+PhD student;PhD student;PhD student;Researcher;Assistant Professor;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nchern2025factool,\ntitle={FacTool: Factuality Detection in Generative {AI} -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},\nauthor={Ethan Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=hJkQL9VtWT}\n}",
        "github": "",
        "project": "",
        "reviewers": "qwzF;BTty;jo5j;7gdn",
        "site": "https://openreview.net/forum?id=hJkQL9VtWT",
        "pdf_size": 0,
        "rating": "4;6;6;7",
        "confidence": "5;1;3;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.0,
            1.4142135623730951
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.6488856845230502
    },
    {
        "id": "hJtvCfDfs1",
        "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the _Global North_ than the _Global South_, and the _Global West_ than the _Global East_. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, _free-form_ evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. \nWe release the dataset (Geo20Q+) and code at https://sites.google.com/view/llmbias20q/home.",
        "keywords": "geographic representation;LLM evaluation;fairness and bias;reasoning capabilities;cross-cultural NLP;interactive question answering",
        "primary_area": "",
        "supplementary_material": "/attachment/074af523bf6b035bfdf44772c9856eb01071c044.zip",
        "author": "Harsh Nishant Lalai;Raj Sanjay Shah;Jiaxin Pei;Sashank Varma;Yi-Chia Wang;Ali Emami",
        "authorids": "~Harsh_Nishant_Lalai1;~Raj_Sanjay_Shah2;~Jiaxin_Pei1;~Sashank_Varma1;~Yi-Chia_Wang2;~Ali_Emami3",
        "gender": "M;M;;;;M",
        "homepage": "https://sites.google.com/view/harsh-nishant-lalai/;https://raj-sanjay-shah.github.io/;;;;http://cosc.brocku.ca/~aemami/",
        "dblp": ";262/0806;228/5526;;71/2302;75/10772",
        "google_scholar": ";mpGOJ0gAAAAJ;bfPz_-8AAAAJ;;9gMgFPQAAAAJ;https://scholar.google.ca/citations?user=Pjdq8cUAAAAJ",
        "orcid": "0009-0009-6285-8479;0000-0002-0847-8426;;;;",
        "linkedin": "harshlalai/;;;;;",
        "or_profile": "~Harsh_Nishant_Lalai1;~Raj_Sanjay_Shah2;~Jiaxin_Pei1;~Sashank_Varma1;~Yi-Chia_Wang2;~Ali_Emami3",
        "aff": "Birla Institute of Technology and Science, Dhirubhai Ambani Institute Of Information and Communication Technology;Georgia Institute of Technology;Stanford University;;Stanford University;Emory University+Brock University",
        "aff_domain": "bits-pilani.ac.in;gatech.edu;stanford.edu;;stanford.edu;emory.edu+brocku.ca",
        "position": "Undergrad student;PhD student;Postdoc;;Researcher;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nlalai2025the,\ntitle={The World According to {LLM}s: How Geographic Origin Influences {LLM}s' Entity Deduction Capabilities},\nauthor={Harsh Nishant Lalai and Raj Sanjay Shah and Jiaxin Pei and Sashank Varma and Yi-Chia Wang and Ali Emami},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=hJtvCfDfs1}\n}",
        "github": "",
        "project": "",
        "reviewers": "2qWz;vWma;g6ic",
        "site": "https://openreview.net/forum?id=hJtvCfDfs1",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "5;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "hLg2rzBJR2",
        "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through **O**utcome **RE**w**A**rd-based reinforcement **L**earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data are available at https://github.com/InternLM/OREAL.",
        "keywords": "Large Language Model;Reinforcement Learning;Mathmatical Reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chengqi Lyu;Songyang Gao;Yuzhe Gu;Wenwei Zhang;Jianfei Gao;Kuikun Liu;Ziyi Wang;Shuaibin Li;Qian Zhao;Haian Huang;Weihan Cao;Jiangning Liu;Hongwei Liu;Junnan Liu;Songyang Zhang;Dahua Lin;Kai Chen",
        "authorids": "~Chengqi_Lyu1;~Songyang_Gao1;~Yuzhe_Gu1;~Wenwei_Zhang1;~Jianfei_Gao1;~Kuikun_Liu1;~Ziyi_Wang30;~Shuaibin_Li2;~Qian_Zhao10;~Haian_Huang1;~Weihan_Cao1;~Jiangning_Liu1;~Hongwei_Liu2;~Junnan_Liu1;~Songyang_Zhang1;~Dahua_Lin1;~Kai_Chen4",
        "gender": "M;M;;M;;M;M;;M;;M;M;M;M;M;M;M",
        "homepage": ";;http://guyuzhe.site/;https://zhangwenwei.cn;;;;;;;https://github.com/HIT-cwh;https://github.com/liujiangning30;https://www.cnblogs.com/liushz-blog/;https://github.com/spankeran;https://www.zhangsongyang.com/;http://dahua.site;https://chenkai.site/",
        "dblp": "319/5244;314/6067;;;;;;;;;;;;206/8503;;53/6088;181/2839-26",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;O42mLrsAAAAJ;NaiWQ5oAAAAJ;QDXADSEAAAAJ;;lcSm6RoAAAAJ;dCFQBKcAAAAJ;;FOgmoGwAAAAJ;;;;;https://scholar.google.com/citations?hl=en;8XQPi7YAAAAJ;GMzzRRUAAAAJ;https://scholar.google.com.hk/citations?user=eGD0b7IAAAAJ",
        "orcid": ";;;0000-0002-2748-4514;;;;;;;;;;;;;0000-0002-6820-2325",
        "linkedin": ";;;wenweizhang-b9769a124/;;;;;;;;;;;;;",
        "or_profile": "~Chengqi_Lyu1;~Songyang_Gao1;~Yuzhe_Gu1;~Wenwei_Zhang1;~Jianfei_Gao1;~Kuikun_Liu1;~Ziyi_Wang30;~Shuaibin_Li2;~Qian_Zhao10;~Haian_Huang1;~Weihan_Cao1;~Jiangning_Liu1;~Hongwei_Liu2;~Junnan_Liu1;~Songyang_Zhang1;~Dahua_Lin1;~Kai_Chen4",
        "aff": "Shanghai AI Laboratory;Shanghai AI Laboratory;Shanghai Jiaotong University+Shanghai AI Laboratory;Shanghai AI Laboratory;;Shanghai AI Laboratory;Shanghai AI Laboratory;;Shanghai AI Laboratory;;Shanghai AI Laboratory;Shanghai AI Laboratory;Shanghai AI Laboratory;Shanghai AI Laboratory;Shanghai AI Laboratory;The Chinese University of Hong Kong;Shanghai AI Laboratory",
        "aff_domain": "pjlab.org.cn;pjlab.org.cn;sjtu.edu.cn+pjlab.org.cn;pjlab.org.cn;;pjlab.org.cn;pjlab.org.cn;;pjlab.org.cn;;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;cuhk.edu.hk;pjlab.org.cn",
        "position": "Researcher;Researcher;PhD student+Intern;Researcher;;Researcher;Researcher;;Researcher;;Researcher;Researcher;Researcher;Intern;Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nlyu2025exploring,\ntitle={Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning},\nauthor={Chengqi Lyu and Songyang Gao and Yuzhe Gu and Wenwei Zhang and Jianfei Gao and Kuikun Liu and Ziyi Wang and Shuaibin Li and Qian Zhao and Haian Huang and Weihan Cao and Jiangning Liu and Hongwei Liu and Junnan Liu and Songyang Zhang and Dahua Lin and Kai Chen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=hLg2rzBJR2}\n}",
        "github": "",
        "project": "",
        "reviewers": "XBDg;cCHz;AWbc;qoai",
        "site": "https://openreview.net/forum?id=hLg2rzBJR2",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "1;3;3;2",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            2.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            17,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.30151134457776363
    },
    {
        "id": "hLjoekkPiJ",
        "title": "Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As visual assistant systems powered by visual language models (VLMs) become more prevalent, concerns over user privacy have grown, particularly for blind and low vision users who may unknowingly capture personal private information in their images. Existing privacy protection methods rely on coarse-grained segmentation, which uniformly masks entire private objects, often at the cost of usability. In this work, we propose FiG-Priv, a fine-grained privacy protection framework that selectively masks only high-risk private information while preserving low-risk information. Our approach integrates fine-grained segmentation with a data-driven risk scoring mechanism. By leveraging a more nuanced understanding of privacy risk, our method enables more effective protection without unnecessarily restricting users\u2019 access to critical information. We evaluate our framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26% of image content, enhancing the ability of VLMs to provide useful responses by 11% and identify the image content by 45%, while ensuring privacy protection.",
        "keywords": "visual language models;privacy;safety;accessibility",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jeffri Murrugarra-Llerena;Haoran Niu;K. Suzanne Barber;Hal Daum\u00e9 III;Yang Trista Cao;Paola Cascante-Bonilla",
        "authorids": "~Jeffri_Murrugarra-Llerena1;~Haoran_Niu1;~K._Suzanne_Barber1;~Hal_Daum\u00e9_III1;~Yang_Trista_Cao1;~Paola_Cascante-Bonilla1",
        "gender": ";F;;M;;",
        "homepage": ";;https://ece.utexas.edu/people/faculty/suzanne-barber;http://hal3.name;;https://paolacascante.com/",
        "dblp": ";;b/KSuzanneBarber;77/2856.html;;242/7369",
        "google_scholar": ";9-7KRd4AAAAJ;;PbEw81gAAAAJ;;4viWbgIAAAAJ",
        "orcid": ";;0000-0003-2906-6583;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Jeffri_Murrugarra-Llerena1;~Haoran_Niu1;~K._Suzanne_Barber1;~Hal_Daum\u00e9_III1;~Yang_Trista_Cao1;~Paola_Cascante-Bonilla1",
        "aff": ";University of Texas at Austin;University of Texas at Austin;University of Maryland, College Park;;State University of New York at Stony Brook+University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park",
        "aff_domain": ";utexas.edu;utexas.edu;umd.edu;;stonybrook.edu+umiacs.umd.edu",
        "position": ";PhD student;Full Professor;Full Professor;;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nmurrugarra-llerena2025beyond,\ntitle={Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users},\nauthor={Jeffri Murrugarra-Llerena and Haoran Niu and K. Suzanne Barber and Hal Daum{\\'e} III and Yang Trista Cao and Paola Cascante-Bonilla},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=hLjoekkPiJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "MxCN;BRNH;LRXX",
        "site": "https://openreview.net/forum?id=hLjoekkPiJ",
        "pdf_size": 0,
        "rating": "6;6;6",
        "confidence": "4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "i1uGbfHHpH",
        "title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language model post-training is applied to refine behaviors and unlock\nnew skills across a wide range of language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying\ntraining data and recipes for post-training are simultaneously the most im-\nportant pieces of the puzzle and the portion with the least transparency. To\nbridge this gap, we introduce T\u00dcLU 3, a family of fully-open state-of-the-art\npost-trained models, alongside its data, code, and training recipes, serving\nas a comprehensive guide for modern post-training techniques. T\u00dcLU 3,\nwhich builds on Llama 3.1 base models at 8B, 70B and 405B parameters,\nachieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5,\nand Mistral at comparable model sizes. The 405B T\u00dcLU 3 performs compet-\nitively against closed models such as GPT-4o-mini and Claude 3.5-Haiku or\nlarge open models like DeepSeek V3. The training algorithms for our mod-\nels include supervised finetuning (SFT), Direct Preference Optimization\n(DPO), and a novel method we call Reinforcement Learning with Verifiable\nRewards (RLVR). We detail varying the objective, model initialization, gen-\neralization, and over-optimization of this new RL finetuning method. With\nT\u00dcLU 3, we build a multi-task evaluation scheme for post-training with\ndevelopment and unseen evaluations, standard benchmark implementa-\ntions, and substantial decontamination of existing open datasets on said\nbenchmarks. The T\u00dcLU 3 release includes model weights, a demo, and the\ncomplete recipe \u2014 datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed recipe for reproducing and further adapting the\nT\u00dcLU 3 approach to more domains.",
        "keywords": "post-training;reinforcement learning;preference learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nathan Lambert;Jacob Morrison;Valentina Pyatkin;Shengyi Huang;Hamish Ivison;Faeze Brahman;Lester James Validad Miranda;Alisa Liu;Nouha Dziri;Xinxi Lyu;Yuling Gu;Saumya Malik;Victoria Graf;Jena D. Hwang;Jiangjiang Yang;Ronan Le Bras;Oyvind Tafjord;Christopher Wilhelm;Luca Soldaini;Noah A. Smith;Yizhong Wang;Pradeep Dasigi;Hannaneh Hajishirzi",
        "authorids": "~Nathan_Lambert1;~Jacob_Morrison2;~Valentina_Pyatkin1;~Shengyi_Huang1;~Hamish_Ivison1;~Faeze_Brahman1;~Lester_James_Validad_Miranda1;~Alisa_Liu1;~Nouha_Dziri2;~Xinxi_Lyu1;~Yuling_Gu1;~Saumya_Malik1;~Victoria_Graf1;~Jena_D._Hwang1;~Jiangjiang_Yang1;~Ronan_Le_Bras1;~Oyvind_Tafjord2;~Christopher_Wilhelm1;~Luca_Soldaini1;~Noah_A._Smith2;~Yizhong_Wang2;~Pradeep_Dasigi1;~Hannaneh_Hajishirzi1",
        "gender": "M;;;M;;F;M;F;;M;;F;F;F;M;M;M;M;Non-Binary;;M;M;F",
        "homepage": "https://natolambert.com;;;https://costa.sh/;https://hamishivi.github.io;https://fabrahman.github.io;https://ljvmiranda921.github.io;https://alisawuffles.github.io/;;;;https://www.linkedin.com/in/saumya-malik-983a11229/;;https://jenahwang.github.io/;;https://rlebras.github.io/index.html;;https://www.semanticscholar.org/author/Chris-Wilhelm/2270038642;https://soldaini.net;;https://yizhong-wang.com;https://pdasigi.github.io/;https://homes.cs.washington.edu/~hannaneh/",
        "dblp": "228/9584.html;;;251/8731;288/1956;276/6005;224/9490;;;314/6814;194/1346;;348/6122.html;83/10905;;;178/8640;;160/1741;;79/3601;27/7184;52/1296",
        "google_scholar": "O4jW7BsAAAAJ;;;kl9YcpEAAAAJ;;viCG2ikAAAAJ;https://scholar.google.co.jp/citations?user=2RtnNKEAAAAJ;3-lTFAwAAAAJ;;;;;0arBo88AAAAJ;9QuMhLgAAAAJ;;8dXLDSsAAAAJ;https://scholar.google.com/citations?hl=en;;3KPvwcgAAAAJ;;y5zpqdAAAAAJ;https://scholar.google.com/citations?authorid=Bpd76vcAAAAJ;LOV6_WIAAAAJ",
        "orcid": "0000-0002-9997-6817;;;;0000-0002-0069-7659;;;;;;;;;;0009-0009-9444-2136;;0000-0003-4190-5618;;0000-0001-6998-9863;;;0000-0001-7127-1316;",
        "linkedin": "nathan-lambert-55093468/;;;costa-huang/;;;;;;;yuling-gu/;;;;;;;;soldni/;;;;",
        "or_profile": "~Nathan_Lambert1;~Jacob_Morrison2;~Valentina_Pyatkin1;~Shengyi_Huang1;~Hamish_Ivison1;~Faeze_Brahman1;~Lester_James_Validad_Miranda1;~Alisa_Liu1;~Nouha_Dziri2;~Xinxi_Lyu1;~Yuling_Gu1;~Saumya_Malik1;~Victoria_Graf1;~Jena_D._Hwang1;~Jiangjiang_Yang1;~Ronan_Le_Bras1;~Oyvind_Tafjord2;~Christopher_Wilhelm1;~Luca_Soldaini1;~Noah_A._Smith2;~Yizhong_Wang2;~Pradeep_Dasigi1;~Hannaneh_Hajishirzi1",
        "aff": "Allen Institute for Artificial Intelligence;;;Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;NVIDIA+University of Washington;;Allen Institute for Artificial Intelligence;New York University+Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Google DeepMind+Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;;Department of Computer Science, University of Washington;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence+University of Washington",
        "aff_domain": "allenai.org;;;allenai.org;uw.edu;allenai.org;allenai.org;nvidia.com+uw.edu;;allenai.org;nyu.edu+allenai.org;allenai.org;uw.edu;allenai.org;allenai.org;allenai.org;google.com+allenai.org;allenai.org;allenai.org;;cs.washington.edu;allenai.org;allenai.org+uw.edu",
        "position": "Researcher;;;Researcher;PhD student;Researcher;Researcher;Intern+PhD student;;Researcher;PhD student+Predoctoral Young Investigator;Researcher;PhD student;Researcher;Engineer;Researcher;Researcher+Researcher;Researcher;Researcher;;PhD student;Research Scientist;senior director+Associate Professor",
        "bibtex": "@inproceedings{\nlambert2025tulu,\ntitle={Tulu 3: Pushing Frontiers in Open Language Model Post-Training},\nauthor={Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James Validad Miranda and Alisa Liu and Nouha Dziri and Xinxi Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Christopher Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=i1uGbfHHpH}\n}",
        "github": "",
        "project": "",
        "reviewers": "WQtE;BY8Z;yFmp",
        "site": "https://openreview.net/forum?id=i1uGbfHHpH",
        "pdf_size": 0,
        "rating": "7;8;9",
        "confidence": "3;4;5",
        "wc_review": "",
        "rating_avg": [
            8.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            23,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "jRGGmbhX2s",
        "title": "Post-training for Efficient Communication via Convention Formation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.",
        "keywords": "Interaction;Communication Efficiency;Linguistic Convention;Post-training;Alignment;LLM;In-context learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yilun Hua;Evan Wang;Yoav Artzi",
        "authorids": "~Yilun_Hua1;~Evan_Wang2;~Yoav_Artzi1",
        "gender": ";M;",
        "homepage": ";;",
        "dblp": ";;",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": ";ewang505/;",
        "or_profile": "~Yilun_Hua1;~Evan_Wang2;~Yoav_Artzi1",
        "aff": ";Cornell University;",
        "aff_domain": ";cornell.edu;",
        "position": ";MS student;",
        "bibtex": "@inproceedings{\nhua2025posttraining,\ntitle={Post-training for Efficient Communication via Convention Formation},\nauthor={Yilun Hua and Evan Wang and Yoav Artzi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jRGGmbhX2s}\n}",
        "github": "",
        "project": "",
        "reviewers": "Usy9;He5h;XAPw;uQr9",
        "site": "https://openreview.net/forum?id=jRGGmbhX2s",
        "pdf_size": 0,
        "rating": "6;6;6;9",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.299038105676658
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "jST2VzWUFb",
        "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Humans acquire language through implicit learning, absorbing complex patterns without explicit awareness. While large language models (LLMs) demonstrate impressive linguistic capabilities, it remains unclear whether they exhibit human-like pattern recognition during in-context learning at inferencing level. We adapted three classic artificial language learning experiments spanning morphology (regular/irregular plural marking), morphosyntax (context-dependent determiners), and syntax (finite state grammar) to systematically evaluate implicit learning at inferencing level in two state-of-the-art Openai models: gpt-4o (optimized for general language tasks) and o3-mini (specifically fine-tuned for explicit reasoning). This comparison allowed us to examine whether models trained to articulate reasoning processes differ in their ability to extract implicit patterns. Our findings reveal a complex picture: o3-mini demonstrated human-like probabilistic learning in morphological regularization, while gpt-4o showed stronger performance in finite state grammar acquisition. Neither model successfully replicated human patterns in the morphosyntax task. Post-experiment probes revealed correlations between models' performance and their ability to articulate underlying patterns, suggesting alignment between implicit recognition and explicit awareness. These results indicate that different LLMs implement distinct in-context processing mechanisms, with architecture and training objectives influencing pattern extraction across linguistic domains. Our study contributes to understanding in-context learning in LLMs and provides a novel framework for evaluating these models through the lens of cognitive science, highlighting both similarities and differences between human implicit learning and machine in-context pattern recognition.",
        "keywords": "implicit learning;artificial language learning;in-context learning;psycholinguistics;cognitive science",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaomeng Ma;Qihui Xu",
        "authorids": "~Xiaomeng_Ma1;~Qihui_Xu1",
        "gender": "F;F",
        "homepage": "https://xiaomeng-ma.github.io/;",
        "dblp": ";",
        "google_scholar": ";P1vb_HwAAAAJ",
        "orcid": ";0000-0002-5892-6442",
        "linkedin": ";",
        "or_profile": "~Xiaomeng_Ma1;~Qihui_Xu1",
        "aff": "Amazon;Ohio State University, Columbus",
        "aff_domain": "amazon.com;ohio-state.edu",
        "position": "Researcher;Postdoc",
        "bibtex": "@inproceedings{\nma2025implicit,\ntitle={Implicit In-Context Learning: Evidence from Artificial Language Experiments},\nauthor={Xiaomeng Ma and Qihui Xu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jST2VzWUFb}\n}",
        "github": "",
        "project": "",
        "reviewers": "CGzC;FBZf;FRjt",
        "site": "https://openreview.net/forum?id=jST2VzWUFb",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "4;4;5",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9999999999999998
    },
    {
        "id": "jSmpq7IRYe",
        "title": "Can Test-Time Scaling Improve World Foundation Model?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. \nAs such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training.\nIn this work, we introduce **SWIFT**, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search.\nEmpirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. Project page: [https://scalingwfm.github.io/](https://scalingwfm.github.io/).",
        "keywords": "world foundation model;test time scaling;autoregressive video generation;evaluation tookit",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenyan Cong;Hanqing Zhu;Peihao Wang;Bangya Liu;Dejia Xu;Kevin Wang;David Z. Pan;Yan Wang;Zhiwen Fan;Zhangyang Wang",
        "authorids": "~Wenyan_Cong1;~Hanqing_Zhu1;~Peihao_Wang1;~Bangya_Liu1;~Dejia_Xu1;~Kevin_Wang4;~David_Z._Pan1;~Yan_Wang10;~Zhiwen_Fan2;~Zhangyang_Wang1",
        "gender": "F;M;M;M;M;M;M;M;;M",
        "homepage": "https://wenyancong.com/;https://zhuhanqing.github.io/;https://peihaowang.github.io/;https://pages.cs.wisc.edu/~bangya/;https://ir1d.github.io;;http://users.ece.utexas.edu/~dpan/;https://www.cs.cornell.edu/~yanwang/;;https://vita-group.github.io",
        "dblp": "247/9471;164/8690;239/4075;275/9810;264/5685;;p/DavidZhigangPan.html;59/2227;;119/4026",
        "google_scholar": "uQV5aCsAAAAJ;myMcrNEAAAAJ;fqf2tBsAAAAJ;https://scholar.google.com/citations?hl=en;ET0e93cAAAAJ;;3aLlroEAAAAJ;nZsD8XwAAAAJ;;pxFyKAIAAAAJ",
        "orcid": ";;;0009-0007-4758-0734;0000-0001-8474-3095;;0000-0002-5705-2501;;;",
        "linkedin": ";;peihao-wang-25a411162/;;;kevin-wang-01/;davidzpan/;;;",
        "or_profile": "~Wenyan_Cong1;~Hanqing_Zhu1;~Peihao_Wang1;~Bangya_Liu1;~Dejia_Xu1;~Kevin_Wang4;~David_Z._Pan1;~Yan_Wang10;~Zhiwen_Fan2;~Zhangyang_Wang1",
        "aff": "University of Texas at Austin;University of Texas, Austin;University of Texas, Austin;University of Wisconsin - Madison;Luma AI+University of Texas at Austin;University of Texas at Austin;University of Texas, Austin;NVIDIA;;University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;wisc.edu;lumalabs.ai+utexas.edu;utexas.edu;utexas.edu;nvidia.com;;utexas.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Researcher+PhD student;PhD student;Professor;Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\ncong2025can,\ntitle={Can Test-Time Scaling Improve World Foundation Model?},\nauthor={Wenyan Cong and Hanqing Zhu and Peihao Wang and Bangya Liu and Dejia Xu and Kevin Wang and David Z. Pan and Yan Wang and Zhiwen Fan and Zhangyang Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jSmpq7IRYe}\n}",
        "github": "",
        "project": "",
        "reviewers": "qtYN;vAmQ;oo4B",
        "site": "https://openreview.net/forum?id=jSmpq7IRYe",
        "pdf_size": 0,
        "rating": "5;7;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.49999999999999983
    },
    {
        "id": "jXP9bgFack",
        "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with relative improvements of 15.66%, 7.42%, and 9.40%. It also generalizes to more challenging benchmarks, marking an early advance in multi-agent cooperative training.",
        "keywords": "reasoning;multi-agent systems;post-training;reinforcement learning;large language models",
        "primary_area": "",
        "supplementary_material": "/attachment/c362d320dbebaac8a22e3ce459c2bf0981b28842.zip",
        "author": "Sumeet Ramesh Motwani;Chandler Smith;Rocktim Jyoti Das;Rafael Rafailov;Philip Torr;Ivan Laptev;Fabio Pizzati;Ronald Clark;Christian Schroeder de Witt",
        "authorids": "~Sumeet_Ramesh_Motwani1;~Chandler_Smith1;~Rocktim_Jyoti_Das2;~Rafael_Rafailov1;~Philip_Torr1;~Ivan_Laptev1;~Fabio_Pizzati1;~Ronald_Clark2;~Christian_Schroeder_de_Witt1",
        "gender": "M;M;;M;;M;;;M",
        "homepage": "https://sumeetmotwani.com;https://chandlersmith.me/;;https://rmrafailov.github.io/;http://www.robots.ox.ac.uk/~tvg/;https://www.di.ens.fr/~laptev/;https://fabvio.github.io;;https://www.schroederdewitt.com",
        "dblp": ";;;272/5358;;41/1854;241/5366;;",
        "google_scholar": ";MW32guUAAAAJ;;TwABcRgAAAAJ;;https://scholar.google.com.tw/citations?user=-9ifK0cAAAAJ;kA_l7GYAAAAJ;;DE60h_0AAAAJ",
        "orcid": ";;;;;;;;",
        "linkedin": ";chandlerdsmith;;;;;;;",
        "or_profile": "~Sumeet_Ramesh_Motwani1;~Chandler_Smith1;~Rocktim_Jyoti_Das2;~Rafael_Rafailov1;~Philip_Torr1;~Ivan_Laptev1;~Fabio_Pizzati1;~Ronald_Clark2;~Christian_Schroeder_de_Witt1",
        "aff": "University of Oxford;Cooperative AI Foundation;;Stanford University;University of Oxford;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;;University of Oxford",
        "aff_domain": "oxford.ac.uk;cooperativeai.org;;stanford.edu;ox.ac.uk;mbzuai.ac.ae;mbzuai.ac.ae;;oxford.ac.uk",
        "position": "PhD student;Researcher;;PhD student;Full Professor;Full Professor;Postdoc;;Lecturer",
        "bibtex": "@inproceedings{\nmotwani2025malt,\ntitle={{MALT}: Improving Reasoning with Multi-Agent {LLM} Training},\nauthor={Sumeet Ramesh Motwani and Chandler Smith and Rocktim Jyoti Das and Rafael Rafailov and Philip Torr and Ivan Laptev and Fabio Pizzati and Ronald Clark and Christian Schroeder de Witt},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jXP9bgFack}\n}",
        "github": "",
        "project": "",
        "reviewers": "Shaf;rgPo;s2iA",
        "site": "https://openreview.net/forum?id=jXP9bgFack",
        "pdf_size": 0,
        "rating": "4;8;9",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            2.160246899469287
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            9,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "jdOC24msVq",
        "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.",
        "keywords": "Encoder;Multilingual;EuroBERT;Training;Vector Representations;Bidirectional;European",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicolas Boizard;Hippolyte Gisserot-Boukhlef;Duarte Miguel Alves;Andre Martins;Ayoub Hammal;Caio Corro;CELINE HUDELOT;Emmanuel Malherbe;Etienne Malaboeuf;Fanny Jourdan;Gabriel Hautreux;Jo\u00e3o Alves;Kevin El Haddad;Manuel Faysse;Maxime Peyrard;Nuno M Guerreiro;Patrick Fernandes;Ricardo Rei;Pierre Colombo",
        "authorids": "~Nicolas_Boizard1;~Hippolyte_Gisserot-Boukhlef1;~Duarte_Miguel_Alves1;~Andre_Martins1;~Ayoub_Hammal1;~Caio_Corro2;~CELINE_HUDELOT1;~Emmanuel_Malherbe3;~Etienne_Malaboeuf1;~Fanny_Jourdan1;~Gabriel_Hautreux2;~Jo\u00e3o_Alves2;~Kevin_El_Haddad1;~Manuel_Faysse1;~Maxime_Peyrard2;~Nuno_M_Guerreiro1;~Patrick_Fernandes1;~Ricardo_Rei1;~Pierre_Colombo2",
        "gender": "M;M;M;M;M;M;F;M;;F;M;M;;M;M;;;;M",
        "homepage": "https://nicolas-bzrd.github.io;;;https://andre-martins.github.io/;;http://caio-corro.fr/;http://perso.ecp.fr/~hudelotc/;;;https://fanny-jourdan.github.io;;;;https://manuelfay.github.io/;https://peyrardm.github.io;https://nunonmg.github.io/;https://coderpat.github.io;;https://pierrecolombo.github.io/",
        "dblp": ";;329/4709;m/AndreFTMartins;355/6465;184/3727;https://dblp.uni-trier.de/pers/hd/h/Hudelot:C=eacute=line;;;;;;;359/3589;184/3721;267/0265;207/6964.html;;",
        "google_scholar": "B-jY8EkAAAAJ;;;https://scholar.google.pt/citations?user=mT7ppvwAAAAJ;r7tQy5gAAAAJ;Q_DmlucAAAAJ;https://scholar.google.fr/citations?user=gFlAh6MAAAAJ;;;ntU8A30AAAAJ;;;S3Q9SAsAAAAJ;ew4xsR4AAAAJ;RFMdKLMAAAAJ;268m9FgAAAAJ;;;yPoMt8gAAAAJ",
        "orcid": ";;0009-0007-2109-9555;;;;0000-0003-3849-4133;;;;0009-0000-9127-336X;;0000-0003-1465-6273;;;;;;",
        "linkedin": "nicolas-boizard/;hippolyte-gisserot-boukhlef-10977519a/;duarte-alves;;;;;emmanuel-malherbe-0b60a440/en;;fanny-jourdan/;;jmc-alves/;kevinelhaddad/;manuel-faysse/;;;;;",
        "or_profile": "~Nicolas_Boizard1;~Hippolyte_Gisserot-Boukhlef1;~Duarte_Miguel_Alves1;~Andre_Martins1;~Ayoub_Hammal1;~Caio_Corro2;~CELINE_HUDELOT1;~Emmanuel_Malherbe3;~Etienne_Malaboeuf1;~Fanny_Jourdan1;~Gabriel_Hautreux2;~Jo\u00e3o_Alves2;~Kevin_El_Haddad1;~Manuel_Faysse1;~Maxime_Peyrard2;~Nuno_M_Guerreiro1;~Patrick_Fernandes1;~Ricardo_Rei1;~Pierre_Colombo2",
        "aff": "CentraleSupelec;CentraleSupelec;Instituto Superior T\u00e9cnico;Instituto Superior T\u00e9cnico+Unbabel;Universit\u00e9 Paris-Saclay;Sorbonne Universit\u00e9;CentraleSupelec;Artefact;;IRT Saint Exupery;LIRMM;Unbabel;Diabolocom+University of Mons;CentraleSupelec;CNRS;Unbabel+Instituto Superior T\u00e9cnico;School of Computer Science, Carnegie Mellon University+Instituto Superior T\u00e9cnico;;CentraleSupelec",
        "aff_domain": "centralesupelec.fr;centralesupelec.fr;tecnico.ulisboa.pt;tecnico.ulisboa.pt+unbabel.com;universite-paris-saclay.fr;isir.upmc.fr;centralesupelec.fr;artefact.com;;irt-saintexupery.com;lirmm.fr;unbabel.com;diabolocom.com+umons.ac.be;centralesupelec.fr;cnrs.fr;unbabel.com+tecnico.ulisboa.pt;cs.cmu.edu+tecnico.ulisboa.pt;;centralesupelec.fr",
        "position": "PhD student;PhD student;PhD student;Associate Professor+Research Scientist;PhD student;Associate Professor;Full Professor;Research director;;Researcher;PhD student;Researcher;Principal Researcher+Principal Researcher;PhD student;Associate Professor;Researcher+PhD student;PhD student+PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nboizard2025eurobert,\ntitle={Euro{BERT}: Scaling Multilingual Encoders for European Languages},\nauthor={Nicolas Boizard and Hippolyte Gisserot-Boukhlef and Duarte Miguel Alves and Andre Martins and Ayoub Hammal and Caio Corro and CELINE HUDELOT and Emmanuel Malherbe and Etienne Malaboeuf and Fanny Jourdan and Gabriel Hautreux and Jo{\\~a}o Alves and Kevin El Haddad and Manuel Faysse and Maxime Peyrard and Nuno M Guerreiro and Patrick Fernandes and Ricardo Rei and Pierre Colombo},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jdOC24msVq}\n}",
        "github": "",
        "project": "",
        "reviewers": "iq5e;aExR;6A47;pgGu",
        "site": "https://openreview.net/forum?id=jdOC24msVq",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            19,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "jeDYcjuZIV",
        "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time computation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft.  Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.",
        "keywords": "LLM;Writing;Reward Models;Alignment;Human Centered NLP;Test Time Compute;Text Editing;Behavioral Science",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tuhin Chakrabarty;Philippe Laban;Chien-Sheng Wu",
        "authorids": "~Tuhin_Chakrabarty2;~Philippe_Laban1;~Chien-Sheng_Wu1",
        "gender": "M;M;M",
        "homepage": "https://tuhinjubcse.github.io/;https://people.eecs.berkeley.edu/~phillab/;http://jasonwu0731.github.io",
        "dblp": "227/2812;220/3590;180/5537",
        "google_scholar": "HCmFuo8AAAAJ;fR5t200AAAAJ;1G4GV2EAAAAJ",
        "orcid": ";;",
        "linkedin": ";;chien-sheng-jason-wu/",
        "or_profile": "~Tuhin_Chakrabarty2;~Philippe_Laban1;~Chien-Sheng_Wu1",
        "aff": "SalesForce Research+State University of New York at Stony Brook;Microsoft;Salesforce AI",
        "aff_domain": "salesforce.com+stonybrook.edu;microsoft.com;salesforce.com",
        "position": "Researcher+Assistant Professor;Researcher;Researcher",
        "bibtex": "@inproceedings{\nchakrabarty2025aislop,\ntitle={{AI}-Slop to {AI}-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time computation},\nauthor={Tuhin Chakrabarty and Philippe Laban and Chien-Sheng Wu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jeDYcjuZIV}\n}",
        "github": "",
        "project": "",
        "reviewers": "YjfD;EPcP;Ac9o;8c7P",
        "site": "https://openreview.net/forum?id=jeDYcjuZIV",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.13245323570650439
    },
    {
        "id": "jnRBe6zatP",
        "title": "FineWeb2: One Pipeline to Scale Them All \u2014 Adapting Pre-Training Data Processing to Every Language",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of 9 diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.",
        "keywords": "multilingual;dataset;pretraining;web data;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Guilherme Penedo;Hynek Kydl\u00ed\u010dek;Vinko Sabol\u010dec;Bettina Messmer;Negar Foroutan;Amir Hossein Kargaran;Colin Raffel;Martin Jaggi;Leandro Von Werra;Thomas Wolf",
        "authorids": "~Guilherme_Penedo1;~Hynek_Kydl\u00ed\u010dek1;~Vinko_Sabol\u010dec1;~Bettina_Messmer1;~Negar_Foroutan1;~Amir_Hossein_Kargaran1;~Colin_Raffel1;~Martin_Jaggi1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "gender": "M;M;Not Specified;;F;M;;M;M;M",
        "homepage": "https://github.com/guipenedo;https://me.hynky.name/;https://people.epfl.ch/vinko.sabolcec?lang=en;https://people.epfl.ch/bettina.messmer;http://negar.foroutan.info/;https://kargaranamir.github.io/;http://colinraffel.com;https://mlo.epfl.ch;https://github.com/lvwerra;https://thomwolf.io",
        "dblp": ";;400/2190;;174/4070;261/9248;149/0082;17/4402;223/1855;",
        "google_scholar": "L-jmoJYAAAAJ;;;;jHeHoScAAAAJ;2idwpjcAAAAJ;I66ZBYwAAAAJ;https://scholar.google.ch/citations?user=r1TJBr8AAAAJ;https://scholar.google.com/citations?hl=en;D2H5EFEAAAAJ",
        "orcid": ";;;;;0000-0001-6253-1315;;0000-0003-1579-5558;;",
        "linkedin": ";;;;;amirkargaran/;;;lvwerra/;",
        "or_profile": "~Guilherme_Penedo1;~Hynek_Kydl\u00ed\u010dek1;~Vinko_Sabol\u010dec1;~Bettina_Messmer1;~Negar_Foroutan1;~Amir_Hossein_Kargaran1;~Colin_Raffel1;~Martin_Jaggi1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "aff": "HuggingFace;Huggingface;EPFL - EPF Lausanne;EPFL - EPF Lausanne;School of Computer and Communication Sciences, EPFL - EPF Lausanne;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen+Hugging Face;Department of Computer Science, University of Toronto+Hugging Face;EPFL;Hugging Face;Hugging Face",
        "aff_domain": "huggingface.co;huggingface.co;epfl.ch;epfl.ch;ic.epfl.ch;lmu.de+huggingface.co;cs.toronto.edu+huggingface.co;epfl.ch;hf.co;huggingface.co",
        "position": "Researcher;Intern;PhD student;PhD student;PhD student;PhD student+Intern;Associate Professor+Researcher;Associate Professor;Researcher;Researcher",
        "bibtex": "@inproceedings{\npenedo2025fineweb,\ntitle={FineWeb2: One Pipeline to Scale Them All {\\textemdash} Adapting Pre-Training Data Processing to Every Language},\nauthor={Guilherme Penedo and Hynek Kydl{\\'\\i}{\\v{c}}ek and Vinko Sabol{\\v{c}}ec and Bettina Messmer and Negar Foroutan and Amir Hossein Kargaran and Colin Raffel and Martin Jaggi and Leandro Von Werra and Thomas Wolf},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=jnRBe6zatP}\n}",
        "github": "",
        "project": "",
        "reviewers": "faKg;RcCQ;A835;5fxD",
        "site": "https://openreview.net/forum?id=jnRBe6zatP",
        "pdf_size": 0,
        "rating": "8;8;8;9",
        "confidence": "3;4;5;4",
        "wc_review": "",
        "rating_avg": [
            8.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "k72RxnoS5g",
        "title": "AdaptMI: Adaptive Skill-based In-context Math Instructions for Small Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model\u2019s (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM\u2019s metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. \nWe investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model\u2019s responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B\u20137B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.",
        "keywords": "Small language models;large language models;in-context learning;natural language processing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yinghui He;Abhishek Panigrahi;Yong Lin;Sanjeev Arora",
        "authorids": "~Yinghui_He1;~Abhishek_Panigrahi1;~Yong_Lin2;~Sanjeev_Arora1",
        "gender": "F;M;;",
        "homepage": "https://ying-hui-he.github.io/;https://abhishekpanigrahi1996.github.io/;;http://www.cs.princeton.edu/~arora/",
        "dblp": ";208/4926;;a/SArora",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.co.in/citations?user=oMhp8p8AAAAJ;;RUP4S68AAAAJ",
        "orcid": ";;;",
        "linkedin": "yinghui-he-8b147321a/;;;",
        "or_profile": "~Yinghui_He1;~Abhishek_Panigrahi1;~Yong_Lin2;~Sanjeev_Arora1",
        "aff": "Princeton University;Princeton University;;Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;;princeton.edu",
        "position": "PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nhe2025adaptmi,\ntitle={Adapt{MI}: Adaptive Skill-based In-context Math Instructions for Small Language Models},\nauthor={Yinghui He and Abhishek Panigrahi and Yong Lin and Sanjeev Arora},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=k72RxnoS5g}\n}",
        "github": "",
        "project": "",
        "reviewers": "Qc6H;8zxM;nCJe;qoNx;HGMM",
        "site": "https://openreview.net/forum?id=k72RxnoS5g",
        "pdf_size": 0,
        "rating": "6;6;7;7;8",
        "confidence": "4;5;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.8,
            0.7483314773547882
        ],
        "confidence_avg": [
            4.0,
            0.6324555320336759
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.42257712736425823
    },
    {
        "id": "kH6LOHGjEl",
        "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important.\nIn particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment.\nIn this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection.\nTo study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions.\nOur analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes.\nSurprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation.\nThese findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration.",
        "keywords": "Large Language Models;Multi-Agent;Cooperation;Social Dilemmas;Public Goods Games;Institutional Choice;Costly Sanctioning;Norm Enforcement;Reasoning",
        "primary_area": "",
        "supplementary_material": "/attachment/e77bf91abe796499b424be1e4bdb5bc6a3541fc2.zip",
        "author": "David Guzman Piedrahita;Yongjin Yang;Mrinmaya Sachan;Giorgia Ramponi;Bernhard Sch\u00f6lkopf;Zhijing Jin",
        "authorids": "~David_Guzman_Piedrahita1;~Yongjin_Yang1;~Mrinmaya_Sachan3;~Giorgia_Ramponi1;~Bernhard_Sch\u00f6lkopf1;~Zhijing_Jin1",
        "gender": "M;M;;F;;",
        "homepage": ";https://yangyongjin.github.io/;;https://gioramponi.github.io/;;",
        "dblp": "382/9812;159/8412;;186/4493;;",
        "google_scholar": "2VxJWs4AAAAJ;qGVZm3sAAAAJ;;xbIAH5gAAAAJ;;",
        "orcid": ";0009-0005-3293-1904;;;;",
        "linkedin": "davidguzman1120/;yongjin-yang-0195a6184/;;;;",
        "or_profile": "~David_Guzman_Piedrahita1;~Yongjin_Yang1;~Mrinmaya_Sachan3;~Giorgia_Ramponi1;~Bernhard_Sch\u00f6lkopf1;~Zhijing_Jin1",
        "aff": "Department of Informatics, University of Zurich, University of Zurich;University of Toronto+KAIST;;Department of Informatics, University of Zurich, University of Zurich;;",
        "aff_domain": "ifi.uzh.ch;cs.toronto.edu+kaist.ac.kr;;ifi.uzh.ch;;",
        "position": "MS student;PhD student+MS student;;Assistant Professor;;",
        "bibtex": "@inproceedings{\npiedrahita2025corrupted,\ntitle={Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games},\nauthor={David Guzman Piedrahita and Yongjin Yang and Mrinmaya Sachan and Giorgia Ramponi and Bernhard Sch{\\\"o}lkopf and Zhijing Jin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=kH6LOHGjEl}\n}",
        "github": "",
        "project": "",
        "reviewers": "gBFf;cqML;WHGr",
        "site": "https://openreview.net/forum?id=kH6LOHGjEl",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "kVOrGZM5N7",
        "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. \nIn this work, we propose RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL$^V$ boosts MATH accuracy by over 20\\% with parallel sampling and enables $8-32\\times$ efficient test-time compute scaling compared to the base RL method. RL$^V$ also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves $1.5-2\\times$ higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.",
        "keywords": "LLM Reasoning;Verifiers;Test-time scaling;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kusha Sareen;Morgane M Moss;Alessandro Sordoni;Arian Hosseini;Rishabh Agarwal",
        "authorids": "~Kusha_Sareen1;~Morgane_M_Moss1;~Alessandro_Sordoni2;~Arian_Hosseini1;~Rishabh_Agarwal2",
        "gender": "M;;;M;M",
        "homepage": "https://kushasareen.github.io/;;;;https://agarwl.github.io",
        "dblp": "348/9545;;;218/5690;",
        "google_scholar": "Ah1II-oAAAAJ;https://scholar.google.fr/citations?user=9U8Cmi4AAAAJ;;https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=aH8AJu4AAAAJ",
        "orcid": ";;;;",
        "linkedin": "https://ca.linkedin.com/in/kushasareen;;;;",
        "or_profile": "~Kusha_Sareen1;~Morgane_M_Moss1;~Alessandro_Sordoni2;~Arian_Hosseini1;~Rishabh_Agarwal2",
        "aff": "Mila - Quebec Artificial Intelligence Institute;Mila - Quebec Artificial Intelligence Institute;;Google+University of Montreal+Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal;Meta Facebook+McGill University+Google DeepMind",
        "aff_domain": "mila.quebec;mila.quebec;;deepmind.com+umontreal.ca+mila.umontreal.ca;meta.com+mcgill.ca+google.com",
        "position": "MS student;PhD student;;Researcher+PhD student+PhD student;Researcher+Adjunct Professor+Research Scientist",
        "bibtex": "@inproceedings{\nsareen2025putting,\ntitle={Putting the Value Back in {RL}: Better Test-Time Scaling by Unifying {LLM} Reasoners With Verifiers},\nauthor={Kusha Sareen and Morgane M Moss and Alessandro Sordoni and Arian Hosseini and Rishabh Agarwal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=kVOrGZM5N7}\n}",
        "github": "",
        "project": "",
        "reviewers": "f9F3;BvLu;Sqzo;SGCT",
        "site": "https://openreview.net/forum?id=kVOrGZM5N7",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "kaPAalWAp3",
        "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from a trained model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic SAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning---offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.",
        "keywords": "unlearning;sparse autoencoders;representation editing;steering vectors;relearning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Aashiq Muhamed;Jacopo Bonato;Mona T. Diab;Virginia Smith",
        "authorids": "~Aashiq_Muhamed1;~Jacopo_Bonato1;~Mona_T._Diab1;~Virginia_Smith1",
        "gender": "M;M;F;F",
        "homepage": "https://github.com/aashiqmuhamed;;https://www.seas.gwu.edu/~mtdiab/;",
        "dblp": "294/0107;;15/4305;120/0921",
        "google_scholar": "GbVC5NYAAAAJ;tC1GFkUAAAAJ;https://scholar.google.com.tw/citations?user=-y6SIhQAAAAJ;",
        "orcid": "0000-0002-8657-0439;0000-0001-6751-3407;;",
        "linkedin": "aashiq-muhamed-52169421/;jacopo-bonato92;mona-diab-55946614/;",
        "or_profile": "~Aashiq_Muhamed1;~Jacopo_Bonato1;~Mona_T._Diab1;~Virginia_Smith1",
        "aff": "Carnegie Mellon University;Prometeia+LeonardoLabs;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;prometeia.com+leonardo.com;cmu.edu;cmu.edu",
        "position": "PhD student;Researcher+Researcher;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nmuhamed2025saes,\ntitle={{SAE}s Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in {LLM}s},\nauthor={Aashiq Muhamed and Jacopo Bonato and Mona T. Diab and Virginia Smith},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=kaPAalWAp3}\n}",
        "github": "",
        "project": "",
        "reviewers": "teuD;mnFd;5A4g;JnnW",
        "site": "https://openreview.net/forum?id=kaPAalWAp3",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;5;2;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.2581988897471611
    },
    {
        "id": "kjNJYWvfPA",
        "title": "How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance across a wide range of vision-language tasks, yet their internal processing dynamics remain underexplored. In this work, we introduce a probing framework to systematically analyze how MLLMs process visual and textual inputs across layers. We train linear classifiers to predict fine-grained visual categories (e.g., dog breeds) from token embeddings extracted at each layer, using a standardized anchor question. To uncover the functional roles of different layers, we evaluate these probes under three types of controlled prompt variations: (1) lexical variants that test sensitivity to surface-level changes, (2) semantic negation variants that flip the expected answer by modifying the visual concept in the prompt, and (3) output format variants that preserve reasoning but alter the answer format. Applying our framework to LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent stage-wise structure in which early layers perform visual grounding, middle layers support lexical integration and semantic reasoning, and final layers prepare task-specific outputs. We further show that while the overall stage-wise structure remains stable across variations in visual tokenization, instruction tuning data, and pretraining corpus, the specific layer allocation to each stage shifts notably with changes in the base LLM architecture. Our findings provide a unified perspective on the layer-wise organization of MLLMs and offer a lightweight, model-agnostic approach for analyzing multimodal representation dynamics.",
        "keywords": "Multimodal LLM;Interpretability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhuoran Yu;Yong Jae Lee",
        "authorids": "~Zhuoran_Yu2;~Yong_Jae_Lee2",
        "gender": "M;",
        "homepage": "https://www.zhuoranyu.com;",
        "dblp": "120/3973;",
        "google_scholar": "txxhxREAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Zhuoran_Yu2;~Yong_Jae_Lee2",
        "aff": "University of Wisconsin, Madison;",
        "aff_domain": "wisc.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nyu2025how,\ntitle={How Multimodal {LLM}s Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding},\nauthor={Zhuoran Yu and Yong Jae Lee},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=kjNJYWvfPA}\n}",
        "github": "",
        "project": "",
        "reviewers": "nNBQ;C29N;oi97;RU2Z",
        "site": "https://openreview.net/forum?id=kjNJYWvfPA",
        "pdf_size": 0,
        "rating": "4;5;6;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.7745966692414834
    },
    {
        "id": "kkBCNLMbGj",
        "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, produces a challenging and relevant query that requires reasoning to match, as well as a plausibly related but ultimately unhelpful hard negative. By training on a mixture of this synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it outperforms other retrievers when combined with our simple-yet-effective tie-breaking LLM reranker (36.9 nDCG@10). When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. \nOur training recipe is general and can be easily extended to future LLMs.",
        "keywords": "retriever;information retrieval;retrieval-augmented generation;reasoning;synthetic data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rulin Shao;Rui Qiao;Varsha Kishore;Niklas Muennighoff;Xi Victoria Lin;Daniela Rus;Bryan Kian Hsiang Low;Sewon Min;Wen-tau Yih;Pang Wei Koh;Luke Zettlemoyer",
        "authorids": "~Rulin_Shao1;~Rui_Qiao3;~Varsha_Kishore1;~Niklas_Muennighoff1;~Xi_Victoria_Lin1;~Daniela_Rus1;~Bryan_Kian_Hsiang_Low1;~Sewon_Min1;~Wen-tau_Yih1;~Pang_Wei_Koh1;~Luke_Zettlemoyer1",
        "gender": ";M;F;M;F;F;M;F;M;M;M",
        "homepage": "https://rulinshao.github.io/;https://qiaoruiyt.github.io/;;https://muennighoff.github.io/;http://victorialin.net;https://www.csail.mit.edu/person/daniela-rus;http://www.comp.nus.edu.sg/~lowkh;https://www.sewonmin.com;http://scottyih.org;http://cs.stanford.edu/~pangwei;https://www.cs.washington.edu/people/faculty/lsz/",
        "dblp": ";31/3517-6;239/5696;281/6745;215/5264;r/DanielaRus;97/4877;203/9401;07/7129;10/10453;21/6793",
        "google_scholar": "Vdwh6bcAAAAJ;Ox5Z9EwAAAAJ;;Me0IoRMAAAAJ;gYUOJwMAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=2P-Q09UAAAAJ;https://scholar.google.ca/citations?user=jU4IZs4AAAAJ;8rDNIMsAAAAJ;Nn990CkAAAAJ;https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ",
        "orcid": ";0000-0002-6719-4490;;;;;;;0000-0003-4263-395X;;",
        "linkedin": ";;;niklasmuennighoff/;xivictorialin/;;;;scottyih/;;luke-zettlemoyer-a0109b226/",
        "or_profile": "~Rulin_Shao1;~Rui_Qiao3;~Varsha_Kishore1;~Niklas_Muennighoff1;~Xi_Victoria_Lin1;~Daniela_Rus1;~Bryan_Kian_Hsiang_Low1;~Sewon_Min1;~Wen-tau_Yih1;~Pang_Wei_Koh1;~Luke_Zettlemoyer1",
        "aff": "University of Washington;national university of singaore, National University of Singapore;Allen Institute for Artificial Intelligence;Stanford University+Contextual AI+Allen Institute for Artificial Intelligence;Department of Computer Science, University of Washington+Department of Computer and Information Science, University of Pennsylvania+Meta;Massachusetts Institute of Technology;National University of Singapore;University of California, Berkeley+Allen Institute for Artificial Intelligence;Meta Platforms, Inc.;Allen Institute for Artificial Intelligence+University of Washington;University of Washington+Meta Facebook+Meta",
        "aff_domain": "uw.edu;u.nus.edu;allenai.org;stanford.edu+gmail.com+allenai.org;cs.washington.edu+cis.upenn.edu+fb.com;mit.edu;nus.edu.sg;berkeley.edu+allenai.org;meta.com;allenai.org+cs.washington.edu;cs.washington.edu+fb.com+meta.com",
        "position": "PhD student;PhD student;Postdoc;PhD student+Researcher+Researcher;PhD student+PhD student+Research Scientist;Full Professor;Associate Professor;Assistant Professor+Research Scientist;Research Scientist;Visiting Research Scientist+Assistant Professor;Full Professor+Researcher+Researcher",
        "bibtex": "@inproceedings{\nshao2025reasonir,\ntitle={Reason{IR}: Training Retrievers for Reasoning Tasks},\nauthor={Rulin Shao and Rui Qiao and Varsha Kishore and Niklas Muennighoff and Xi Victoria Lin and Daniela Rus and Bryan Kian Hsiang Low and Sewon Min and Wen-tau Yih and Pang Wei Koh and Luke Zettlemoyer},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=kkBCNLMbGj}\n}",
        "github": "",
        "project": "",
        "reviewers": "MLfg;P7d1;4Dkf;EF6c",
        "site": "https://openreview.net/forum?id=kkBCNLMbGj",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "klPszYDIRT",
        "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs), such as OpenAI\u2019s o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies  reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (**S**teerable r**EA**soning ca**L**ibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL,  up to a 11\\% improvement in accuracy while reducing reasoning tokens by 11.8\\% to 50.4\\%.",
        "keywords": "LLM;Reasoning;Representation Engineering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Runjin Chen;Zhenyu Zhang;Junyuan Hong;Souvik Kundu;Zhangyang Wang",
        "authorids": "~Runjin_Chen1;~Zhenyu_Zhang4;~Junyuan_Hong1;~Souvik_Kundu2;~Zhangyang_Wang1",
        "gender": ";M;M;M;M",
        "homepage": ";https://zhenyu.gallery;https://jyhong.gitlab.io/;https://ksouvik52.github.io;https://vita-group.github.io",
        "dblp": ";01/1844-15;185/1316;126/2210;119/4026",
        "google_scholar": ";ZLyJRxoAAAAJ;7Cbv6doAAAAJ;https://scholar.google.com/citations?hl=en;pxFyKAIAAAAJ",
        "orcid": ";;0000-0002-5718-5187;0000-0002-3533-9405;",
        "linkedin": ";zhenyu-allen-zhang-a9b1391a3/;;souvik-kundu-64922b50/;",
        "or_profile": "~Runjin_Chen1;~Zhenyu_Zhang4;~Junyuan_Hong1;~Souvik_Kundu2;~Zhangyang_Wang1",
        "aff": ";University of Texas at Austin;University of Texas at Austin;Intel+Intel;University of Texas at Austin",
        "aff_domain": ";utexas.edu;utexas.edu;intel.com+intel.com;utexas.edu",
        "position": ";PhD student;Postdoc;Principal Researcher+Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nchen2025seal,\ntitle={{SEAL}: Steerable Reasoning Calibration of Large Language Models for Free},\nauthor={Runjin Chen and Zhenyu Zhang and Junyuan Hong and Souvik Kundu and Zhangyang Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=klPszYDIRT}\n}",
        "github": "",
        "project": "",
        "reviewers": "ssbp;goZm;b7rX;hU5P",
        "site": "https://openreview.net/forum?id=klPszYDIRT",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;3;5;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7071067811865475
    },
    {
        "id": "lEQnUI5lEA",
        "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important step when researchers investigate scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. We demonstrate the pipeline\u2019s validity and accuracy with multiple sets of human-expert annotations. We evaluated a diverse set of language models and retrieval systems on the benchmark and found that model performances still fall significantly short of the expert level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to facilitate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench",
        "keywords": "Biomedical Benchmark;Scientific Information Extraction;Large Language Models;BioNLP",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jianyou Wang;Weili Cao;Kaicheng Wang;Xiaoyue Wang;Ashish Dalvi;Gino Prasad;Qishan Liang;Hsuan-lin Her;Mingwang;Qin Yang;Gene W. Yeo;David E Neal;Maxim Khan;Christopher D. Rosin;Ramamohan Paturi;Leon Bergen",
        "authorids": "~Jianyou_Wang1;~Weili_Cao1;~Kaicheng_Wang1;~Xiaoyue_Wang3;~Ashish_Dalvi1;~Gino_Prasad1;~Qishan_Liang1;~Hsuan-lin_Her1;~Mingwang1;~Qin_Yang5;~Gene_W._Yeo1;~David_E_Neal1;~Maxim_Khan1;~Christopher_D._Rosin1;~Ramamohan_Paturi1;~Leon_Bergen1",
        "gender": "M;M;M;F;M;M;F;F;M;F;M;M;;M;Not Specified;",
        "homepage": ";https://weilicao.github.io/;https://www.semanticscholar.org/author/Kaicheng-Wang/2257044695;https://dpwxy.github.io/xiaoyuewang15.github.io/;;;;https://scholar.google.com/citations?user=ppdAoDgAAAAJ&hl=en;;;https://yeolab.com;https://www.clarehall.cam.ac.uk/our-people/professor-david-neal;;https://chrisrosin.com;https://cseweb.ucsd.edu/~paturi/;",
        "dblp": "251/3315;371/2464.html;256/5299;;;;;;;;;;;49/3517;p/RPaturi.html;136/8736",
        "google_scholar": "4nysj5kAAAAJ;BDrdcwQAAAAJ;;;;opMrU98AAAAJ;E6PDIn8AAAAJ;;;;6D8iNc0AAAAJ;;;ggaVQvQAAAAJ;;0FclEuAAAAAJ",
        "orcid": ";;;;;0000-0003-4590-1278;0000-0003-4351-3140;;0009-0006-4553-5254;0000-0003-0582-057X;0000-0002-0799-6037;0000-0002-6033-5086;;;;",
        "linkedin": ";;;xiaoyue-wang-611029232/;ashish-dalvi/;ginoprasad/;;;;;geneyeo/;david-e-neal-a6129882/;maximkhan/;chris-rosin-844aa6/;;",
        "or_profile": "~Jianyou_Wang1;~Weili_Cao1;~Kaicheng_Wang1;~Xiaoyue_Wang3;~Ashish_Dalvi1;~Gino_Prasad1;~Qishan_Liang1;~Hsuan-lin_Her1;~Mingwang1;~Qin_Yang5;~Gene_W._Yeo1;~David_E_Neal1;~Maxim_Khan1;~Christopher_D._Rosin1;~Ramamohan_Paturi1;~Leon_Bergen1",
        "aff": "University of California, San Diego;University of California, San Diego;University of Southern California;Stanford University;University of California, Irvine;University of California, San Diego;The Scripps Research Institute;University of California, San Diego;University of Electronic Science and Technology of China;Southwest Jiaotong University;University of California, San Diego;University of Oxford+University of Cambridge;Elsevier;Constructive.Codes;University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;usc.edu;stanford.edu;uci.edu;ucsd.edu;scripps.edu;ucsd.edu;uestc.edu.cn;swjtu.edu.cn;ucsd.edu;ox.ac.uk+cam.ac.uk;elsevier.com;constructive.codes;ucsd.edu;ucsd.edu",
        "position": "PhD student;MS student;PhD student;MS student;PhD student;PhD student;Postdoc;PhD student;Associate Professor;Principal Researcher;Full Professor;Emeritus+Emeritus;Product Manager;Researcher;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nwang2025evidencebench,\ntitle={EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers},\nauthor={Jianyou Wang and Weili Cao and Kaicheng Wang and Xiaoyue Wang and Ashish Dalvi and Gino Prasad and Qishan Liang and Hsuan-lin Her and Mingwang and Qin Yang and Gene W. Yeo and David E Neal and Maxim Khan and Christopher D. Rosin and Ramamohan Paturi and Leon Bergen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lEQnUI5lEA}\n}",
        "github": "",
        "project": "",
        "reviewers": "Gnr9;F3p6;b6XD;4ogA",
        "site": "https://openreview.net/forum?id=lEQnUI5lEA",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            16,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5222329678670935
    },
    {
        "id": "lEaHNs2qEv",
        "title": "Overcoming Vocabulary Constraints with Pixel-level Fallback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Subword tokenization requires balancing computational efficiency and vocabulary coverage, often leading to suboptimal performance on languages and scripts not prioritized during training. \nWe propose to augment pretrained language models with a vocabulary-free encoder that generates input embeddings from text rendered to pixels.\nThrough experiments on English-centric language models, we demonstrate that our approach substantially improves machine translation performance and facilitates effective cross-lingual transfer, outperforming tokenizer-based methods.\nFurthermore, we find that pixel-based representations outperform byte-level approaches and standard vocabulary expansion.\nOur approach enhances the multilingual capabilities of monolingual language models without extensive retraining and reduces decoding latency via input compression.",
        "keywords": "pixel-based text representations;machine translation;multilinguality;cross-lingual transfer;unseen scripts",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jonas F. Lotz;Hendra Setiawan;Stephan Peitz;Yova Kementchedjhieva",
        "authorids": "~Jonas_F._Lotz1;~Hendra_Setiawan1;~Stephan_Peitz1;~Yova_Kementchedjhieva1",
        "gender": "M;M;;F",
        "homepage": ";;;",
        "dblp": ";22/5447;68/11425;225/7708",
        "google_scholar": "rQi0nEcAAAAJ;;NMSCU3MAAAAJ;",
        "orcid": "0000-0001-6405-0590;;;0009-0000-7465-5702",
        "linkedin": "jonas-f-lotz-ab7805113/;;stephan-peitz-9844708b/;",
        "or_profile": "~Jonas_F._Lotz1;~Hendra_Setiawan1;~Stephan_Peitz1;~Yova_Kementchedjhieva1",
        "aff": "University of Copenhagen;Apple;Apple;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "diku.dk;apple.com;apple.com;mbzuai.ac.ae",
        "position": "PhD student;Machine Learning Scientist;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nlotz2025overcoming,\ntitle={Overcoming Vocabulary Constraints with Pixel-level Fallback},\nauthor={Jonas F. Lotz and Hendra Setiawan and Stephan Peitz and Yova Kementchedjhieva},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lEaHNs2qEv}\n}",
        "github": "",
        "project": "",
        "reviewers": "KQmA;scWL;QyNG",
        "site": "https://openreview.net/forum?id=lEaHNs2qEv",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "4;4;5",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8660254037844385
    },
    {
        "id": "lEpPFmGH3L",
        "title": "Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans across *historical* information. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 197k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. The code, prompts, and model outputs are available on [GitHub](https://github.com/gtfintechlab/knowledge-gap).",
        "keywords": "Large Language Models;Knowledge Cutoff;Model Hallucinations",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Agam Shah;Liqin Ye;Sebastian Jaskowski;Wei Xu;Sudheer Chava",
        "authorids": "~Agam_Shah1;~Liqin_Ye1;~Sebastian_Jaskowski1;~Wei_Xu5;~Sudheer_Chava1",
        "gender": "M;M;M;F;Not Specified",
        "homepage": "https://shahagam4.github.io/;;https://www.sebastianjaskowski.com;https://cocoxu.github.io/;https://fintech.gatech.edu",
        "dblp": "206/6806;;;32/1213-4.html;61/9926",
        "google_scholar": "https://scholar.google.co.in/citations?user=wGA2umEAAAAJ;YxM-Z3AAAAAJ;;BfOdG-oAAAAJ;AXYf-i8AAAAJ",
        "orcid": "0000-0002-9062-2430;0009-0001-0390-8802;;;0000-0001-8330-682X",
        "linkedin": "agam-shah/;liqin-ye-b48981212/;sjaskowski/;;",
        "or_profile": "~Agam_Shah1;~Liqin_Ye1;~Sebastian_Jaskowski1;~Wei_Xu5;~Sudheer_Chava1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;PhD student;Undergrad student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nshah2025beyond,\ntitle={Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge},\nauthor={Agam Shah and Liqin Ye and Sebastian Jaskowski and Wei Xu and Sudheer Chava},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lEpPFmGH3L}\n}",
        "github": "",
        "project": "",
        "reviewers": "xcLv;2yqz;YwGF;hcdm",
        "site": "https://openreview.net/forum?id=lEpPFmGH3L",
        "pdf_size": 0,
        "rating": "4;6;7;8",
        "confidence": "4;5;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.479019945774904
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.09759000729485331
    },
    {
        "id": "lI4LgGv4sX",
        "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Evaluating whether vision\u2013language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce SEAM, a benchmark that pairs semantically equivalent inputs across four domains that have existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notation and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.",
        "keywords": "Vision-Language Models;Benchmark;Modality Imbalance",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhenwei Tang;Difan Jiao;Blair Yang;Ashton Anderson",
        "authorids": "~Zhenwei_Tang1;~Difan_Jiao1;~Blair_Yang1;~Ashton_Anderson1",
        "gender": "M;M;M;",
        "homepage": "https://lilv98.github.io/;;https://www.cs.toronto.edu/~blair/;http://www.cs.toronto.edu/~ashton/",
        "dblp": "271/4450.html;362/0706.html;;21/8524",
        "google_scholar": "R46GZk0AAAAJ;HTuHhzQAAAAJ;;https://scholar.google.co.uk/citations?user=FMSltawAAAAJ",
        "orcid": "0000-0002-8742-9146;;;",
        "linkedin": "zhenwei-tang-631611250/;difan-jiao/;;",
        "or_profile": "~Zhenwei_Tang1;~Difan_Jiao1;~Blair_Yang1;~Ashton_Anderson1",
        "aff": "University of Toronto;Department of Computer Science, University of Toronto;University of Toronto;Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;utoronto.ca;cs.toronto.edu",
        "position": "PhD student;PhD student;Undergrad student;Associate Professor",
        "bibtex": "@inproceedings{\ntang2025seam,\ntitle={{SEAM}: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models},\nauthor={Zhenwei Tang and Difan Jiao and Blair Yang and Ashton Anderson},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lI4LgGv4sX}\n}",
        "github": "",
        "project": "",
        "reviewers": "NLx2;yu5A;hEvb;EVRg",
        "site": "https://openreview.net/forum?id=lI4LgGv4sX",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "lODGn1Rp5t",
        "title": "Task Vectors in In-Context Learning: Emergence, Formation, and Benefits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-context learning is a remarkable capability of transformers, referring to their ability to adapt to specific tasks based on a short history or context. Previous research has found that task-specific information is locally encoded within models, though their emergence and functionality remain unclear due to opaque pre-training processes. In this work, we investigate the formation of task vectors in a controlled setting, using models trained from scratch on synthetic datasets. Our findings confirm that task vectors naturally emerge under certain conditions, but the tasks may be relatively weakly and/or non-locally encoded within the model. To promote strong task vectors encoded at a prescribed location within the model, we propose an auxiliary training mechanism based on a task vector prompting loss (TVP-loss). This method eliminates the need to search for task-correlated encodings within the trained model and demonstrably improves robustness and generalization.",
        "keywords": "in-context learning; task vector",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Liu Yang;Ziqian Lin;Kangwook Lee;Dimitris Papailiopoulos;Robert D Nowak",
        "authorids": "~Liu_Yang6;~Ziqian_Lin1;~Kangwook_Lee1;~Dimitris_Papailiopoulos1;~Robert_D_Nowak1",
        "gender": ";M;M;M;M",
        "homepage": "https://leiay.github.io/;https://myhakureimu.github.io/;http://kangwooklee.com/;http://papail.io;http://nowak.ece.wisc.edu",
        "dblp": ";245/3453;88/9826-1;;n/RobertDNowak",
        "google_scholar": "ul5MsOIAAAAJ;0nOdbCoAAAAJ;sCEl8r-n5VEC;hYi6i9sAAAAJ;fn13u8IAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Liu_Yang6;~Ziqian_Lin1;~Kangwook_Lee1;~Dimitris_Papailiopoulos1;~Robert_D_Nowak1",
        "aff": "University of Wisconsin - Madison+Google;Google+University of Wisconsin - Madison;KRAFTON+University of Wisconsin - Madison+University of Wisconsin - Madison;Microsoft Research+University of Wisconsin - Madison;University of Wisconsin, Madison+Toyota Technological Institute at Chicago+Rice University+University of Wisconsin-Madison+University of Wisconsin - Madison",
        "aff_domain": "wisc.edu+google.com;google.com+wisc.edu;krafton.com+wisc.edu+wisc.edu;research.microsoft.com+wisc.edu;wisc.edu+ttic.edu+rice.edu++",
        "position": "PhD student+Intern;Researcher+PhD student;Researcher+Associate Professor+Assistant Professor;Principal Researcher+Associate Professor;Full Professor+Full Professor+Full Professor++Full Professor",
        "bibtex": "@inproceedings{\nyang2025task,\ntitle={Task Vectors in In-Context Learning: Emergence, Formation, and Benefits},\nauthor={Liu Yang and Ziqian Lin and Kangwook Lee and Dimitris Papailiopoulos and Robert D Nowak},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lODGn1Rp5t}\n}",
        "github": "",
        "project": "",
        "reviewers": "g1s8;GxZE;BR3X;9mvp",
        "site": "https://openreview.net/forum?id=lODGn1Rp5t",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "lSWOMjonL7",
        "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Personalized preference alignment for large language models (LLMs), the process of tailoring LLMs to individual users' preferences, is an emerging research direction spanning the area of NLP and personalization. In this survey, we present an analysis of works on personalized alignment and modeling for LLMs. We introduce a taxonomy of preference alignment techniques, including training time, inference time, and heuristic-driven methods. We provide analysis and discussion on the strengths and limitations of each group of techniques and then cover evaluation, benchmarks, as well as open problems in the field.",
        "keywords": "Preference Alignment;Personalization;Pluralistic Alignment;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhouhang Xie;Junda Wu;Yiran Shen;Raghav Jain;Yu Xia;Xintong Li;Aaron Chang;Ryan A. Rossi;Tong Yu;Sachin Kumar;Bodhisattwa Prasad Majumder;Jingbo Shang;Prithviraj Ammanabrolu;Julian McAuley",
        "authorids": "~Zhouhang_Xie1;~Junda_Wu1;~Yiran_Shen2;~Raghav_Jain1;~Yu_Xia9;~Xintong_Li2;~Aaron_Chang1;~Ryan_A._Rossi2;~Tong_Yu3;~Sachin_Kumar1;~Bodhisattwa_Prasad_Majumder1;~Jingbo_Shang2;~Prithviraj_Ammanabrolu1;~Julian_McAuley1",
        "gender": "M;M;;M;M;F;M;;;M;;M;M;M",
        "homepage": "https://zhouhanxie.github.io/;https://scholar.google.com/citations?user=_iKeQFwAAAAJ&hl=en;;;https://andree-9.github.io/;https://kaylee0501.github.io/;;;https://www.linkedin.com/in/tong-yu-42790744;https://shocheen.com;https://www.majumderb.com/;https://shangjingbo1226.github.io/;http://prithvirajva.com;http://cseweb.ucsd.edu/~jmcauley/",
        "dblp": "299/9534;295/8249;;;28/4326-7;;;;32/1593-1;31/4484-9;138/6177;151/3145.html;202/2351;29/3483",
        "google_scholar": "https://scholar.google.com/citations?hl=en;_iKeQFwAAAAJ;;;sTVqEUMAAAAJ;Sw5mq4cAAAAJ;;;https://scholar.google.com/citations?hl=en;qO38fRIAAAAJ;cEM1a5gAAAAJ;0SkFI4MAAAAJ;2yaiWZ8AAAAJ;icbo4M0AAAAJ",
        "orcid": ";;;;;;;;0000-0002-5991-2050;;;;;0000-0003-0955-7588",
        "linkedin": ";;jennyshen56/;raghav-jain-3a8076214;;xintong-li-970ab31b5/;aaronyuanchang/;;tong-yu-42790744;;;;rajammanabrolu/;",
        "or_profile": "~Zhouhang_Xie1;~Junda_Wu1;~Yiran_Shen2;~Raghav_Jain1;~Yu_Xia9;~Xintong_Li2;~Aaron_Chang1;~Ryan_A._Rossi2;~Tong_Yu3;~Sachin_Kumar1;~Bodhisattwa_Prasad_Majumder1;~Jingbo_Shang2;~Prithviraj_Ammanabrolu1;~Julian_McAuley1",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;Indian Institute of Technology, Patna.;University of California, San Diego;University of California, San Diego;University of California, Los Angeles;;Adobe Research;Ohio State University, Columbus;Allen Institute for Artificial Intelligence;University of California, San Diego;University of California, San Diego;University of California, San Diego, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;iitp.ac.in;ucsd.edu;ucsd.edu;ucla.edu;;adobe.com;osu.edu;allenai.org;ucsd.edu;ucsd.edu;eng.ucsd.edu",
        "position": "PhD student;PhD student;PhD student;Researcher;PhD student;PhD student;Undergrad student;;Senior Research Scientist;Assistant Professor;Researcher;Associate Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nxie2025a,\ntitle={A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models},\nauthor={Zhouhang Xie and Junda Wu and Yiran Shen and Raghav Jain and Yu Xia and Xintong Li and Aaron Chang and Ryan A. Rossi and Tong Yu and Sachin Kumar and Bodhisattwa Prasad Majumder and Jingbo Shang and Prithviraj Ammanabrolu and Julian McAuley},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lSWOMjonL7}\n}",
        "github": "",
        "project": "",
        "reviewers": "ckzC;Voas;9asZ;oXrd",
        "site": "https://openreview.net/forum?id=lSWOMjonL7",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "lcDRvffeNP",
        "title": "SuperBPE: Space Travel for Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delimiter of meaning, as evidenced by multi-word expressions (e.g., \"by the way\"), crosslingual variation in the number of words needed to express a concept (e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do not use whitespace at all (e.g., Chinese). To explore the potential of tokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE, which incorporates a simple pretokenization curriculum into the byte-pair encoding (BPE) algorithm to first learn subwords, then superwords that bridge whitespace. This brings dramatic improvements in encoding efficiency: when fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B transformer LMs from scratch while fixing the model size, vocabulary size, and train compute, varying *only* the algorithm for learning the vocabulary. Our model trained with SuperBPE achieves an average +4.0% absolute improvement over the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while simultaneously requiring 27% less compute at inference time. In analysis, we find that SuperBPE results in segmentations of text that are more uniform in per-token difficulty. Qualitatively, this may be because SuperBPE tokens often capture common multi-word expressions that function semantically as a single unit. SuperBPE is a straightforward, local modification to tokenization that improves both encoding efficiency and downstream performance, yielding better language models overall.",
        "keywords": "tokenization;language modeling;efficiency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alisa Liu;Jonathan Hayase;Valentin Hofmann;Sewoong Oh;Noah A. Smith;Yejin Choi",
        "authorids": "~Alisa_Liu1;~Jonathan_Hayase2;~Valentin_Hofmann1;~Sewoong_Oh3;~Noah_A._Smith2;~Yejin_Choi1",
        "gender": "F;M;;;;F",
        "homepage": "https://alisawuffles.github.io/;https://jhayase.github.io/;https://valentinhofmann.github.io/;;;https://yejinc.github.io/",
        "dblp": ";244/9599;264/4665;;;89/579-1",
        "google_scholar": "3-lTFAwAAAAJ;Zw-l1d8AAAAJ;bbHOPKwAAAAJ;;;vhP-tlcAAAAJ",
        "orcid": ";0000-0002-3757-6586;;;;",
        "linkedin": ";jonathan-hayase-5ab849128;;;;",
        "or_profile": "~Alisa_Liu1;~Jonathan_Hayase2;~Valentin_Hofmann1;~Sewoong_Oh3;~Noah_A._Smith2;~Yejin_Choi1",
        "aff": "NVIDIA+University of Washington;University of Washington;Allen Institute for Artificial Intelligence;;;Computer Science Department, Stanford University+NVIDIA",
        "aff_domain": "nvidia.com+uw.edu;washington.edu;allenai.org;;;cs.stanford.edu+nvidia.com",
        "position": "Intern+PhD student;PhD student;Postdoc;;;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nliu2025superbpe,\ntitle={Super{BPE}: Space Travel for Language Models},\nauthor={Alisa Liu and Jonathan Hayase and Valentin Hofmann and Sewoong Oh and Noah A. Smith and Yejin Choi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lcDRvffeNP}\n}",
        "github": "",
        "project": "",
        "reviewers": "o42C;85bC;ydMz;LFP8",
        "site": "https://openreview.net/forum?id=lcDRvffeNP",
        "pdf_size": 0,
        "rating": "5;9;9;9",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            8.0,
            1.7320508075688772
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "lkjhBdz3rn",
        "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the \"data wall\" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data. We make our high-quality synthetic data publicly available at https://huggingface.co/datasets/facebook/recycling_the_web.",
        "keywords": "pretraining;synthetic data;rewriting;data curation for LLMs;data filtering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Thao Nguyen;Yang Li;Olga Golovneva;Luke Zettlemoyer;Sewoong Oh;Ludwig Schmidt;Xian Li",
        "authorids": "~Thao_Nguyen3;~Yang_Li112;~Olga_Golovneva1;~Luke_Zettlemoyer1;~Sewoong_Oh3;~Ludwig_Schmidt1;~Xian_Li1",
        "gender": "F;;F;M;;M;",
        "homepage": "https://thaonguyen19.github.io/;;;https://www.cs.washington.edu/people/faculty/lsz/;;http://people.csail.mit.edu/ludwigs/;",
        "dblp": "77/2922;;280/3377;21/6793;;141/2720;82/1763-3.html",
        "google_scholar": "DvJG-_8AAAAJ;;;https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ;;SWMKy70AAAAJ;v_sIgawAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": ";yangli625/;olgagolovneva/;luke-zettlemoyer-a0109b226/;;ludwig-schmidt-87ba3612/;",
        "or_profile": "~Thao_Nguyen3;~Yang_Li112;~Olga_Golovneva1;~Luke_Zettlemoyer1;~Sewoong_Oh3;~Ludwig_Schmidt1;~Xian_Li1",
        "aff": "University of Washington, Seattle+Meta;Meta Facebook;Meta Facebook;University of Washington+Meta Facebook+Meta;;Stanford University+Anthropic;Facebook AI",
        "aff_domain": "uw.edu+meta.com;meta.com;fb.com;cs.washington.edu+fb.com+meta.com;;stanford.edu+anthropic.com;fb.com",
        "position": "PhD student+Visiting Researcher;Researcher;Researcher;Full Professor+Researcher+Researcher;;Assistant Professor+Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nnguyen2025recycling,\ntitle={Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models},\nauthor={Thao Nguyen and Yang Li and Olga Golovneva and Luke Zettlemoyer and Sewoong Oh and Ludwig Schmidt and Xian Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lkjhBdz3rn}\n}",
        "github": "",
        "project": "",
        "reviewers": "NFsi;VaHx;Mvmh;dApK",
        "site": "https://openreview.net/forum?id=lkjhBdz3rn",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.816496580927726
    },
    {
        "id": "lqC5J7pBP9",
        "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways\u2014our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \u201cre-mixing\u201d the experts in different layers jointly for each test sample. Since the test sample\u2019s ground truth is unknown, we propose to optimize a surrogate objective defined by the sample\u2019s \u201csuccessful neighbors\u201d from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts\u2019 mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \u201cCritical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\u201d. We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE\u2019s advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.",
        "keywords": "Mixture-of-Experts;Large Language Models;Test-Time Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhongyang Li;Ziyue Li;Tianyi Zhou",
        "authorids": "~Zhongyang_Li5;~Ziyue_Li1;~Tianyi_Zhou2",
        "gender": "M;F;",
        "homepage": ";https://litzy0619.github.io/;",
        "dblp": ";;",
        "google_scholar": ";NQVzCSkAAAAJ;",
        "orcid": ";;",
        "linkedin": "zhongyangli0124/;litzyli/;",
        "or_profile": "~Zhongyang_Li5;~Ziyue_Li1;~Tianyi_Zhou2",
        "aff": "Johns Hopkins University;University of Maryland, College Park;",
        "aff_domain": "johnshopkins.edu;umd.edu;",
        "position": "MS student;PhD student;",
        "bibtex": "@inproceedings{\nli2025cpo,\ntitle={C3{PO}: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing},\nauthor={Zhongyang Li and Ziyue Li and Tianyi Zhou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lqC5J7pBP9}\n}",
        "github": "",
        "project": "",
        "reviewers": "Qu61;hK3h;7Z5F",
        "site": "https://openreview.net/forum?id=lqC5J7pBP9",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9999999999999997
    },
    {
        "id": "lsAY6fWsog",
        "title": "Inducing Programmatic Skills for Agentic Tasks",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7\u201315.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.",
        "keywords": "agent;skill learning;web navigation;scalability;generalization",
        "primary_area": "",
        "supplementary_material": "/attachment/557818dcceefef95e993d00d56cf6bd03deaf89f.zip",
        "author": "Zora Zhiruo Wang;Apurva Gandhi;Graham Neubig;Daniel Fried",
        "authorids": "~Zora_Zhiruo_Wang1;~Apurva_Gandhi1;~Graham_Neubig1;~Daniel_Fried1",
        "gender": ";M;M;M",
        "homepage": ";;http://phontron.com;https://dpfried.github.io/",
        "dblp": ";;03/8155;117/4804",
        "google_scholar": ";elzCF8sAAAAJ;wlosgkoAAAAJ;sJDqACEAAAAJ",
        "orcid": ";;;",
        "linkedin": ";apurvaga/;;",
        "or_profile": "~Zora_Zhiruo_Wang1;~Apurva_Gandhi1;~Graham_Neubig1;~Daniel_Fried1",
        "aff": ";Carnegie Mellon University;Carnegie Mellon University;Meta AI+Carnegie Mellon University",
        "aff_domain": ";cmu.edu;cmu.edu;meta.com+cmu.edu",
        "position": ";PhD student;Associate Professor;Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025inducing,\ntitle={Inducing Programmatic Skills for Agentic Tasks},\nauthor={Zora Zhiruo Wang and Apurva Gandhi and Graham Neubig and Daniel Fried},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lsAY6fWsog}\n}",
        "github": "",
        "project": "",
        "reviewers": "5eir;nthk;HU68;vSJS",
        "site": "https://openreview.net/forum?id=lsAY6fWsog",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "lv0cJ2pWVd",
        "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While effective in closed, narrowly scoped environments, this approach presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks demonstrate that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases.",
        "keywords": "LLM;LLM agents",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Dang Nguyen;Viet Dac Lai;Seunghyun Yoon;Ryan A. Rossi;Handong Zhao;Ruiyi Zhang;Puneet Mathur;Nedim Lipka;Yu Wang;Trung Bui;Franck Dernoncourt;Tianyi Zhou",
        "authorids": "~Dang_Nguyen3;~Viet_Dac_Lai1;~Seunghyun_Yoon1;~Ryan_A._Rossi2;~Handong_Zhao3;~Ruiyi_Zhang3;~Puneet_Mathur2;~Nedim_Lipka1;~Yu_Wang41;~Trung_Bui1;~Franck_Dernoncourt1;~Tianyi_Zhou2",
        "gender": ";M;M;;;;M;;M;M;;",
        "homepage": ";http://laiviet.github.io;https://david-yoon.github.io/;;;;https://themadaiguy.github.io/;;https://yuwang0103.github.io/;https://sites.google.com/site/trungbuistanford/;http://francky.me;",
        "dblp": ";251/8546;68/3020-2;;;;148/3798;;02/5889-160;180/0632;132/4043;",
        "google_scholar": ";TtxmNccAAAAJ;https://scholar.google.com/citations?hl=en;;;;https://scholar.google.co.in/citations?user=Zb4-A0AAAAAJ;;XPCmiz4AAAAJ;FpFTduYAAAAJ;kz2aIc8AAAAJ;",
        "orcid": ";;0000-0002-7262-3579;;;;;;0000-0001-6908-508X;0000-0002-0871-349X;0000-0002-1119-1346;",
        "linkedin": ";laidacviet/;david-s-yoon/;;;;PuneetMathurUMD/;;;trung-bui-4333322/;franckdernoncourt;",
        "or_profile": "~Dang_Nguyen3;~Viet_Dac_Lai1;~Seunghyun_Yoon1;~Ryan_A._Rossi2;~Handong_Zhao3;~Ruiyi_Zhang3;~Puneet_Mathur2;~Nedim_Lipka1;~Yu_Wang41;~Trung_Bui1;~Franck_Dernoncourt1;~Tianyi_Zhou2",
        "aff": ";Adobe Systems;Adobe Research;;;;Adobe Systems;;University of Oregon+Vanderbilt University;Adobe Research;Adobe Systems;",
        "aff_domain": ";adobe.com;adobe.com;;;;adobe.com;;uoregon.edu+vanderbilt.edu;adobe.com;adobe.com;",
        "position": ";Researcher;Researcher;;;;Researcher;;Assistant Professor+PhD student;Researcher;Researcher;",
        "bibtex": "@inproceedings{\nnguyen2025dynasaur,\ntitle={DynaSaur: Large Language Agents Beyond Predefined Actions},\nauthor={Dang Nguyen and Viet Dac Lai and Seunghyun Yoon and Ryan A. Rossi and Handong Zhao and Ruiyi Zhang and Puneet Mathur and Nedim Lipka and Yu Wang and Trung Bui and Franck Dernoncourt and Tianyi Zhou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lv0cJ2pWVd}\n}",
        "github": "",
        "project": "",
        "reviewers": "CTUg;rSiz;P4Yg;coxz",
        "site": "https://openreview.net/forum?id=lv0cJ2pWVd",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "lvQwn8eiRf",
        "title": "How does Watermarking Affect Visual Language Models in Document Understanding?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Visual Language Models (VLMs) have become foundational models for document understanding tasks, widely used in the processing of complex multimodal documents across domains such as finance, law, and academia. However, documents often contain noise-like information, such as watermarks, which inevitably leads us to inquire: Do watermarks degrade the performance of VLMs in document understanding? To address this, we propose a novel evaluation framework to investigate the effect of visible watermarks on VLMs performance.\nWe takes into account various factors, including different types of document data, the positions of watermarks within documents and variations in watermark content.\nOur experimental results reveal that VLMs performance can be significantly compromised by watermarks, with performance drop rates reaching up to 36\\%. We discover that \\emph{scattered} watermarks cause stronger interference than centralized ones, and that \\emph{semantic contents} in watermarks creates greater disruption than simple visual occlusion. Through attention mechanism analysis and embedding similarity examination, we find that the performance drops are mainly attributed to that watermarks 1) force widespread attention redistribution, and 2) alter semantic representation in the embedding space. Our research not only highlights significant challenges in deploying VLMs for document understanding, but also provides insights towards developing robust inference mechanisms on watermarked documents.",
        "keywords": "VLMs;Document Understanding;Robustness",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Chunxue Xu;Yiwei Wang;Bryan Hooi;Yujun Cai;Songze Li",
        "authorids": "~Chunxue_Xu1;~Yiwei_Wang2;~Bryan_Hooi1;~Yujun_Cai1;~Songze_Li1",
        "gender": "F;M;;F;M",
        "homepage": "https://snow-like-kk.github.io/chunxue.github.io/;;http://bhooi.github.io;;https://s3di-lab.github.io/",
        "dblp": ";50/5889-1;169/9975;227/4399;119/2630",
        "google_scholar": ";https://scholar.google.com.hk/citations?user=Sh9QvBkAAAAJ;;https://scholar.google.com/citations?hl=en;vcGuNDYAAAAJ",
        "orcid": ";;0000-0002-5645-1754;;",
        "linkedin": ";;;;",
        "or_profile": "~Chunxue_Xu1;~Yiwei_Wang2;~Bryan_Hooi1;~Yujun_Cai1;~Songze_Li1",
        "aff": "Southeast University+Donghua University, Shanghai;University of California, Merced;National University of Singapore;The University of Queensland;Southeast University",
        "aff_domain": "seu.edu.cn+dhu.edu.cn;ucmerced.edu;nus.edu.sg;uq.edu.au;seu.edu.cn",
        "position": "MS student+Undergrad student;Assistant Professor;Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nxu2025how,\ntitle={How does Watermarking Affect Visual Language Models in Document Understanding?},\nauthor={Chunxue Xu and Yiwei Wang and Bryan Hooi and Yujun Cai and Songze Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=lvQwn8eiRf}\n}",
        "github": "",
        "project": "",
        "reviewers": "bZfS;64kT;stAf;Ndiu",
        "site": "https://openreview.net/forum?id=lvQwn8eiRf",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "m4F3kQCfGX",
        "title": "LLM Unlearning Without an Expert Curated Dataset",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning\u2014the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets\u2014datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at [https://github.com/xyzhu123/Synthetic_Textbook](https://github.com/xyzhu123/Synthetic_Textbook).",
        "keywords": "NLP;Machine Unlearning;Synthetic Data Generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaoyuan Zhu;Muru Zhang;Ollie Liu;Robin Jia;Willie Neiswanger",
        "authorids": "~Xiaoyuan_Zhu2;~Muru_Zhang1;~Ollie_Liu1;~Robin_Jia1;~Willie_Neiswanger2",
        "gender": ";M;M;M;M",
        "homepage": ";https://nanami18.github.io/;https://ollieliu.com;https://robinjia.github.io/;https://willieneis.github.io/",
        "dblp": ";325/4648.html;;182/2556;120/7593.html",
        "google_scholar": ";OJIXk7wAAAAJ;https://scholar.google.com/citations?view_op=list_works;ajZ-_O0AAAAJ;QwKHApEAAAAJ",
        "orcid": ";;;;",
        "linkedin": "xiaoyuan-zhu-38005a224/;muruzhang/;oliu/;;",
        "or_profile": "~Xiaoyuan_Zhu2;~Muru_Zhang1;~Ollie_Liu1;~Robin_Jia1;~Willie_Neiswanger2",
        "aff": "University of Southern California;University of Southern California;University of Southern California;University of Southern California;University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "position": "Undergrad student;PhD student;PhD;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhu2025llm,\ntitle={{LLM} Unlearning Without an Expert Curated Dataset},\nauthor={Xiaoyuan Zhu and Muru Zhang and Ollie Liu and Robin Jia and Willie Neiswanger},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=m4F3kQCfGX}\n}",
        "github": "",
        "project": "",
        "reviewers": "Smv6;zZHD;SAXR;GsKB",
        "site": "https://openreview.net/forum?id=m4F3kQCfGX",
        "pdf_size": 0,
        "rating": "6;7;8;9",
        "confidence": "4;4;5;3",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3162277660168379
    },
    {
        "id": "m6nBgFSMTL",
        "title": "ICQuant: Index Coding enables Low-bit LLM Quantization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The rapid deployment of Large Language Models (LLMs) highlights the need for efficient low-bit post-training quantization (PTQ) due to their high memory costs. A key challenge in weight quantization is the presence of outliers, which inflate quantization ranges and lead to large errors. While a number of outlier suppression techniques have been proposed, they either: fail to effectively shrink the quantization range, or incur (relatively) high bit overhead. In this paper, we present ICQuant, a novel framework that leverages outlier statistics to design an efficient index coding scheme for outlier-aware weight-only quantization. Compared to existing outlier suppression techniques requiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant requires only $\\approx 0.25$ bits; a significant saving in low bit regimes (e.g., 2-3 bits). ICQuant can be used on top of any existing quantizers to eliminate outliers, improving the quantization quality. Using just 2.3 bits per weight and simple scalar quantizers, \\ours improves the zero-shot accuracy of the 2-bit Llama3-70B model by up to 130\\% and 150\\% relative to QTIP (Tseng et al. (2024b)) and QuIP\\# (Tseng et al. (2024a) respectively; and it achieves comparable performance to the best-known fine-tuned quantizer (Malinovskii et al. (2024)) without any fine-tuning.",
        "keywords": "LLM Quantization;LLM Compression;Post Training Quantization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinlin Li;Osama Hanna;Christina Fragouli;Suhas Diggavi",
        "authorids": "~Xinlin_Li3;~Osama_Hanna1;~Christina_Fragouli1;~Suhas_Diggavi1",
        "gender": "F;M;F;",
        "homepage": ";https://www.arni.ee.ucla.edu/people/osama-hanna/;https://www.arni.ee.ucla.edu;https://www.ee.ucla.edu/suhas-diggavi/",
        "dblp": ";;87/5736;d/SNDiggavi.html#j15",
        "google_scholar": "tKcDMGoAAAAJ;;sJIAF-gAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Xinlin_Li3;~Osama_Hanna1;~Christina_Fragouli1;~Suhas_Diggavi1",
        "aff": "University of California, Los Angeles;Meta Facebook;University of California, Los Angeles;University of California, Los Angeles",
        "aff_domain": "ucla.edu;meta.com;ucla.edu;ucla.edu",
        "position": "PhD student;Research Scientist;Full Professor;Professor",
        "bibtex": "@inproceedings{\nli2025icquant,\ntitle={{ICQ}uant: Index Coding enables Low-bit {LLM} Quantization},\nauthor={Xinlin Li and Osama Hanna and Christina Fragouli and Suhas Diggavi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=m6nBgFSMTL}\n}",
        "github": "",
        "project": "",
        "reviewers": "PnYh;ykoc;bufP;BVJU",
        "site": "https://openreview.net/forum?id=m6nBgFSMTL",
        "pdf_size": 0,
        "rating": "5;5;5;5",
        "confidence": "5;3;4;2",
        "wc_review": "",
        "rating_avg": [
            5.0,
            0.0
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "mTJW8Y1nd8",
        "title": "Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLMs have demonstrated remarkable performance across various tasks but face challenges related to unintentionally generating outputs containing sensitive information. A straightforward approach to address this issue is to retrain the model after excluding the problematic data. However, this approach incurs prohibitively high computational costs. To overcome this limitation, machine unlearning has emerged as a promising solution that can effectively remove sensitive information without the need to retrain the model from scratch. Recently, FILA has been proposed as a parameter-efficient unlearning method by integrating LoRA adapters. Specifically, it calculates the Fisher information to identify parameters associated with the forget set and assigns them to LoRA adapters for updates. Despite its innovative approach, FILA still requires access to all model parameters and does not adequately account for fundamental assumptions underlying Fisher information, leading to inaccuracies in importance estimation. To address these limitations, we propose VILA, a novel unlearning framework that explicitly considers the assumptions overlooked in FILA, thereby enhancing the accuracy of parameter identification for the forget set. Moreover, VILA significantly reduces computational costs by enabling parameter identification without accessing the entire model. Our method achieves up to 100\u00d7 higher parameter efficiency and 40\u00d7 faster training speed compared to FILA, and sets new state-of-the-art performance on benchmarks including TOFU, WMDP, and MUSE. Our code is available at https://github.com/kyj93790/VILA.",
        "keywords": "Machine Unlearning;Large Language Model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yejin Kim;Eunwon Kim;Buru Chang;Junsuk Choe",
        "authorids": "~Yejin_Kim7;~Eunwon_Kim1;~Buru_Chang1;~Junsuk_Choe1",
        "gender": "F;F;Not Specified;M",
        "homepage": "https://sites.google.com/view/yejin-c-kim/home?authuser=0;https://lilab.korea.ac.kr/members;https://sites.google.com/view/buru-chang;https://sites.google.com/site/junsukchoe/",
        "dblp": ";391/5702;221/3390;169/7682",
        "google_scholar": ";https://scholar.google.co.kr/citations?user=CLv6v9gAAAAJ;https://scholar.google.co.kr/citations?hl=ko;1H2H7XAAAAAJ",
        "orcid": "0009-0002-0294-2280;;0000-0002-7595-9035;0000-0003-4726-4436",
        "linkedin": "yejin-kim-912458268/;eunwon-kim-480a83321/;;junsukchoe/",
        "or_profile": "~Yejin_Kim7;~Eunwon_Kim1;~Buru_Chang1;~Junsuk_Choe1",
        "aff": "Sogang University;Sogang University;Korea University;Sogang University",
        "aff_domain": "sogang.ac.kr;sogang.ac.kr;korea.ac.kr;sogang.ac.kr",
        "position": "MS student;MS student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025improving,\ntitle={Improving Fisher Information Estimation and Efficiency for Lo{RA}-based {LLM} Unlearning},\nauthor={Yejin Kim and Eunwon Kim and Buru Chang and Junsuk Choe},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=mTJW8Y1nd8}\n}",
        "github": "",
        "project": "",
        "reviewers": "5shu;ZX4F;WfNG;Fpuh",
        "site": "https://openreview.net/forum?id=mTJW8Y1nd8",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;3;5;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "mgsS73kvOA",
        "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming.",
        "keywords": "Large Language Models;Planning;Game;Benchmark;Social Intelligence",
        "primary_area": "",
        "supplementary_material": "/attachment/b27f210e2adf29e68a923dd43e9fb24d62894bf2.zip",
        "author": "Jianzhu Yao;Kevin Wang;Ryan Hsieh;Haisu Zhou;Tianqing Zou;Zerui Cheng;Zhangyang Wang;Pramod Viswanath",
        "authorids": "~Jianzhu_Yao1;~Kevin_Wang4;~Ryan_Hsieh2;~Haisu_Zhou1;~Tianqing_Zou1;~Zerui_Cheng1;~Zhangyang_Wang1;~Pramod_Viswanath2",
        "gender": "M;M;M;;F;M;M;M",
        "homepage": ";;;;https://github.com/cooodeBrew;https://www.zerui-cheng.com/;https://vita-group.github.io;http://pramodv.ece.illinois.edu",
        "dblp": ";;;;;330/4351.html;119/4026;",
        "google_scholar": "L5XDYTIAAAAJ;;;;;BK4DaPAAAAAJ;pxFyKAIAAAAJ;lPycXNcAAAAJ",
        "orcid": ";;;;;0000-0003-3397-9844;;",
        "linkedin": "jianzhu-yao-369243290/;kevin-wang-01/;ryan-hsieh-b60827282/;%E6%B5%B7%E7%B2%9F-%E5%91%A8-a376222b7/;;zerui-cheng/;;",
        "or_profile": "~Jianzhu_Yao1;~Kevin_Wang4;~Ryan_Hsieh2;~Haisu_Zhou1;~Tianqing_Zou1;~Zerui_Cheng1;~Zhangyang_Wang1;~Pramod_Viswanath2",
        "aff": "Princeton University;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;Princeton University;University of Texas at Austin;University of Illinois, Urbana Champaign",
        "aff_domain": "princeton.edu;utexas.edu;utexas.edu;utexas.edu;utexas.edu;princeton.edu;utexas.edu;illinois.edu",
        "position": "PhD student;PhD student;Undergrad student;Undergrad student;Undergrad student;PhD student;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nyao2025spinbench,\ntitle={{SPIN}-Bench: How Well Do {LLM}s Plan Strategically and Reason Socially?},\nauthor={Jianzhu Yao and Kevin Wang and Ryan Hsieh and Haisu Zhou and Tianqing Zou and Zerui Cheng and Zhangyang Wang and Pramod Viswanath},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=mgsS73kvOA}\n}",
        "github": "",
        "project": "",
        "reviewers": "CDqy;p6AN;nB76;onko;Jxuz",
        "site": "https://openreview.net/forum?id=mgsS73kvOA",
        "pdf_size": 0,
        "rating": "7;7;7;8;8",
        "confidence": "3;5;4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.4,
            0.48989794855663565
        ],
        "confidence_avg": [
            3.8,
            0.7483314773547882
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.32732683535398854
    },
    {
        "id": "mpTIzK4Zca",
        "title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy and language modeling capabilities, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.",
        "keywords": "Multilingual continual pretraining;data mixing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zihao Li;Shaoxiong Ji;Hengyu Luo;J\u00f6rg Tiedemann",
        "authorids": "~Zihao_Li13;~Shaoxiong_Ji1;~Hengyu_Luo1;~J\u00f6rg_Tiedemann1",
        "gender": "M;;;M",
        "homepage": "https://www.zihao.cool/;;;https://blogs.helsinki.fi/tiedeman/",
        "dblp": ";227/0291;;15/670",
        "google_scholar": "mSHvZpAAAAAJ;;;j6V-rOUAAAAJ",
        "orcid": "0009-0008-9329-5341;;;0000-0003-3065-7989",
        "linkedin": "zihao-li-968170212/;;hengyu-l-582434112/;",
        "or_profile": "~Zihao_Li13;~Shaoxiong_Ji1;~Hengyu_Luo1;~J\u00f6rg_Tiedemann1",
        "aff": "University of Helsinki+University of Helsinki;Technical University of Darmstadt;University of Helsinki;University of Helsinki",
        "aff_domain": "helsinki.fi+helsinki.fi;tu-darmstadt.de;helsinki.fi;helsinki.fi",
        "position": "PhD student+MS student;Researcher;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nli2025rethinking,\ntitle={Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting {LLM}s Across Languages and Resources},\nauthor={Zihao Li and Shaoxiong Ji and Hengyu Luo and J{\\\"o}rg Tiedemann},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=mpTIzK4Zca}\n}",
        "github": "",
        "project": "",
        "reviewers": "DAf3;Betk;fFA7",
        "site": "https://openreview.net/forum?id=mpTIzK4Zca",
        "pdf_size": 0,
        "rating": "6;6;6",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "mxcCg9YRqj",
        "title": "Fluid Language Model Benchmarking",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language model (LM) benchmarking faces several challenges: comprehensive evaluations are costly, benchmarks often fail to measure the intended capabilities, and evaluation quality can degrade due to labeling errors and benchmark saturation. Although various strategies have been proposed to mitigate these issues, they tend to address individual aspects in isolation, neglecting broader questions about overall evaluation quality. Here, we introduce Fluid Benchmarking, a new evaluation approach that advances LM benchmarking across multiple dimensions. Inspired by psychometrics, Fluid Benchmarking is based on the insight that the relative value of benchmark items depends on an LM's capability level, suggesting that evaluation should adapt to each LM. Methodologically, Fluid Benchmarking estimates an item response model based on existing LM evaluation results and uses the inferred quantities to select evaluation items dynamically, similar to computerized adaptive testing in education. In our experiments, we compare Fluid Benchmarking against the common practice of random item sampling as well as more sophisticated baselines, including alternative methods grounded in item response theory. We examine four dimensions&mdash;efficiency, validity, variance, and saturation&mdash;and find that Fluid Benchmarking achieves superior performance in all of them (e.g., higher validity and less variance on MMLU with fifty times fewer items). Our analysis shows that the two components of Fluid Benchmarking have distinct effects: item response theory, used to map performance into a latent ability space, increases validity, while dynamic item selection reduces variance. Overall, our results suggest that LM benchmarking can be substantially improved by moving beyond static evaluation.",
        "keywords": "language models;evaluation;item response theory;efficiency;robustness",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Valentin Hofmann;David Heineman;Ian Magnusson;Kyle Lo;Jesse Dodge;Maarten Sap;Pang Wei Koh;Chun Wang;Hannaneh Hajishirzi;Noah A. Smith",
        "authorids": "~Valentin_Hofmann1;~David_Heineman1;~Ian_Magnusson1;~Kyle_Lo1;~Jesse_Dodge1;~Maarten_Sap1;~Pang_Wei_Koh1;~Chun_Wang8;~Hannaneh_Hajishirzi1;~Noah_A._Smith2",
        "gender": ";M;;;M;M;M;F;F;",
        "homepage": "https://valentinhofmann.github.io/;https://davidheineman.com;;https://kyleclo.github.io/;http://www.cs.cmu.edu/~jessed/;http://maartensap.com;http://cs.stanford.edu/~pangwei;https://education.uw.edu/about/directory/chun-wang;https://homes.cs.washington.edu/~hannaneh/;",
        "dblp": "264/4665;336/4616;;220/2020;49/11425;153/9519;10/10453;;52/1296;",
        "google_scholar": "bbHOPKwAAAAJ;JO2Q6CUAAAAJ;;VJS12uMAAAAJ;nHy_1doAAAAJ;gFN4QUYAAAAJ;Nn990CkAAAAJ;6j3ABHUAAAAJ;LOV6_WIAAAAJ;",
        "orcid": ";;;;;;;;;",
        "linkedin": ";;;kylelo/;;;;;;",
        "or_profile": "~Valentin_Hofmann1;~David_Heineman1;~Ian_Magnusson1;~Kyle_Lo1;~Jesse_Dodge1;~Maarten_Sap1;~Pang_Wei_Koh1;~Chun_Wang8;~Hannaneh_Hajishirzi1;~Noah_A._Smith2",
        "aff": "Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Carnegie Mellon University;Allen Institute for Artificial Intelligence+University of Washington;University of Washington;Allen Institute for Artificial Intelligence+University of Washington;",
        "aff_domain": "allenai.org;allenai.org;;allenai.org;allenai.org;cmu.edu;allenai.org+cs.washington.edu;uw.edu;allenai.org+uw.edu;",
        "position": "Postdoc;Researcher;;Researcher;Researcher;Assistant Professor;Visiting Research Scientist+Assistant Professor;Full Professor;senior director+Associate Professor;",
        "bibtex": "@inproceedings{\nhofmann2025fluid,\ntitle={Fluid Language Model Benchmarking},\nauthor={Valentin Hofmann and David Heineman and Ian Magnusson and Kyle Lo and Jesse Dodge and Maarten Sap and Pang Wei Koh and Chun Wang and Hannaneh Hajishirzi and Noah A. Smith},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=mxcCg9YRqj}\n}",
        "github": "",
        "project": "",
        "reviewers": "22Eb;JcBQ;5T1x",
        "site": "https://openreview.net/forum?id=mxcCg9YRqj",
        "pdf_size": 0,
        "rating": "7;7;9",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.666666666666667,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            8,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.49999999999999983
    },
    {
        "id": "n3rZJrWPLE",
        "title": "Mixture of Attention Spans: Optimizing LLM Inference Efficiency with Heterogeneous Sliding-Window Lengths",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Sliding-window attention offers a hardware-efficient solution to the memory and throughput challenges of Large Language Models (LLMs) in long-context scenarios. Existing methods typically employ a single window length across all attention heads and input sizes. However, this uniform approach fails to capture the heterogeneous attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose *Mixture of Attention Spans* (MoA), which automatically tailors distinct sliding-window length configurations to different heads and layers. MoA constructs and navigates a search space of various window lengths and their scaling rules relative to input sizes. It profiles the model, evaluates potential configurations, and pinpoints the optimal length configurations for each head. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer inputs, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by 3.9\u00d7 with the same average sliding-window length, boosting retrieval accuracy by 1.5-7.1\u00d7 over the uniform-window baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models. Moreover, MoA narrows the performance gap with full attention, reducing the maximum relative performance drop from 9%-36% to within 5% across three long-context understanding benchmarks. MoA achieves a 1.2-1.4\u00d7 GPU memory reduction, boosting decode throughput by 6.6-8.2\u00d7 and 1.7-1.9\u00d7 over FlashAttention2 and vLLM, with minimal performance impact. Our code is available at https://github.com/thu-nics/MoA.",
        "keywords": "Efficient Attention;Sparse Attention;KV Cache Management;Large Language Models;Efficiency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tianyu Fu;Haofeng Huang;Xuefei Ning;Genghan Zhang;Boju Chen;Tianqi Wu;Hongyi Wang;Zixiao Huang;Shiyao Li;Shengen Yan;Guohao Dai;Huazhong Yang;Yu Wang",
        "authorids": "~Tianyu_Fu3;~Haofeng_Huang3;~Xuefei_Ning1;~Genghan_Zhang1;~Boju_Chen1;~Tianqi_Wu2;~Hongyi_Wang8;~Zixiao_Huang2;~Shiyao_Li2;~Shengen_Yan1;~Guohao_Dai4;~Huazhong_Yang2;~Yu_Wang3",
        "gender": "M;M;Not Specified;M;M;M;M;M;M;M;M;M;M",
        "homepage": "http://nicsefc.ee.tsinghua.edu.cn/people/TianyuFu;https://github.com/jason-huang03/jason-huang03.github.io;https://nics-effalg.com/ningxuefei/;https://zhang677.github.io/;http://nicsefc.ee.tsinghua.edu.cn/people/BojuChen;http://nicsefc.ee.tsinghua.edu.cn/people/TianqiWu;;;http://nicsefc.ee.tsinghua.edu.cn/people/ShiyaoLi;;https://nicsefc.ee.tsinghua.edu.cn/people/guohao-dai/;http://web.ee.tsinghua.edu.cn/yanghuazhong/en/index.htm;https://nicsefc.ee.tsinghua.edu.cn",
        "dblp": "219/6025-4;;202/9525;329/3725;;;;;;117/6968;147/1470;94/1128.html;w/YuWang2.html",
        "google_scholar": "Mnfue94AAAAJ;;oVslpJsAAAAJ;;OWseyN4AAAAJ;;;;JWaexW0AAAAJ;SvE3bdUAAAAJ;gz3Tkl0AAAAJ;;https://scholar.google.com.hk/citations?user=j8JGVvoAAAAJ",
        "orcid": "0000-0003-3508-1755;;;;;;0009-0008-7095-7963;0009-0000-1273-2573;;;;0000-0003-2421-353X;0000-0001-6108-5157",
        "linkedin": ";;;genghan-zhang/;;;;;;;;;",
        "or_profile": "~Tianyu_Fu3;~Haofeng_Huang3;~Xuefei_Ning1;~Genghan_Zhang1;~Boju_Chen1;~Tianqi_Wu2;~Hongyi_Wang8;~Zixiao_Huang2;~Shiyao_Li2;~Shengen_Yan1;~Guohao_Dai4;~Huazhong_Yang2;~Yu_Wang3",
        "aff": "Tsinghua University;Tsinghua University;Tsinghua University;Stanford University;Tsinghua University;Tsinghua University+Infinigence;Tsinghua University;Tsinghua University;Infinigence+Tsinghua University;Tsinghua University;Shanghai Jiaotong University;Tsinghua University;Tsinghua University",
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;stanford.edu;mails.tsinghua.edu.cn;tsinghua.edu.cn+infini-ai.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;infini-ai.com+tsinghua.edu.cn;tsinghua.edu.cn;sjtu.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;Undergrad student;Research Assistant Professor;PhD student;PhD student;MS student+Intern;MS student;Undergrad student;Intern+PhD student;Associate Professor;Associate Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nfu2025mixture,\ntitle={Mixture of Attention Spans: Optimizing {LLM} Inference Efficiency with Heterogeneous Sliding-Window Lengths},\nauthor={Tianyu Fu and Haofeng Huang and Xuefei Ning and Genghan Zhang and Boju Chen and Tianqi Wu and Hongyi Wang and Zixiao Huang and Shiyao Li and Shengen Yan and Guohao Dai and Huazhong Yang and Yu Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=n3rZJrWPLE}\n}",
        "github": "",
        "project": "",
        "reviewers": "dfKb;HcQe;RJum",
        "site": "https://openreview.net/forum?id=n3rZJrWPLE",
        "pdf_size": 0,
        "rating": "5;7;7",
        "confidence": "5;5;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            4.333333333333333,
            0.9428090415820634
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.49999999999999983
    },
    {
        "id": "n4JdyBGu6T",
        "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)\u2014adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers.\nDespite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations.\nSpecifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation.\nThis behavior makes MICL still unimodal and largely restricts its practical utility. \nMore importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context.\nAs a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored.\nTo address these issues, we first introduce Dynamic Attention Reallocation  (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens.\nIn addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information\u2014particularly visual content\u2014for correct task completion.\nExtensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the MICL capabilities.\nCode and datasets are available at [here](https://chenxshuo.github.io/true-micl-colm/).",
        "keywords": "Multimodal In-Context Learning;Multimodal Large Language Models;Vision Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuo Chen;Jianzhe Liu;Zhen Han;Yan Xia;Daniel Cremers;Philip Torr;Volker Tresp;Jindong Gu",
        "authorids": "~Shuo_Chen12;~Jianzhe_Liu1;~Zhen_Han3;~Yan_Xia5;~Daniel_Cremers1;~Philip_Torr1;~Volker_Tresp1;~Jindong_Gu1",
        "gender": "M;M;M;M;M;;M;",
        "homepage": "https://chenxshuo.github.io;https://github.com/Timbermorph;https://sites.google.com/view/zhenhan/home;https://yan-xia.github.io/;https://vision.in.tum.de/members/cremers;http://www.robots.ox.ac.uk/~tvg/;https://www.dbs.ifi.lmu.de/~tresp/;",
        "dblp": "00/6472-14;;;17/6518-3;c/DanielCremers;;t/VolkerTresp;",
        "google_scholar": "BKvdGiwAAAAJ;;HMdgrwoAAAAJ;xkBn4mMAAAAJ;cXQciMEAAAAJ;;xIJHTUwAAAAJ;",
        "orcid": "0000-0001-7305-3793;;;;;;0000-0001-9428-3686;",
        "linkedin": ";;zhen-han-08a769128/;;;;volker-tresp-8110a118/;",
        "or_profile": "~Shuo_Chen12;~Jianzhe_Liu1;~Zhen_Han3;~Yan_Xia5;~Daniel_Cremers1;~Philip_Torr1;~Volker_Tresp1;~Jindong_Gu1",
        "aff": "Siemens Corporate Research+University of Munich, Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen+Amazon;Technische Universit\u00e4t M\u00fcnchen;Amazon;University of Science and Technology of China+Technische Universit\u00e4t M\u00fcnchen;Technical University Munich;University of Oxford;Ludwig Maximilian University of Munich+Siemens Corporate Research;",
        "aff_domain": "siemens.com+campus.lmu.de+amazon.de;tum.de;amazon.com;ustc.edu.cn+tum.de;tum.de;ox.ac.uk;lmu.de+siemens.com;",
        "position": "Researcher+PhD student+Intern;MS student;Researcher;Associate Professor+Researcher;Full Professor;Full Professor;Associate Professor+Principal Researcher;",
        "bibtex": "@inproceedings{\nchen2025true,\ntitle={True Multimodal In-Context Learning Needs Attention to the Visual Context},\nauthor={Shuo Chen and Jianzhe Liu and Zhen Han and Yan Xia and Daniel Cremers and Philip Torr and Volker Tresp and Jindong Gu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=n4JdyBGu6T}\n}",
        "github": "",
        "project": "",
        "reviewers": "1vtb;Ro7V;gEKm;Lzs5",
        "site": "https://openreview.net/forum?id=n4JdyBGu6T",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            26,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "n5hmtkdl7k",
        "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria -- high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text by transforming it into hate speech, while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs.",
        "keywords": "LLM watermarking;Spoofing attack;Piggyback Spoofing attack",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Li An;Yujian Liu;Yepeng Liu;Yang Zhang;Yuheng Bu;Shiyu Chang",
        "authorids": "~Li_An3;~Yujian_Liu1;~Yepeng_Liu1;~Yang_Zhang3;~Yuheng_Bu1;~Shiyu_Chang2",
        "gender": "F;M;;M;M;Unspecified",
        "homepage": ";https://yujianll.github.io;;;https://buyuheng.github.io/;http://people.csail.mit.edu/chang87/",
        "dblp": ";206/8853;;06/6785-1;168/8338;28/9988",
        "google_scholar": ";rLetNLIAAAAJ;;_-5PSgQAAAAJ;1jPQEVMAAAAJ;r21asW4AAAAJ",
        "orcid": ";;;;0000-0002-3479-4553;",
        "linkedin": "li-an-842aa7206/;;;;bu-yuheng-36560039/;",
        "or_profile": "~Li_An3;~Yujian_Liu1;~Yepeng_Liu1;~Yang_Zhang3;~Yuheng_Bu1;~Shiyu_Chang2",
        "aff": "University of California, Santa Barbara;University of California, Santa Barbara;;International Business Machines;University of California, Santa Barbara+University of Florida;University of California, Santa Barbara",
        "aff_domain": "ucsb.edu;ucsb.edu;;ibm.com;ucsb.edu+ufl.edu;ucsb.edu",
        "position": "PhD student;PhD student;;Research Staff Employee;Assistant Professor+Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nan2025defending,\ntitle={Defending {LLM} Watermarking Against Spoofing Attacks with Contrastive Representation Learning},\nauthor={Li An and Yujian Liu and Yepeng Liu and Yang Zhang and Yuheng Bu and Shiyu Chang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=n5hmtkdl7k}\n}",
        "github": "",
        "project": "",
        "reviewers": "78SG;u26n;6rXN",
        "site": "https://openreview.net/forum?id=n5hmtkdl7k",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;2;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.0,
            0.816496580927726
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "n6mTO5JS4j",
        "title": "Teaching Models to Understand (but not Generate) High-risk Data",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language model developers typically filter out high-risk content\u2014such as toxic or copyrighted text\u2014from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models\u2019 ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.",
        "keywords": "LLMs;Language Models;Pre-training Data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ryan Yixiang Wang;Matthew Finlayson;Luca Soldaini;Swabha Swayamdipta;Robin Jia",
        "authorids": "~Ryan_Yixiang_Wang1;~Matthew_Finlayson1;~Luca_Soldaini1;~Swabha_Swayamdipta1;~Robin_Jia1",
        "gender": ";M;Non-Binary;F;M",
        "homepage": ";https://mattf1n.github.io;https://soldaini.net;http://swabhs.com/;https://robinjia.github.io/",
        "dblp": ";55/3614;160/1741;121/2036;182/2556",
        "google_scholar": ";_ODwk4EAAAAJ;3KPvwcgAAAAJ;3uTVQt0AAAAJ;ajZ-_O0AAAAJ",
        "orcid": ";;0000-0001-6998-9863;0000-0002-5851-8254;",
        "linkedin": ";;soldni/;swabhaswayamdipta;",
        "or_profile": "~Ryan_Yixiang_Wang1;~Matthew_Finlayson1;~Luca_Soldaini1;~Swabha_Swayamdipta1;~Robin_Jia1",
        "aff": ";University of Southern California;Allen Institute for Artificial Intelligence;University of Southern California;University of Southern California",
        "aff_domain": ";usc.edu;allenai.org;usc.edu;usc.edu",
        "position": ";PhD student;Researcher;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025teaching,\ntitle={Teaching Models to Understand (but not Generate) High-risk Data},\nauthor={Ryan Yixiang Wang and Matthew Finlayson and Luca Soldaini and Swabha Swayamdipta and Robin Jia},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=n6mTO5JS4j}\n}",
        "github": "",
        "project": "",
        "reviewers": "J4DB;P4Xk;zbxy;RnzU",
        "site": "https://openreview.net/forum?id=n6mTO5JS4j",
        "pdf_size": 0,
        "rating": "5;5;6;7",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "nSV8Depcpx",
        "title": "Plancraft: an evaluation dataset for planning with LLM agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.",
        "keywords": "planning;multi-modal;agents;LLMs;tool use;minecraft;RAG",
        "primary_area": "",
        "supplementary_material": "/attachment/73266a3ae8ba8b61a11ec99084fc33ef47b66c57.zip",
        "author": "Gautier Dagan;Frank Keller;Alex Lascarides",
        "authorids": "~Gautier_Dagan1;~Frank_Keller1;~Alex_Lascarides1",
        "gender": "M;M;F",
        "homepage": "https://www.gautier.tech/;https://homepages.inf.ed.ac.uk/keller/;http://homepages.inf.ed.ac.uk/alex",
        "dblp": "234/2268;30/4872;14/1635",
        "google_scholar": "fyqu2nIAAAAJ;https://scholar.google.co.uk/citations?user=-lbtnAgAAAAJ;https://scholar.google.co.uk/citations?user=fVRKVf4AAAAJ",
        "orcid": "0000-0002-1867-4201;0000-0002-8242-4362;0000-0003-1704-1864",
        "linkedin": "gautier-dagan/;;alex-lascarides-9361463/?originalSubdomain=uk",
        "or_profile": "~Gautier_Dagan1;~Frank_Keller1;~Alex_Lascarides1",
        "aff": "University of Edinburgh;University of Edinburgh;University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "position": "PhD student;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\ndagan2025plancraft,\ntitle={Plancraft: an evaluation dataset for planning with {LLM} agents},\nauthor={Gautier Dagan and Frank Keller and Alex Lascarides},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=nSV8Depcpx}\n}",
        "github": "",
        "project": "",
        "reviewers": "rg18;Q4bq;ME3a;87j3",
        "site": "https://openreview.net/forum?id=nSV8Depcpx",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "nVQmW1af6j",
        "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency.\n\nThe Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL.\n\nWe open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine \"fully open\" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.",
        "keywords": "multimodal large language model;efficient pre-training;high quality image-text filtering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weizhi Wang;Yu Tian;Linjie Yang;Heng Wang;Xifeng Yan",
        "authorids": "~Weizhi_Wang1;~Yu_Tian4;~Linjie_Yang4;~Heng_Wang2;~Xifeng_Yan1",
        "gender": "M;;M;;",
        "homepage": "https://victorwz.github.io;;https://sites.google.com/site/linjieyang89/;;https://sites.cs.ucsb.edu/~xyan/",
        "dblp": "98/6969;;126/6794;;y/XifengYan",
        "google_scholar": "UC2_V1MAAAAJ;;;;XZV2eogAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Weizhi_Wang1;~Yu_Tian4;~Linjie_Yang4;~Heng_Wang2;~Xifeng_Yan1",
        "aff": "University of California, Santa Barbara;;ByteDance Inc.;;UC Santa Barbara",
        "aff_domain": "ucsb.edu;;bytedance.com;;ucsb.edu",
        "position": "PhD student;;Research Scientist;;Full Professor",
        "bibtex": "@inproceedings{\nwang2025openqwenvl,\ntitle={Open-Qwen2{VL}: Compute-Efficient Pre-Training of Fully-Open Multimodal {LLM}s on Academic Resources},\nauthor={Weizhi Wang and Yu Tian and Linjie Yang and Heng Wang and Xifeng Yan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=nVQmW1af6j}\n}",
        "github": "",
        "project": "",
        "reviewers": "G2dF;eK8t;RPfn;aZwW",
        "site": "https://openreview.net/forum?id=nVQmW1af6j",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "naEyNVTLsh",
        "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Vision-language models (VLMs) have advanced rapidly in processing multimodal information, but their ability to reconcile conflicting signals across modalities remains underexplored. This study investigates how VLMs process ASCII art, a unique medium where textual elements collectively form visual patterns, potentially creating semantic-visual conflicts. We introduce a novel evaluation framework that systematically challenges five state-of-the-art models (including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where character-level semantics deliberately contradict global visual patterns. Our experiments reveal a strong text-priority bias: VLMs consistently prioritize textual information over visual patterns, with visual recognition ability declining dramatically as semantic complexity increases. Various mitigation attempts through visual parameter tuning and prompt engineering yielded only modest improvements, suggesting that this limitation requires architectural-level solutions. These findings uncover fundamental flaws in how current VLMs integrate multimodal information, providing important guidance for future model development while highlighting significant implications for content moderation systems vulnerable to adversarial examples.",
        "keywords": "vision language model;ASCII art;sentiment analysis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhaochen Wang;Bryan Hooi;Yiwei Wang;Ming-Hsuan Yang;Zi Huang;Yujun Cai",
        "authorids": "~Zhaochen_Wang1;~Bryan_Hooi1;~Yiwei_Wang2;~Ming-Hsuan_Yang1;~Zi_Huang1;~Yujun_Cai1",
        "gender": "M;;M;M;F;F",
        "homepage": "https://github.com/George0ne;http://bhooi.github.io;;https://faculty.ucmerced.edu/mhyang/;https://staff.itee.uq.edu.au/huang/;",
        "dblp": ";169/9975;50/5889-1;79/3711.html;70/6862;227/4399",
        "google_scholar": ";;https://scholar.google.com.hk/citations?user=Sh9QvBkAAAAJ;p9-ohHsAAAAJ;https://scholar.google.com.au/citations?user=iAWMsgEAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-5645-1754;;0000-0003-4848-2304;;",
        "linkedin": ";;;minghsuanyang/;;",
        "or_profile": "~Zhaochen_Wang1;~Bryan_Hooi1;~Yiwei_Wang2;~Ming-Hsuan_Yang1;~Zi_Huang1;~Yujun_Cai1",
        "aff": "The University of Queensland;National University of Singapore;University of California, Merced;Google DeepMind+University of California at Merced;University of Queensland;The University of Queensland",
        "aff_domain": "uq.edu.au;nus.edu.sg;ucmerced.edu;google.com+umcerced.edu;uq.edu.au;uq.edu.au",
        "position": "MS student;Assistant Professor;Assistant Professor;Senior Staff Research Scientist+Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025text,\ntitle={Text Speaks Louder than Vision: {ASCII} Art Reveals Textual Biases in Vision-Language Models},\nauthor={Zhaochen Wang and Bryan Hooi and Yiwei Wang and Ming-Hsuan Yang and Zi Huang and Yujun Cai},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=naEyNVTLsh}\n}",
        "github": "",
        "project": "",
        "reviewers": "ygS7;3Tgy;hQ3b;Fq3A",
        "site": "https://openreview.net/forum?id=naEyNVTLsh",
        "pdf_size": 0,
        "rating": "5;5;6;7",
        "confidence": "3;3;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9045340337332909
    },
    {
        "id": "nqX9UYW9Af",
        "title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.",
        "keywords": "collaborative inference;efficient inference;token-level routing;large language model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenhao Zheng;Yixiao Chen;Weitong Zhang;Souvik Kundu;Yun Li;Zhengzhong Liu;Eric P. Xing;Hongyi Wang;Huaxiu Yao",
        "authorids": "~Wenhao_Zheng4;~Yixiao_Chen2;~Weitong_Zhang2;~Souvik_Kundu2;~Yun_Li7;~Zhengzhong_Liu1;~Eric_Xing1;~Hongyi_Wang1;~Huaxiu_Yao1",
        "gender": "M;M;;M;Not Specified;M;M;M;M",
        "homepage": ";;;https://ksouvik52.github.io;https://yunliweb.its.unc.edu;https://hunterhector.github.io/;http://www.cs.cmu.edu/~epxing/;https://hwang595.github.io/;http://huaxiuyao.mystrikingly.com",
        "dblp": ";;;126/2210;;166/0352;36/3855;15/832-1.html;197/1635",
        "google_scholar": "dR1J_4EAAAAJ;;;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;S9E-hMwAAAAJ;https://scholar.google.com.tw/citations?user=5pKTRxEAAAAJ;zYdZORsAAAAJ;A20BZnQAAAAJ",
        "orcid": "0000-0002-7108-370X;;;0000-0002-3533-9405;0000-0002-9275-4189;;;;",
        "linkedin": ";yixiao-chen-94853221a/;;souvik-kundu-64922b50/;;hunterhector/;;hongyi-wang-b89651102/;huaxiuyao/",
        "or_profile": "~Wenhao_Zheng4;~Yixiao_Chen2;~Weitong_Zhang2;~Souvik_Kundu2;~Yun_Li7;~Zhengzhong_Liu1;~Eric_Xing1;~Hongyi_Wang1;~Huaxiu_Yao1",
        "aff": "University of North Carolina at Chapel Hill;Harvard University+University of North Carolina at Chapel Hill;;Intel+Intel;University of North Carolina at Chapel Hill;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed Univeristy of AI+School of Computer Science, Carnegie Mellon University;Rutgers University+GenBio AI;Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "unc.edu;harvard.edu+unc.edu;;intel.com+intel.com;unc.edu;mbzuai.ac.ae;mbzuai.ac.ae+cs.cmu.edu;rutgers.edu+genbio.ai;cs.unc.edu",
        "position": "PhD student;MS student+Undergrad student;;Principal Researcher+Researcher;Full Professor;Researcher;Full Professor+Full Professor;Assistant Professor+Head of Infrastructure;Assistant Professor",
        "bibtex": "@inproceedings{\nzheng2025citer,\ntitle={{CITER}: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing},\nauthor={Wenhao Zheng and Yixiao Chen and Weitong Zhang and Souvik Kundu and Yun Li and Zhengzhong Liu and Eric P. Xing and Hongyi Wang and Huaxiu Yao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=nqX9UYW9Af}\n}",
        "github": "",
        "project": "",
        "reviewers": "xyvj;iqRc;mhTX;ZgLP",
        "site": "https://openreview.net/forum?id=nqX9UYW9Af",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "4;3;4;5",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "nrZysNmJ0n",
        "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The syntactic structures of sentences can be readily read-out from the activations of large language models (LLMs). However, the ``structural probes'' that have been developed to reveal this phenomenon are typically evaluated on an indiscriminate set of sentences. Consequently, it remains unclear whether structural and/or statistical factors systematically affect these syntactic representations. To address this issue, we conduct an in-depth analysis of structural probes on three controlled benchmarks. Our results are fourfold. First, structural probes are biased by a superficial property: the closer two words are in a sentence, the more likely structural probes will consider them as syntactically linked. Second, structural probes are challenged by linguistic properties: they poorly represent deep syntactic structures, and get interfered by interacting nouns or ungrammatical verb forms. Third, structural probes do not appear to be affected by the LLMs' predictability of individual words. Fourth, despite these challenges, structural probes still reveal syntactic links far more accurately than the linear baseline or the LLMs' raw activation spaces. Taken together, this work sheds light on both the challenges and the successes of current structural probes and provides a benchmark made of controlled stimuli to better evaluate their performance.",
        "keywords": "Syntax;LLMs;Probing;Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pablo J. Diego Simon;Emmanuel Chemla;Jean-Remi King;Yair Lakretz",
        "authorids": "~Pablo_J._Diego_Simon1;~Emmanuel_Chemla1;~Jean-Remi_King1;~Yair_Lakretz2",
        "gender": "M;;M;M",
        "homepage": "https://www.linkedin.com/in/pablo-j-diego-sim%C3%B3n-b3475a212/;http://www.emmanuel.chemla.free.fr;https://kingjr.github.io/;https://yairlak.github.io/",
        "dblp": ";85/8850;;166/5196",
        "google_scholar": ";;XZOgIwEAAAAJ;https://scholar.google.co.il/citations?user=cNnJ5YUAAAAJ",
        "orcid": ";0000-0002-8423-5880;;0000-0001-8774-6427",
        "linkedin": ";;;",
        "or_profile": "~Pablo_J._Diego_Simon1;~Emmanuel_Chemla1;~Jean-Remi_King1;~Yair_Lakretz2",
        "aff": "ENS - PSL;Earth Species Project+CNRS;CNRS;Ecole Normale Sup\u00e9rieure de Paris",
        "aff_domain": "psl.eu;earthspecies.org+cnrs.fr;cnrs.fr;ens.fr",
        "position": "PhD student;Principal Researcher+Full Professor;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nsimon2025probing,\ntitle={Probing Syntax in Large Language Models: Successes and Remaining Challenges},\nauthor={Pablo J. Diego Simon and Emmanuel Chemla and Jean-Remi King and Yair Lakretz},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=nrZysNmJ0n}\n}",
        "github": "",
        "project": "",
        "reviewers": "D9Dc;tL7W;qy93;dAJ4",
        "site": "https://openreview.net/forum?id=nrZysNmJ0n",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "3;4;2;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "oGO0fNVWrN",
        "title": "Plato: Plan to Efficient Decode for Large Language Model Inference",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural language tasks, but their inference incurs substantial computational and memory overhead.\nTo improve efficiency, parallel decoding methods like Skeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent processing. However, these methods significantly compromise answer quality by treating semantically linked sub-problems as independent.\nWe propose Plato, a novel approach that co-designs algorithms and systems for semantic-aware parallel decoding. Plato leverages LLMs to organize sub-problems into a dependency graph based on logical and causal relationships, enabling concurrent decoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding stages, implements a global context cache, and carefully structures node inference prompts to maximize key-value cache reuse and minimize overhead. Our evaluations show that Plato improves throughput by up to 68% over autoregressive decoding while achieving a 40% net win rate in answer quality. Compared to SoT, Plato demonstrates a remarkable 90% quality net-win rate. Ablation studies reveal that our pipeline design improves speedup by 29%, while our KV cache reuse optimization reduces overhead by 75%.",
        "keywords": "Efficient LLM Inference",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuowei Jin;Xueshen Liu;Yongji Wu;Haizhong Zheng;Qingzhao Zhang;Atul Prakash;Matthew Lentz;Danyang Zhuo;Feng Qian;Zhuoqing Mao",
        "authorids": "~Shuowei_Jin1;~Xueshen_Liu1;~Yongji_Wu1;~Haizhong_Zheng1;~Qingzhao_Zhang1;~Atul_Prakash1;~Matthew_Lentz1;~Danyang_Zhuo1;~Feng_Qian4;~Zhuoqing_Mao1",
        "gender": "M;M;M;M;M;;;M;;F",
        "homepage": "https://shuoweijin.com/;https://xenshinu.github.io/;;http://zhenghaizhong.com/;https://zqzqz.github.io;https://www.eecs.umich.edu/~aprakash;;https://danyangzhuo.com/;https://feng-qian.github.io/;https://web.eecs.umich.edu/~zmao/",
        "dblp": "246/2987;;;158/4817;132/5633;p/AtulPrakash;;151/7537;54/476-1;",
        "google_scholar": "hxeEwm8AAAAJ;3AxUf6QAAAAJ;Yw_1RuoAAAAJ;Zx6pKsQAAAAJ;ZSXIPHgAAAAJ;kIkHa2IAAAAJ;;E3yOuvEAAAAJ;-ExSBb0AAAAJ;Ba_Ci9UAAAAJ",
        "orcid": ";0009-0001-0227-7463;0009-0000-6297-1599;0000-0003-3723-8701;0000-0003-2598-5988;0000-0002-4907-3687;;;0000-0001-8509-2650;",
        "linkedin": ";xueshen-liu-a75718205/;;haizhong-zheng-1093a0a7/;qingzhao-zhang-zqzqz/;atul-prakash-8729a44/;;;;",
        "or_profile": "~Shuowei_Jin1;~Xueshen_Liu1;~Yongji_Wu1;~Haizhong_Zheng1;~Qingzhao_Zhang1;~Atul_Prakash1;~Matthew_Lentz1;~Danyang_Zhuo1;~Feng_Qian4;~Zhuoqing_Mao1",
        "aff": "University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of California, Berkeley;Carnegie Mellon University;University of Michigan - Ann Arbor;University of Michigan;;Duke University;ByteDance Inc.+University of Southern California;Google+University of Michigan",
        "aff_domain": "umich.edu;umich.edu;berkeley.edu;andrew.cmu.edu;umich.edu;umich.edu;;duke.edu;bytedance.com+usc.edu;google.com+umich.edu",
        "position": "PhD student;PhD student;Postdoc;Postdoc;PhD student;Professor;;Assistant Professor;Researcher+Associate Professor;consultant+Professor",
        "bibtex": "@inproceedings{\njin2025plato,\ntitle={Plato: Plan to Efficient Decode for Large Language Model Inference},\nauthor={Shuowei Jin and Xueshen Liu and Yongji Wu and Haizhong Zheng and Qingzhao Zhang and Atul Prakash and Matthew Lentz and Danyang Zhuo and Feng Qian and Zhuoqing Mao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oGO0fNVWrN}\n}",
        "github": "",
        "project": "",
        "reviewers": "qy58;NYGo;5cB7;JH4e",
        "site": "https://openreview.net/forum?id=oGO0fNVWrN",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.30151134457776363
    },
    {
        "id": "oHR862dpMC",
        "title": "ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reasoning models have demonstrated impressive performance on difficult tasks that traditional language models struggle at. However, many are plagued with the problem of overthinking---generating large amounts of unnecessary tokens which don't improve accuracy on a question. We introduce approximate measures of problem-level difficulty and demonstrate that a clear relationship between problem difficulty and optimal token spend exists, and evaluate how well calibrated a variety of reasoning models are in terms of efficiently allocating the optimal token count. We find that in general, reasoning models are poorly calibrated, particularly on easy problems. To evaluate calibration on easy questions we introduce DUMB500, a dataset of extremely easy math, reasoning, code, and task problems, and jointly evaluate reasoning model on these simple examples and extremely difficult examples from existing frontier benchmarks on the same task domain. Finally, we introduce ThoughtTerminator, a training-free black box decoding technique that significantly improves reasoning model calibration.",
        "keywords": "reasoning model;overthinking;decoding;tool use;evaluation;benchmark",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiao Pu;Michael Saxon;Wenyue Hua;William Yang Wang",
        "authorids": "~Xiao_Pu2;~Michael_Saxon1;~Wenyue_Hua1;~William_Yang_Wang2",
        "gender": "F;M;F;",
        "homepage": ";https://saxon.me;;",
        "dblp": "91/4650-3;222/6656;278/7993;",
        "google_scholar": "rRazhgkAAAAJ;pAlwjdgAAAAJ;Yqw8P-QAAAAJ;",
        "orcid": ";;0009-0008-2043-2704;",
        "linkedin": ";;wenyue-hua-094b6b176/;",
        "or_profile": "~Xiao_Pu2;~Michael_Saxon1;~Wenyue_Hua1;~William_Yang_Wang2",
        "aff": "University of California, Santa Barbara;UC Santa Barbara;University of California, Santa Barbara;",
        "aff_domain": "ucsb.edu;ucsb.edu;ucsb.edu;",
        "position": "PhD student;PhD student;Postdoc;",
        "bibtex": "@inproceedings{\npu2025thoughtterminator,\ntitle={ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models},\nauthor={Xiao Pu and Michael Saxon and Wenyue Hua and William Yang Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oHR862dpMC}\n}",
        "github": "",
        "project": "",
        "reviewers": "U1jN;aNwS;1isg;HZWD",
        "site": "https://openreview.net/forum?id=oHR862dpMC",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "5;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "oKdVFxngy1",
        "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Podcasts have become daily companions for half a billion users. Given the enormous amount of podcast content available, highlights provide a valuable signal that helps viewers get the gist of an episode and decide if they want to invest in listening to it in its entirety. However, identifying highlights automatically is challenging due to the unstructured and long-form nature of the content. We introduce Rhapsody, a dataset of 13K podcast episodes paired with segment-level highlight scores derived from YouTube's 'most replayed' feature. We frame the podcast highlight detection as a segment-level binary classification task. We explore various baseline approaches, including zero-shot prompting of language models and lightweight fine-tuned language models using segment-level classification heads. Our experimental results indicate that even state-of-the-art language models like GPT-4o and Gemini struggle with this task, while models fine-tuned with in-domain data significantly outperform their zero-shot performance. The fine-tuned model benefits from leveraging both speech signal features and transcripts. These findings highlight the challenges for fine-grained information access in long-form spoken media.",
        "keywords": "podcast highlight detection;long-context reasoning;spoken language processing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Younghan Park;Anuj Diwan;David Harwath;Eunsol Choi",
        "authorids": "~Younghan_Park1;~Anuj_Diwan1;~David_Harwath1;~Eunsol_Choi1",
        "gender": "M;M;M;",
        "homepage": "https://younghanstark.github.io/;https://ajd12342.github.io/;https://www.cs.utexas.edu/~harwath/index.html;https://eunsol.github.io/",
        "dblp": ";276/7585;;116/2765",
        "google_scholar": ";RiiYXH0AAAAJ;C0kDOzcAAAAJ;6wulN88AAAAJ",
        "orcid": ";0000-0002-1783-3717;;0000-0003-3607-9104",
        "linkedin": "younghan-park;;;",
        "or_profile": "~Younghan_Park1;~Anuj_Diwan1;~David_Harwath1;~Eunsol_Choi1",
        "aff": "Yonsei University;University of Texas at Austin;University of Texas, Austin;New York University",
        "aff_domain": "yonsei.ac.kr;utexas.edu;utexas.edu;nyu.edu",
        "position": "Undergrad student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\npark2025rhapsody,\ntitle={Rhapsody: A Dataset for Highlight Detection in Podcasts},\nauthor={Younghan Park and Anuj Diwan and David Harwath and Eunsol Choi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oKdVFxngy1}\n}",
        "github": "",
        "project": "",
        "reviewers": "HBck;ZtNz;7EoE;csU1",
        "site": "https://openreview.net/forum?id=oKdVFxngy1",
        "pdf_size": 0,
        "rating": "6;6;6;8",
        "confidence": "5;5;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.8660254037844386
        ],
        "confidence_avg": [
            4.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "oN9STRYQVa",
        "title": "Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus is shifting towards solving more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5\\%, 12.3\\%, 14.8\\%, 11.1\\%, and 15.3\\% in relative accuracy on GSM8k, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8k (a math dataset) by 16.9\\%.",
        "keywords": "Large Language Models;Reinforcement Learning;Multi-Step Reasoning;Tool Use;Synthetic Data",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anna Goldie;Azalia Mirhoseini;Hao Zhou;Irene Cai;Christopher D Manning",
        "authorids": "~Anna_Goldie2;~Azalia_Mirhoseini3;~Hao_Zhou46;~Irene_Cai1;~Christopher_D_Manning1",
        "gender": "F;;M;F;M",
        "homepage": "http://annagoldie.com;;;;https://nlp.stanford.edu/~manning/",
        "dblp": ";;;;m/ChristopherDManning",
        "google_scholar": ";;6Va6E-gAAAAJ;;1zmDOdwAAAAJ",
        "orcid": ";;;;0000-0001-6155-649X",
        "linkedin": ";;;yunling-cai;christopher-manning-011575/",
        "or_profile": "~Anna_Goldie2;~Azalia_Mirhoseini3;~Hao_Zhou46;~Irene_Cai1;~Christopher_D_Manning1",
        "aff": "Google DeepMind+Stanford University;;Google;Google;Computer Science Department, Stanford University",
        "aff_domain": "anthropic.com+stanford.edu;;google.com;deepmind.com;cs.stanford.edu",
        "position": "Researcher+PhD student;;Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\ngoldie2025synthetic,\ntitle={Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use},\nauthor={Anna Goldie and Azalia Mirhoseini and Hao Zhou and Irene Cai and Christopher D Manning},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oN9STRYQVa}\n}",
        "github": "",
        "project": "",
        "reviewers": "WJVa;SNTP;SiLC;4922",
        "site": "https://openreview.net/forum?id=oN9STRYQVa",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "oP3b5YBFoP",
        "title": "Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This study investigates the layerwise importance of feed-forward networks (FFNs) in transformer-based language models during pretraining.\nWe introduce an experimental approach that, while maintaining the total parameter count, increases the FFN dimensions in some layers and completely removes the FFNs from other layers.\nFurthermore, since our focus is on the importance of FFNs during pretraining, we train models from scratch to examine whether the importance of FFNs varies depending on their layer positions, rather than using publicly available pretrained models as is frequently done.\nThrough comprehensive evaluations of models with varying sizes (285M, 570M, and 1.2B parameters) and layer counts (12, 24, and 40 layers), we demonstrate that concentrating FFNs in 70\\% of the consecutive middle layers consistently outperforms standard configurations for multiple downstream tasks.",
        "keywords": "Feed-Forward Networks;Model Architecture;Knowledge Representation;Pre-training",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wataru Ikeda;Kazuki Yano;Ryosuke Takahashi;Jaesung Lee;KeigoShibata;Jun Suzuki",
        "authorids": "~Wataru_Ikeda2;~Kazuki_Yano1;~Ryosuke_Takahashi2;~Jaesung_Lee3;~KeigoShibata1;~Jun_Suzuki1",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://wataruuuuu.github.io/;https://kazuki-ya.github.io/;https://r-takahashi.webflow.io/;https://x.com/2225333_;https://x.com/butternese;https://www.nlp.ecei.tohoku.ac.jp/~jun/",
        "dblp": ";97/428;35/5848;;;78/6923",
        "google_scholar": ";;-tE--yUAAAAJ;;;https://scholar.google.co.jp/citations?user=XO5CrIsAAAAJ",
        "orcid": ";;;;;0000-0003-2108-1340",
        "linkedin": ";;ryosuke-takahashi-859977280/;;;",
        "or_profile": "~Wataru_Ikeda2;~Kazuki_Yano1;~Ryosuke_Takahashi2;~Jaesung_Lee3;~KeigoShibata1;~Jun_Suzuki1",
        "aff": "Tohoku University, Tokyo Institute of Technology+Tohoku University;Tohoku University+Tohoku University;Tohoku University, Tokyo Institute of Technology+Tohoku University;Tohoku University;Tohoku University+Tohoku University;Tohoku University",
        "aff_domain": "tohoku.ac.jp+tohoku.ac.jp;tohoku.ac.jp+tohoku.ac.jp;tohoku.ac.jp+tohoku.ac.jp;tohoku.ac.jp;tohoku.ac.jp+tohoku.ac.jp;tohoku.ac.jp",
        "position": "MS student+Undergrad student;PhD student+MS student;PhD student+MS student;MS student;MS student+Undergrad student;Full Professor",
        "bibtex": "@inproceedings{\nikeda2025layerwise,\ntitle={Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models},\nauthor={Wataru Ikeda and Kazuki Yano and Ryosuke Takahashi and Jaesung Lee and KeigoShibata and Jun Suzuki},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oP3b5YBFoP}\n}",
        "github": "",
        "project": "",
        "reviewers": "8WSS;Nedb;dJhd;XrFb",
        "site": "https://openreview.net/forum?id=oP3b5YBFoP",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "oPAjXGV8qQ",
        "title": "Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Pre-tokenization, the initial step in many modern tokenization pipelines, segments text into smaller units called pretokens, typically splitting on whitespace and punctuation. While this process encourages having full, individual words as tokens, it introduces a fundamental limitation in most tokenization algorithms such as Byte Pair Encoding (BPE). Specifically, pre-tokenization causes the distribution of tokens in a corpus to heavily skew towards common, full-length words. This skewed distribution limits the benefits of expanding to larger vocabularies, since the additional tokens appear with progressively lower counts. To overcome this barrier, we propose BoundlessBPE, a modified BPE algorithm that relaxes the pretoken boundary constraint. Our approach selectively merges two complete pretokens into a larger unit we term a superword. Superwords are not necessarily semantically cohesive. For example, the pretokens \" of\" and \" the\" might be combined to form the superword \" of the\". This merging strategy results in a substantially more uniform distribution of tokens across a corpus than standard BPE, and compresses text more effectively, with an approximate 20% increase in bytes per token.",
        "keywords": "tokenization;Byte Pair Encoding;BPE;subword tokenization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Craig W Schmidt;Varshini Reddy;Chris Tanner;Yuval Pinter",
        "authorids": "~Craig_W_Schmidt1;~Varshini_Reddy2;~Chris_Tanner1;~Yuval_Pinter1",
        "gender": "M;F;M;M",
        "homepage": ";;http://www.chriswtanner.com;http://www.yuvalpinter.com",
        "dblp": ";303/5599;165/0792;153/5384",
        "google_scholar": "dihkMwgAAAAJ;A6BwHbYAAAAJ;FmXNcCoAAAAJ;aYAcXccAAAAJ",
        "orcid": ";;;0000-0003-3174-1621",
        "linkedin": "craig-w-schmidt/;varshinireddy/;chriswtanner/;yuvalpinter",
        "or_profile": "~Craig_W_Schmidt1;~Varshini_Reddy2;~Chris_Tanner1;~Yuval_Pinter1",
        "aff": "Kensho Technologies;Kensho Technologies;Massachusetts Institute of Technology+Kensho;Ben-Gurion University of the Negev+Amazon Science",
        "aff_domain": "kensho.com;kensho.com;mit.edu+kensho.com;bgu.ac.il+amazon.com",
        "position": "Researcher;Researcher;Lecturer+Principal Researcher;Senior Lecturer+Visiting Academic",
        "bibtex": "@inproceedings{\nschmidt2025boundless,\ntitle={Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier},\nauthor={Craig W Schmidt and Varshini Reddy and Chris Tanner and Yuval Pinter},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oPAjXGV8qQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "sXwb;s1uu;bq5M;DcHp;SuJi",
        "site": "https://openreview.net/forum?id=oPAjXGV8qQ",
        "pdf_size": 0,
        "rating": "4;6;6;6;8",
        "confidence": "4;3;4;3;5",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.2649110640673518
        ],
        "confidence_avg": [
            3.8,
            0.7483314773547882
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.42257712736425823
    },
    {
        "id": "oSub7DiyjL",
        "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite significant advances in vision-language models (VLMs), image captioning often suffers from a lack of detail, with base models producing short, generic captions. This limitation persists even though VLMs are equipped with strong vision and language backbones. While supervised data and complex reward functions have been proposed to improve detailed image captioning, we identify a simpler underlying issue: a bias towards the end-of-sequence (EOS) token, which is introduced during cross-entropy training. We propose an unsupervised method to debias the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the generation of longer, more detailed captions without the need for intricate reward functions or supervision. Our approach is straightforward, effective, and easily applicable to any pretrained model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed captioning benchmarks. Our results show a substantial increase in caption length and relevant details, albeit with an expected increase in the rate of hallucinations.",
        "keywords": "Detailed image captioning; sequence training; reinforcement learning; vision langauge models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abdelrahman Mohamed;Yova Kementchedjhieva",
        "authorids": "~Abdelrahman_Mohamed3;~Yova_Kementchedjhieva1",
        "gender": "M;F",
        "homepage": ";",
        "dblp": ";225/7708",
        "google_scholar": "WL4TvKMAAAAJ;",
        "orcid": "0000-0003-3187-2883;0009-0000-7465-5702",
        "linkedin": "abdelrahman-helmy-91212713b/;",
        "or_profile": "~Abdelrahman_Mohamed3;~Yova_Kementchedjhieva1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae",
        "position": "Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nmohamed2025the,\ntitle={The Devil is in the {EOS}: Sequence Training for Detailed Image Captioning},\nauthor={Abdelrahman Mohamed and Yova Kementchedjhieva},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oSub7DiyjL}\n}",
        "github": "",
        "project": "",
        "reviewers": "m9TX;881E;Pbv2",
        "site": "https://openreview.net/forum?id=oSub7DiyjL",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "oaCUsn391F",
        "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their substantial memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we propose \\textit{SlimMoE}, a multi-stage compression framework that transforms large MoE models into significantly smaller and more efficient variants without the cost of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation typical of one-shot pruning.\nUsing SlimMoE, we compress Phi-3.5-MoE (41.9B total / 6.6B activated parameters) into two smaller models: Phi-mini-MoE (7.6B total / 2.4B activated) and Phi-tiny-MoE (3.8B total / 1.1B activated), using only 400B tokens -- less than 10\\% of the original training data. These models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them well suited for academic and resource-limited settings. \nOur experiments show that the compressed models outperform others of similar size and remain competitive with larger models. For example, Phi-mini-MoE matches or exceeds the performance of Phi-3-mini while using only two-thirds of the activated parameters and achieves comparable MMLU scores to LLaMA 3.1 8B with significantly lower latency. These results highlight that structured pruning combined with multi-stage distillation is an effective strategy for building high-quality, compact MoE models, enabling broader adoption of MoE architectures across diverse computational environments. We release our models at \\url{https://huggingface.co/microsoft/Phi-mini-MoE-instruct} and \\url{https://huggingface.co/microsoft/Phi-tiny-MoE-instruct}.",
        "keywords": "Large Language Model;Mixture of Experts;Structured Pruning;Knowledge Distillation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zichong Li;Chen Liang;Zixuan Zhang;Ilgee Hong;Young Jin Kim;Weizhu Chen;Tuo Zhao",
        "authorids": "~Zichong_Li2;~Chen_Liang3;~Zixuan_Zhang5;~Ilgee_Hong1;~Young_Jin_Kim1;~Weizhu_Chen1;~Tuo_Zhao2",
        "gender": "M;F;F;M;M;M;",
        "homepage": "https://github.com/zichongli5/zichongli5.github.io;https://cliang1453.github.io/;https://www.isye.gatech.edu/users/zixuan-zhang;https://ilgeehong.github.io/;https://www.microsoft.com/en-us/research/people/youki/;https://www.microsoft.com/en-us/research/people/wzchen/;",
        "dblp": ";35/3221-6;;;00/8110-1.html;79/2536;",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;;;LG_E-4EAAAAJ;",
        "orcid": ";;;;;;",
        "linkedin": ";;;;ykim362/;;",
        "or_profile": "~Zichong_Li2;~Chen_Liang3;~Zixuan_Zhang5;~Ilgee_Hong1;~Young_Jin_Kim1;~Weizhu_Chen1;~Tuo_Zhao2",
        "aff": "Georgia Institute of Technology;Microsoft;Georgia Institute of Technology;Georgia Institute of Technology+Amazon;Microsoft;Microsoft GenAI;",
        "aff_domain": "gatech.edu;microsoft.com;gatech.edu;gatech.edu+amazon.com;microsoft.com;microsoft.com;",
        "position": "PhD student;Researcher;PhD student;PhD student+Intern;Principal Researcher;Vice President;",
        "bibtex": "@inproceedings{\nli2025slimmoe,\ntitle={SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation},\nauthor={Zichong Li and Chen Liang and Zixuan Zhang and Ilgee Hong and Young Jin Kim and Weizhu Chen and Tuo Zhao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oaCUsn391F}\n}",
        "github": "",
        "project": "",
        "reviewers": "e3xh;MkJW;6UVt",
        "site": "https://openreview.net/forum?id=oaCUsn391F",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "oj3ETSitjb",
        "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication. However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.",
        "keywords": "Vision-Language Models;Pragmatics;Referring Expression Generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ziqiao Ma;Jing Ding;Xuejun Zhang;Dezhi Luo;Jiahe Ding;Sihan Xu;Yuchen Huang;Run Peng;Joyce Chai",
        "authorids": "~Ziqiao_Ma1;~Jing_Ding3;~Xuejun_Zhang4;~Dezhi_Luo1;~Jiahe_Ding1;~Sihan_Xu2;~Yuchen_Huang5;~Run_Peng1;~Joyce_Chai2",
        "gender": "Not Specified;F;F;;M;M;M;M;",
        "homepage": "http://mars-tin.github.io/;;https://xuejunzhang2002.github.io/;https://ihzedoul.com;;https://sihanxu.github.io/;;https://roihn.github.io/;",
        "dblp": "287/7595-1.html;;38/1691;84/8729;;;;354/3815;",
        "google_scholar": "WbybssYAAAAJ;;l2Kpd2wAAAAJ;m9YSDXMAAAAJ;;;;dqTJFVcAAAAJ;",
        "orcid": "0000-0002-0760-4638;;;;;;;;",
        "linkedin": ";\u5a67-\u4e01-5b8384357/;;;jiahe-ding-886814338;;yuchen-huang-206756274/;;",
        "or_profile": "~Ziqiao_Ma1;~Jing_Ding3;~Xuejun_Zhang4;~Dezhi_Luo1;~Jiahe_Ding1;~Sihan_Xu2;~Yuchen_Huang5;~Run_Peng1;~Joyce_Chai2",
        "aff": "University of Michigan+International Business Machines+Adobe Research;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor+Shanghai Jiaotong University;University of Michigan - Ann Arbor+University College London, University of London;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor+University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;",
        "aff_domain": "umich.edu+ibm.com+adobe.com;umich.edu;umich.edu+sjtu.edu.cn;umich.edu+ucl.ac.uk;umich.edu;umich.edu+umich.edu;umich.edu;umich.edu;",
        "position": "PhD student+Research Intern+Research Intern;Undergrad student;Undergrad student+Undergrad student;Undergrad student+Visiting Student;Undergrad student;PhD student+Undergrad student;Undergrad student;PhD student;",
        "bibtex": "@inproceedings{\nma2025visionlanguage,\ntitle={Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation},\nauthor={Ziqiao Ma and Jing Ding and Xuejun Zhang and Dezhi Luo and Jiahe Ding and Sihan Xu and Yuchen Huang and Run Peng and Joyce Chai},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=oj3ETSitjb}\n}",
        "github": "",
        "project": "",
        "reviewers": "D8WT;drYW;tVi1;GKs2",
        "site": "https://openreview.net/forum?id=oj3ETSitjb",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "4;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8703882797784891
    },
    {
        "id": "p0BwJk3R1p",
        "title": "LLMs as Research Tools: A Large Scale Survey of Researchers\u2019 Usage and Perceptions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers actually use LLMs and why or why not. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into aspects of their research workflow. We also find that some traditionally disadvantaged groups in academia (non-white, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns. Our study provides much-needed evidence, rather than speculation, about how LLMs are currently being used as research tools.",
        "keywords": "survey;large language model;research;societal impact",
        "primary_area": "",
        "supplementary_material": "/attachment/27da1dfac11a3cf6ed57dad4725449633edef625.zip",
        "author": "Zhehui Liao;Maria Antoniak;Inyoung Cheong;Evie Yu-Yen Cheng;Ai-Heng Lee;Kyle Lo;Joseph Chee Chang;Amy X Zhang",
        "authorids": "~Zhehui_Liao1;~Maria_Antoniak1;~Inyoung_Cheong1;~Evie_Yu-Yen_Cheng1;~Ai-Heng_Lee1;~Kyle_Lo1;~Joseph_Chee_Chang1;~Amy_X_Zhang1",
        "gender": "F;;;;Not Specified;;;F",
        "homepage": ";https://maria-antoniak.github.io/;https://inyoungcheong.github.io;;;https://kyleclo.github.io/;;https://homes.cs.washington.edu/~axz/",
        "dblp": ";162/6913;;;;220/2020;;133/8390",
        "google_scholar": ";lNaynLcAAAAJ;xwZI_jcAAAAJ;;;VJS12uMAAAAJ;;thZJZaYAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": "simonaliao/;;https://linkedin.com/my/inyoungcheong;;leeaiheng/;kylelo/;;",
        "or_profile": "~Zhehui_Liao1;~Maria_Antoniak1;~Inyoung_Cheong1;~Evie_Yu-Yen_Cheng1;~Ai-Heng_Lee1;~Kyle_Lo1;~Joseph_Chee_Chang1;~Amy_X_Zhang1",
        "aff": "University of Washington;University of Colorado at Boulder+Copenhagen University;Princeton University;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;Allen Institute for Artificial Intelligence;;Department of Computer Science",
        "aff_domain": "uw.edu;colorado.edu+ku.dk;princeton.edu;allenai.org;allenai.org;allenai.org;;cs.washington.edu",
        "position": "MS student;Assistant Professor+Postdoc;Postdoc;Researcher;Product Marketing Manager;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\nliao2025llms,\ntitle={{LLM}s as Research Tools: A Large Scale Survey of Researchers{\\textquoteright} Usage and Perceptions},\nauthor={Zhehui Liao and Maria Antoniak and Inyoung Cheong and Evie Yu-Yen Cheng and Ai-Heng Lee and Kyle Lo and Joseph Chee Chang and Amy X Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=p0BwJk3R1p}\n}",
        "github": "",
        "project": "",
        "reviewers": "u4YT;5NYA;iVRx",
        "site": "https://openreview.net/forum?id=p0BwJk3R1p",
        "pdf_size": 0,
        "rating": "4;7;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5
    },
    {
        "id": "p4ujQsKmPV",
        "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferences\u2014yet current preference models typically treat human judgment as a black box. We introduce PrefPalette, a framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in a human-interpretable way. PrefPalette operationalizes a cognitive science principle known as multi-attribute decision making in two ways: (1) a scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling\nand transparent, interpretable insights, and serves as a first step toward\nmore trustworthy, value-aware personalized applications.",
        "keywords": "Social Reasoning;Preference Modeling;Explainability",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuyue Stella Li;Melanie Sclar;Hunter Lang;Ansong Ni;Jacqueline He;Puxin Xu;Andrew Cohen;Chan Young Park;Yulia Tsvetkov;Asli Celikyilmaz",
        "authorids": "~Shuyue_Stella_Li1;~Melanie_Sclar1;~Hunter_Lang1;~Ansong_Ni1;~Jacqueline_He1;~Puxin_Xu1;~Andrew_Cohen4;~Chan_Young_Park1;~Yulia_Tsvetkov1;~Asli_Celikyilmaz1",
        "gender": "F;F;M;M;F;M;M;F;F;F",
        "homepage": "http://stellalisy.com/;https://msclar.github.io;http://web.mit.edu/hjl/www/;https://niansong1996.github.io/;http://jacqueline-he.github.io;https://github.com/jacobyxu;;https://chan0park.github.io;https://homes.cs.washington.edu/~yuliats/;https://asli.us",
        "dblp": "312/6501;274/6796;210/2358.html;202/1480;319/3146.html;345/2002;;15/480;75/8157;15/3724",
        "google_scholar": "CRfOlOEAAAAJ;4uNPtZgAAAAJ;;4IA1clAAAAAJ;;;;https://scholar.google.com/citations?hl=en;SEDPkrsAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;;;;0000-0002-4634-7128;",
        "linkedin": ";melanie-sclar-077047b5/;;;;puxin-xu-jacob/;andrew-cohen-17a7aa15b;;;aslicelikyilmaz/",
        "or_profile": "~Shuyue_Stella_Li1;~Melanie_Sclar1;~Hunter_Lang1;~Ansong_Ni1;~Jacqueline_He1;~Puxin_Xu1;~Andrew_Cohen4;~Chan_Young_Park1;~Yulia_Tsvetkov1;~Asli_Celikyilmaz1",
        "aff": "Department of Computer Science, University of Washington;Meta Facebook+University of Washington, Seattle;Massachusetts Institute of Technology;Meta AI;University of Washington;Facebook AI Research;Meta Platforms;Microsoft+Department of Computer Science, University of Washington;Department of Computer Science, University of Washington;FAIR ",
        "aff_domain": "cs.washington.edu;meta.com+uw.edu;mit.edu;meta.com;uw.edu;meta.com;meta.com;microsoft.com+cs.washington.edu;cs.washington.edu;meta.com",
        "position": "PhD student;Researcher+PhD student;PhD student;Researcher;PhD student;Research Data Engineer;Researcher;Postdoc+Postdoc;Associate Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nli2025prefpalette,\ntitle={PrefPalette: Personalized Preference Modeling with Latent Attributes},\nauthor={Shuyue Stella Li and Melanie Sclar and Hunter Lang and Ansong Ni and Jacqueline He and Puxin Xu and Andrew Cohen and Chan Young Park and Yulia Tsvetkov and Asli Celikyilmaz},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=p4ujQsKmPV}\n}",
        "github": "",
        "project": "",
        "reviewers": "3hQX;do5a;bsUc;CeCr",
        "site": "https://openreview.net/forum?id=p4ujQsKmPV",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "p4wZfBFgyI",
        "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, thus providing insights into the reliability of LLM's output. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a formal reasoning topology perspective. By designing a structural elicitation strategy, we can decompose the explanation into the knowledge and reasoning dimensions, which allows us to not only quantify reasoning uncertainty but also assess knowledge redundancy and provide interpretable insights into the model\u2019s reasoning structure. Our method offers a systematic way to interpret the LLM reasoning process, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations, offering a new perspective on evaluating and improving reasoning capabilities.",
        "keywords": "Uncertainty Quantification;LLM Explanations;Graph Mining",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Longchao Da;Xiaoou Liu;Jiaxin Dai;Lu Cheng;Yaqing Wang;Hua Wei",
        "authorids": "~Longchao_Da1;~Xiaoou_Liu1;~Jiaxin_Dai2;~Lu_Cheng2;~Yaqing_Wang1;~Hua_Wei1",
        "gender": "M;F;;F;M;M",
        "homepage": "https://longchaoda.github.io/;https://xiao0o0o.github.io/;;https://lcheng.org/;https://yaqingwang.github.io/;https://labs.engineering.asu.edu/hw/",
        "dblp": "334/1633;;;17/4969-1;147/1393;01/6961-1",
        "google_scholar": "https://scholar.google.com.hk/citations?user=jic73NsAAAAJ;8nDIsz8AAAAJ;;9rpkTSkAAAAJ;_Rfg2CAAAAAJ;F1CEAKwAAAAJ",
        "orcid": "0009-0000-8631-9634;0009-0008-1082-8326;;0000-0002-2503-2522;;0000-0002-3735-1635",
        "linkedin": "longchao-da-b8014624b;ellen-xiaoou-liu-40a71a1b9/;jiaxin-dai/;;;",
        "or_profile": "~Longchao_Da1;~Xiaoou_Liu1;~Jiaxin_Dai2;~Lu_Cheng2;~Yaqing_Wang1;~Hua_Wei1",
        "aff": "Arizona State University;Arizona State University;Arizona State University;University of Illinois at Chicago;Google DeepMind;Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;uic.edu;google.com;asu.edu",
        "position": "PhD student;PhD student;MS student;Assistant Professor;Research Scientist;Assistant Professor",
        "bibtex": "@inproceedings{\nda2025understanding,\ntitle={Understanding the Uncertainty of {LLM} Explanations: A Perspective Based on Reasoning Topology},\nauthor={Longchao Da and Xiaoou Liu and Jiaxin Dai and Lu Cheng and Yaqing Wang and Hua Wei},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=p4wZfBFgyI}\n}",
        "github": "",
        "project": "",
        "reviewers": "GC6U;ymzu;oZN2",
        "site": "https://openreview.net/forum?id=p4wZfBFgyI",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "2;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.0,
            0.816496580927726
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "pQm66IPmeE",
        "title": "Traceable and Explainable Multimodal Large Language Models: An Information-Theoretic View",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing multimodal large language models (MLLMs) often lack traceable and explainable mechanisms for visual-textual alignment, making it challenging to understand how textual instructions shape multimodal representations. To address this shortcoming, we propose an information-theoretic framework that clarifies how MLLMs handle and transform both text and visual inputs. In particular, we measure the visual information gain that arises from textual instructions and multimodal encodings, thereby illuminating how different modalities interact and contribute to the model\u2019s overall processing.\nOur framework decomposes the multimodal encoding process into layer-wise mutual information measures for better explainability, quantifying the visual contribution as the difference between unconditional and text-conditional mutual information. Specifically, inspired by the Information Bottleneck framework, we introduce a Concept Bottleneck that maps high-dimensional multimodal representations into an interpretable space, enabling tractable variational upper bounds on the mutual information between visual inputs and the model\u2019s internal states. Furthermore, we quantify the contextual contribution introduced by textual cues via an InfoNCE mechanism that contrasts multimodal representations computed with and without text guidance. This dual perspective, facilitated by tractable variational upper bounds, provides insight into how visual information is encoded and filtered by textual instructions, while also highlighting the contextual information induced and enhanced by MLLMs. \nEmpirical findings demonstrate underexplored dynamics of visual-textual interaction within MLLMs, \nunderscoring how textual instructions distinctly shape visual representations and demonstrating how visual prompts, \nwhen effectively paired with instructions, enhance multimodal understanding.",
        "keywords": "multimodal LLM;information theory",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zihan Huang;Junda Wu;Rohan Surana;Raghav Jain;Tong Yu;Raghavendra Addanki;David Arbour;Sungchul Kim;Julian McAuley",
        "authorids": "~Zihan_Huang1;~Junda_Wu1;~Rohan_Surana1;~Raghav_Jain1;~Tong_Yu3;~Raghavendra_Addanki1;~David_Arbour1;~Sungchul_Kim1;~Julian_McAuley1",
        "gender": "M;M;M;M;;M;;M;M",
        "homepage": "https://huang-zihan.github.io/;https://scholar.google.com/citations?user=_iKeQFwAAAAJ&hl=en;;;https://www.linkedin.com/in/tong-yu-42790744;https://raddanki.github.io/;http://darbour.github.io;https://sites.google.com/site/subright;http://cseweb.ucsd.edu/~jmcauley/",
        "dblp": ";295/8249;;;32/1593-1;218/5579;87/7578;61/1573;29/3483",
        "google_scholar": ";_iKeQFwAAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com/citations?hl=en;SUPaOhgAAAAJ;prj0heYAAAAJ;v8ISLgIAAAAJ;icbo4M0AAAAJ",
        "orcid": ";;;;0000-0002-5991-2050;;;0000-0003-3580-5290;0000-0003-0955-7588",
        "linkedin": ";;;raghav-jain-3a8076214;tong-yu-42790744;;david-arbour/;;",
        "or_profile": "~Zihan_Huang1;~Junda_Wu1;~Rohan_Surana1;~Raghav_Jain1;~Tong_Yu3;~Raghavendra_Addanki1;~David_Arbour1;~Sungchul_Kim1;~Julian_McAuley1",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;Indian Institute of Technology, Patna.;Adobe Research;Adobe Systems;Adobe Systems;Adobe Systems;University of California, San Diego, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;iitp.ac.in;adobe.com;adobe.com;adobe.com;adobe.com;eng.ucsd.edu",
        "position": "MS student;PhD student;MS student;Researcher;Senior Research Scientist;Research Scientist;Research Scientist;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nhuang2025traceable,\ntitle={Traceable and Explainable Multimodal Large Language Models: An Information-Theoretic View},\nauthor={Zihan Huang and Junda Wu and Rohan Surana and Raghav Jain and Tong Yu and Raghavendra Addanki and David Arbour and Sungchul Kim and Julian McAuley},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=pQm66IPmeE}\n}",
        "github": "",
        "project": "",
        "reviewers": "fJZx;JoXW;WuoZ;j1jo",
        "site": "https://openreview.net/forum?id=pQm66IPmeE",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;3;3;5",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "pm9ykfhknK",
        "title": "CoLa: Learning to Interactively Collaborate with Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for  collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies.",
        "keywords": "Human Simulation;Interactive Learning;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abhishek Sharma;Dan Goldwasser",
        "authorids": "~Abhishek_Sharma7;~Dan_Goldwasser1",
        "gender": "M;M",
        "homepage": "https://www.cs.purdue.edu/homes/sharm271/;https://www.cs.purdue.edu/homes/dgoldwas/",
        "dblp": ";38/3382",
        "google_scholar": ";https://scholar.google.com.tw/citations?user=u8358QgAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Abhishek_Sharma7;~Dan_Goldwasser1",
        "aff": "Purdue University;Purdue University+Purdue University",
        "aff_domain": "cs.purdue.edu;purdue.edu+purdue.edu",
        "position": "PhD student;Assistant Professor+Associate Professor",
        "bibtex": "@inproceedings{\nsharma2025cola,\ntitle={CoLa: Learning to Interactively Collaborate with Large Language Models},\nauthor={Abhishek Sharma and Dan Goldwasser},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=pm9ykfhknK}\n}",
        "github": "",
        "project": "",
        "reviewers": "iRuu;xE3z;zghj",
        "site": "https://openreview.net/forum?id=pm9ykfhknK",
        "pdf_size": 0,
        "rating": "6;7;8",
        "confidence": "5;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.816496580927726
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            26,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5
    },
    {
        "id": "ptmgWRCWmu",
        "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes\u2014inconsistencies in a storyline that break the internal logic or rules of a story\u2019s world\u2014requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMs\u2019 plot hole detection abilities in stories \u2014FlawedFictions\u2014robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with 50%+ and 100%+ increases in plot hole detection rates with respect to human-written originals.",
        "keywords": "narrative understanding;reasoning;synthetic data generation;test time scaling;evaluation;natural language generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kabir Ahuja;Melanie Sclar;Yulia Tsvetkov",
        "authorids": "~Kabir_Ahuja1;~Melanie_Sclar1;~Yulia_Tsvetkov1",
        "gender": "M;F;F",
        "homepage": "https://kabirahuja2431.github.io/;https://msclar.github.io;https://homes.cs.washington.edu/~yuliats/",
        "dblp": "https://dblp.uni-trier.de/pid/265/5632;274/6796;75/8157",
        "google_scholar": "xQ4sUrYAAAAJ;4uNPtZgAAAAJ;SEDPkrsAAAAJ",
        "orcid": ";;0000-0002-4634-7128",
        "linkedin": "kabirahuja2431/;melanie-sclar-077047b5/;",
        "or_profile": "~Kabir_Ahuja1;~Melanie_Sclar1;~Yulia_Tsvetkov1",
        "aff": "Microsoft;Meta Facebook+University of Washington, Seattle;Department of Computer Science, University of Washington",
        "aff_domain": "microsoft.com;meta.com+uw.edu;cs.washington.edu",
        "position": "Research Fellow;Researcher+PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nahuja2025finding,\ntitle={Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection},\nauthor={Kabir Ahuja and Melanie Sclar and Yulia Tsvetkov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ptmgWRCWmu}\n}",
        "github": "",
        "project": "",
        "reviewers": "9VkJ;tF48;KNPv;smF7",
        "site": "https://openreview.net/forum?id=ptmgWRCWmu",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "qG4dL0bart",
        "title": "Benchmarking Retrieval-Augmented Generation for Chemistry",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. \nDespite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. \nIn this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. \nThe accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. \nIn addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. \nUsing ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\u2014achieving an average relative improvement of 17.4\\% over direct inference methods. \nWe further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain.",
        "keywords": "Retrieval-Augmented Generation;RAG;Benchmark;AI for Science;LLM;Large Language Model;Chemistry",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xianrui Zhong;Bowen Jin;Siru Ouyang;Yanzhen Shen;Qiao Jin;Yin Fang;Zhiyong Lu;Jiawei Han",
        "authorids": "~Xianrui_Zhong1;~Bowen_Jin1;~Siru_Ouyang1;~Yanzhen_Shen1;~Qiao_Jin1;~Yin_Fang1;~Zhiyong_Lu1;~Jiawei_Han1",
        "gender": "M;M;F;M;M;F;;M",
        "homepage": "https://xianruizhong.github.io;https://peterjin.me/;https://ozyyshr.github.io;;https://andy-jqa.github.io/;https://github.com/Fangyinfff;;http://hanj.cs.illinois.edu/",
        "dblp": "326/7308;235/8066;https://dblp.org/search/pid/api?q=author:Siru_Ouyang:;350/0200;96/5382-1;231/7716;66/6604;h/JiaweiHan.html",
        "google_scholar": "F8izpj0AAAAJ;https://scholar.google.com/citations?hl=zh-CN;fetoihAAAAAJ;RKBZ8isAAAAJ;tYy-bzgAAAAJ;4rWspjsAAAAJ;;https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ",
        "orcid": "0000-0002-2100-6474;0000-0003-1295-2829;0009-0001-1331-424X;;0000-0002-1268-7239;0000-0001-9538-848X;;0000-0002-3629-2696",
        "linkedin": "xianrui-zhong/;bowen-peter-jin/;;yanzhen-shen-890102228/;qiao-jin-andy/;;;",
        "or_profile": "~Xianrui_Zhong1;~Bowen_Jin1;~Siru_Ouyang1;~Yanzhen_Shen1;~Qiao_Jin1;~Yin_Fang1;~Zhiyong_Lu1;~Jiawei_Han1",
        "aff": "University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;University of Illinois Urbana-Champaign Champaign;Stanford University+University of Illinois Urbana-Champaign;National Institutes of Health;National Institutes of Health;National Institutes of Health;University of Illinois at Urbana-Champaign (UIUC)",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;stanford.edu+cs.illinois.edu;nih.gov;nih.gov;nih.gov;illinois.edu",
        "position": "MS student;PhD student;PhD student;MS student+Undergrad student;Postdoc;Postdoc;Senior Investigator;Full Professor",
        "bibtex": "@inproceedings{\nzhong2025benchmarking,\ntitle={Benchmarking Retrieval-Augmented Generation for Chemistry},\nauthor={Xianrui Zhong and Bowen Jin and Siru Ouyang and Yanzhen Shen and Qiao Jin and Yin Fang and Zhiyong Lu and Jiawei Han},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qG4dL0bart}\n}",
        "github": "",
        "project": "",
        "reviewers": "cNEq;JvHc;vCXR;YkjK",
        "site": "https://openreview.net/forum?id=qG4dL0bart",
        "pdf_size": 0,
        "rating": "4;5;6;7",
        "confidence": "3;4;3;4",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            29,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4472135954999579
    },
    {
        "id": "qMUbhGUFUb",
        "title": "SmolVLM: Redefining small and efficient multimodal models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.\n\nWe introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on both image and video tasks within minimal memory footprints.\n\nOur smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.\n\nOur results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.",
        "keywords": "Vision Language Models;Large Multimodal Models;Vision Understanding;Video Understanding",
        "primary_area": "",
        "supplementary_material": "/attachment/341f5c3957a28cc47659fed5751c69c213885514.zip",
        "author": "Andr\u00e9s Marafioti;Orr Zohar;Miquel Farr\u00e9;Merve noyan;Elie Bakouch;Pedro Manuel Cuenca Jim\u00e9nez;Cyril Zakka;Loubna Ben allal;Anton Lozhkov;Nouamane Tazi;Vaibhav Srivastav;Joshua Lochner;Hugo Larcher;Mathieu Morlon;Lewis Tunstall;Leandro Von Werra;Thomas Wolf",
        "authorids": "~Andr\u00e9s_Marafioti1;~Orr_Zohar1;~Miquel_Farr\u00e91;~Merve_noyan1;~Elie_Bakouch1;~Pedro_Manuel_Cuenca_Jim\u00e9nez1;~Cyril_Zakka1;~Loubna_Ben_allal1;~Anton_Lozhkov1;~Nouamane_Tazi1;~Vaibhav_Srivastav2;~Joshua_Lochner1;~Hugo_Larcher1;~Mathieu_Morlon1;~Lewis_Tunstall1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "gender": ";M;M;;M;M;M;F;;;M;M;M;;M;M;M",
        "homepage": ";https://orrzohar.github.io/;https://huggingface.co;;;;https://cyrilzakka.github.io;https://loubnabnl.github.io/;;;https://vaibhavs10.github.io;;;https://github.com/glutamatt;https://lewtun.github.io/blog/;https://github.com/lvwerra;https://thomwolf.io",
        "dblp": "228/9304;335/1624;;;;;;;;;;;;;;223/1855;",
        "google_scholar": ";Jjw4rL0AAAAJ;0_1BRB0AAAAJ;;;;AnQY3l8AAAAJ;reU1i-sAAAAJ;xlMMVCAAAAAJ;q2bZs1IAAAAJ;;;;;Hc6MI0QAAAAJ;https://scholar.google.com/citations?hl=en;D2H5EFEAAAAJ",
        "orcid": ";;;;;0000-0002-0198-0659;0000-0001-8446-2349;;;;;0000-0002-5823-071X;;;;;",
        "linkedin": ";orr-zohar/;miquelangelfarre/;merve-noyan-28b1a113a/;eliebak/;;;https://www.linkedin.com/mwlite/in/loubna-ben-allal-238690152;anton-lozhkov/;nouamanetazi/;;;hlarcher;;lewis-tunstall/;lvwerra/;",
        "or_profile": "~Andr\u00e9s_Marafioti1;~Orr_Zohar1;~Miquel_Farr\u00e91;~Merve_noyan1;~Elie_Bakouch1;~Pedro_Manuel_Cuenca_Jim\u00e9nez1;~Cyril_Zakka1;~Loubna_Ben_allal1;~Anton_Lozhkov1;~Nouamane_Tazi1;~Vaibhav_Srivastav2;~Joshua_Lochner1;~Hugo_Larcher1;~Mathieu_Morlon1;~Lewis_Tunstall1;~Leandro_Von_Werra1;~Thomas_Wolf1",
        "aff": "Hugging Face;Stanford University;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face;Hugging Face",
        "aff_domain": "huggingface.co;stanford.edu;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;hugggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;huggingface.co;hf.co;huggingface.co",
        "position": "Researcher;PhD student;Researcher;Researcher;Researcher;ML Engineer;Researcher;Researcher;Machine Learning Engineer;Researcher;Researcher;Researcher;Researcher;Researcher;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nmarafioti2025smolvlm,\ntitle={Smol{VLM}: Redefining small and efficient multimodal models},\nauthor={Andr{\\'e}s Marafioti and Orr Zohar and Miquel Farr{\\'e} and Merve noyan and Elie Bakouch and Pedro Manuel Cuenca Jim{\\'e}nez and Cyril Zakka and Loubna Ben allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro Von Werra and Thomas Wolf},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qMUbhGUFUb}\n}",
        "github": "",
        "project": "",
        "reviewers": "RaaY;jgcB;ofar;BnL7",
        "site": "https://openreview.net/forum?id=qMUbhGUFUb",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "3;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            17,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6488856845230502
    },
    {
        "id": "qPsmGjpq1j",
        "title": "Interpreting the linear structure of vision-language model embedding spaces",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Vision-language models encode images and text in a joint space, minimizing the distance between corresponding image and text pairs. How are language and images organized in this joint space, and how do the models encode meaning and modality? To investigate this, we train and release sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, and AIMv2). SAEs approximate model embeddings as sparse linear combinations of learned directions, or ``concepts''. We find that, compared to other methods of linear feature learning, SAEs are better at reconstructing the real embeddings, while also able to retain the most sparsity. Retraining SAEs with different seeds or different data diet leads to two findings: the rare, specific concepts captured by the SAEs are liable to change drastically, but we also show that commonly-activating concepts are remarkably stable across runs. Interestingly, while most concepts activate primarily for one modality, we find they are not merely encoding modality per se. Many are almost orthogonal to the subspace that defines modality, and the concept directions do not function as good modality classifiers, suggesting that they encode cross-modal semantics. To quantify this bridging behavior, we introduce the Bridge Score, a metric that identifies concept pairs which are both co-activated across aligned image-text inputs and geometrically aligned in the shared space. This reveals that even single-modality concepts can collaborate to support cross-modal integration. We release interactive demos of the SAEs for all models, allowing researchers to explore the organization of the concept spaces. Overall, our findings uncover a sparse linear structure within VLM embedding spaces that is shaped by modality, yet stitched together through latent bridges\u2014offering new insight into how multimodal meaning is constructed.",
        "keywords": "vision-language models;interpretability;cross-modality meaning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Isabel Papadimitriou;Huangyuan Su;Thomas Fel;Sham M. Kakade;Stephanie Gil",
        "authorids": "~Isabel_Papadimitriou1;~Huangyuan_Su1;~Thomas_Fel2;~Sham_M._Kakade1;~Stephanie_Gil2",
        "gender": "F;;;M;F",
        "homepage": "https://www.isabelpapad.com/;;;https://shamulent.github.io;",
        "dblp": "264/0034;;;s/SMKakade;",
        "google_scholar": ";;;https://scholar.google.com.tw/citations?user=wb-DKCIAAAAJ;",
        "orcid": "0000-0003-0214-0659;;;;0000-0002-4951-5350",
        "linkedin": ";;;;",
        "or_profile": "~Isabel_Papadimitriou1;~Huangyuan_Su1;~Thomas_Fel2;~Sham_M._Kakade1;~Stephanie_Gil2",
        "aff": "Harvard University;;;Harvard University;Harvard University",
        "aff_domain": "g.harvard.edu;;;harvard.edu;harvard.edu",
        "position": "Postdoc;;;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\npapadimitriou2025interpreting,\ntitle={Interpreting the linear structure of vision-language model embedding spaces},\nauthor={Isabel Papadimitriou and Huangyuan Su and Thomas Fel and Sham M. Kakade and Stephanie Gil},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qPsmGjpq1j}\n}",
        "github": "",
        "project": "",
        "reviewers": "Hk33;VzYu;WVqz;6WMN",
        "site": "https://openreview.net/forum?id=qPsmGjpq1j",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "qQb1JLrwol",
        "title": "Hidden in plain sight: VLMs overlook their visual representations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Language provides a natural interface to specify and evaluate performance on visual tasks. \nTo realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information.\nOur work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the \\textit{entire} model, and they inherit their language biases. \n   Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.",
        "keywords": "vision;language;representation;benchmark;encoder;vlm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Stephanie Fu;tyler bonnen;Devin Guillory;Trevor Darrell",
        "authorids": "~Stephanie_Fu1;~tyler_bonnen1;~Devin_Guillory1;~Trevor_Darrell2",
        "gender": "F;M;M;",
        "homepage": "https://stephanie-fu.github.io/;https://tzler.github.io/;https://www.devinguillory.com/;",
        "dblp": "270/1541;;188/1061;",
        "google_scholar": "Rx-h05AAAAAJ;https://scholar.google.co.uk/citations?user=6ZkcZUAAAAAJ;t4dSV4YAAAAJ;",
        "orcid": "0000-0001-6591-6026;0000-0001-8709-1651;;",
        "linkedin": "stephanie-fu/;;devin-guillory-78528958/;",
        "or_profile": "~Stephanie_Fu1;~tyler_bonnen1;~Devin_Guillory1;~Trevor_Darrell2",
        "aff": "University of California, Berkeley;Electrical Engineering & Computer Science Department, University of California, Berkeley;University of California, Berkeley;",
        "aff_domain": "berkeley.edu;eecs.berkeley.edu;berkeley.edu;",
        "position": "PhD student;Postdoc;PhD student;",
        "bibtex": "@inproceedings{\nfu2025hidden,\ntitle={Hidden in plain sight: {VLM}s overlook their visual representations},\nauthor={Stephanie Fu and tyler bonnen and Devin Guillory and Trevor Darrell},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qQb1JLrwol}\n}",
        "github": "",
        "project": "",
        "reviewers": "Nkzv;zfkz;6sUQ;34qn",
        "site": "https://openreview.net/forum?id=qQb1JLrwol",
        "pdf_size": 0,
        "rating": "5;7;7;8",
        "confidence": "4;4;2;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.48420012470625223
    },
    {
        "id": "qSFr5wJPGc",
        "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies\u2014dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.",
        "keywords": "Quality Estimation;Machine Translation;Translation Quality",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Archchana Sindhujan;Shenbin Qian;Chan Chi Chun Matthew;Constantin Orasan;Diptesh Kanojia",
        "authorids": "~Archchana_Sindhujan1;~Shenbin_Qian1;~Chan_Chi_Chun_Matthew1;~Constantin_Orasan1;~Diptesh_Kanojia1",
        "gender": "F;M;M;;M",
        "homepage": ";;;https://dinel.org.uk;http://dipteshkanojia.github.io",
        "dblp": "306/9400.html;321/0536;;83/316;127/0183",
        "google_scholar": "BlObQHUAAAAJ;mJFaiMwAAAAJ;;BIFDGBkAAAAJ;https://scholar.google.co.in/citations?user=UNCgCAEAAAAJ",
        "orcid": "0000-0002-6467-6873;;;;0000-0001-8814-0080",
        "linkedin": ";shenbin-qian-063b95229/;matthew-chan-72ba96180;;dipteshkanojia/",
        "or_profile": "~Archchana_Sindhujan1;~Shenbin_Qian1;~Chan_Chi_Chun_Matthew1;~Constantin_Orasan1;~Diptesh_Kanojia1",
        "aff": "University of Surrey;University of Surrey;Centre for Perceptual and Interactive Intelligence;University of Surrey;University of Surrey",
        "aff_domain": "surrey.ac.uk;surrey.ac.uk;cpii.hk;surrey.ac.uk;surrey.ac.uk",
        "position": "PhD student;PhD student;AI Programmer;Full Professor;Senior Lecturer",
        "bibtex": "@inproceedings{\nsindhujan2025alope,\ntitle={{ALOPE}: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models},\nauthor={Archchana Sindhujan and Shenbin Qian and Chan Chi Chun Matthew and Constantin Orasan and Diptesh Kanojia},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qSFr5wJPGc}\n}",
        "github": "",
        "project": "",
        "reviewers": "6xjS;ffUc;m5bM",
        "site": "https://openreview.net/forum?id=qSFr5wJPGc",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "qbWpEufkqk",
        "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM: Reasoning over Embodied Multi-Frame Trajectories, a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
        "keywords": "Multimodal Reasoning;Embodied AI;VLMs;Spatial Reasoning;Object Permanence;Visuospatial Representation;Large Language Models (LLMs);Egocentric Vision;Video Understanding;Evaluation Benchmarks;Synthetic Environments;Long-horizon Reasoning;Numerical Tracking;Temporal Ordering;Scene Understanding;LLM Limitations",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jacob Thompson;Emiliano Garcia-Lopez;Yonatan Bisk",
        "authorids": "~Jacob_Thompson1;~Emiliano_Garcia-Lopez1;~Yonatan_Bisk1",
        "gender": "M;M;M",
        "homepage": ";;http://www.YonatanBisk.com",
        "dblp": ";;38/9282",
        "google_scholar": ";;bWoGh8UAAAAJ",
        "orcid": ";;0000-0002-2111-9081",
        "linkedin": "jacob-thompson-557080194/;emiliano-gl;yonatanbisk/",
        "or_profile": "~Jacob_Thompson1;~Emiliano_Garcia-Lopez1;~Yonatan_Bisk1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;cmu.edu",
        "position": "MS student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nthompson2025rem,\ntitle={{REM}: Evaluating {LLM} Embodied Spatial Reasoning through Multi-Frame Trajectories},\nauthor={Jacob Thompson and Emiliano Garcia-Lopez and Yonatan Bisk},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qbWpEufkqk}\n}",
        "github": "",
        "project": "",
        "reviewers": "gAU9;eEuK;2bj9;iEfL",
        "site": "https://openreview.net/forum?id=qbWpEufkqk",
        "pdf_size": 0,
        "rating": "5;5;6;7",
        "confidence": "4;4;3;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9045340337332909
    },
    {
        "id": "qiLJVU4I8P",
        "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features.",
        "keywords": "vision language models;font recognition;texture or semantics;stroop effect",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhecheng Li;Guoxian Song;Yujun Cai;Zhen Xiong;Junsong Yuan;Yiwei Wang",
        "authorids": "~Zhecheng_Li1;~Guoxian_Song1;~Yujun_Cai1;~Zhen_Xiong2;~Junsong_Yuan2;~Yiwei_Wang2",
        "gender": "M;M;F;M;M;M",
        "homepage": ";https://guoxiansong.github.io/homepage/index.html;;;https://cse.buffalo.edu/~jsyuan/;",
        "dblp": ";189/7103;227/4399;26/9887;42/3332;50/5889-1",
        "google_scholar": ";https://scholar.google.com.sg/citations?user=EMyFIYgAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;fJ7seq0AAAAJ;https://scholar.google.com.hk/citations?user=Sh9QvBkAAAAJ",
        "orcid": ";0000-0002-3664-572X;;;0000-0002-7901-8793;",
        "linkedin": "zhechengli2002/;guoxian-song-101558117/;;xiong-zhen;;",
        "or_profile": "~Zhecheng_Li1;~Guoxian_Song1;~Yujun_Cai1;~Zhen_Xiong2;~Junsong_Yuan2;~Yiwei_Wang2",
        "aff": "University of California, San Diego;Bytedance Inc;The University of Queensland;University of Southern California;State University of New York at Buffalo;University of California, Merced",
        "aff_domain": "ucsd.edu;bytedance.com;uq.edu.au;usc.edu;buffalo.edu;ucmerced.edu",
        "position": "MS student;Researcher;Assistant Professor;MS student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025texture,\ntitle={Texture or Semantics? Vision-Language Models Get Lost in Font Recognition},\nauthor={Zhecheng Li and Guoxian Song and Yujun Cai and Zhen Xiong and Junsong Yuan and Yiwei Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=qiLJVU4I8P}\n}",
        "github": "",
        "project": "",
        "reviewers": "P7Qz;A1JS;FEXv;A9ZJ",
        "site": "https://openreview.net/forum?id=qiLJVU4I8P",
        "pdf_size": 0,
        "rating": "5;5;6;8",
        "confidence": "3;4;3;5",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.224744871391589
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7385489458759963
    },
    {
        "id": "r0AXK5Cnhr",
        "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 15 LLMs on LV-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that:\n(i) Moonshot-v1 and recent large-scale open-source models, such as Qwen-2.5-72B and Llama-3.1-70B, achieve the highest performance on LV-Eval, particularly at lengths below $64k$. (ii) Models exhibit distinct score trends. For example, GLM-4-9B-128k, Yi-6B-200k, and Llama3-8B-1M exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of \"needle in a haystack\". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval.",
        "keywords": "large language model;long-context benchmark;knowledge leakage mitigation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tao Yuan;Xuefei Ning;Dong Zhou;Zhijie Yang;Shiyao Li;Minghui Zhuang;Zheyue Tan;Zhuyu Yao;Dahua Lin;Boxun Li;Guohao Dai;Shengen Yan;Yu Wang",
        "authorids": "~Tao_Yuan7;~Xuefei_Ning1;~Dong_Zhou8;~Zhijie_Yang3;~Shiyao_Li2;~Minghui_Zhuang1;~Zheyue_Tan1;~Zhuyu_Yao1;~Dahua_Lin1;~Boxun_Li2;~Guohao_Dai4;~Shengen_Yan1;~Yu_Wang3",
        "gender": ";Not Specified;;;M;M;M;M;M;M;M;M;M",
        "homepage": ";https://nics-effalg.com/ningxuefei/;https://email.whu.edu.cn;https://github.com/followall;http://nicsefc.ee.tsinghua.edu.cn/people/ShiyaoLi;https://dblp.org/pid/218/6609.html;https://github.com/tanzeyy;https://scholar.google.com/citations?hl=zh-CN&user=O8_HpXcAAAAJ;http://dahua.site;;https://nicsefc.ee.tsinghua.edu.cn/people/guohao-dai/;;https://nicsefc.ee.tsinghua.edu.cn",
        "dblp": ";202/9525;;;;218/6609.html;;;53/6088;135/8082.html;147/1470;117/6968;w/YuWang2.html",
        "google_scholar": ";oVslpJsAAAAJ;;;JWaexW0AAAAJ;gRMUhbsAAAAJ;;;GMzzRRUAAAAJ;;gz3Tkl0AAAAJ;SvE3bdUAAAAJ;https://scholar.google.com.hk/citations?user=j8JGVvoAAAAJ",
        "orcid": ";;;;;;;;;;;;0000-0001-6108-5157",
        "linkedin": ";;;;;;;;;;;;",
        "or_profile": "~Tao_Yuan7;~Xuefei_Ning1;~Dong_Zhou8;~Zhijie_Yang3;~Shiyao_Li2;~Minghui_Zhuang1;~Zheyue_Tan1;~Zhuyu_Yao1;~Dahua_Lin1;~Boxun_Li2;~Guohao_Dai4;~Shengen_Yan1;~Yu_Wang3",
        "aff": ";Tsinghua University;infini-ai;Zhejiang University;Infinigence+Tsinghua University;Peking University;Infinigence;Infinigence-AI;The Chinese University of Hong Kong;Infinigence-AI;Shanghai Jiaotong University;Tsinghua University;Tsinghua University",
        "aff_domain": ";tsinghua.edu.cn;infini-ai.com;zju.edu.cn;infini-ai.com+tsinghua.edu.cn;pku.edu.cn;infini-ai.com;infini-ai.com;cuhk.edu.hk;infini-ai.com;sjtu.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": ";Research Assistant Professor;Researcher;PhD student;Intern+PhD student;MS student;Researcher;Researcher;Associate Professor;Principal Researcher;Associate Professor;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nyuan2025lveval,\ntitle={{LV}-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K},\nauthor={Tao Yuan and Xuefei Ning and Dong Zhou and Zhijie Yang and Shiyao Li and Minghui Zhuang and Zheyue Tan and Zhuyu Yao and Dahua Lin and Boxun Li and Guohao Dai and Shengen Yan and Yu Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=r0AXK5Cnhr}\n}",
        "github": "",
        "project": "",
        "reviewers": "3FgU;67DQ;iK3F;P4e8",
        "site": "https://openreview.net/forum?id=r0AXK5Cnhr",
        "pdf_size": 0,
        "rating": "4;5;7;9",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.920286436967152
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.676481425202546
    },
    {
        "id": "r61s1FNYlj",
        "title": "TRELLIS: Learning to Compress Key-Value Memory in Attention Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis,  a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory.  To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length increases, highlighting its potential for long-context applications.",
        "keywords": "Sequence Models;Language models;Recurrent Neural Nets;Test Time Training",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mahdi Karami;Ali Behrouz;Praneeth Kacham;Vahab Mirrokni",
        "authorids": "~Mahdi_Karami2;~Ali_Behrouz1;~Praneeth_Kacham1;~Vahab_Mirrokni2",
        "gender": "M;M;M;M",
        "homepage": "https://karami-m.github.io/;https://Abehrouz.github.io;https://www.praneethkacham.com;https://people.csail.mit.edu/mirrokni/Welcome.html",
        "dblp": "90/394.html;220/4163;255/5684;m/VahabSMirrokni",
        "google_scholar": "https://scholar.google.com/citations?hl=en;UbwVuqIAAAAJ;hKhPmTkAAAAJ;opbZfw0AAAAJ",
        "orcid": ";;;",
        "linkedin": "mahdi-karami-2957412a/;ali-behrouz-506aa2127;;",
        "or_profile": "~Mahdi_Karami2;~Ali_Behrouz1;~Praneeth_Kacham1;~Vahab_Mirrokni2",
        "aff": "Research, Google+University of Waterloo;Google+Cornell University;Google;Google Research",
        "aff_domain": "research.google.com+cs.uwaterloo.ca;google.com+cornell.edu;google.com;google.com",
        "position": "Researcher+Researcher;Intern+PhD student;Researcher;VP, Google Fellow",
        "bibtex": "@inproceedings{\nkarami2025trellis,\ntitle={{TRELLIS}: Learning to Compress Key-Value Memory in Attention Models},\nauthor={Mahdi Karami and Ali Behrouz and Praneeth Kacham and Vahab Mirrokni},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=r61s1FNYlj}\n}",
        "github": "",
        "project": "",
        "reviewers": "M9Vs;RaMN;1Hgq",
        "site": "https://openreview.net/forum?id=r61s1FNYlj",
        "pdf_size": 0,
        "rating": "6;6;10",
        "confidence": "3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            1.8856180831641267
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5
    },
    {
        "id": "r8nloXtluk",
        "title": "ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their ability to support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], which is then used to query a citation database. The retrieved references are fed into the model to augment the generation process.\nWe jointly optimize both the generation and citation tasks within a single framework to improve efficiency. Our model is built upon Qwen-2.5-7B and trained on 500K papers from arXiv. It achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%).\nOn a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality--measured across relevance, coherence, academic rigor, completeness, and innovation--significantly surpassing all existing models, including much larger ones like the Retrieval-Augmented Qwen2.5-72B-Instruct. Human studies further demonstrate that ScholarCopilot, despite being a 7B model, significantly outperforms ChatGPT, achieving 100% preference in citation quality and over 70% in overall usefulness.",
        "keywords": "RAG",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yubo Wang;Xueguang Ma;Ping Nie;Huaye Zeng;Zhiheng Lyu;Yuxuan Zhang;Benjamin Schneider;Yi Lu;Xiang Yue;Wenhu Chen",
        "authorids": "~Yubo_Wang9;~Xueguang_Ma1;~Ping_Nie1;~Huaye_Zeng2;~Zhiheng_Lyu2;~Yuxuan_Zhang12;~Benjamin_Schneider1;~Yi_Lu9;~Xiang_Yue1;~Wenhu_Chen3",
        "gender": "M;M;;M;;;M;M;;",
        "homepage": ";;;;;;https://github.com/Ben-Schneider-code;https://eigentom.github.io/;;",
        "dblp": ";44/9030;;377/3893;;;55/9497;;;",
        "google_scholar": "s_ZW7voAAAAJ;4kvcmkQAAAAJ;;;;;;MrOZk28AAAAJ;;",
        "orcid": ";;;;;;;0009-0003-5864-1142;;",
        "linkedin": ";;;wyettzeng/;;;;yi-lu-tom/;;",
        "or_profile": "~Yubo_Wang9;~Xueguang_Ma1;~Ping_Nie1;~Huaye_Zeng2;~Zhiheng_Lyu2;~Yuxuan_Zhang12;~Benjamin_Schneider1;~Yi_Lu9;~Xiang_Yue1;~Wenhu_Chen3",
        "aff": "University of Waterloo;University of Waterloo;;Harvard University+University of Waterloo;;;University of Waterloo;Opus AI Research+University of Toronto;;",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;;g.harvard.edu+uwaterloo.ca;;;uwaterloo.ca;opus.pro+utoronto.ca;;",
        "position": "PhD student;PhD student;;MS student+Undergrad student;;;MS student;Intern+MS student;;",
        "bibtex": "@inproceedings{\nwang2025scholarcopilot,\ntitle={ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations},\nauthor={Yubo Wang and Xueguang Ma and Ping Nie and Huaye Zeng and Zhiheng Lyu and Yuxuan Zhang and Benjamin Schneider and Yi Lu and Xiang Yue and Wenhu Chen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=r8nloXtluk}\n}",
        "github": "",
        "project": "",
        "reviewers": "UtQh;T1dK;mo4f;caCT",
        "site": "https://openreview.net/forum?id=r8nloXtluk",
        "pdf_size": 0,
        "rating": "3;6;6;8",
        "confidence": "4;3;5;5",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.7853571071357126
        ],
        "confidence_avg": [
            4.25,
            0.82915619758885
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.37998029782867415
    },
    {
        "id": "rAR7iPI8Kh",
        "title": "When Splitting Makes Stronger: A Theoretical and Empirical Analysis of Divide-and-Conquer Prompting in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Foundation models, particularly Large Language Models (LLMs), have garnered significant interest due to their wide range of applications.  Yet these models demonstrate notable weaknesses when confronted with tasks involving iterative sub-problems or deliberately misleading content\u2014exemplified by complex arithmetic operations and comprehensive fake news evaluation. \nConventional instructional prompting frequently produces flawed outputs in these scenarios. While research has established that advanced techniques such as Chain-of-Thoughts and Least-to-Most methodologies can dramatically enhance LLM performance, emerging investigation indicates that a more streamlined divide-and-conquer (DaC) approach\u2014which systematically partitions input sequences into discrete components\u2014can yield remarkable improvements for particular problem classes like misinformation assessment. Our investigation rigorously examines the efficacy of DaC prompting strategies and precisely delineates the task characteristics that benefit most from this methodology. Through comprehensive theoretical analysis, we establish formal guarantees for performance enhancement in specifically identified task categories. We validate our theoretical framework through focused empirical studies on large integer multiplication and factual verification tasks, where experimental outcomes robustly confirm our analytical predictions, demonstrating DaC's practical superiority in these challenging domains.",
        "keywords": "Program-guided Prompt;Divide-and-Conquer;Foundation Model;Large Language Models;Deceptive content",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yizhou Zhang;Defu Cao;Lun Du;Qiang Fu;Yan Liu",
        "authorids": "~Yizhou_Zhang3;~Defu_Cao1;~Lun_Du1;~Qiang_Fu7;~Yan_Liu1",
        "gender": "M;M;M;M;F",
        "homepage": "https://yizhouzhang1997.netlify.app/;https://idevede.github.io/;https://www.microsoft.com/en-us/research/people/ludu/;;http://www-bcf.usc.edu/~liu32/",
        "dblp": ";274/1535;213/3199;;150/4295",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;3XUANDAAAAAJ;bwTLZSIAAAAJ;UUKLPMYAAAAJ",
        "orcid": ";0000-0003-0240-3818;;0000-0002-5821-7267;0000-0002-7055-9518",
        "linkedin": ";;;qiang-fu-08301285/;",
        "or_profile": "~Yizhou_Zhang3;~Defu_Cao1;~Lun_Du1;~Qiang_Fu7;~Yan_Liu1",
        "aff": "Amazon;University of Southern California;Ant Research;Microsoft;University of Southern California",
        "aff_domain": "amazon.com;usc.edu;antgroup.com;microsoft.com;usc.edu",
        "position": "Applied Scientist;PhD student;Researcher;Researcher;Professor",
        "bibtex": "@inproceedings{\nzhang2025when,\ntitle={When Splitting Makes Stronger: A Theoretical and Empirical Analysis of Divide-and-Conquer Prompting in {LLM}s},\nauthor={Yizhou Zhang and Defu Cao and Lun Du and Qiang Fu and Yan Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=rAR7iPI8Kh}\n}",
        "github": "",
        "project": "",
        "reviewers": "kq9V;N6F9;uHuS;1hW4",
        "site": "https://openreview.net/forum?id=rAR7iPI8Kh",
        "pdf_size": 0,
        "rating": "7;7;8;8",
        "confidence": "4;2;2;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.5
        ],
        "confidence_avg": [
            3.0,
            1.0
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "rGNAyHReSg",
        "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.",
        "keywords": "In-Context Learning;Mechanistic Interpretability",
        "primary_area": "",
        "supplementary_material": "/attachment/264eaf273f446b8b60871c95b423bf71648a3819.zip",
        "author": "Aleksandra Bakalova;Yana Veitsman;Xinting Huang;Michael Hahn",
        "authorids": "~Aleksandra_Bakalova2;~Yana_Veitsman1;~Xinting_Huang2;~Michael_Hahn1",
        "gender": "F;F;M;M",
        "homepage": ";https://yveitsman.xyz;;https://www.mhahn.info/",
        "dblp": ";378/1390;240/7147;https://dblp.uni-trier.de/pid/44/9903",
        "google_scholar": ";3j3Myg0AAAAJ;BpCALOYAAAAJ;",
        "orcid": ";;;",
        "linkedin": "aleksandra-bakalova;yveitsman/;;",
        "or_profile": "~Aleksandra_Bakalova2;~Yana_Veitsman1;~Xinting_Huang2;~Michael_Hahn1",
        "aff": "Universit\u00e4t des Saarlandes;Universit\u00e4t des Saarlandes;Universit\u00e4t des Saarlandes;Universit\u00e4t des Saarlandes",
        "aff_domain": "uni-saarland.de;uni-saarland.de;uni-saarland.de;uni-saarland.de",
        "position": "MS student;MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nbakalova2025contextualizethenaggregate,\ntitle={Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B},\nauthor={Aleksandra Bakalova and Yana Veitsman and Xinting Huang and Michael Hahn},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=rGNAyHReSg}\n}",
        "github": "",
        "project": "",
        "reviewers": "cuaP;mZBA;JCLW;SjLE",
        "site": "https://openreview.net/forum?id=rGNAyHReSg",
        "pdf_size": 0,
        "rating": "5;7;7;9",
        "confidence": "2;4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4264014327112209
    },
    {
        "id": "rJOkPauru9",
        "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Although large language models (LLMs) have become more capable and accurate across many tasks, some fundamental sources of unreliability remain in their behavior. One key limitation is their inconsistency at reporting the same information when prompts are changed. In this paper, we consider the discrepancy between a model\u2019s generated answer and their own verification of that answer, the generator-validator gap. We define this gap in a more stringent way than prior work: we expect correlation of scores from a generator and a validator over the entire set of candidate answers, i.e., candidate completions that could possibly arise during ordinary language use without breaking Gricean norms. We show that according to this measure, a large gap exists in various settings, including question answering, lexical semantics tasks, and next-word prediction. We then propose RankAlign, a ranking-based training method, and show that it significantly closes the gap, surpassing all baseline methods. Moreover, this approach generalizes well to out-of-domain tasks and lexical items.",
        "keywords": "consistency;robustness;ranking loss;generalizability;generator-validator gap",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Juan Diego Rodriguez;Wenxuan Ding;Katrin Erk;Greg Durrett",
        "authorids": "~Juan_Diego_Rodriguez1;~Wenxuan_Ding1;~Katrin_Erk1;~Greg_Durrett1",
        "gender": ";F;F;M",
        "homepage": ";https://wenwen-d.github.io/;http://www.katrinerk.com/;http://www.cs.utexas.edu/~gdurrett/",
        "dblp": ";36/1339-1;23/556;69/7968",
        "google_scholar": ";GyHBjwQAAAAJ;v7kFHRoAAAAJ;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ",
        "orcid": ";;;",
        "linkedin": ";wenxuan-ding-0b299923b/;;",
        "or_profile": "~Juan_Diego_Rodriguez1;~Wenxuan_Ding1;~Katrin_Erk1;~Greg_Durrett1",
        "aff": ";University of Texas at Austin;University of Texas, Austin;University of Texas at Austin",
        "aff_domain": ";utexas.edu;utexas.edu;utexas.edu",
        "position": ";PhD student;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nrodriguez2025rankalign,\ntitle={RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models},\nauthor={Juan Diego Rodriguez and Wenxuan Ding and Katrin Erk and Greg Durrett},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=rJOkPauru9}\n}",
        "github": "",
        "project": "",
        "reviewers": "p48J;Zso5;R1Jh;JEXo",
        "site": "https://openreview.net/forum?id=rJOkPauru9",
        "pdf_size": 0,
        "rating": "3;6;6;7",
        "confidence": "3;2;3;3",
        "wc_review": "",
        "rating_avg": [
            5.5,
            1.5
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.19245008972987526
    },
    {
        "id": "rgq9BFXSFl",
        "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models.\nWe introduce *hyperschedules*, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (*e.g.*, GPT) and conventional diffusion models (*e.g.*, SEDD, MDLM) as special cases. \nSecond, we propose two \\emph{hybrid token-wise noising processes} that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a *novel inference algorithm* that leverages this new feature in a simplified context inspired from MDLM.\nTo support efficient training and inference, we design attention masks compatible with KV-caching.\nOur methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.\nSee code and resources at https://hdlm-colm.github.io/ .",
        "keywords": "discrete diffusion;generative diffusion models;language models;autoregressive language models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nima Fathi;Torsten Scholak;Pierre-Andre Noel",
        "authorids": "~Nima_Fathi1;~Torsten_Scholak1;~Pierre-Andre_Noel1",
        "gender": "M;M;M",
        "homepage": "http://cim.mcgill.ca/~nimafh/;https://tscholak.github.com;",
        "dblp": "322/1220;277/0957;47/9226.html",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=BgkjtKgAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0008-2062-4415;;0000-0001-6979-1873",
        "linkedin": "nimafathi/;tscholak;panoel/",
        "or_profile": "~Nima_Fathi1;~Torsten_Scholak1;~Pierre-Andre_Noel1",
        "aff": "ServiceNow Inc+McGill University+Mila - Quebec Artificial Intelligence Institute;ServiceNow Research;ServiceNow",
        "aff_domain": "servicenow.com+mcgill.ca+mila.quebec;servicenow.com;servicenow.com",
        "position": "Intern+MS student+Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nfathi2025unifying,\ntitle={Unifying Autoregressive and Diffusion-Based Sequence Generation},\nauthor={Nima Fathi and Torsten Scholak and Pierre-Andre Noel},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=rgq9BFXSFl}\n}",
        "github": "",
        "project": "",
        "reviewers": "WiDd;AaDn;XPkA;NmzT",
        "site": "https://openreview.net/forum?id=rgq9BFXSFl",
        "pdf_size": 0,
        "rating": "4;7;7;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.299038105676658
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "ruWC5LIMSo",
        "title": "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens.\nWe introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. \nThese tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). \nFurthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. \nWe evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. \nNotably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks.\nReasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training.\nFurther analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations.\nThese findings highlight critical limitations in current LCLMs and suggest substantial room for improvement.",
        "keywords": "Keywords: Large language models;long-context;natural language processing",
        "primary_area": "",
        "supplementary_material": "/attachment/38208434b51c7f317e26844b704854a0274f2798.zip",
        "author": "Xi Ye;Fangcong Yin;Yinghui He;Joie Zhang;Howard Yen;Tianyu Gao;Greg Durrett;Danqi Chen",
        "authorids": "~Xi_Ye2;~Fangcong_Yin1;~Yinghui_He1;~Joie_Zhang1;~Howard_Yen1;~Tianyu_Gao1;~Greg_Durrett1;~Danqi_Chen1",
        "gender": ";M;F;F;M;M;M;F",
        "homepage": "https://xiye17.github.io/;https://fangcong-yin-2.github.io/;https://ying-hui-he.github.io/;;https://howard-yen.github.io;https://gaotianyu.xyz/about/;http://www.cs.utexas.edu/~gdurrett/;https://www.cs.princeton.edu/~danqic/",
        "dblp": ";362/8553;;;348/5988.html;207/8893-1.html;69/7968;87/7949",
        "google_scholar": "qH83GlAAAAAJ;u_-1TRIAAAAJ;https://scholar.google.com/citations?hl=en;;8rJOrBEAAAAJ;il-F8YYAAAAJ;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ;sVR8ktkAAAAJ",
        "orcid": ";;;;;0000-0002-5178-0866;;",
        "linkedin": ";;yinghui-he-8b147321a/;joie-zhang/;;;;",
        "or_profile": "~Xi_Ye2;~Fangcong_Yin1;~Yinghui_He1;~Joie_Zhang1;~Howard_Yen1;~Tianyu_Gao1;~Greg_Durrett1;~Danqi_Chen1",
        "aff": "Princeton University;University of Texas at Austin;Princeton University;Princeton University;Princeton University;Princeton University;University of Texas at Austin;Princeton University",
        "aff_domain": "princeton.edu;utexas.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu;utexas.edu;cs.princeton.edu",
        "position": "Postdoc;PhD student;PhD student;Undergrad student;PhD student;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nye2025longproc,\ntitle={LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation},\nauthor={Xi Ye and Fangcong Yin and Yinghui He and Joie Zhang and Howard Yen and Tianyu Gao and Greg Durrett and Danqi Chen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ruWC5LIMSo}\n}",
        "github": "",
        "project": "",
        "reviewers": "kGCV;yAzi;eTor;5YMC",
        "site": "https://openreview.net/forum?id=ruWC5LIMSo",
        "pdf_size": 0,
        "rating": "7;7;8;8",
        "confidence": "5;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.5
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "rujwIvjooA",
        "title": "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Domain reweighting is an emerging research area aimed at adjusting the relative weights of different data sources to improve the effectiveness and efficiency of LLM pre-training. We show that data mixtures that perform well at smaller scales may not retain their advantage at larger scales, challenging the existing practice of determining competitive mixtures in small-scale experiments and *directly* applying them at much larger scales. To address this, we propose AutoScale, a two-stage, scale-aware data composition framework. First, AutoScale fits a parametric model that predicts the model\u2019s loss under different data compositions, then uses it to find an approximate best allocation at smaller, more manageable budgets. Next, leveraging a novel theoretical analysis of how optimal compositions evolve with scale, AutoScale extrapolates that composition to larger budgets without further retraining. Empirically, AutoScale accelerates convergence and improves downstream performance.\nFor instance, when pre-training GPT-2 Large, it achieves a 28\\% faster perplexity reduction than baselines and up to a 38\\% speed-up over unweighted training, while yielding best-average results on various downstream tasks. Overall, our findings illustrate how domain importance shifts with training scale, underscoring the need for scale-dependent data curation in LLM training. \nOur code is open-sourced.",
        "keywords": "Large Language Models (LLM);Data Curation;Domain Reweighting;Scaling Laws;Data-centric AI",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Feiyang Kang;Yifan Sun;Bingbing Wen;Si Chen;Dawn Song;Rafid Mahmood;Ruoxi Jia",
        "authorids": "~Feiyang_Kang1;~Yifan_Sun8;~Bingbing_Wen1;~Si_Chen5;~Dawn_Song1;~Rafid_Mahmood1;~Ruoxi_Jia1",
        "gender": "M;M;F;;F;;",
        "homepage": ";https://yifansun99.github.io/;https://bbwen.github.io/;;;http://rafidrm.github.io;https://ruoxijia.info/",
        "dblp": "218/1175;99/10261-10.html;318/9589;;s/DXSong;164/5832;147/5355-1",
        "google_scholar": "_6mV_iEAAAAJ;;Jt0E6FEAAAAJ;;;https://scholar.google.ca/citations?user=NoPweUQAAAAJ;JCrug-YAAAAJ",
        "orcid": ";;0000-0001-7980-9555;;;;",
        "linkedin": ";;bingbing-wen-18593570/;;;;",
        "or_profile": "~Feiyang_Kang1;~Yifan_Sun8;~Bingbing_Wen1;~Si_Chen5;~Dawn_Song1;~Rafid_Mahmood1;~Ruoxi_Jia1",
        "aff": "Meta FAIR+Virginia Tech;University of Illinois, Urbana Champaign;University of Washington;;University of California, Berkeley;University of Ottawa+NVIDIA;Virginia Tech",
        "aff_domain": "meta.com+vt.edu;illinois.edu;uw.edu;;berkeley.edu;uottawa.ca+nvidia.com;vt.edu",
        "position": "Intern+PhD student;PhD student;PhD student;;Full Professor;Assistant Professor+Research Scientist;Assistant Professor",
        "bibtex": "@inproceedings{\nkang2025autoscale,\ntitle={AutoScale: Scale-Aware Data Mixing for Pre-Training {LLM}s},\nauthor={Feiyang Kang and Yifan Sun and Bingbing Wen and Si Chen and Dawn Song and Rafid Mahmood and Ruoxi Jia},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=rujwIvjooA}\n}",
        "github": "",
        "project": "",
        "reviewers": "jtwN;HTn5;TYrQ",
        "site": "https://openreview.net/forum?id=rujwIvjooA",
        "pdf_size": 0,
        "rating": "5;5;6",
        "confidence": "3;5;3",
        "wc_review": "",
        "rating_avg": [
            5.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.9428090415820634
        ],
        "replies_avg": [
            26,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "ryTr83DxRq",
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
        "keywords": "LLM Agents;Tool Use;Benchmark;AI Research Agents",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Deepak Nathani;Lovish Madaan;Nicholas Roberts;Nikolay Bashlykov;Ajay Menon;Vincent Moens;Mikhail Plekhanov;Amar Budhiraja;Despoina Magka;Vladislav Vorotilov;Gaurav Chaurasia;Dieuwke Hupkes;Ricardo Silveira Cabral;Tatiana Shavrina;Jakob Nicolaus Foerster;Yoram Bachrach;William Yang Wang;Roberta Raileanu",
        "authorids": "~Deepak_Nathani2;~Lovish_Madaan1;~Nicholas_Roberts2;~Nikolay_Bashlykov1;~Ajay_Menon1;~Vincent_Moens3;~Mikhail_Plekhanov1;~Amar_Budhiraja1;~Despoina_Magka2;~Vladislav_Vorotilov1;~Gaurav_Chaurasia1;~Dieuwke_Hupkes1;~Ricardo_Silveira_Cabral1;~Tatiana_Shavrina1;~Jakob_Nicolaus_Foerster1;~Yoram_Bachrach2;~William_Yang_Wang2;~Roberta_Raileanu2",
        "gender": "M;;;M;M;M;M;M;F;;M;;M;F;M;;;",
        "homepage": "https://www.dnathani.net;https://lovishmadaan.github.io;;;;https://github.com/vmoens;;;;;https://gchauras.github.io;https://github.com/google/BIG-bench;;https://tatianashavrina.github.io/;https://www.jakobfoerster.com;;;",
        "dblp": "222/9844;245/6079;;;;220/5625;;172/7740.html;52/8336.html;;;184/8838;65/10697;242/9350;176/5095;;;",
        "google_scholar": "ItTsP6IAAAAJ;zc2WyXkAAAAJ;;vfSAlwEAAAAJ;https://scholar.google.com/citations?hl=en;8l-tvFoAAAAJ;Nty9hAYAAAAJ;;itFfzv0AAAAJ;;pBaL_DgAAAAJ;https://scholar.google.nl/citations?user=tAtSMTcAAAAJ;NB9xXQcAAAAJ;sdmdZh8AAAAJ;6z4lQzMAAAAJ;;;",
        "orcid": ";;;;;;;;;;;;;;;;;",
        "linkedin": "deepak-nathani/;;;nikolaybashlykov/;;vincent-moens-9bb91972/;mikeplekhanov/;;despoina-magka-04372015/;vvorotilov/;;;rscabral/;;;;;",
        "or_profile": "~Deepak_Nathani2;~Lovish_Madaan1;~Nicholas_Roberts2;~Nikolay_Bashlykov1;~Ajay_Menon1;~Vincent_Moens3;~Mikhail_Plekhanov1;~Amar_Budhiraja1;~Despoina_Magka2;~Vladislav_Vorotilov1;~Gaurav_Chaurasia1;~Dieuwke_Hupkes1;~Ricardo_Silveira_Cabral1;~Tatiana_Shavrina1;~Jakob_Nicolaus_Foerster1;~Yoram_Bachrach2;~William_Yang_Wang2;~Roberta_Raileanu2",
        "aff": "University of California, Santa Barbara;Meta+University College London, University of London;;Meta Facebook;Meta Facebook;Meta;Meta Facebook;Meta Facebook;FAIR at Meta;Independent;Meta Facebook;Meta Facebook;Meta Facebook;Meta Facebook;University of Oxford;;;",
        "aff_domain": "ucsb.edu;meta.com+ucl.ac.uk;;meta.com;meta.com;fb.com;meta.com;meta.com;ai.meta.com;gmail.com;meta.com;facebook.com;meta.com;meta.com;eng.ox.ac.uk;;;",
        "position": "PhD student;Researcher+PhD student;;Researcher;Software Engineer;Applied ML Scientist;Researcher;Researcher;Researcher;Independent Researcher;Researcher;Research Scientist;Researcher;Researcher;Associate Professor;;;",
        "bibtex": "@inproceedings{\nnathani2025mlgym,\ntitle={{MLG}ym: A New Framework and Benchmark for Advancing {AI} Research Agents},\nauthor={Deepak Nathani and Lovish Madaan and Nicholas Roberts and Nikolay Bashlykov and Ajay Menon and Vincent Moens and Mikhail Plekhanov and Amar Budhiraja and Despoina Magka and Vladislav Vorotilov and Gaurav Chaurasia and Dieuwke Hupkes and Ricardo Silveira Cabral and Tatiana Shavrina and Jakob Nicolaus Foerster and Yoram Bachrach and William Yang Wang and Roberta Raileanu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ryTr83DxRq}\n}",
        "github": "",
        "project": "",
        "reviewers": "33qt;5pzF;xDsa",
        "site": "https://openreview.net/forum?id=ryTr83DxRq",
        "pdf_size": 0,
        "rating": "5;6;6",
        "confidence": "3;3;4",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            18,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "s0p9xpORgP",
        "title": "Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Fine-tuning large pre-trained LLMs generally demands extensive GPU memory. Traditional first-order optimizers like SGD encounter substantial difficulties due to increased memory requirements from storing activations and gradients during both the forward and backward phases as the model size expands. Alternatively, zeroth-order (ZO) techniques can compute gradients using just forward operations, eliminating the need to store activations. Furthermore, by leveraging CPU capabilities, it's feasible to enhance both the memory and processing power available to a single GPU.\nWe propose a novel framework, ZO2 (Zeroth-Order Offloading), for efficient zeroth-order fine-tuning of LLMs with only limited GPU memory. Our framework dynamically shifts model parameters between the CPU and GPU as required, optimizing computation flow and maximizing GPU usage by minimizing downtime. This integration of parameter adjustments with ZO's double forward operations reduces unnecessary data movement, enhancing the fine-tuning efficacy. Additionally, our framework supports an innovative low-bit precision approach in AMP (Automatic Mixed Precision) mode to streamline data exchanges between the CPU and GPU.\nEmploying this approach allows us to fine-tune extraordinarily large models, such as the OPT-175B with 175 billion parameters, on a mere 18GB GPU. Moreover, our framework achieves these results with almost no additional time overhead and absolutely no accuracy loss compared to standard zeroth-order methods. ZO2's code has been open-sourced in https://github.com/liangyuwang/zo2.",
        "keywords": "Zeroth-Order Optimization;LLMs;Fine-Tuning",
        "primary_area": "",
        "supplementary_material": "/attachment/d1a8cbfa2abae3229e3889fc60a25827e31d5d45.zip",
        "author": "Liangyu Wang;Jie Ren;Hang Xu;Junxiao Wang;Huanyi Xie;David E. Keyes;Di Wang",
        "authorids": "~Liangyu_Wang1;~Jie_Ren4;~Hang_Xu3;~Junxiao_Wang1;~Huanyi_Xie1;~David_E._Keyes1;~Di_Wang1",
        "gender": "M;;M;M;M;M;",
        "homepage": ";https://jieren98.github.io/;;http://jxiao.wang/;https://www.kth.se/profile/huanyi?l=en;https://www.kaust.edu.sa/en/study/faculty/david-keyes;",
        "dblp": ";;;;;27/854;",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;wlVxP3QAAAAJ;UhUecFUAAAAJ;H6RsGygAAAAJ;ud1kAH8AAAAJ;G3dEJEQAAAAJ;",
        "orcid": "0009-0006-4328-4287;;;0000-0001-7263-174X;;0000-0002-4052-7224;",
        "linkedin": "https://www.linkedin.cn/incareer/in/liangyu-wang-728526214;;;junxiao-wang/;;david-keyes-2798236/;",
        "or_profile": "~Liangyu_Wang1;~Jie_Ren4;~Hang_Xu3;~Junxiao_Wang1;~Huanyi_Xie1;~David_E._Keyes1;~Di_Wang1",
        "aff": "King Abdullah University of Science and Technology;King Abdullah University of Science and Technology;Independent Researcher;Guangzhou University+King Abdullah University of Science and Technology;KTH Royal Institute of Technology;King Abdullah University of Science and Technology+Columbia University;",
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;xu.com;gzhu.edu.cn+kaust.edu.sa;kth.se;kaust.edu.sa+columbia.edu;",
        "position": "PhD student;PhD student;Researcher;Associate Professor+Postdoc;MS student;Full Professor+Full Professor;",
        "bibtex": "@inproceedings{\nwang2025scalable,\ntitle={Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited {GPU} Memory},\nauthor={Liangyu Wang and Jie Ren and Hang Xu and Junxiao Wang and Huanyi Xie and David E. Keyes and Di Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=s0p9xpORgP}\n}",
        "github": "",
        "project": "",
        "reviewers": "Ka4C;SBMa;pm1a",
        "site": "https://openreview.net/forum?id=s0p9xpORgP",
        "pdf_size": 0,
        "rating": "5;6;8",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            1.247219128924647
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.18898223650461363
    },
    {
        "id": "sX4OoLKSW2",
        "title": "Supposedly Equivalent Facts That Aren\u2019t? Entity Frequency in Pre-training Induces Asymmetry in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, we demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, we leverage the fully open-source $\\texttt{OLMo}$ series by indexing its $\\texttt{Dolma}$ dataset to estimate entity frequencies. Using relational facts (represented as triples) from $\\texttt{Wikidata5M}$, we construct probing datasets to isolate this effect. Our experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.",
        "keywords": "Large Language Models;Asymmetry;Equivalent Facts;Entity Frequency;Pre-training Bias;Knowledge Probing;Hallucinations;Knowledge Graphs",
        "primary_area": "",
        "supplementary_material": "/attachment/e5908026cff40ccda236868d7ddebec90ec6823c.zip",
        "author": "Yuan He;Bailan He;Zifeng Ding;Alisia Maria Lupidi;Yuqicheng Zhu;Shuo Chen;Caiqi Zhang;Jiaoyan Chen;Yunpu Ma;Volker Tresp;Ian Horrocks",
        "authorids": "~Yuan_He5;~Bailan_He1;~Zifeng_Ding1;~Alisia_Maria_Lupidi1;~Yuqicheng_Zhu1;~Shuo_Chen12;~Caiqi_Zhang2;~Jiaoyan_Chen1;~Yunpu_Ma1;~Volker_Tresp1;~Ian_Horrocks1",
        "gender": "M;;;F;M;M;;M;M;M;",
        "homepage": "https://www.yuanhe.wiki/;;;https://www.linkedin.com/in/alisia-maria-lupidi/;https://zhuyuqicheng.github.io/;https://chenxshuo.github.io;;https://chenjiaoyan.github.io/;https://dblp.org/pid/199/8143.html;https://www.dbs.ifi.lmu.de/~tresp/;http://www.cs.ox.ac.uk/ian.horrocks/",
        "dblp": "11/1735-8;;283/5849;;330/2167;00/6472-14;;56/8110-1;199/8143.html;t/VolkerTresp;h/IanHorrocks",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;https://scholar.google.com/citations?hl=en;5xzdzq4AAAAJ;TE5jy5cAAAAJ;BKvdGiwAAAAJ;;https://scholar.google.ch/citations?user=5Cy4z8wAAAAJ;fj5DzgcAAAAJ;xIJHTUwAAAAJ;0ypdmcYAAAAJ",
        "orcid": "0000-0002-4486-1262;;;;;0000-0001-7305-3793;;0000-0003-4643-6750;;0000-0001-9428-3686;",
        "linkedin": "yuan-he-0557781aa/;;zifeng-ding-9361b0186/;;yuqicheng-zhu-531658161/;;;;yunpu-ma-05a9b41b0/?originalSubdomain=de;volker-tresp-8110a118/;",
        "or_profile": "~Yuan_He5;~Bailan_He1;~Zifeng_Ding1;~Alisia_Maria_Lupidi1;~Yuqicheng_Zhu1;~Shuo_Chen12;~Caiqi_Zhang2;~Jiaoyan_Chen1;~Yunpu_Ma1;~Volker_Tresp1;~Ian_Horrocks1",
        "aff": "Amazon+University of Oxford;;University of Cambridge;University of Oxford+Meta Facebook;Universit\u00e4t Stuttgart;Siemens Corporate Research+University of Munich, Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen+Amazon;;The University of Manchester+The University of Manchester+University of Oxford;Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen+Siemens Corporate Research;Ludwig Maximilian University of Munich+Siemens Corporate Research;University of Oxford",
        "aff_domain": "amazon.com+cs.ox.ac.uk;;cam.ac.uk;ox.ac.uk+meta.com;uni-stuttgart.de;siemens.com+campus.lmu.de+amazon.de;;manchester.ac.uk+manchester.ac.uk+cs.ox.ac.uk;lmu.de+siemens.com;lmu.de+siemens.com;ox.ac.uk",
        "position": "Applied Scientist+Postdoc;;Postdoc;PhD student+Researcher;PhD student;Researcher+PhD student+Intern;;Associate Professor+Assistant Professor+Senior Researcher;Assistant Professor+Researcher;Associate Professor+Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nhe2025supposedly,\ntitle={Supposedly Equivalent Facts That Aren{\\textquoteright}t? Entity Frequency in Pre-training Induces Asymmetry in {LLM}s},\nauthor={Yuan He and Bailan He and Zifeng Ding and Alisia Maria Lupidi and Yuqicheng Zhu and Shuo Chen and Caiqi Zhang and Jiaoyan Chen and Yunpu Ma and Volker Tresp and Ian Horrocks},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=sX4OoLKSW2}\n}",
        "github": "",
        "project": "",
        "reviewers": "H939;WBVB;HfnW;Ubaa",
        "site": "https://openreview.net/forum?id=sX4OoLKSW2",
        "pdf_size": 0,
        "rating": "4;6;6;7",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            1.0897247358851685
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "sy71y74U80",
        "title": "D3: A Dataset for Training Code LMs to Act Diff-by-Diff",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce D3 (\"Diverse Data for Diff-by-Diff Coding\"), a large dataset for training LMs to iteratively synthesize general-purpose Python source code by generating file diffs. D3 frames code synthesis as a goal-conditioned sequential decision-making problem, where goals, states, and actions are represented by token sequences corresponding to the description of a functionality to add, the current contents of a file, and a file diff, respectively.  The dataset contains 8 billion tokens of instruction + file-state + file-diff-sequence examples sampled from  850,000 human-written Python source files.  To construct D3, we filter, augment, and annotate source code from The Stack by sampling synthetic file-diff sequences with a code analysis tool and labeling each sample with an LLM-generated rationale. In our experiments, we show that mid-training LMs like Llama 3.2 1b and 3b on D3 prior to supervised fine-tuning (SFT) on task-curated data improves performance on synthesis & editing tasks. On benchmarks like HumanEvalSynth and HumanEvalFix, we observe improvements in pass@1 of 3 to 6 points compared to direct SFT. D3-trained models are particularly strong at completing partial human-written solutions to programming problems.",
        "keywords": "data filtering;synthetic data;code synthesis;code editing;file diffs;midtraining;SFT;LM agents",
        "primary_area": "",
        "supplementary_material": "/attachment/db1dce296e8bcb2b3440badc8ce5c55718a09742.zip",
        "author": "Ulyana Piterbarg;Kanishk Gandhi;Lerrel Pinto;Noah Goodman;Rob Fergus",
        "authorids": "~Ulyana_Piterbarg1;~Kanishk_Gandhi1;~Lerrel_Pinto1;~Noah_Goodman1;~Rob_Fergus1",
        "gender": "F;;M;;M",
        "homepage": "https://upiterbarg.github.io/;;https://www.lerrelpinto.com/;https://cocolab.stanford.edu/;http://cs.nyu.edu/fergus/",
        "dblp": "284/4477;;168/8304;96/1216;77/3763",
        "google_scholar": ";;pmVPj94AAAAJ;OUpIbcQAAAAJ;https://scholar.google.com.tw/citations?user=GgQ9GEkAAAAJ",
        "orcid": "0000-0002-8363-9648;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Ulyana_Piterbarg1;~Kanishk_Gandhi1;~Lerrel_Pinto1;~Noah_Goodman1;~Rob_Fergus1",
        "aff": "Meta Superintelligence Labs+New York University;;New York University;Stanford University;Meta+New York University+Google",
        "aff_domain": "meta.com+cims.nyu.edu;;cs.nyu.edu;stanford.edu;meta.com+nyu.edu+google.com",
        "position": "Intern+PhD student;;Assistant Professor;Full Professor;Researcher+Professor+Research scientist",
        "bibtex": "@inproceedings{\npiterbarg2025d,\ntitle={D3: A Dataset for Training Code {LM}s to Act Diff-by-Diff},\nauthor={Ulyana Piterbarg and Kanishk Gandhi and Lerrel Pinto and Noah Goodman and Rob Fergus},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=sy71y74U80}\n}",
        "github": "",
        "project": "",
        "reviewers": "Wokx;9pRV;zkKL",
        "site": "https://openreview.net/forum?id=sy71y74U80",
        "pdf_size": 0,
        "rating": "5;6;7",
        "confidence": "3;3;5",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.9428090415820634
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.8660254037844387
    },
    {
        "id": "tK8GHR62EX",
        "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Training large, general-purpose language models poses significant challenges. The growing availability of specialized *expert* models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SpectR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SpectR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.",
        "keywords": "MoE;routing;merging;LoRA;adapters;experts",
        "primary_area": "",
        "supplementary_material": "",
        "author": "William Fleshman;Benjamin Van Durme",
        "authorids": "~William_Fleshman1;~Benjamin_Van_Durme2",
        "gender": "M;",
        "homepage": ";",
        "dblp": "222/1705;",
        "google_scholar": "I_p1VXUAAAAJ;",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~William_Fleshman1;~Benjamin_Van_Durme2",
        "aff": "Johns Hopkins University;",
        "aff_domain": "jh.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nfleshman2025spectr,\ntitle={SpectR: Dynamically Composing {LM} Experts with Spectral Routing},\nauthor={William Fleshman and Benjamin Van Durme},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=tK8GHR62EX}\n}",
        "github": "",
        "project": "",
        "reviewers": "Efhf;iGcM;N8tG",
        "site": "https://openreview.net/forum?id=tK8GHR62EX",
        "pdf_size": 0,
        "rating": "6;6;6",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "te7UC87Zbw",
        "title": "Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Merging parameter-efficient task experts has recently gained growing attention as a way to build modular architectures that can be rapidly adapted on the fly for specific downstream tasks, without requiring additional fine-tuning. Typically, LoRA serves as the foundational building block of such parameter-efficient modular architectures, leveraging low-rank weight structures to reduce the number of trainable parameters. In this paper, we study the properties of sparse adapters, which train only a subset of weights in the base neural network, as potential building blocks of modular architectures. First, we propose a simple method for training highly effective sparse adapters, which is conceptually simpler than existing methods in the literature and surprisingly outperforms both LoRA and full fine-tuning in our setting. Next, we investigate the merging properties of these sparse adapters by merging adapters for up to 20 natural language processing tasks, thus scaling beyond what is usually studied in the literature. Our findings demonstrate that sparse adapters yield superior in-distribution performance post-merging compared to LoRA or full model merging. Achieving strong held-out performance remains a challenge for all methods considered.",
        "keywords": "Sparse adapter;Parameter-efficient finetuning;Model merging;LLM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Samin Yeasar Arnob;Zhan Su;Minseon Kim;Oleksiy Ostapenko;Riyasat Ohib;Esra'a Saleh;Doina Precup;Lucas Caccia;Alessandro Sordoni",
        "authorids": "~Samin_Yeasar_Arnob1;~Zhan_Su1;~Minseon_Kim1;~Oleksiy_Ostapenko1;~Riyasat_Ohib1;~Esra'a_Saleh1;~Doina_Precup1;~Lucas_Caccia1;~Alessandro_Sordoni2",
        "gender": "M;M;;M;M;F;F;M;",
        "homepage": "https://www.linkedin.com/in/samin-yeasar-arnob/;https://shuishen112.github.io/zhansu.github.io/;https://kim-minseon.github.io/;;https://www.riyasatohib.com/;https://esraasaleh.com;http://cs.mcgill.ca/~dprecup/;https://www.cs.mcgill.ca/~lpagec/;",
        "dblp": ";02/6524;247/5952;;;;p/DoinaPrecup;;",
        "google_scholar": "RMPv4RQAAAAJ;VzEpVpoAAAAJ;ZwObZNwAAAAJ;mqLVUGgAAAAJ;;xWqwajsAAAAJ;https://scholar.google.com.tw/citations?user=j54VcVEAAAAJ;fuvIITUAAAAJ;",
        "orcid": ";0000-0001-5189-9165;;;;;;;",
        "linkedin": ";;minseon-kim-707a84174;;;;;;",
        "or_profile": "~Samin_Yeasar_Arnob1;~Zhan_Su1;~Minseon_Kim1;~Oleksiy_Ostapenko1;~Riyasat_Ohib1;~Esra'a_Saleh1;~Doina_Precup1;~Lucas_Caccia1;~Alessandro_Sordoni2",
        "aff": "McGill University;Universit\u00e9 de Montr\u00e9al+Mila - Quebec Artificial Intelligence Institute;Microsoft;ServiceNow Inc;Georgia Institute of Technology;Universit\u00e9 de Montr\u00e9al;Google DeepMind+McGill University;Microsoft;",
        "aff_domain": "mcgill.ca;umontreal.ca+mila.quebec;microsoft.com;servicenow.com;gatech.edu;umontreal.ca;google.com+mcgill.ca;microsoft.com;",
        "position": "PhD student;Postdoc+Intern;Postdoc;Researcher;PhD student;PhD student;Research Team Lead+Associate Professor;Researcher;",
        "bibtex": "@inproceedings{\narnob2025exploring,\ntitle={Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts},\nauthor={Samin Yeasar Arnob and Zhan Su and Minseon Kim and Oleksiy Ostapenko and Riyasat Ohib and Esra'a Saleh and Doina Precup and Lucas Caccia and Alessandro Sordoni},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=te7UC87Zbw}\n}",
        "github": "",
        "project": "",
        "reviewers": "u9fH;MeHe;GEzS",
        "site": "https://openreview.net/forum?id=te7UC87Zbw",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "teW4nIZ1gy",
        "title": "One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Steering vectors (SVs) have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations.\nWe propose directly optimizing SVs through gradient descent on a single training example, and systematically investigate how these SVs generalize.\nWe consider several SV optimization techniques and find that the resulting SVs effectively mediate safety-relevant behaviors in multiple models.\nIndeed, in experiments on an alignment-faking model, we are able to optimize one-shot SVs that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples.\nAnd in experiments on refusal suppression, we demonstrate that one-shot optimized SVs can transfer across inputs, yielding a Harmbench attack success rate of 96.9%.\nFurthermore, we extend work on \"emergent misalignment\" and show that SVs optimized to induce a model to write vulnerable code cause the model to respond harmfully on unrelated open-ended prompts. \nFinally, we use one-shot SV optimization to investigate how an instruction-tuned LLM recovers from outputting false information, and find that this ability is independent of the model's explicit verbalization that the information was false.\nOverall, our findings suggest that optimizing SVs on a single example can mediate a wide array of misaligned behaviors in LLMs.\nCode can be found at https://github.com/jacobdunefsky/one-shot-steering-repro and https://github.com/jacobdunefsky/one-shot-steering-misalignment.",
        "keywords": "steering vectors;interpretability;alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jacob Dunefsky;Arman Cohan",
        "authorids": "~Jacob_Dunefsky1;~Arman_Cohan1",
        "gender": "M;M",
        "homepage": "https://jacobdunefsky.github.io;http://www.armancohan.com",
        "dblp": ";160/1727",
        "google_scholar": ";https://scholar.google.com/citations?hl=en",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Jacob_Dunefsky1;~Arman_Cohan1",
        "aff": "Department of Computer Science, Yale University;Yale University+Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.yale.edu;yale.edu+allenai.org",
        "position": "PhD student;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\ndunefsky2025oneshot,\ntitle={One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in {LLM}s},\nauthor={Jacob Dunefsky and Arman Cohan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=teW4nIZ1gy}\n}",
        "github": "",
        "project": "",
        "reviewers": "9bVR;dKkm;KGN7;g4Zf",
        "site": "https://openreview.net/forum?id=teW4nIZ1gy",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "tfTn8616Gf",
        "title": "A Taxonomy of Transcendence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Although language models are trained to mimic humans, the resulting systems display capabilities beyond the scope of any one person. To understand this phenomenon, we use a controlled setting to identify properties of the training data that lead a model to transcend the performance of its data sources. We build on previous work to outline three modes of transcendence, which we call \\textit{skill denoising}, \\textit{skill selection}, and \\textit{skill generalization}. We then introduce a knowledge graph-based setting in which simulated experts generate data based on their individual expertise. We highlight several aspects of data diversity that help to enable the model's transcendent capabilities. Additionally, our data generation setting offers a controlled testbed that we hope is valuable for future research in the area.",
        "keywords": "language models;data diversity;composition;knowledge graph",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Natalie Abreu;Edwin Zhang;Eran Malach;Naomi Saphra",
        "authorids": "~Natalie_Abreu1;~Edwin_Zhang2;~Eran_Malach3;~Naomi_Saphra1",
        "gender": ";;;F",
        "homepage": ";https://eddie.win;;http://nsaphra.github.io/",
        "dblp": ";;;131/6883",
        "google_scholar": ";;;TPhVfX8AAAAJ",
        "orcid": ";;;",
        "linkedin": "https://www.linkedin.com/natalie-abreu;;;naomi-saphra-028b8060/",
        "or_profile": "~Natalie_Abreu1;~Edwin_Zhang2;~Eran_Malach3;~Naomi_Saphra1",
        "aff": "Harvard University;OpenAI+Harvard University;;Harvard University",
        "aff_domain": "g.harvard.edu;openai.com+harvard.edu;;harvard.edu",
        "position": "PhD student;Researcher+PhD student;;Fellow",
        "bibtex": "@inproceedings{\nabreu2025a,\ntitle={A Taxonomy of Transcendence},\nauthor={Natalie Abreu and Edwin Zhang and Eran Malach and Naomi Saphra},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=tfTn8616Gf}\n}",
        "github": "",
        "project": "",
        "reviewers": "vsp5;76dv;tCzd;nXy4",
        "site": "https://openreview.net/forum?id=tfTn8616Gf",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "tfriX0r2Sg",
        "title": "Towards User-level Private Reinforcement Learning with Human Feedback",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) has emerged as an influential technique, enabling the alignment of large language models (LLMs) with human preferences. However, how to protect user preference privacy has become a crucial issue, as LLMs tend to remember users' preferences. Most previous work has focused on using  differential privacy (DP) to protect the privacy of individual data. However, they have concentrated primarily on item-level privacy protection and have unsatisfactory performance for user-level privacy, which is more common in RLHF. This study proposes a novel framework, AUP-RLHF, which integrates user-level label DP into RLHF. We first show that the classical random response algorithm, which achieves an acceptable performance in item-level privacy, leads to suboptimal utility when in the user-level settings. We then establish a lower bound for the user-level label DP-RLHF and develop the AUP-RLHF algorithm, which guarantees $(\\varepsilon, \\delta)$ user-level privacy and achieves an improved estimation error. Experimental results show that AUP-RLHF outperforms existing baseline methods in sentiment generation and summarization tasks, achieving a better privacy-utility trade-off.",
        "keywords": "Differential Privacy;RLHF;LLM alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiaming Zhang;Mingxi Lei;Meng Ding;Mengdi Li;Zihang Xiang;Difei Xu;Jinhui Xu;Di Wang",
        "authorids": "~Jiaming_Zhang15;~Mingxi_Lei1;~Meng_Ding3;~Mengdi_Li1;~Zihang_Xiang1;~Difei_Xu1;~Jinhui_Xu1;~Di_Wang1",
        "gender": "M;;F;M;M;;M;",
        "homepage": "https://traderbxy.github.io/jiamingzhang.github.io//;https://mingxilei.github.io;;https://mengdi-li.github.io/;http://zihangxiang.github.io;;https://www.cse.buffalo.edu/~jinhui/;",
        "dblp": ";268/5761;;;345/1541;;24/6437-1.html;",
        "google_scholar": ";xWNNQ_IAAAAJ;Ipwvf8oAAAAJ;0W7UjrcAAAAJ;A7N2C0kAAAAJ;;https://scholar.google.com/citations?hl=en;",
        "orcid": ";;;0009-0000-2650-2891;0009-0008-9352-4810;;;",
        "linkedin": ";;;limengdi2/;zihang-xiang/;;;",
        "or_profile": "~Jiaming_Zhang15;~Mingxi_Lei1;~Meng_Ding3;~Mengdi_Li1;~Zihang_Xiang1;~Difei_Xu1;~Jinhui_Xu1;~Di_Wang1",
        "aff": "Renmin University of China;State University of New York at Buffalo;State University of New York at Buffalo;King Abdullah University of Science and Technology;King Abdullah University of Science and Technology;;University of Science and Technology of China+University at Buffalo, State University of New York;",
        "aff_domain": "ruc.edu.cn;buffalo.edu;buffalo.edu;kaust.edu.sa;kaust.edu.sa;;ustc.edu.cn+buffalo.edu;",
        "position": "Undergrad student;PhD student;PhD student;Postdoc;PhD student;;Full Professor+Full Professor;",
        "bibtex": "@inproceedings{\nzhang2025towards,\ntitle={Towards User-level Private Reinforcement Learning with Human Feedback},\nauthor={Jiaming Zhang and Mingxi Lei and Meng Ding and Mengdi Li and Zihang Xiang and Difei Xu and Jinhui Xu and Di Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=tfriX0r2Sg}\n}",
        "github": "",
        "project": "",
        "reviewers": "Kvf8;c7jp;vYQH;WBhh;Bw9U;yUzC;1im1",
        "site": "https://openreview.net/forum?id=tfriX0r2Sg",
        "pdf_size": 0,
        "rating": "4;4;5;6;7;7;7",
        "confidence": "3;3;4;2;2;3;3",
        "wc_review": "",
        "rating_avg": [
            5.714285714285714,
            1.2777531299998797
        ],
        "confidence_avg": [
            2.857142857142857,
            0.6388765649999398
        ],
        "replies_avg": [
            29,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4
    },
    {
        "id": "tu4dFUsW5z",
        "title": "Why do LLMs attend to the first token?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We run experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.",
        "keywords": "Large Language Models;Attention Sinks;Information Propagation;Pre-training",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Federico Barbero;Alvaro Arroyo;Xiangming Gu;Christos Perivolaropoulos;Petar Veli\u010dkovi\u0107;Razvan Pascanu;Michael M. Bronstein",
        "authorids": "~Federico_Barbero1;~Alvaro_Arroyo1;~Xiangming_Gu1;~Christos_Perivolaropoulos1;~Petar_Veli\u010dkovi\u01071;~Razvan_Pascanu1;~Michael_M._Bronstein1",
        "gender": ";M;M;M;M;M;M",
        "homepage": "https://federicobarbero.com;https://scholar.google.co.uk/citations?user=P1qHzNYAAAAJ&hl=en;https://guxm2021.github.io;https://cperivol.com;https://petar-v.com;https://razp.info;http://www.inf.usi.ch/bronstein/",
        "dblp": ";;276/5844;;184/4786.html;65/8368.html;07/2668",
        "google_scholar": "jpYtKMEAAAAJ;;BkxEuIoAAAAJ;;https://scholar.google.co.uk/citations?user=kcTK_FAAAAAJ;https://scholar.google.ca/citations?user=eSPY8LwAAAAJ;UU3N6-UAAAAJ",
        "orcid": ";;;;0000-0002-2820-4692;;",
        "linkedin": ";;xiangming-gu/;cperivol/;petarvelickovic;;mbronstein/",
        "or_profile": "~Federico_Barbero1;~Alvaro_Arroyo1;~Xiangming_Gu1;~Christos_Perivolaropoulos1;~Petar_Veli\u010dkovi\u01071;~Razvan_Pascanu1;~Michael_M._Bronstein1",
        "aff": "University of Oxford;University of Oxford;Google Deepmind+National University of Singapore;Google;University of Cambridge+Google DeepMind;Mila - Quebec Artificial Intelligence Institute+Google DeepMind;University of Oxford",
        "aff_domain": "ox.ac.uk;ox.ac.uk;deepmind.com+nus.edu.sg;deepmind.com;cam.ac.uk+google.com;mila.quebec+google.com;ox.ac.uk",
        "position": "PhD student;PhD student;Intern+PhD student;Researcher;Affiliated Lecturer+Senior Staff Research Scientist;Affiliate Member+Research Scientist;Full Professor",
        "bibtex": "@inproceedings{\nbarbero2025why,\ntitle={Why do {LLM}s attend to the first token?},\nauthor={Federico Barbero and Alvaro Arroyo and Xiangming Gu and Christos Perivolaropoulos and Petar Veli{\\v{c}}kovi{\\'c} and Razvan Pascanu and Michael M. Bronstein},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=tu4dFUsW5z}\n}",
        "github": "",
        "project": "",
        "reviewers": "4hVJ;x9S1;Z1TA;Maou",
        "site": "https://openreview.net/forum?id=tu4dFUsW5z",
        "pdf_size": 0,
        "rating": "5;6;7;9",
        "confidence": "3;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            1.479019945774904
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.09759000729485331
    },
    {
        "id": "tybbSo6wba",
        "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs\u2019 safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the \u201ctrue intention\u201d of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user\u2019s potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua.",
        "keywords": "Jailbreak Attacks Defense;Prompt Compression",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yucheng Li;Surin Ahn;Huiqiang Jiang;Amir H. Abdi;Yuqing Yang;Lili Qiu",
        "authorids": "~Yucheng_Li5;~Surin_Ahn1;~Huiqiang_Jiang2;~Amir_H._Abdi1;~Yuqing_Yang1;~Lili_Qiu3",
        "gender": ";;M;;;",
        "homepage": ";;https://hqjiang.com;;;https://www.microsoft.com/en-us/research/people/liliqiu/",
        "dblp": ";;204/2497;;91/9064-1.html;",
        "google_scholar": ";;99KtvpYAAAAJ;;4BtNQAEAAAAJ;",
        "orcid": ";;0000-0002-1327-4882;;0000-0003-3518-5212;",
        "linkedin": ";;;;;",
        "or_profile": "~Yucheng_Li5;~Surin_Ahn1;~Huiqiang_Jiang2;~Amir_H._Abdi1;~Yuqing_Yang1;~Lili_Qiu3",
        "aff": ";;Microsoft;;Microsoft Research;Microsoft+University of Texas at Austin",
        "aff_domain": ";;microsoft.com;;research.microsoft.com;microsoft.com+utexas.edu",
        "position": ";;RSDE;;Researcher;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nli2025securitylingua,\ntitle={SecurityLingua: Efficient Defense of {LLM} Jailbreak Attacks via Security-Aware Prompt Compression},\nauthor={Yucheng Li and Surin Ahn and Huiqiang Jiang and Amir H. Abdi and Yuqing Yang and Lili Qiu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=tybbSo6wba}\n}",
        "github": "",
        "project": "",
        "reviewers": "8yJ5;5PWa;oVEx;hYDA",
        "site": "https://openreview.net/forum?id=tybbSo6wba",
        "pdf_size": 0,
        "rating": "5;5;6;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "u9JXu4L17I",
        "title": "DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Information retrieval systems are crucial for enabling effective access to large document collections. Recent approaches have leveraged Large Language Models (LLMs) to enhance retrieval performance through query augmentation, but often rely on expensive supervised learning or distillation techniques that require significant computational resources and hand-labeled data. We introduce DeepRetrieval, a reinforcement learning approach that trains LLMs for query generation through trial and error without supervised data for reference query. Using retrieval metrics as rewards, our system generates queries that maximize retrieval performance. DeepRetrieval outperforms state-of-the-art methods on literature search with 65.07\\% (vs.\\ previous SOTA 24.68\\%) recall for publication search and 63.18\\% (vs.\\ previous SOTA 32.11\\%) recall for trial search using real-world search engines. DeepRetrieval also dominates in evidence-seeking retrieval, classic information retrieval and SQL database search. With only 3B parameters, it outperforms industry-leading models like GPT-4o and Claude-3.5-Sonnet on those tasks. These results demonstrate that our reinforcement learning approach offers a more efficient and effective paradigm for information retrieval.",
        "keywords": "Large Language Models;Information Retrieval;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/e1c13f35faae5175020c5266f60439cdda5f1f8f.zip",
        "author": "Pengcheng Jiang;Jiacheng Lin;Lang Cao;Runchu Tian;SeongKu Kang;Zifeng Wang;Jimeng Sun;Jiawei Han",
        "authorids": "~Pengcheng_Jiang2;~Jiacheng_Lin3;~Lang_Cao2;~Runchu_Tian1;~SeongKu_Kang1;~Zifeng_Wang3;~Jimeng_Sun3;~Jiawei_Han1",
        "gender": "M;M;M;M;M;M;;M",
        "homepage": "https://pat-jj.github.io/;https://linjc16.github.io/;https://github.com/windszzlang;https://github.com/Rachum-thu;https://seongku-kang.github.io/;https://zifengwang.xyz;http://sunlab.org;http://hanj.cs.illinois.edu/",
        "dblp": "60/8352;;133/4286;344/9908;251/9613.html;;;h/JiaweiHan.html",
        "google_scholar": "TejDN9wAAAAJ;https://scholar.google.com.tw/citations?user=h9tJLt8AAAAJ;CefzkpEAAAAJ;u6ex8v8AAAAJ;fB0K-fMAAAAJ;kMlWwTAAAAAJ;9jmmp5sAAAAJ;https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ",
        "orcid": "0000-0001-9925-3777;;0000-0003-0011-5724;;0000-0001-5528-1426;;0000-0003-1512-6426;0000-0002-3629-2696",
        "linkedin": "patrick-j-3492b4235/;;lang-cao-a73657260/;;;;jimengsun/;",
        "or_profile": "~Pengcheng_Jiang2;~Jiacheng_Lin3;~Lang_Cao2;~Runchu_Tian1;~SeongKu_Kang1;~Zifeng_Wang3;~Jimeng_Sun3;~Jiawei_Han1",
        "aff": "University of Illinois, Urbana Champaign+Google;Department of Computer Science, University of Illinois;University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign;Korea University+University of Illinois Urbana-Champaign;Keiji AI+University of Illinois, Urbana Champaign;University of Illinois, Urbana Champaign+College of Computing+Georgia Institute of Technology;University of Illinois at Urbana-Champaign (UIUC)",
        "aff_domain": "cs.illinois.edu+google.com;cs.illinois.edu;cs.illinois.edu;illinois.edu;korea.ac.kr+cs.illinois.edu;keiji.ai+illinois.edu;illinois.edu+cc.gatech.edu+gatech.edu;illinois.edu",
        "position": "PhD student+Student Researcher;PhD student;PhD student;MS student;Assistant Professor+Postdoc;Researcher+PhD student;Professor+Associate Professor+Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\njiang2025deepretrieval,\ntitle={DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning},\nauthor={Pengcheng Jiang and Jiacheng Lin and Lang Cao and Runchu Tian and SeongKu Kang and Zifeng Wang and Jimeng Sun and Jiawei Han},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=u9JXu4L17I}\n}",
        "github": "",
        "project": "",
        "reviewers": "C5L2;fHLD;HAPX",
        "site": "https://openreview.net/forum?id=u9JXu4L17I",
        "pdf_size": 0,
        "rating": "7;7;8",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9999999999999997
    },
    {
        "id": "uBAubFwymy",
        "title": "VaPR - Vision-language Preference alignment for Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL \\& Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on VaPR, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page https://vap-r.github.io/vap-r/",
        "keywords": "Vision Language Models;Preference Optimization;DPO;Data Generation;Reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rohan Wadhawan;Fabrice Y Harel-Canada;Zi-Yi Dou;Suhaila Shakiah;Robinson Piramuthu;Nanyun Peng",
        "authorids": "~Rohan_Wadhawan1;~Fabrice_Y_Harel-Canada1;~Zi-Yi_Dou1;~Suhaila_Shakiah1;~Robinson_Piramuthu1;~Nanyun_Peng1",
        "gender": ";M;;F;M;F",
        "homepage": ";https://fabrice.harel-canada.com/;https://zdou0830.github.io/;https://www.linkedin.com/in/suhailashakiah/;https://scholar.google.com/citations?user=2CkqEGcAAAAJ&hl=en;https://violetpeng.github.io/",
        "dblp": ";274/6818.html;205/8985;271/2364;29/1333;117/4036",
        "google_scholar": ";AY9hnu8AAAAJ;RWogNsEAAAAJ;IP6H8LYAAAAJ;https://scholar.google.cl/citations?user=2CkqEGcAAAAJ;XxRXvX0AAAAJ",
        "orcid": ";;;;0000-0002-1767-8382;",
        "linkedin": ";;;suhailashakiah/;rpiramuthu/;",
        "or_profile": "~Rohan_Wadhawan1;~Fabrice_Y_Harel-Canada1;~Zi-Yi_Dou1;~Suhaila_Shakiah1;~Robinson_Piramuthu1;~Nanyun_Peng1",
        "aff": ";University of California, Los Angeles;University of California, Los Angeles;Amazon;Amazon Inc;University of California, Los Angeles",
        "aff_domain": ";ucla.edu;ucla.edu;amazon.com;amazon.com;ucla.edu",
        "position": ";PhD student;PhD student;Researcher;Principal Scientist;Associate Professor",
        "bibtex": "@inproceedings{\nwadhawan2025vapr,\ntitle={Va{PR} - Vision-language Preference alignment for Reasoning},\nauthor={Rohan Wadhawan and Fabrice Y Harel-Canada and Zi-Yi Dou and Suhaila Shakiah and Robinson Piramuthu and Nanyun Peng},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uBAubFwymy}\n}",
        "github": "",
        "project": "",
        "reviewers": "gWgY;2MKa;W6PJ;iz1D",
        "site": "https://openreview.net/forum?id=uBAubFwymy",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;4;2;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            24,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "uBg8PClMUu",
        "title": "ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Speech language models refer to language models with speech processing and understanding capabilities. One key desirable capability for speech language models is the ability to capture the intricate interdependency between content and prosody. The existing mainstream paradigm of training speech language models, which converts speech into discrete tokens before feeding them into LLMs, is sub-optimal in learning prosody information --- we find that the resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone. To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed tokenization scheme retains more complete prosody information, and is more understandable to text-based LLMs. We find that ProsodyLM can learn surprisingly diverse emerging prosody processing capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining prosody consistency in long contexts.",
        "keywords": "Speech LM: Multi-modal LLM",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kaizhi Qian;Xulin Fan;Junrui Ni;Slava Shechtman;Mark A. Hasegawa-Johnson;Chuang Gan;Yang Zhang",
        "authorids": "~Kaizhi_Qian1;~Xulin_Fan1;~Junrui_Ni1;~Slava_Shechtman1;~Mark_A._Hasegawa-Johnson1;~Chuang_Gan1;~Yang_Zhang3",
        "gender": ";M;M;M;M;M;M",
        "homepage": ";;;;http://speechtechnology.web.illinois.edu;http://people.csail.mit.edu/ganchuang/;",
        "dblp": "212/6254;;249/5376;55/8052.html;70/3186;139/6993;06/6785-1",
        "google_scholar": ";fU7hjTYAAAAJ;;gFAD6c8AAAAJ;18O0OAwAAAAJ;PTeSCbIAAAAJ;_-5PSgQAAAAJ",
        "orcid": ";;0009-0004-1666-3842;;0000-0002-5631-2893;;",
        "linkedin": ";;junrui-ni-931051156/;slava-shechtman-9b04b0/?originalSubdomain=il;mark-hasegawa-johnson-21a86825/;;",
        "or_profile": "~Kaizhi_Qian1;~Xulin_Fan1;~Junrui_Ni1;~Slava_Shechtman1;~Mark_A._Hasegawa-Johnson1;~Chuang_Gan1;~Yang_Zhang3",
        "aff": "International Business Machines;University of Illinois Urbana-Champaign;University of Illinois, Urbana Champaign;International Business Machines;University of Illinois, Urbana Champaign;University of Massachusetts at Amherst;International Business Machines",
        "aff_domain": "ibm.com;illinois.edu;illinous.edu;ibm.com;illinois.edu;umass.edu;ibm.com",
        "position": "Researcher;PhD student;PhD student;Researcher;Full Professor;Assistant Professor;Research Staff Employee",
        "bibtex": "@inproceedings{\nqian2025prosodylm,\ntitle={Prosody{LM}: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models},\nauthor={Kaizhi Qian and Xulin Fan and Junrui Ni and Slava Shechtman and Mark A. Hasegawa-Johnson and Chuang Gan and Yang Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uBg8PClMUu}\n}",
        "github": "",
        "project": "",
        "reviewers": "GTN9;HdRD;mFRG",
        "site": "https://openreview.net/forum?id=uBg8PClMUu",
        "pdf_size": 0,
        "rating": "6;7;7",
        "confidence": "4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "uLl7tSUOir",
        "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80\\% TLS and 70\\% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly at https://github.com/thunlp/BlockFFN.",
        "keywords": "mixture-of-experts;activation sparsity;inference acceleration",
        "primary_area": "",
        "supplementary_material": "/attachment/64657ad2b7008be33d41cfe2edd34b44a1e24875.zip",
        "author": "Chenyang Song;Weilin Zhao;Xu Han;Chaojun Xiao;Yingfa Chen;Yuxuan Li;Zhiyuan Liu;Maosong Sun",
        "authorids": "~Chenyang_Song1;~Weilin_Zhao1;~Xu_Han2;~Chaojun_Xiao1;~Yingfa_Chen1;~Yuxuan_Li19;~Zhiyuan_Liu1;~Maosong_Sun1",
        "gender": "M;M;;M;M;M;M;M",
        "homepage": ";https://brawny-college-5b2.notion.site/Weilin-Zhao-11d20b7deb8280388213d5f5ed072992?pvs=4;;https://xcjthu.github.io/;https://www.github.com/chen-yingfa;;http://nlp.csai.tsinghua.edu.cn/~lzy;https://www.cs.tsinghua.edu.cn/csen/info/1312/4394.htm",
        "dblp": ";197/5702.html;;223/4856;;;53/3245-1;95/3291-1",
        "google_scholar": "4L39cy0AAAAJ;_CR92HUAAAAJ;;xoC8smYAAAAJ;https://scholar.google.com/citations?hl=en;nAJmE48AAAAJ;dT0v5u0AAAAJ;https://scholar.google.com.tw/citations?user=zIgT0HMAAAAJ",
        "orcid": "0000-0002-8249-5216;0000-0001-8016-1952;;;;;0000-0002-7709-2543;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Chenyang_Song1;~Weilin_Zhao1;~Xu_Han2;~Chaojun_Xiao1;~Yingfa_Chen1;~Yuxuan_Li19;~Zhiyuan_Liu1;~Maosong_Sun1",
        "aff": "Tsinghua University;Tsinghua University;;Tsinghua University+Tsinghua University;Tsinghua University;, Tsinghua University;Tsinghua University;Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;;tsinghua.edu.cn+tsinghua.edu.cn;tsinghua.edu.cn;cs.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;PhD student;;Postdoc+PhD student;PhD student;Postdoc;Associate Professor;Full Professor",
        "bibtex": "@inproceedings{\nsong2025blockffn,\ntitle={Block{FFN}: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity},\nauthor={Chenyang Song and Weilin Zhao and Xu Han and Chaojun Xiao and Yingfa Chen and Yuxuan Li and Zhiyuan Liu and Maosong Sun},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uLl7tSUOir}\n}",
        "github": "",
        "project": "",
        "reviewers": "ohTG;G7oy;SU7d",
        "site": "https://openreview.net/forum?id=uLl7tSUOir",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "uXR2KsA4L9",
        "title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advancements in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks. However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the logical rigor crucial for mathematical problem-solving. The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination. To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities. Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions. We also found that occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning. These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the rigor and coherence of mathematical arguments rather than merely the correctness of final answers.",
        "keywords": "Mathematical Reasoning;Human Evaluation;Reasoning Evaluation;Math Problem-Solving",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hamed Mahdavi;Alireza Hashemi;Majid Daliri;Pegah Mohammadipour;Alireza Farhadi;Samira Malek;Yekta Yazdanifard;Amir Khasahmadi;Vasant G Honavar",
        "authorids": "~Hamed_Mahdavi1;~Alireza_Hashemi2;~Majid_Daliri1;~Pegah_Mohammadipour1;~Alireza_Farhadi2;~Samira_Malek1;~Yekta_Yazdanifard1;~Amir_Khasahmadi1;~Vasant_G_Honavar1",
        "gender": "M;M;M;F;M;F;M;;M",
        "homepage": "https://hamedmahdavi72.github.io/;;https://majid-daliri.github.io/;https://science.psu.edu/math/people/pmm5999;https://alirezafarhadi01.github.io/;https://samiramalek.github.io/;;;http://faculty.ist.psu.edu/vhonavar",
        "dblp": ";;315/4544;;;;;;https://dblp.uni-trier.de/pid/h/VasantHonavar.html",
        "google_scholar": "Ld-BcToAAAAJ;FEnk76cAAAAJ;MatRqXUAAAAJ;;;3S6LVy4AAAAJ;8qivQ74AAAAJ;;GPqMVRkAAAAJ",
        "orcid": ";;0000-0003-4001-4346;;;;;;0000-0001-5399-3489",
        "linkedin": ";;;;alireza-farhadi/;samira-malek-733b9889/;;;vhonavar/",
        "or_profile": "~Hamed_Mahdavi1;~Alireza_Hashemi2;~Majid_Daliri1;~Pegah_Mohammadipour1;~Alireza_Farhadi2;~Samira_Malek1;~Yekta_Yazdanifard1;~Amir_Khasahmadi1;~Vasant_G_Honavar1",
        "aff": "Pennsylvania State University;City University of New York;New York University;Pennsylvania State University;Amirkabir University of Technology;Pennsylvania State University;Bocconi University;;Pennsylvania State University",
        "aff_domain": "psu.edu;cuny.edu;nyu.edu;psu.edu;aut.ac.ir;psu.edu;unibocconi.it;;ist.psu.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;MS student;PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nmahdavi2025brains,\ntitle={Brains vs. Bytes: Evaluating {LLM} Proficiency in Olympiad Mathematics},\nauthor={Hamed Mahdavi and Alireza Hashemi and Majid Daliri and Pegah Mohammadipour and Alireza Farhadi and Samira Malek and Yekta Yazdanifard and Amir Khasahmadi and Vasant G Honavar},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uXR2KsA4L9}\n}",
        "github": "",
        "project": "",
        "reviewers": "qtx8;hUEG;ZMU4;1MfA",
        "site": "https://openreview.net/forum?id=uXR2KsA4L9",
        "pdf_size": 0,
        "rating": "5;7;8;8",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            1.224744871391589
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "ufozo2Wc9e",
        "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. \n    Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. \n    Surprisingly, LLMs not specifically trained for reasoning exhibit much better critical thinking ability, producing much shorter responses that quickly identify ill-posed queries and ask for the MiP. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. \n    To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. \n    Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses.\n    These results improve the understanding of overthinking and shed novel insights into mitigating the problem. \n    Our code and data can be found in: https://github.com/tianyi-lab/MiP-Overthinking.",
        "keywords": "LLM;Reasoning Model;Overthinking;Abstain",
        "primary_area": "",
        "supplementary_material": "/attachment/825254c1747951fd04bf0c9067828ba4da07f36f.zip",
        "author": "Chenrui Fan;Ming Li;Lichao Sun;Tianyi Zhou",
        "authorids": "~Chenrui_Fan1;~Ming_Li18;~Lichao_Sun1;~Tianyi_Zhou2",
        "gender": "M;M;M;",
        "homepage": "https://www.linkedin.com/in/chenrui-fan-a1b102298/;https://mingliiii.github.io/;https://lichao-sun.github.io/;",
        "dblp": "342/1752.html;;121/0780-1.html;",
        "google_scholar": "IR8PuQUAAAAJ;MpEoJegAAAAJ;WhGUE7AAAAAJ;",
        "orcid": "0000-0003-3183-2224;0009-0001-6491-4827;;",
        "linkedin": "chenrui-fan-a1b102298/;;lichao-sun-b273a290/;",
        "or_profile": "~Chenrui_Fan1;~Ming_Li18;~Lichao_Sun1;~Tianyi_Zhou2",
        "aff": "University of Maryland, College Park;University of Maryland, College Park;Lehigh University;",
        "aff_domain": "umd.edu;umd.edu;lehigh.edu;",
        "position": "MS student;PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\nfan2025missing,\ntitle={Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?},\nauthor={Chenrui Fan and Ming Li and Lichao Sun and Tianyi Zhou},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ufozo2Wc9e}\n}",
        "github": "",
        "project": "",
        "reviewers": "teYv;hrUF;gLKw;4FcV",
        "site": "https://openreview.net/forum?id=ufozo2Wc9e",
        "pdf_size": 0,
        "rating": "5;6;7;7",
        "confidence": "3;4;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.82915619758885
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "uh0Sf8yN7n",
        "title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advances in long-context reasoning abilities of language models led to interesting applications in large-scale multi-document summarization. However, prior work has shown that these long-context models are not effective at their claimed context windows. To this end, retrieval-augmented systems provide an efficient and effective alternative. However, their performance can be highly sensitive to the choice of retrieval context length. In this work, we present a hybrid method that combines retrieval-augmented systems with long-context windows supported by recent language models. Our method first estimates the optimal retrieval length as a function of the retriever, summarizer, and dataset. On a randomly sampled subset of the dataset, we use a panel of LMs to generate a pool of silver references. We use these silver references to estimate the optimal context length for a given RAG system configuration. Our results on the multi-document summarization task showcase the effectiveness of our method across model classes and sizes. We compare against length estimates from strong long-context benchmarks such as RULER and HELMET. Our analysis also highlights the effectiveness of our estimation method for very long-context LMs and its generalization to new classes of LMs.",
        "keywords": "retrieval-augmented generation;long-context;multi-document summarization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Adithya Pratapa;Teruko Mitamura",
        "authorids": "~Adithya_Pratapa1;~Teruko_Mitamura1",
        "gender": "M;F",
        "homepage": "https://adithya7.github.io/;http://www.cs.cmu.edu/~teruko",
        "dblp": "222/9370;90/785",
        "google_scholar": "BAT6abIAAAAJ;gjsxBCkAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Adithya_Pratapa1;~Teruko_Mitamura1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu",
        "position": "PhD student;Research Professor",
        "bibtex": "@inproceedings{\npratapa2025estimating,\ntitle={Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization},\nauthor={Adithya Pratapa and Teruko Mitamura},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uh0Sf8yN7n}\n}",
        "github": "",
        "project": "",
        "reviewers": "iKj6;3Jr7;h4ue;jEKZ",
        "site": "https://openreview.net/forum?id=uh0Sf8yN7n",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;3;2;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.7071067811865476
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.816496580927726
    },
    {
        "id": "uyX5Vnow3U",
        "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) are widely used as proxies for human labelers in both training (Reinforcement Learning from AI Feedback) and large-scale response evaluation (LLM-as-a-judge). Alignment and evaluation are critical components in the development of reliable LLMs, and the choice of feedback protocol plays a central role in both but remains understudied. In this work, we show that the choice of feedback protocol for evaluation (absolute scores versus relative preferences) can significantly affect evaluation reliability and induce systematic biases. In the context of LLM-as-a-judge evaluation, we show that pairwise protocols are more vulnerable to **distracted evaluation**. Generator models can exploit spurious attributes (or distractor features) favored by the LLM judge, resulting in inflated scores for lower-quality outputs. We find that absolute scoring is more robust to such manipulation, producing judgments that better reflect response quality and are less influenced by distractor features. Our results demonstrate that generator models can flip preferences by embedding distractor features, skewing LLM-as-a-judge comparisons and leading to inaccurate conclusions about model quality in benchmark evaluations. **Pairwise preferences flip in about 35\\% of the cases, compared to only 9\\% for absolute scores**. We offer recommendations for choosing feedback protocols based on dataset characteristics and evaluation objectives.",
        "keywords": "evaluation;data;alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tuhina Tripathi;Manya Wadhwa;Greg Durrett;Scott Niekum",
        "authorids": "~Tuhina_Tripathi1;~Manya_Wadhwa1;~Greg_Durrett1;~Scott_Niekum1",
        "gender": "F;;M;M",
        "homepage": "https://tuhina2313.github.io/;https://manyawadhwa.github.io/;http://www.cs.utexas.edu/~gdurrett/;https://people.cs.umass.edu/~sniekum/index.php",
        "dblp": ";192/2524.html;69/7968;62/8399",
        "google_scholar": ";4_uMjzgAAAAJ;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ;4wXYfSUAAAAJ",
        "orcid": ";;;",
        "linkedin": ";manya-wadhwa-9798a79a/;;",
        "or_profile": "~Tuhina_Tripathi1;~Manya_Wadhwa1;~Greg_Durrett1;~Scott_Niekum1",
        "aff": "University of Massachusetts at Amherst;University of Texas at Austin;University of Texas at Austin;University of Massachusetts at Amherst",
        "aff_domain": "umass.edu;utexas.edu;utexas.edu;umass.edu",
        "position": "PhD student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\ntripathi2025pairwise,\ntitle={Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in {LLM}-Based Evaluation},\nauthor={Tuhina Tripathi and Manya Wadhwa and Greg Durrett and Scott Niekum},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uyX5Vnow3U}\n}",
        "github": "",
        "project": "",
        "reviewers": "nMV3;9wPX;kig2;C657",
        "site": "https://openreview.net/forum?id=uyX5Vnow3U",
        "pdf_size": 0,
        "rating": "4;5;5;6",
        "confidence": "3;4;4;5",
        "wc_review": "",
        "rating_avg": [
            5.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "uzauWUW9u3",
        "title": "News is More than a Collection of Facts: Moral Frame Preserving News Summarization",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "News articles are more than collections of facts; they reflect journalists' framing, shaping how events are presented to the audience. One key aspect of framing is the choice to write in (or quote verbatim) morally charged language as opposed to using neutral terms. This moral framing carries implicit judgments that automated news summarizers should recognize and preserve to maintain the original intent of the writer. In this work, we perform the first study on the preservation of moral framing in AI-generated news summaries. We propose an approach that leverages the intuition that journalists intentionally use or report specific moral-laden words, which should be retained in summaries. Through automated, crowd-sourced, and expert evaluations, we demonstrate that our approach enhances the preservation of moral framing while maintaining overall summary quality.",
        "keywords": "LLMs;news;summarization;morality;framing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Enrico Liscio;Michela Lorandi;Pradeep K. Murukannaiah",
        "authorids": "~Enrico_Liscio1;~Michela_Lorandi1;~Pradeep_K._Murukannaiah1",
        "gender": "M;F;",
        "homepage": "https://enricoliscio.github.io/;;",
        "dblp": "294/2674;287/9344;",
        "google_scholar": "g8X6yQkAAAAJ;USpEfyQAAAAJ;",
        "orcid": "0000-0002-8285-5867;0000-0002-6131-8763;",
        "linkedin": "enrico-liscio-aa7a64135/;michela-lorandi-0322881a0;",
        "or_profile": "~Enrico_Liscio1;~Michela_Lorandi1;~Pradeep_K._Murukannaiah1",
        "aff": "Delft University of Technology;Dublin City University;",
        "aff_domain": "tudelft.nl;dcu.ie;",
        "position": "Postdoc;PhD student;",
        "bibtex": "@inproceedings{\nliscio2025news,\ntitle={News is More than a Collection of Facts: Moral Frame Preserving News Summarization},\nauthor={Enrico Liscio and Michela Lorandi and Pradeep K. Murukannaiah},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=uzauWUW9u3}\n}",
        "github": "",
        "project": "",
        "reviewers": "zVNp;RS8W;iyXe;thoM",
        "site": "https://openreview.net/forum?id=uzauWUW9u3",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "4;3;4;5",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.6324555320336758
    },
    {
        "id": "vBcGnragkr",
        "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models accumulate vast amounts of knowledge during their pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, with performance plateauing before they acquire precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall.\nSecond, the training data distribution significantly impacts learning dynamics, with imbalanced distributions shortening the plateau.\nFinally, hallucinations appear simultaneously to knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric associative memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.",
        "keywords": "learning dynamics;factual recall;curricula;data distribution;hallucinations",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Nicolas Zucchet;Jorg Bornschein;Stephanie C.Y. Chan;Andrew Kyle Lampinen;Razvan Pascanu;Soham De",
        "authorids": "~Nicolas_Zucchet1;~Jorg_Bornschein1;~Stephanie_C.Y._Chan1;~Andrew_Kyle_Lampinen1;~Razvan_Pascanu1;~Soham_De2",
        "gender": "M;M;F;M;M;M",
        "homepage": ";;https://scychan.github.io/;https://github.com/google/BIG-bench;https://razp.info;https://sohamde.github.io",
        "dblp": "289/6252;13/8510;255/7866;https://dblp.uni-trier.de/pers/hd/l/Lampinen:Andrew_K=;65/8368.html;124/9197",
        "google_scholar": "cLhZY44AAAAJ;X7kZFnoAAAAJ;https://scholar.google.com/citations?hl=en;_N44XxAAAAAJ;https://scholar.google.ca/citations?user=eSPY8LwAAAAJ;lHf55pF3KVQC",
        "orcid": ";0000-0002-3356-7922;;;;",
        "linkedin": "nicolas-zucchet-7a84a6139/;;scychan;;;",
        "or_profile": "~Nicolas_Zucchet1;~Jorg_Bornschein1;~Stephanie_C.Y._Chan1;~Andrew_Kyle_Lampinen1;~Razvan_Pascanu1;~Soham_De2",
        "aff": "ETHZ - ETH Zurich;Google Deepmind;Google DeepMind;Google DeepMind;Mila - Quebec Artificial Intelligence Institute+Google DeepMind;Google DeepMind",
        "aff_domain": "ethz.ch;google.com;deepmind.com;google.com;mila.quebec+google.com;google.com",
        "position": "PhD student;Research Scientist;Research Scientist;Research Scientist;Affiliate Member+Research Scientist;Research Scientist",
        "bibtex": "@inproceedings{\nzucchet2025how,\ntitle={How do language models learn facts? Dynamics, curricula and hallucinations},\nauthor={Nicolas Zucchet and Jorg Bornschein and Stephanie C.Y. Chan and Andrew Kyle Lampinen and Razvan Pascanu and Soham De},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vBcGnragkr}\n}",
        "github": "",
        "project": "",
        "reviewers": "EVVD;ERnd;84Ed;HLX1",
        "site": "https://openreview.net/forum?id=vBcGnragkr",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.816496580927726
    },
    {
        "id": "vDr0RV3590",
        "title": "Do Biased Models Have Biased Thoughts?",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: *Do biased models have biased thoughts*? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.",
        "keywords": "Bias in language models;Large Language Models;biased thoughts;Chain-of-Thought prompting",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Swati Rajwal;Shivank Garg;Reem Abdel-Salam;Abdelrahman Zayed",
        "authorids": "~Swati_Rajwal2;~Shivank_Garg1;~Reem_Abdel-Salam1;~Abdelrahman_Zayed1",
        "gender": "F;M;;M",
        "homepage": "https://swati-rajwal.github.io/;;;https://abdelrahmanzayed.github.io/",
        "dblp": "325/7923;358/6393;;",
        "google_scholar": "6AfEraYAAAAJ;;https://scholar.google.com/citations?hl=en;kg5hcO0AAAAJ",
        "orcid": "0000-0002-3826-5069;;0000-0002-4594-211X;",
        "linkedin": "swatirajwal/;shivank-garg21/;;abdelrahman-zayed-669382103/",
        "or_profile": "~Swati_Rajwal2;~Shivank_Garg1;~Reem_Abdel-Salam1;~Abdelrahman_Zayed1",
        "aff": "Emory University+Boston Children's Hospital (aff. Harvard Medical School);Indian Institute of Technology,Roorkee;Faculty of Engineering Cairo University, Cairo University;Montreal Institute for Learning Algorithms, University of Montreal, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "emory.edu+harvard.edu;iitr.ac.in;eng.cu.edu.eg;mila.umontreal.ca",
        "position": "PhD student+Researcher;Undergrad student;PhD student;PhD student",
        "bibtex": "@inproceedings{\nrajwal2025do,\ntitle={Do Biased Models Have Biased Thoughts?},\nauthor={Swati Rajwal and Shivank Garg and Reem Abdel-Salam and Abdelrahman Zayed},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vDr0RV3590}\n}",
        "github": "",
        "project": "",
        "reviewers": "iGTF;bsru;kJ6k;d6Ma",
        "site": "https://openreview.net/forum?id=vDr0RV3590",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "4;2;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "vNJbDhgrM4",
        "title": "Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent evidence suggests Large Language Models (LLMs) display Theory of Mind (ToM) abilities. Most ToM experiments place participants in a spectatorial role, wherein they predict and interpret other agents' behavior. However, human ToM also contributes to dynamically planning action and strategically intervening on others' mental states. We present MindGames: a novel `planning theory of mind' (PToM) task which requires agents to infer an interlocutor's beliefs and desires to persuade them to alter their behavior. Unlike previous evaluations, we explicitly evaluate use cases of ToM. We find that humans significantly outperform o1-preview (an LLM) at our PToM task (11% higher; $p=0.006$). We hypothesize this is because humans have an implicit causal model of other agents (e.g., they know, as our task requires, to ask about people's preferences). In contrast, o1-preview outperforms humans in a baseline condition which requires a similar amount of planning but minimal mental state inferences (e.g., o1-preview is better than humans at planning when already given someone's preferences). These results suggest a significant gap between human-like social reasoning and LLM abilities.",
        "keywords": "theory of mind;planning;causal model;persuasion",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jared Moore;Ned Cooper;Rasmus Overmark;Beba Cibralic;Cameron Robert Jones;Nick Haber",
        "authorids": "~Jared_Moore1;~Ned_Cooper1;~Rasmus_Overmark1;~Beba_Cibralic1;~Cameron_Robert_Jones1;~Nick_Haber1",
        "gender": ";;M;;M;",
        "homepage": ";;https://research-portal.st-andrews.ac.uk/en/persons/rasmus-overmark;;https://camrobjones.com;",
        "dblp": ";;;;;179/4983",
        "google_scholar": ";;https://scholar.google.co.uk/citations?user=i8tTD_QAAAAJ;;mhU_tUgAAAAJ;euNCoVYAAAAJ",
        "orcid": ";;0000-0002-1554-6765;;0000-0002-6609-8966;0000-0001-8804-7804",
        "linkedin": ";;;beba-cibralic-phd-a74106232;;",
        "or_profile": "~Jared_Moore1;~Ned_Cooper1;~Rasmus_Overmark1;~Beba_Cibralic1;~Cameron_Robert_Jones1;~Nick_Haber1",
        "aff": ";;University of St. Andrews;RAND Corporation;University of California, San Diego;Stanford University",
        "aff_domain": ";;st-andrews.ac.uk;rand.org;ucsd.edu;stanford.edu",
        "position": ";;PhD student;Researcher;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nmoore2025do,\ntitle={Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task},\nauthor={Jared Moore and Ned Cooper and Rasmus Overmark and Beba Cibralic and Cameron Robert Jones and Nick Haber},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vNJbDhgrM4}\n}",
        "github": "",
        "project": "",
        "reviewers": "UEVQ;CxkX;7dfj;g1gx",
        "site": "https://openreview.net/forum?id=vNJbDhgrM4",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "5;4;2;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.75,
            1.0897247358851685
        ],
        "replies_avg": [
            34,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5129891760425771
    },
    {
        "id": "vSMCBUgrQj",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models\u2014a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities.\nIn this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. \nLeveraging several key design strategies\u2014such as adjusting format reward and controlling query difficulty\u2014we achieve substantial improvements in both reasoning accuracy and response length across most settings.\nHowever, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the ``aha moment'' for the first time in small models not from the Qwen family.\nWe share the key designs that enable successful zero RL training, along with our findings and practices. \nTo facilitate further research, we open-source the  code, models, and analysis tools.",
        "keywords": "Reasoning;Large Language Model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weihao Zeng;Yuzhen Huang;Qian Liu;Wei Liu;Keqing He;Zejun MA;Junxian He",
        "authorids": "~Weihao_Zeng2;~Yuzhen_Huang2;~Qian_Liu2;~Wei_Liu25;~Keqing_He1;~Zejun_MA1;~Junxian_He1",
        "gender": "M;M;M;M;;M;M",
        "homepage": "https://zeng-wh.github.io/;https://hyz17.github.io/;http://siviltaram.github.io/;https://vpeterv.github.io/;https://helicqin.github.io/about/index.html;;https://jxhe.github.io",
        "dblp": "174/3836;;;49/3283-131;79/2314;;188/6127.html",
        "google_scholar": ";XZK8cewAAAAJ;bcbeUo0AAAAJ;https://scholar.google.com/citations?hl=zh-CN;811USNoAAAAJ;https://scholar.google.com/citations?hl=zh-CN;BIFGeoUAAAAJ",
        "orcid": ";;;0000-0003-2195-2310;;;",
        "linkedin": ";;;;;zejun-ma-58614365/;",
        "or_profile": "~Weihao_Zeng2;~Yuzhen_Huang2;~Qian_Liu2;~Wei_Liu25;~Keqing_He1;~Zejun_MA1;~Junxian_He1",
        "aff": "Microsoft+Beijing University of Posts and Telecommunications;Hong Kong University of Science and Technology;TikTok;Hong Kong University of Science and Technology;Meituan Group;TikTok;Hong Kong University of Science and Technology",
        "aff_domain": "microsoft.com+bupt.edu.cn;ust.hk;bytedance.com;connect.ust.hk;meituan.com;bytedance.com;ust.hk",
        "position": "Intern+MS student;PhD student;Researcher;PhD student;Researcher;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nzeng2025simplerlzoo,\ntitle={Simple{RL}-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild},\nauthor={Weihao Zeng and Yuzhen Huang and Qian Liu and Wei Liu and Keqing He and Zejun MA and Junxian He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vSMCBUgrQj}\n}",
        "github": "",
        "project": "",
        "reviewers": "S5Rk;7p56;ccrE;9Rvw",
        "site": "https://openreview.net/forum?id=vSMCBUgrQj",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "3;3;2;3",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5222329678670935
    },
    {
        "id": "vTAz44GgOA",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we propose Critique Fine-Tuning (CFT), a method more effective than SFT for reasoning tasks. Instead of simply imitating correct responses, CFT trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding--traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct multiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where GPT-4o serves as the teacher to generate critiques in the form of ([query; noisy response], critique). Experiments on these datasets demonstrate that CFT consistently outperforms SFT by 4--10% across six mathematical reasoning benchmarks, and is effective across different base models including Qwen2.5, Qwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only requires 1 hour of training on 8xH100 over the 50K examples, yet matches or outperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it matches the performance of SimpleRL, which is a DeepSeek-r1 replication trained with 140x more compute. Experiments on IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance the model's general generation and instruction-following capabilities, outperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies show that CFT is robust to noisy response sources and teacher critique models. These findings highlight that CFT offers a more effective alternative to advance the reasoning of language models.",
        "keywords": "Reasoning;Large Language Model;Fine-Tuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yubo Wang;Xiang Yue;Wenhu Chen",
        "authorids": "~Yubo_Wang9;~Xiang_Yue1;~Wenhu_Chen3",
        "gender": "M;;",
        "homepage": ";;",
        "dblp": ";;",
        "google_scholar": "s_ZW7voAAAAJ;;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Yubo_Wang9;~Xiang_Yue1;~Wenhu_Chen3",
        "aff": "University of Waterloo;;",
        "aff_domain": "uwaterloo.ca;;",
        "position": "PhD student;;",
        "bibtex": "@inproceedings{\nwang2025critique,\ntitle={Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate},\nauthor={Yubo Wang and Xiang Yue and Wenhu Chen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vTAz44GgOA}\n}",
        "github": "",
        "project": "",
        "reviewers": "XEXb;tGLF;G4No;Yors",
        "site": "https://openreview.net/forum?id=vTAz44GgOA",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "3;4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "vgmiRvpCLA",
        "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation).\nHowever, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align.\nTherefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering.\nWe propose a method to transform each dataset to enable parallel probability- and generation-based evaluation.\nThen, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances.\nFinally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations.\nBased on our findings, we provide recommendations for future evaluations of LLM misgendering.\nOur results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.",
        "keywords": "fairness;meta-evaluation;misgendering",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arjun Subramonian;Vagrant Gautam;Preethi Seshadri;Dietrich Klakow;Kai-Wei Chang;Yizhou Sun",
        "authorids": "~Arjun_Subramonian1;~Vagrant_Gautam1;~Preethi_Seshadri2;~Dietrich_Klakow1;~Kai-Wei_Chang1;~Yizhou_Sun1",
        "gender": ";Agender;F;M;M;F",
        "homepage": ";https://dippedrusk.com/;https://preethiseshadri518.github.io/;https://www.lsv.uni-saarland.de/;http://kwchang.net;http://web.cs.ucla.edu/~yzsun/",
        "dblp": ";344/3375;175/6462.html;00/1846;18/2428;37/3868",
        "google_scholar": ";BG7ORjIAAAAJ;;https://scholar.google.de/citations?user=_HtGYmoAAAAJ;fqDBtzYAAAAJ;https://scholar.google.com.tw/citations?user=TQgOjK0AAAAJ",
        "orcid": ";0000-0002-7263-8578;;0000-0002-4147-9690;0000-0001-5365-0072;",
        "linkedin": ";dippedrusk/;preethi-seshadri/;https://www.linkedin.com/feed/?trk=DACH-SEM_google-adwords_brand-ghpwwww.l;kai-wei-chang-41239040;",
        "or_profile": "~Arjun_Subramonian1;~Vagrant_Gautam1;~Preethi_Seshadri2;~Dietrich_Klakow1;~Kai-Wei_Chang1;~Yizhou_Sun1",
        "aff": ";Saarland University;University of California, Irvine;Saarland University;University of California, Los Angeles+Amazon;University of California, Los Angeles",
        "aff_domain": ";uni-saarland.de;uci.edu;saarland.de;ucla.edu+amazon.com;ucla.edu",
        "position": ";PhD student;PhD student;Full Professor;Associate Professor+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nsubramonian2025agree,\ntitle={Agree to Disagree? A Meta-Evaluation of {LLM} Misgendering},\nauthor={Arjun Subramonian and Vagrant Gautam and Preethi Seshadri and Dietrich Klakow and Kai-Wei Chang and Yizhou Sun},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vgmiRvpCLA}\n}",
        "github": "",
        "project": "",
        "reviewers": "Zan3;U3xB;fDmf;xsTw",
        "site": "https://openreview.net/forum?id=vgmiRvpCLA",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "3;2;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.25,
            0.82915619758885
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "vlUk8z8LaM",
        "title": "Positional Biases Shift as Inputs Approach Context Window Limits",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) often struggle to use information across long inputs effectively. \nPrior work has identified positional biases, such as the Lost in the Middle (LiM) effect, where models perform better when information appears at the beginning (primacy bias) or end (recency bias) of the input, rather than in the middle. \nHowever, long-context studies have not consistently replicated these effects, raising questions about their intensity and the conditions under which they manifest.\nTo address this, we conducted a comprehensive analysis using relative rather than absolute input lengths, defined with respect to each model\u2019s context window. \nOur findings reveal that the LiM effect is strongest when inputs occupy up to 50\\% of a model\u2019s context window.\nBeyond that, the primacy bias weakens, while recency bias remains relatively stable.\nThis effectively eliminates the LiM effect; instead, we observe a distance-based bias, where model performance is better when relevant information is closer to the end of the input.\nFurthermore, our results suggest that successful retrieval is a prerequisite for reasoning in LLMs, and that the observed positional biases in reasoning are largely inherited from retrieval. These insights have implications for long-context tasks, the design of future LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.",
        "keywords": "Long-context understanding;positional biases",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Blerta Veseli;Julian Chibane;Mariya Toneva;Alexander Koller",
        "authorids": "~Blerta_Veseli1;~Julian_Chibane1;~Mariya_Toneva1;~Alexander_Koller2",
        "gender": "F;;;",
        "homepage": "https://www.mpi-sws.org/people/bveseli/;https://virtualhumans.mpi-inf.mpg.de/people/Chibane.html;;",
        "dblp": ";260/0133;;",
        "google_scholar": ";;;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Blerta_Veseli1;~Julian_Chibane1;~Mariya_Toneva1;~Alexander_Koller2",
        "aff": "MPI-SWS+Universit\u00e4t des Saarlandes;Saarland Informatics Campus, Max-Planck Institute;;",
        "aff_domain": "mpi-sws.org+uni-saarland.de;mpi-inf.mpg.de;;",
        "position": "PhD student+PhD student;PhD student;;",
        "bibtex": "@inproceedings{\nveseli2025positional,\ntitle={Positional Biases Shift as Inputs Approach Context Window Limits},\nauthor={Blerta Veseli and Julian Chibane and Mariya Toneva and Alexander Koller},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vlUk8z8LaM}\n}",
        "github": "",
        "project": "",
        "reviewers": "DcAC;db9o;rJZo",
        "site": "https://openreview.net/forum?id=vlUk8z8LaM",
        "pdf_size": 0,
        "rating": "6;6;6",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            11,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "vlyl9xZVAL",
        "title": "Improving Table Understanding with LLMs and Entity-Oriented Search",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Our work addresses the challenges of understanding tables. Existing methods often struggle with the unpredictable nature of table content, leading to a reliance on preprocessing and keyword matching. They also face limitations due to the lack of contextual information, which complicates the reasoning processes of large language models (LLMs). To overcome these challenges, we introduce an entity-oriented search method to improve table understanding with LLMs. This approach effectively leverages the semantic similarities between questions and table data, as well as the implicit relationships between table cells, minimizing the need for data preprocessing and keyword matching. Additionally, it focuses on table entities, ensuring that table cells are semantically tightly bound, thereby enhancing contextual clarity. Furthermore, we pioneer the use of a graph query language for table understanding, establishing a new research direction. Experiments show that our approach achieves new state-of-the-art performances on standard benchmarks WikiTableQuestions and TabFact.",
        "keywords": "table understanding;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Thi-Nhung Nguyen;Hoang Ngo;Dinh Phung;Thuy-Trang Vu;Dat Quoc Nguyen",
        "authorids": "~Thi-Nhung_Nguyen1;~Hoang_Ngo1;~Dinh_Phung2;~Thuy-Trang_Vu1;~Dat_Quoc_Nguyen1",
        "gender": "F;;;;",
        "homepage": "https://nhungnt7.github.io/;;;;http://datquocnguyen.github.io",
        "dblp": "305/9765;;;228/5538;23/9125",
        "google_scholar": "LvHj1fAAAAAJ;;;https://scholar.google.com.au/citations?user=cx2eAe0AAAAJ;HVl7vyEAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Thi-Nhung_Nguyen1;~Hoang_Ngo1;~Dinh_Phung2;~Thuy-Trang_Vu1;~Dat_Quoc_Nguyen1",
        "aff": "VinAI Research;;;Monash University;Qualcomm AI Research+VinAI Research, Vietnam",
        "aff_domain": "vinai.io;;;monash.edu;qti.qualcomm.com+vinai.io",
        "position": "Researcher;;;Assistant Professor;Principal Researcher+Senior Research Scientist",
        "bibtex": "@inproceedings{\nnguyen2025improving,\ntitle={Improving Table Understanding with {LLM}s and Entity-Oriented Search},\nauthor={Thi-Nhung Nguyen and Hoang Ngo and Dinh Phung and Thuy-Trang Vu and Dat Quoc Nguyen},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vlyl9xZVAL}\n}",
        "github": "",
        "project": "",
        "reviewers": "uSxF;eigU;G4kS;k1Zo",
        "site": "https://openreview.net/forum?id=vlyl9xZVAL",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;5;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            15,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.3333333333333333
    },
    {
        "id": "vqN8uom4A1",
        "title": "Base Models Beat Aligned Models at Randomness and Creativity",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate ``7'' over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.",
        "keywords": "alignment;pretrained;limitations;limits;capabilities;randomness;creativity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peter West;Christopher Potts",
        "authorids": "~Peter_West1;~Christopher_Potts1",
        "gender": "M;M",
        "homepage": "https://peterwestai.notion.site/;http://web.stanford.edu/~cgpotts/",
        "dblp": "179/4587;13/2617",
        "google_scholar": "https://scholar.google.ca/citations?user=9ubCBYwAAAAJ;3j08YoAAAAAJ",
        "orcid": ";0000-0002-7978-6055",
        "linkedin": ";",
        "or_profile": "~Peter_West1;~Christopher_Potts1",
        "aff": "Stanford University;Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "position": "Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nwest2025base,\ntitle={Base Models Beat Aligned Models at Randomness and Creativity},\nauthor={Peter West and Christopher Potts},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vqN8uom4A1}\n}",
        "github": "",
        "project": "",
        "reviewers": "bHor;u55E;beTY;D7L6;wJrW;ze1Q",
        "site": "https://openreview.net/forum?id=vqN8uom4A1",
        "pdf_size": 0,
        "rating": "5;6;6;6;7;8",
        "confidence": "4;3;4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.9428090415820634
        ],
        "confidence_avg": [
            3.8333333333333335,
            0.3726779962499649
        ],
        "replies_avg": [
            26,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.158113883008419
    },
    {
        "id": "vv1ZyQF8LD",
        "title": "The Zero Body Problem: Probing LLM Use of Sensory Language",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Sensory language expresses embodied experiences ranging from taste and sound to excitement and stomachache. It is of interest to scholars from a wide range of domains including robotics, narratology, linguistics, and cognitive science. In this work, we explore whether language models, which are not embodied, can approximate human use of embodied language. To do this, we extend an existing corpus of parallel human and model responses to short story prompts with an additional 18,000 stories generated by 18 popular language models. We find that all models generate stories that differ significantly from human usage of sensory language. However, the direction of these differences varies considerably between model families; Gemini models use significantly more sensory language than humans along most axes whereas most models from the remaining five families use significantly less. Linear probes run on five models suggest that they are capable of \\textit{identifying} sensory language, meaning an inability to recognize sensory content is unlikely to be the cause of the observed differences. Instead, we find preliminary evidence indicating that instruction tuning may discourage usage of sensory language in some models. To support further work, we release \\href{https://github.com/srhm-ca/sensorylanguage}{our expanded story dataset.}",
        "keywords": "model evaluation;model interpretability;sensory language;model creativity",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rebecca M. M. Hicke;Sil Hamilton;David Mimno",
        "authorids": "~Rebecca_M._M._Hicke1;~Sil_Hamilton1;~David_Mimno1",
        "gender": "F;M;M",
        "homepage": "https://rmatouschekh.github.io/;https://srhm.ca/;https://mimno.infosci.cornell.edu/",
        "dblp": ";331/2834;39/5487",
        "google_scholar": "mBqvEqMAAAAJ;dn6wkPAAAAAJ;uBFV6SUAAAAJ",
        "orcid": ";0000-0002-6579-4628;",
        "linkedin": ";srhm/;",
        "or_profile": "~Rebecca_M._M._Hicke1;~Sil_Hamilton1;~David_Mimno1",
        "aff": "Cornell University;Cornell University;Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu",
        "position": "PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nhicke2025the,\ntitle={The Zero Body Problem: Probing {LLM} Use of Sensory Language},\nauthor={Rebecca M. M. Hicke and Sil Hamilton and David Mimno},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=vv1ZyQF8LD}\n}",
        "github": "",
        "project": "",
        "reviewers": "CgD1;gy69;3hwK;GD7q",
        "site": "https://openreview.net/forum?id=vv1ZyQF8LD",
        "pdf_size": 0,
        "rating": "5;5;6;7",
        "confidence": "4;4;2;4",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.82915619758885
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.17407765595569782
    },
    {
        "id": "w5DSwn9wTC",
        "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.",
        "keywords": "Mechanistic Interpretability;Instruction-tuning;Post-training;Alignment",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hongzhe Du;Weikai Li;Min Cai;Karim Saraipour;Zimin Zhang;Himabindu Lakkaraju;Yizhou Sun;Shichang Zhang",
        "authorids": "~Hongzhe_Du1;~Weikai_Li2;~Min_Cai2;~Karim_Saraipour1;~Zimin_Zhang1;~Himabindu_Lakkaraju1;~Yizhou_Sun1;~Shichang_Zhang2",
        "gender": "M;M;;M;;;F;M",
        "homepage": ";https://weikai-li.github.io;https://github.com/HenryCai11;;;;http://web.cs.ucla.edu/~yzsun/;https://shichangzh.github.io/",
        "dblp": ";157/3533-2;;;;;37/3868;234/4118",
        "google_scholar": "SQAtJvwAAAAJ;https://scholar.google.com/citations?hl=en;;;;;https://scholar.google.com.tw/citations?user=TQgOjK0AAAAJ;TYqG0x4AAAAJ",
        "orcid": ";0000-0002-5801-9500;;;0009-0001-4221-9268;;;0000-0003-0954-5018",
        "linkedin": ";weikai-li;;karimsara/;zimin-zhang/;;;shichang-zhang-4430a4106/",
        "or_profile": "~Hongzhe_Du1;~Weikai_Li2;~Min_Cai2;~Karim_Saraipour1;~Zimin_Zhang1;~Himabindu_Lakkaraju1;~Yizhou_Sun1;~Shichang_Zhang2",
        "aff": "UCLA Computer Science Department, University of California, Los Angeles;UCLA Computer Science Department, University of California, Los Angeles;University of Alberta;UCLA Computer Science Department, University of California, Los Angeles;University of Illinois, Urbana Champaign;;University of California, Los Angeles;Harvard Business School",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;ualberta.ca;cs.ucla.edu;illinois.edu;;ucla.edu;hbs.edu",
        "position": "MS student;PhD student;PhD student;MS student;MS student;;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\ndu2025how,\ntitle={How Post-Training Reshapes {LLM}s: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence},\nauthor={Hongzhe Du and Weikai Li and Min Cai and Karim Saraipour and Zimin Zhang and Himabindu Lakkaraju and Yizhou Sun and Shichang Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=w5DSwn9wTC}\n}",
        "github": "",
        "project": "",
        "reviewers": "K28J;vQ6Y;WfuA;bJdd",
        "site": "https://openreview.net/forum?id=w5DSwn9wTC",
        "pdf_size": 0,
        "rating": "6;7;8;9",
        "confidence": "4;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            1.118033988749895
        ],
        "confidence_avg": [
            4.0,
            0.0
        ],
        "replies_avg": [
            20,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "wKVtjs0w4a",
        "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation meth-\nods often overlook biases in long-form responses and the intrinsic variability of\nLLM outputs. To address these challenges, we propose FiSCo (Fine-grained Se-\nmantic Comparison), a novel statistical framework to evaluate group-level fairness\nin LLMs by detecting subtle semantic differences in long-form responses across\ndemographic groups. Unlike prior work focusing on sentiment or token-level\ncomparisons, FiSCo goes beyond surface-level analysis by operating at the claim\nlevel, leveraging entailment checks to assess the consistency of meaning across\nresponses. We decompose model outputs into semantically distinct claims and\napply statistical hypothesis testing to compare inter- and intra-group similarities,\nenabling robust detection of subtle biases. We formalize a new group counterfac-\ntual fairness definition and validate FiSCo on both synthetic and human-annotated\ndatasets spanning gender, race, and age. Experiments show that FiSCo more\nreliably identifies nuanced biases while reducing the impact of stochastic LLM\nvariability, outperforming various evaluation metrics.",
        "keywords": "Fairness;Bias;Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weijie Xu;Yiwen Wang;Chi Xue;Xiangkun Hu;Xi Fang;Guimin Dong;Chandan K. Reddy",
        "authorids": "~Weijie_Xu1;~Yiwen_Wang4;~Chi_Xue1;~Xiangkun_Hu1;~Xi_Fang3;~Guimin_Dong1;~Chandan_K._Reddy1",
        "gender": "M;;F;M;F;M;M",
        "homepage": "https://www.weijiexu.com;;;;;https://scholar.google.com/citations?user=Ce7TZKkAAAAJ&hl=en;https://creddy.net/",
        "dblp": "195/1675;;91/8492;224/5990;;289/6358;42/1341",
        "google_scholar": "lWjp-dQAAAAJ;;GkDQfSkAAAAJ;_-0MpawAAAAJ;L3n-CJIAAAAJ;Ce7TZKkAAAAJ;LoXnMOIAAAAJ",
        "orcid": ";;0000-0001-5918-6928;;;;",
        "linkedin": "weijie-xu-936b23101/;;;;xi-fang-397620142/;;",
        "or_profile": "~Weijie_Xu1;~Yiwen_Wang4;~Chi_Xue1;~Xiangkun_Hu1;~Xi_Fang3;~Guimin_Dong1;~Chandan_K._Reddy1",
        "aff": "Amazon;;Amazon;Amazon;Amazon;Amazon;Virginia Tech+Amazon",
        "aff_domain": "amazon.com;;amazon.com;amazon.com;amazon.com;amazon.com;vt.edu+amazon.com",
        "position": "Researcher;;Applied Scientist;Applied Scientist;Researcher;Researcher;Full Professor+Amazon Scholar",
        "bibtex": "@inproceedings{\nxu2025quantifying,\ntitle={Quantifying Fairness in {LLM}s Beyond Tokens: A Semantic and Statistical Perspective},\nauthor={Weijie Xu and Yiwen Wang and Chi Xue and Xiangkun Hu and Xi Fang and Guimin Dong and Chandan K. Reddy},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=wKVtjs0w4a}\n}",
        "github": "",
        "project": "",
        "reviewers": "Uir7;QtrT;iCei;DXZz",
        "site": "https://openreview.net/forum?id=wKVtjs0w4a",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "wRcTCcb0H5",
        "title": "Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a Moderately Resourced Setting",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English.  To ensure effective and responsible alignment, we leverage translated instruction datasets, a Kazakhstan-specific instruction dataset that is automatically constructed and manually verified, and Kazakh-specific safety data. We release Sherkala-Chat (8B) as an open-weight model, along with a detailed description of its training, alignment, and evaluation, to support research and real-world applications for Kazakh speakers.",
        "keywords": "LLM;LLaMA-3.1;Kazakh;low-resource language modeling;fine-tuning;safety alignment;model evaluation;generative AI",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Fajri Koto;Rituraj Joshi;Nurdaulet Mukhituly;Yuxia Wang;Zhuohan Xie;Rahul Pal;Daniil Orel;Parvez Mullah;Diana Turmakhan;Maiya Goloburda;Mohammed Kamran;Samujjwal Ghosh;Bokang Jia;Jonibek Mansurov;Mukhammed Togmanov;Debopriyo Banerjee;Nurkhan Laiyk;Akhmed Sakip;Xudong Han;Ekaterina Kochmar;Alham Fikri Aji;Aaryamonvikram Singh;Alok Anil Jadhav;Satheesh Katipomu;Samta Kamboj;Monojit Choudhury;Gurpreet Gosal;Gokulakrishnan Ramakrishnan;Biswajit Mishra;Sarath Chandran;Avraham Sheinin;Natalia Vassilieva;Neha Sengupta;Preslav Nakov",
        "authorids": "~Fajri_Koto1;~Rituraj_Joshi1;~Nurdaulet_Mukhituly1;~Yuxia_Wang1;~Zhuohan_Xie1;~Rahul_Pal1;~Daniil_Orel1;~Parvez_Mullah1;~Diana_Turmakhan1;~Maiya_Goloburda1;~Mohammed_Kamran1;~Samujjwal_Ghosh1;~Bokang_Jia1;~Jonibek_Mansurov1;~Mukhammed_Togmanov1;~Debopriyo_Banerjee1;~Nurkhan_Laiyk1;~Akhmed_Sakip2;~Xudong_Han2;~Ekaterina_Kochmar2;~Alham_Fikri_Aji1;~Aaryamonvikram_Singh1;~Alok_Anil_Jadhav1;~Satheesh_Katipomu1;~Samta_Kamboj1;~Monojit_Choudhury1;~Gurpreet_Gosal2;~Gokulakrishnan_Ramakrishnan1;~Biswajit_Mishra1;~Sarath_Chandran1;~Avraham_Sheinin1;~Natalia_Vassilieva1;~Neha_Sengupta1;~Preslav_Nakov2",
        "gender": "M;M;M;F;M;M;Not Specified;M;F;F;M;M;;M;M;M;;M;M;;M;M;M;M;;M;M;M;M;M;M;F;F;M",
        "homepage": "https://fajrikoto.com/;https://www.riturajnotes.wordpress.com;;;https://www.linkedin.com/in/zhuohanxie/;;https://www.linkedin.com/in/daniilorel/;;;;;;;;https://scholar.google.ru/citations?user=XSyRQakAAAAJ&hl=ru&authuser=4&oi=ao;https://dpbanerjee.site;;;https://hanxudong.github.io/;https://ekochmar.github.io/about/;;;;;;https://mbzuai.ac.ae/study/faculty/monojit-choudhury/;;;;;;;;https://mbzuai.ac.ae/study/faculty/preslav-nakov/",
        "dblp": "160/0019;;400/4193.html;78/7732;220/7055;;343/4317;;;397/8203.html;;218/0839;;348/5307;;154/8755.html;397/7359;;22/10151;140/3465.html;188/8762;401/5838;;;;29/5841;;;;;;97/1208.html;24/10955;https://dblp.uni-trier.de/pid/19/1947",
        "google_scholar": "RA9l3s4AAAAJ;;;dciz7yMAAAAJ;W9mk-R4AAAAJ;;https://scholar.google.com/citations?hl=en;;;kbLuVE4AAAAJ;;https://scholar.google.co.in/citations?user=kROmZdsAAAAJ;;;https://scholar.google.ru/citations?user=XSyRQakAAAAJ;GOOXm9gAAAAJ;https://scholar.google.co.uk/citations?hl=en;;snUr8n8AAAAJ;https://scholar.google.co.uk/citations?user=e2HTYnkAAAAJ;0Cyfqv4AAAAJ;;;;;WR1ImCMAAAAJ;https://scholar.google.com/citations?view_op=list_works;;;https://scholar.google.com/citations?hl=en;;https://scholar.google.ru/citations?user=nqkl6MwAAAAJ;WOlrxxwAAAAJ;DfXsKZ4AAAAJ",
        "orcid": ";;;;0009-0008-2650-2857;;;;;;;0000-0003-2859-7358;;0000-0003-2249-4024;;0000-0001-9773-776X;;;0000-0002-6877-7006;0000-0003-3328-1374;;;;;;0000-0001-7473-7839;;;;;;0009-0003-1038-1572;;0000-0002-3600-1510",
        "linkedin": "fajri-koto-02705860/;rituraj-joshi-67300574?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;https://linkedin.com/in/mukhituly;yuxia-wang-0220a72b1/;zhuohanxie/;rahulpal7/;;parvezmullah;diana-turmakhan/;maiya-goloburda/;mohammed-kamran/;samujjwal/;;jonibek-mansurov-baa667212/;;debzban/;;akhmedsakip;;ekaterina-kochmar-0a655b14/;;aaryamonvikram/;alok-jadhav-9138369b/;;samta2092/;monojit-choudhury-54225898/;https://ca.linkedin.com/in/gurpreetgosal;gokul-ramakrishnan-a577472;biswajit-mishra-97771811/;https://linkedin.com/in/sarathchandranm;avraham-sheinin/;nataliavassilieva/;;preslavnakov/",
        "or_profile": "~Fajri_Koto1;~Rituraj_Joshi1;~Nurdaulet_Mukhituly1;~Yuxia_Wang1;~Zhuohan_Xie1;~Rahul_Pal1;~Daniil_Orel1;~Parvez_Mullah1;~Diana_Turmakhan1;~Maiya_Goloburda1;~Mohammed_Kamran1;~Samujjwal_Ghosh1;~Bokang_Jia1;~Jonibek_Mansurov1;~Mukhammed_Togmanov1;~Debopriyo_Banerjee1;~Nurkhan_Laiyk1;~Akhmed_Sakip2;~Xudong_Han2;~Ekaterina_Kochmar2;~Alham_Fikri_Aji1;~Aaryamonvikram_Singh1;~Alok_Anil_Jadhav1;~Satheesh_Katipomu1;~Samta_Kamboj1;~Monojit_Choudhury1;~Gurpreet_Gosal2;~Gokulakrishnan_Ramakrishnan1;~Biswajit_Mishra1;~Sarath_Chandran1;~Avraham_Sheinin1;~Natalia_Vassilieva1;~Neha_Sengupta1;~Preslav_Nakov2",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence;Cerebras Systems, Inc;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Group 42;Mohamed bin Zayed University of Artificial Intelligence;G42;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;G42 , Abu Dhabi;Inception, G42;G42;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence;G42;Core42;Mohamed bin Zayed University of Artificial Intelligence;Cerebras Systems, Inc;Cerebras Systems, Inc;Cerebras Systems, Inc+Intel;Cerebras Systems, Inc;Cerebras Systems, Inc;Cerebras Systems, Inc;Inception Institute of Artificial Intelligence;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;cerebras.net;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;g42.ai;mbzuai.ac.ae;g42.ai;mbzuai.ac.ae;mbzuai.ac.ae;g42.ai;inceptionai.ai;g42.ai;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;mbzuai.ac.ae;g42.ai;core42.ai;mbzuai.ac.ae;cerebras.net;cerebras.net;cerebras.net+intel.com;cerebras.net;cerebras.net;cerebras.net;inceptionai.ai;mbzuai.ac.ae",
        "position": "Assistant Professor;Researcher;PhD student;Postdoc;Postdoc;Researcher;MS student;Researcher;MS student;MS student;Researcher;Researcher;Researcher;PhD student;MS student;Postdoc;MS student;PhD student;Postdoc;Assistant Professor;Assistant Professor;Researcher;Researcher;Researcher;Researcher;Full Professor;Researcher;Researcher;Senior Applied Scientist +AI software Architect;Researcher;technical product manager;Field CTO;Principal Researcher;Full Professor",
        "bibtex": "@inproceedings{\nkoto2025sherkalachat,\ntitle={Sherkala-Chat: Building a State-of-the-Art {LLM} for Kazakh in a Moderately Resourced Setting},\nauthor={Fajri Koto and Rituraj Joshi and Nurdaulet Mukhituly and Yuxia Wang and Zhuohan Xie and Rahul Pal and Daniil Orel and Parvez Mullah and Diana Turmakhan and Maiya Goloburda and Mohammed Kamran and Samujjwal Ghosh and Bokang Jia and Jonibek Mansurov and Mukhammed Togmanov and Debopriyo Banerjee and Nurkhan Laiyk and Akhmed Sakip and Xudong Han and Ekaterina Kochmar and Alham Fikri Aji and Aaryamonvikram Singh and Alok Anil Jadhav and Satheesh Katipomu and Samta Kamboj and Monojit Choudhury and Gurpreet Gosal and Gokulakrishnan Ramakrishnan and Biswajit Mishra and Sarath Chandran and Avraham Sheinin and Natalia Vassilieva and Neha Sengupta and Preslav Nakov},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=wRcTCcb0H5}\n}",
        "github": "",
        "project": "",
        "reviewers": "uizC;EVms;b2Hx;EowQ",
        "site": "https://openreview.net/forum?id=wRcTCcb0H5",
        "pdf_size": 0,
        "rating": "5;6;6;8",
        "confidence": "4;4;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            1.0897247358851685
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            34,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.13245323570650439
    },
    {
        "id": "wXOUYzNv5k",
        "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings. The code is available at \\href{https://github.com/cacayaya/More-is-Less}{github.com/cacayaya/More-is-Less}.",
        "keywords": "Alignment;Synthetic Data;Safety;Large Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yifan Wang;Runjin Chen;Bolian Li;David Cho;Yihe Deng;Ruqi Zhang;Tianlong Chen;Zhangyang Wang;Ananth Grama;Junyuan Hong",
        "authorids": "~Yifan_Wang14;~Runjin_Chen1;~Bolian_Li1;~David_Cho2;~Yihe_Deng1;~Ruqi_Zhang1;~Tianlong_Chen1;~Zhangyang_Wang1;~Ananth_Grama1;~Junyuan_Hong1",
        "gender": "F;;M;M;F;F;M;M;M;M",
        "homepage": "https://cacayaya.github.io/;;https://lblaoke.github.io/;;;https://ruqizhang.github.io/;https://tianlong-chen.github.io;https://vita-group.github.io;https://www.cs.purdue.edu/homes/ayg/;https://jyhong.gitlab.io/",
        "dblp": ";;304/3220.html;;230/8011;;;119/4026;g/AnanthGrama.html;185/1316",
        "google_scholar": "hqL5jWYAAAAJ;;wNDoepwAAAAJ;;7Lix1poAAAAJ;4ojpmc8AAAAJ;LE3ctn0AAAAJ;pxFyKAIAAAAJ;https://scholar.google.com.tw/citations?user=bpsZlEQAAAAJ;7Cbv6doAAAAJ",
        "orcid": ";;0000-0002-1977-0764;;;;0000-0001-7774-8197;;;0000-0002-5718-5187",
        "linkedin": "yifan-wang-66521524b/;;bolian-li-554001297/;david-cho-85b038250/;;;tianlong-chen-783862167/;;;",
        "or_profile": "~Yifan_Wang14;~Runjin_Chen1;~Bolian_Li1;~David_Cho2;~Yihe_Deng1;~Ruqi_Zhang1;~Tianlong_Chen1;~Zhangyang_Wang1;~Ananth_Grama1;~Junyuan_Hong1",
        "aff": "Purdue University;;Purdue University;Purdue University;University of California, Los Angeles;Purdue University;University of North Carolina at Chapel Hill;University of Texas at Austin;Purdue University;University of Texas at Austin",
        "aff_domain": "purdue.edu;;purdue.edu;purdue.edu;ucla.edu;purdue.edu;unc.edu;utexas.edu;purdue.edu;utexas.edu",
        "position": "PhD student;;PhD student;PhD student;PhD student;Assistant Professor;Assistant Professor;Associate Professor;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nwang2025more,\ntitle={More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in {DPO} Safety Alignment},\nauthor={Yifan Wang and Runjin Chen and Bolian Li and David Cho and Yihe Deng and Ruqi Zhang and Tianlong Chen and Zhangyang Wang and Ananth Grama and Junyuan Hong},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=wXOUYzNv5k}\n}",
        "github": "",
        "project": "",
        "reviewers": "5pKC;1EjX;r9Sn;HTMa",
        "site": "https://openreview.net/forum?id=wXOUYzNv5k",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "wbAWKXNeQ4",
        "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release POLYGUARD, a new state-of-the-art multilingual safety model for\nsafeguarding LLM generations, and the corresponding training and evaluation datasets. POLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training corpus to date containing 1.91M samples across 17 languages (e.g., Chinese, Czech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality multilingual benchmark with 29K samples for the evaluation of safety guardrails. Created by combining naturally occurring multilingual human-LLM interactions and human-verified machine translations of an English-only safety dataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output pairs with labels of prompt harmfulness, response harmfulness, and response refusal. Through extensive evaluations across multiple safety and toxicity benchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art open-weight and commercial safety classifiers by 5.5%. Our contributions advance\nefforts toward safer multilingual LLMs for all global users.",
        "keywords": "ai safety;hate-speech detection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Priyanshu Kumar;Devansh Jain;Akhila Yerukola;Liwei Jiang;Himanshu Beniwal;Thomas Hartvigsen;Maarten Sap",
        "authorids": "~Priyanshu_Kumar1;~Devansh_Jain1;~Akhila_Yerukola1;~Liwei_Jiang2;~Himanshu_Beniwal1;~Thomas_Hartvigsen1;~Maarten_Sap1",
        "gender": "M;;;F;M;M;M",
        "homepage": ";;https://akhila-yerukola.github.io/;https://liweijiang.me;https://himanshubeniwal.github.io/;https://www.tomhartvigsen.com;http://maartensap.com",
        "dblp": ";;249/5606.html;;336/5761;211/5752;153/9519",
        "google_scholar": "SHQikPwAAAAJ;;Y7j60UQAAAAJ;lcPsDgUAAAAJ;https://scholar.google.co.in/citations?hl=en;rIjeeRsAAAAJ;gFN4QUYAAAAJ",
        "orcid": ";;;;0000-0001-6389-576X;;",
        "linkedin": ";;akhilayerukola;;himanshubeniwal/;;",
        "or_profile": "~Priyanshu_Kumar1;~Devansh_Jain1;~Akhila_Yerukola1;~Liwei_Jiang2;~Himanshu_Beniwal1;~Thomas_Hartvigsen1;~Maarten_Sap1",
        "aff": "Carnegie Mellon University;;Carnegie Mellon University;University of Washington;Indian Institute of Technology Gandhinagar;University of Virginia, Charlottesville;Carnegie Mellon University",
        "aff_domain": "cmu.edu;;cmu.edu;washington.edu;iitgn.ac.in;virginia.edu;cmu.edu",
        "position": "MS student;;PhD student;PhD student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkumar2025polyguard,\ntitle={PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages},\nauthor={Priyanshu Kumar and Devansh Jain and Akhila Yerukola and Liwei Jiang and Himanshu Beniwal and Thomas Hartvigsen and Maarten Sap},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=wbAWKXNeQ4}\n}",
        "github": "",
        "project": "",
        "reviewers": "NvCT;woca;bW1v;4mLP",
        "site": "https://openreview.net/forum?id=wbAWKXNeQ4",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "whXh2YxMbt",
        "title": "Elucidating the Design Space of Decay in Linear Attention",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "This paper presents a comprehensive investigation into the decay mechanisms inherent in linear complexity sequence models. We systematically delineate the design space of decay mechanisms across four pivotal dimensions: parameterization strategy, which refers to the computational methodology for decay; parameter sharing, which involves the utilization of supplementary parameters for decay computation; decay granularity. comparing scalar versus vector-based decay; and compatibility with relative positional encoding methods, such as Rotary Position Embedding (RoPE).\nThrough an extensive series of experiments conducted on diverse language modeling tasks, we uncovered several critical insights. Firstly, the design of the parameterization strategy for decay requires meticulous consideration. Our findings indicate that effective configurations are typically confined to a specific range of parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small, thereby significantly impacting performance. Thirdly, under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our analysis reveals that RoPE, a commonly employed relative positional encoding method, typically fails to provide tangible benefits to the majority of linear attention mechanisms.",
        "keywords": "Linear Attention",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhen Qin;Xuyang Shen;Yiran Zhong",
        "authorids": "~Zhen_Qin6;~Xuyang_Shen1;~Yiran_Zhong1",
        "gender": ";M;M",
        "homepage": "https://github.com/Doraemonzzz;;",
        "dblp": ";274/2342;158/9624",
        "google_scholar": "https://scholar.google.com.sg/citations?user=IcBRtycAAAAJ;k6Q1mcoAAAAJ;https://scholar.google.com.sg/citations?user=E9NVOBUAAAAJ",
        "orcid": ";0000-0002-1968-7055;",
        "linkedin": ";;",
        "or_profile": "~Zhen_Qin6;~Xuyang_Shen1;~Yiran_Zhong1",
        "aff": "TapTap;MiniMax;MiniMax+Shanghai AI Lab",
        "aff_domain": "xd.com;minimaxi.com;minimaxi.com+pjlab.org.cn",
        "position": "Researcher;Researcher;Principal Researcher+PI",
        "bibtex": "@inproceedings{\nqin2025elucidating,\ntitle={Elucidating the Design Space of Decay in Linear Attention},\nauthor={Zhen Qin and Xuyang Shen and Yiran Zhong},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=whXh2YxMbt}\n}",
        "github": "",
        "project": "",
        "reviewers": "xr9X;wY2B;R6Me;kU6m",
        "site": "https://openreview.net/forum?id=whXh2YxMbt",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "3;3;2;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            17,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.2581988897471611
    },
    {
        "id": "wyYL5Jov6e",
        "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to *implicitly reason* whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance *online*, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM *offline* to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time.\nFurthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers.\nWe evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost.\nOverall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs.",
        "keywords": "retrieval;offline enrichment;implicit reasoning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Peter Baile Chen;Tomer Wolfson;Mike Cafarella;Dan Roth",
        "authorids": "~Peter_Baile_Chen1;~Tomer_Wolfson1;~Mike_Cafarella1;~Dan_Roth3",
        "gender": ";M;M;M",
        "homepage": "https://peterbaile.github.io/;;https://people.csail.mit.edu/michjc/;https://www.cis.upenn.edu/~danroth/",
        "dblp": "276/5747;225/5206.html;82/231;r/DanRoth",
        "google_scholar": "hVnM4FgAAAAJ;6jFwxg4AAAAJ;https://scholar.google.com.tw/citations?user=r1quzEkAAAAJ;E-bpPWgAAAAJ",
        "orcid": ";0009-0002-7404-7736;0000-0001-6122-0590;",
        "linkedin": ";;mikecafarella/;dan-roth-8667361/",
        "or_profile": "~Peter_Baile_Chen1;~Tomer_Wolfson1;~Mike_Cafarella1;~Dan_Roth3",
        "aff": "Massachusetts Institute of Technology;University of Pennsylvania;Massachusetts Institute of Technology;Oracle+University of Pennsylvania",
        "aff_domain": "mit.edu;seas.upenn.edu;mit.edu;oracle.com+upenn.edu",
        "position": "PhD student;Postdoc;Principal Researcher;Chief Scientist+Eduardo D. Glandt Distinguished Professor",
        "bibtex": "@inproceedings{\nchen2025enrichindex,\ntitle={EnrichIndex: Using {LLM}s to Enrich Retrieval Indices Offline},\nauthor={Peter Baile Chen and Tomer Wolfson and Mike Cafarella and Dan Roth},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=wyYL5Jov6e}\n}",
        "github": "",
        "project": "",
        "reviewers": "LPFq;Cq8Z;u3K1;DMiz",
        "site": "https://openreview.net/forum?id=wyYL5Jov6e",
        "pdf_size": 0,
        "rating": "6;7;7;7",
        "confidence": "4;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            35,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "x2y9i2HDjD",
        "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art performance in automated formal proof generation for mathematical problems.  A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches. First, we train statement formalizers to translate natural language math problems from Numina into the formal language Lean 4, and use an LLM to verify that the formal statements accurately preserve the content of the original problems. This results in a dataset of 1.64 million formal statements. We then iteratively build a large dataset of formal proofs by training a series of provers: each prover is able to prove many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. Despite using only supervised fine-tuning, our final prover (fine-tuned on DeepSeek-Prover-V1.5-base) significantly outperforms the previous best open-source model, DeepSeek-Prover-V1.5, which uses reinforcement learning. \nOn the MiniF2F benchmark, our model achieves a success rate of 57.6\\% (Pass@32), surpassing DeepSeek-Prover-V1.5 by 7.6\\%. \nOn PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. \nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by prior work. \nWe provide extensive discussion of our training methodology, highlighting the key design choices that contribute to Goedel-Prover\u2019s strong performance. Finally, we explore reinforcement learning on top of Goedel-Prover-SFT, offering insights into its potential benefits and limitations.",
        "keywords": "Formal reasoning;verification;Lean;self-improvement",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yong Lin;Shange Tang;Bohan Lyu;Jiayun Wu;Hongzhou Lin;Kaiyu Yang;Jia LI;Mengzhou Xia;Danqi Chen;Sanjeev Arora;Chi Jin",
        "authorids": "~Yong_Lin2;~Shange_Tang1;~Bohan_Lyu1;~Jiayun_Wu1;~Hongzhou_Lin1;~Kaiyu_Yang1;~Jia_LI18;~Mengzhou_Xia1;~Danqi_Chen1;~Sanjeev_Arora1;~Chi_Jin1",
        "gender": ";M;M;M;M;M;M;F;F;;M",
        "homepage": ";https://shangetang.github.io/;https://lyubh.cn;https://ic-hub.github.io;;https://yangky11.github.io;;https://xiamengzhou.github.io/;https://www.cs.princeton.edu/~danqic/;http://www.cs.princeton.edu/~arora/;https://sites.google.com/view/cjin/home",
        "dblp": ";255/5774;355/3278-1;00/9456;178/3313;177/9276;;241/9329;87/7949;a/SArora;126/1802-1",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;vMMFoTsAAAAJ;https://scholar.google.com/citations?hl=en;;FciCu4EAAAAJ;;zyJn1IcAAAAJ;sVR8ktkAAAAJ;RUP4S68AAAAJ;GINhGvwAAAAJ",
        "orcid": ";;;0009-0007-7131-7290;;0000-0002-2777-612X;;;;;",
        "linkedin": ";;;jiayun-wu-4aa86323a/;;kaiyuy;jia-li-50106957/;;;;",
        "or_profile": "~Yong_Lin2;~Shange_Tang1;~Bohan_Lyu1;~Jiayun_Wu1;~Hongzhou_Lin1;~Kaiyu_Yang1;~Jia_LI18;~Mengzhou_Xia1;~Danqi_Chen1;~Sanjeev_Arora1;~Chi_Jin1",
        "aff": ";Princeton University;Tsinghua University;Carnegie Mellon University+Tsinghua University;Amazon;Meta;Project Numina;Princeton University;Princeton University;Princeton University;Princeton University",
        "aff_domain": ";princeton.edu;tsinghua.edu.cn;andrew.cmu.edu+mails.tsinghua.edu.cn;amazon.com;meta.com;projectnumina.ai;princeton.edu;cs.princeton.edu;princeton.edu;princeton.edu",
        "position": ";PhD student;Undergrad student;PhD student+MS student;Applied Scientist;Researcher;Researcher;PhD student;Assistant Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlin2025goedelprover,\ntitle={Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},\nauthor={Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia LI and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=x2y9i2HDjD}\n}",
        "github": "",
        "project": "",
        "reviewers": "e6im;Jesv;r5wC;Zigm",
        "site": "https://openreview.net/forum?id=x2y9i2HDjD",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "5;5;4;2",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            4.0,
            1.224744871391589
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "x4sdXZ7Jdu",
        "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advances in large language models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self\u2010guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT\\&CK Matrix, a proven penetration testing kill chain, to constrain the LLM's reasoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and 78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments.",
        "keywords": "Penetration Testing;Large Language Models;Autonomous Penetration Testing Agents",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Katsuaki Nakano;Reza Fayyazi;Shanchieh Yang;Michael Zuzak",
        "authorids": "~Katsuaki_Nakano1;~Reza_Fayyazi1;~Shanchieh_Yang1;~Michael_Zuzak1",
        "gender": "M;M;M;Not Specified",
        "homepage": ";https://scholar.google.com/citations?user=uVn2j3kAAAAJ&hl=en;;https://mzuzak.github.io/",
        "dblp": ";;;",
        "google_scholar": "32ysD6YAAAAJ;;flJTFn4AAAAJ;dESHYYcAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;shanchieh-jay-yang-26327015/;michael-zuzak/",
        "or_profile": "~Katsuaki_Nakano1;~Reza_Fayyazi1;~Shanchieh_Yang1;~Michael_Zuzak1",
        "aff": "Rochester Institute of Technology;Rochester Institute of Technology;Gonzaga University;Rochester Institute of Technology",
        "aff_domain": "rit.edu;rit.edu;gonzaga.edu;rit.edu",
        "position": "PhD student;PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nnakano2025guided,\ntitle={Guided Reasoning in {LLM}-Driven Penetration Testing Using Structured Attack Trees},\nauthor={Katsuaki Nakano and Reza Fayyazi and Shanchieh Yang and Michael Zuzak},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=x4sdXZ7Jdu}\n}",
        "github": "",
        "project": "",
        "reviewers": "j49b;dWYt;kkHX",
        "site": "https://openreview.net/forum?id=x4sdXZ7Jdu",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "3;4;5",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            4.0,
            0.816496580927726
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "x6evCULIOQ",
        "title": "Energy-Based Reward Models for Robust Language Model Alignment",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preference. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce \\emph{Energy-Based Reward Model} (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization.\nEBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks.\nEmpirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97\\% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines.",
        "keywords": "Reward Models;Alignment;Energy Based Models",
        "primary_area": "",
        "supplementary_material": "/attachment/d94995fd2c118eb604ec15b3b03867d739a2ce0c.zip",
        "author": "Anamika Lochab;Ruqi Zhang",
        "authorids": "~Anamika_Lochab1;~Ruqi_Zhang1",
        "gender": "F;F",
        "homepage": "https://anamikalochab.github.io/AL/;https://ruqizhang.github.io/",
        "dblp": ";",
        "google_scholar": ";4ojpmc8AAAAJ",
        "orcid": "0009-0002-7186-524X;",
        "linkedin": ";",
        "or_profile": "~Anamika_Lochab1;~Ruqi_Zhang1",
        "aff": "Purdue University;Purdue University",
        "aff_domain": "purdue.edu;purdue.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nlochab2025energybased,\ntitle={Energy-Based Reward Models for Robust Language Model Alignment},\nauthor={Anamika Lochab and Ruqi Zhang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=x6evCULIOQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "7x94;rdh4;JZMs;gRuK",
        "site": "https://openreview.net/forum?id=x6evCULIOQ",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5773502691896257
    },
    {
        "id": "xNj14CY5S1",
        "title": "Adaptive Computation Pruning for the Forgetting Transformer",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. In particular, our method performs *provably safe* pruning via a dynamically set pruning threshold that guarantees the pruned attention weights are negligible. \nWe apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs and memory accesses in softmax attention by around 70\\% across different model sizes and context lengths, resulting in a roughly 50\\% to 70\\% reduction in attention runtime (or a 2--3$\\times$ speedup) and a roughly 10\\% to 40\\% increase in end-to-end training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
        "keywords": "transformer;forgetting transformer;efficient transformer;sequence modeling;adaptive computation pruning;forget gate;sparse attention;FlashAttention;hardware-aware optimization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zhixuan Lin;Johan Obando-Ceron;Xu Owen He;Aaron Courville",
        "authorids": "~Zhixuan_Lin1;~Johan_Obando-Ceron1;~Xu_Owen_He1;~Aaron_Courville3",
        "gender": "M;;;",
        "homepage": "https//www.zhixuanlin.com;;;",
        "dblp": "254/1249;;;56/1688",
        "google_scholar": "BiyrJeMAAAAJ;;;https://scholar.google.ca/citations?user=km6CP8cAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Zhixuan_Lin1;~Johan_Obando-Ceron1;~Xu_Owen_He1;~Aaron_Courville3",
        "aff": "Universit\u00e9 de Montr\u00e9al;;;University of Montreal+Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "umontreal.ca;;;umontreal.ca+ ",
        "position": "PhD student;;;Assistant Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nlin2025adaptive,\ntitle={Adaptive Computation Pruning for the Forgetting Transformer},\nauthor={Zhixuan Lin and Johan Obando-Ceron and Xu Owen He and Aaron Courville},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=xNj14CY5S1}\n}",
        "github": "",
        "project": "",
        "reviewers": "s3o6;KGU7;tEnv",
        "site": "https://openreview.net/forum?id=xNj14CY5S1",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "5;4;2",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            1.247219128924647
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.944911182523068
    },
    {
        "id": "xZi2rMUcAO",
        "title": "CALLME: Call Graph Augmentation with Large Language Models for Javascript",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Building precise call graphs for Javascript programs is a fundamental build-\ning block for many important software engineering and security applications\nsuch as bug detection, program repair, and refactoring. However, resolving\ndynamic calls using static analysis is challenging because it requires\nenumerating all possible values of both the object and the field. As a result,\nstatic call graph construction algorithms for Javascript ignore such dynamic\ncalls, resulting in missed edges and a high false negative rate. We present\na new approach, CALLME, that combines Language Models (LMs) with a\ncustom static analyzer to address this challenge. Our key insight is in using\nLMs to incorporate additional modalities such as variable names, natural\nlanguage documentation, and calling contexts, which are often sufficient to\nresolve dynamic property calls, but are difficult to incorporate in traditional\nstatic analysis. We implement our approach in CALLME and evaluate it\non a dataset of call edges that are dependent on dynamic property accesses.\nCALLME achieves 80% accuracy and .79 F1, outperforming the state-of-the-\nart static analyzer by 30% and .60, respectively. To study the effectiveness\nof CALLME on downstream analysis tasks, we evaluate it on our manually\ncurated dataset with 25 known Javascript vulnerabilities. CALLME can\ndetect 24 vulnerabilities with only 3 false positives, whereas static analysis\ntools based on current call graph construction algorithms miss all of them.",
        "keywords": "Javascript;program analysis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Michael Wang;Kexin Pei;Armando Solar-Lezama",
        "authorids": "~Michael_Wang1;~Kexin_Pei1;~Armando_Solar-Lezama1",
        "gender": "M;M;M",
        "homepage": ";https://sites.google.com/site/kexinpeisite/;https://people.csail.mit.edu/asolar/",
        "dblp": ";145/6061;95/6919",
        "google_scholar": "X6tpl0IAAAAJ;XzSkny0AAAAJ;https://scholar.google.com.tw/citations?user=8BX3BokAAAAJ",
        "orcid": ";0000-0001-5052-9808;",
        "linkedin": ";kexin-pei/;",
        "or_profile": "~Michael_Wang1;~Kexin_Pei1;~Armando_Solar-Lezama1",
        "aff": "Massachusetts Institute of Technology;The University of Chicago;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;uchicago.edu;mit.edu",
        "position": "PhD student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nwang2025callme,\ntitle={{CALLME}: Call Graph Augmentation with Large Language Models for Javascript},\nauthor={Michael Wang and Kexin Pei and Armando Solar-Lezama},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=xZi2rMUcAO}\n}",
        "github": "",
        "project": "",
        "reviewers": "vcVB;NECr;7TUv;JHym",
        "site": "https://openreview.net/forum?id=xZi2rMUcAO",
        "pdf_size": 0,
        "rating": "4;5;6;9",
        "confidence": "3;3;3;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            1.8708286933869707
        ],
        "confidence_avg": [
            3.25,
            0.4330127018922193
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.9258200997725515
    },
    {
        "id": "xhDcG8qtw9",
        "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated \\emph{probabilistic} predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.",
        "keywords": "Large Language Model;Probabilistic Reasoning;Semantics;Calibration",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Liaoyaqi Wang;Zhengping Jiang;Anqi Liu;Benjamin Van Durme",
        "authorids": "~Liaoyaqi_Wang1;~Zhengping_Jiang1;~Anqi_Liu2;~Benjamin_Van_Durme2",
        "gender": "F;M;F;",
        "homepage": ";https://zipjiang.github.io/;https://anqiliu-ai.github.io/;",
        "dblp": ";170/0087;;",
        "google_scholar": "RPw7AlUAAAAJ;skgb6DUAAAAJ;Q8yp6zQAAAAJ;",
        "orcid": ";;0000-0002-0468-5698;",
        "linkedin": ";zhengping-jiang-12a591169/;;",
        "or_profile": "~Liaoyaqi_Wang1;~Zhengping_Jiang1;~Anqi_Liu2;~Benjamin_Van_Durme2",
        "aff": "Johns Hopkins University;Johns Hopkins University;Johns Hopkins University+California Institute of Technology+University of Illinois, Chicago;",
        "aff_domain": "jh.edu;jhu.edu;jhu.edu+caltech.edu+uic.edu;",
        "position": "MS student;PhD student;Assistant Professor+Postdoc+PhD student;",
        "bibtex": "@inproceedings{\nwang2025always,\ntitle={Always Tell Me The Odds: Fine-grained Conditional Probability Estimation},\nauthor={Liaoyaqi Wang and Zhengping Jiang and Anqi Liu and Benjamin Van Durme},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=xhDcG8qtw9}\n}",
        "github": "",
        "project": "",
        "reviewers": "C4re;eG4a;oXxq;YjWc",
        "site": "https://openreview.net/forum?id=xhDcG8qtw9",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "xqIwK9mNkj",
        "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "A chatbot\u2019s personality design is key to interaction quality. As chatbots evolved from rule-based systems to those powered by large language models (LLMs), evaluating the effectiveness of their personality design has become increasingly complex, particularly due to the open-ended nature of interactions. A recent and widely adopted method for assessing the personality design of LLM-based chatbots is the use of self-report questionnaires. These questionnaires, often borrowed from established human personality inventories, ask the chatbot to rate itself on various personality traits. Can LLM-based chatbots meaningfully \"self-report\" their personality? We created 500 chatbots with distinct personality designs and evaluated the validity of their self-report personality scores by examining human perceptions formed during interactions with these chatbots. Our findings indicate that the chatbot's answers on human personality scales exhibit weak correlations with both human-perceived personality traits and the overall interaction quality. These findings raise concerns about both the criterion validity and the predictive validity of self-report methods in this context. Further analysis revealed the role of task context and interaction in the chatbot's personality design assessment. We further discuss design implications for creating more contextualized and interactive evaluation.",
        "keywords": "human factors in NLP; evaluation methodologies",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Huiqi Zou;Pengda Wang;Zihan Yan;Tianjun Sun;Ziang Xiao",
        "authorids": "~Huiqi_Zou1;~Pengda_Wang1;~Zihan_Yan1;~Tianjun_Sun1;~Ziang_Xiao1",
        "gender": "F;M;F;F;",
        "homepage": ";https://wpengda.github.io/;;https://profiles.rice.edu/faculty/tianjun-sun;",
        "dblp": "339/2194;;;;196",
        "google_scholar": ";-C0UxKQAAAAJ;https://scholar.google.com/citations?hl=en;dn24ECQAAAAJ;MjkODLEAAAAJ",
        "orcid": "0000-0003-2948-550X;0009-0001-5791-1206;;0000-0002-3655-0042;",
        "linkedin": ";pengda-wang/;;tianjunsun/;",
        "or_profile": "~Huiqi_Zou1;~Pengda_Wang1;~Zihan_Yan1;~Tianjun_Sun1;~Ziang_Xiao1",
        "aff": "Northeastern University+Johns Hopkins University;Rice University;University of Illinois Urbana-Champaign;Rice University;Department of Computer Science, Whiting School of Engineering",
        "aff_domain": "northeastern.edu+jhu.edu;rice.edu;illinois.edu;rice.edu;cs.jhu.edu",
        "position": "PhD student+Researcher;PhD student;PhD student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzou2025can,\ntitle={Can {LLM} ''Self-report''?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in {LLM}-based Chatbots},\nauthor={Huiqi Zou and Pengda Wang and Zihan Yan and Tianjun Sun and Ziang Xiao},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=xqIwK9mNkj}\n}",
        "github": "",
        "project": "",
        "reviewers": "ezgD;BH6H;2DCa",
        "site": "https://openreview.net/forum?id=xqIwK9mNkj",
        "pdf_size": 0,
        "rating": "7;7;10",
        "confidence": "4;4;5",
        "wc_review": "",
        "rating_avg": [
            8.0,
            1.4142135623730951
        ],
        "confidence_avg": [
            4.333333333333333,
            0.4714045207910317
        ],
        "replies_avg": [
            29,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 1.0
    },
    {
        "id": "y56BuSo8Uj",
        "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents.\n\nHowever, real-world applications demand not only accuracy but also interpretability. \nDense retrieval methods provide high accuracy but lack interpretability, while sparse retrieval is transparent but often misses query intent due to keyword matching.\nThus, balancing accuracy and interpretability remains a challenge.\n\nTo address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. \nIterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords.\n\nAcross four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval based RAG and prior iterative query refinement methods using dense models.\nIn summary, IterKey is a novel BM25-based iterative RAG framework that leverages LLMs to balance accuracy and interpretability.",
        "keywords": "retrieval-augmented generation;RAG;sparse retrieval;LLM;Iterative",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kazuki Hayashi;Hidetaka Kamigaito;Shinya Kouda;Taro Watanabe",
        "authorids": "~Kazuki_Hayashi1;~Hidetaka_Kamigaito2;~Shinya_Kouda1;~Taro_Watanabe1",
        "gender": "M;M;M;M",
        "homepage": "https://scholar.google.co.jp/citations?hl=ja&user=9V70ziYAAAAJ;https://sites.google.com/site/hidetakakamigaito/;https://www.tdse.jp/;https://sites.google.com/site/tarowtnb/",
        "dblp": "132/8215;124/2384;;50/4741",
        "google_scholar": "https://scholar.google.co.jp/citations?hl=ja;https://scholar.google.co.jp/citations?user=cyZpch8AAAAJ;;zsEEy7kAAAAJ",
        "orcid": "0009-0007-7327-5311;0000-0002-5249-5813;;0000-0001-8349-3522",
        "linkedin": "kazuki-hayashi-a34047277/;;shinya-kouda/;",
        "or_profile": "~Kazuki_Hayashi1;~Hidetaka_Kamigaito2;~Shinya_Kouda1;~Taro_Watanabe1",
        "aff": "Nara Institute of Science and Technology, Japan;Nara Institute of Science and Technology;AndResearch+TDSE, Inc.;Nara Institute of Science and Technology, Japan",
        "aff_domain": "naist.jp;naist.jp;and-research.com+tdse.jp;naist.jp",
        "position": "MS student;Associate Professor;Researcher+Researcher;Full Professor",
        "bibtex": "@inproceedings{\nhayashi2025iterkey,\ntitle={IterKey: Iterative Keyword Generation with {LLM}s for Enhanced Retrieval Augmented Generation},\nauthor={Kazuki Hayashi and Hidetaka Kamigaito and Shinya Kouda and Taro Watanabe},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=y56BuSo8Uj}\n}",
        "github": "",
        "project": "",
        "reviewers": "g9QR;NWKh;KgZB;cbBx",
        "site": "https://openreview.net/forum?id=y56BuSo8Uj",
        "pdf_size": 0,
        "rating": "6;6;6;6",
        "confidence": "5;4;4;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.0
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            37,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "yGQqTuSJPK",
        "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce ThoughtTracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate ThoughtTracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o3 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.",
        "keywords": "theory of mind;reasoning;large language model;inference-time algorithm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hyunwoo Kim;Melanie Sclar;Tan Zhi-Xuan;Lance Ying;Sydney Levine;Yang Liu;Joshua B. Tenenbaum;Yejin Choi",
        "authorids": "~Hyunwoo_Kim3;~Melanie_Sclar1;~Tan_Zhi-Xuan1;~Lance_Ying1;~Sydney_Levine1;~Yang_Liu60;~Joshua_B._Tenenbaum1;~Yejin_Choi1",
        "gender": "M;F;Non-Binary;M;F;F;;F",
        "homepage": "http://hyunwookim.com;https://msclar.github.io;;https://www.lanceying.com;http://sites.google.com/site/sydneymlevine;;;https://yejinc.github.io/",
        "dblp": "02/8768-2;274/6796;267/5392;301/8985;175/9604;51/3710-4;t/JoshuaBTenenbaum;89/579-1",
        "google_scholar": "https://scholar.google.co.kr/citations?user=PAXFuxsAAAAJ;4uNPtZgAAAAJ;Yscjyf8AAAAJ;https://scholar.google.fr/citations?user=oGtDR60AAAAJ;Yt2H6lwAAAAJ;w90wOucAAAAJ;;vhP-tlcAAAAJ",
        "orcid": "0009-0002-2714-1287;;0000-0002-1549-8492;;;;;",
        "linkedin": "hyunw-kim/;melanie-sclar-077047b5/;;;;yang-liu-8555143/;;",
        "or_profile": "~Hyunwoo_Kim3;~Melanie_Sclar1;~Tan_Zhi-Xuan1;~Lance_Ying1;~Sydney_Levine1;~Yang_Liu60;~Joshua_B._Tenenbaum1;~Yejin_Choi1",
        "aff": "NVIDIA;Meta Facebook+University of Washington, Seattle;Massachusetts Institute of Technology;School of Engineering and Applied Sciences, Harvard University+Massachusetts Institute of Technology;Allen Institute for Artificial Intelligence;Amazon;Massachusetts Institute of Technology;Computer Science Department, Stanford University+NVIDIA",
        "aff_domain": "nvidia.com;meta.com+uw.edu;mit.edu;seas.harvard.edu+mit.edu;allenai.org;amazon.com;mit.edu;cs.stanford.edu+nvidia.com",
        "position": "Postdoc;Researcher+PhD student;PhD student;PhD student+Researcher;Researcher;Principal Researcher;Professor;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nkim2025hypothesisdriven,\ntitle={Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models},\nauthor={Hyunwoo Kim and Melanie Sclar and Tan Zhi-Xuan and Lance Ying and Sydney Levine and Yang Liu and Joshua B. Tenenbaum and Yejin Choi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yGQqTuSJPK}\n}",
        "github": "",
        "project": "",
        "reviewers": "gn2m;dhRR;jvBW",
        "site": "https://openreview.net/forum?id=yGQqTuSJPK",
        "pdf_size": 0,
        "rating": "6;6;7",
        "confidence": "3;3;3",
        "wc_review": "",
        "rating_avg": [
            6.333333333333333,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.0,
            0.0
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "yGa8CYT8kS",
        "title": "Multilingual and Multi-Accent Jailbreaking of Audio LLMs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Audio Language Models (LALMs) have significantly advanced audio understanding but introduce critical security risks, particularly through audio jailbreaks. While prior work has focused on English-centric attacks, we expose a far more severe vulnerability: adversarial multilingual and multi-accent audio jailbreaks, where linguistic and acoustic variations dramatically amplify attack success. In this paper, we introduce Multi-AudioJail, the first systematic framework to exploit these vulnerabilities through (1) a novel dataset of adversarially perturbed multilingual/multi-accent audio jailbreaking prompts, and (2) a hierarchical evaluation pipeline revealing that how acoustic perturbations (e.g., reverberation, echo, and whisper effects) interacts with cross-lingual phonetics to cause jailbreak success rates (JSRs) to surge by up to +57.25 percentage points (e.g., reverberated Kenyan-accented attack on MERaLiON). Crucially, our work further reveals that multimodal LLMs are inherently more vulnerable than unimodal systems: attackers need only exploit the weakest link (e.g., non-English audio inputs) to compromise the entire model, which we empirically show by multilingual audio-only attacks achieving 3.1x higher success rates than text-only attacks. We plan to release our dataset to spur research into cross-modal defenses, urging the community to address this expanding attack surface in multimodality as LALMs evolve.",
        "keywords": "Audio;LLM;Jailbreak;Multilingual;Security",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jaechul Roh;Virat Shejwalkar;Amir Houmansadr",
        "authorids": "~Jaechul_Roh1;~Virat_Shejwalkar1;~Amir_Houmansadr1",
        "gender": "M;M;M",
        "homepage": "https://jrohsc.github.io/;https://people.cs.umass.edu/~vshejwalkar/;https://www.cs.umass.edu/~amir/",
        "dblp": ";243/3113.html;22/1797",
        "google_scholar": "knCeRjsAAAAJ;M6GAEdUAAAAJ;https://scholar.google.com.tw/citations?user=cTTFHNwAAAAJ",
        "orcid": ";;",
        "linkedin": "jaechul-roh-572363155/;;",
        "or_profile": "~Jaechul_Roh1;~Virat_Shejwalkar1;~Amir_Houmansadr1",
        "aff": "University of Massachusetts at Amherst;Google;University of Massachusetts, Amherst",
        "aff_domain": "umass.edu;google.com;umass.edu",
        "position": "PhD student;Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nroh2025multilingual,\ntitle={Multilingual and  Multi-Accent Jailbreaking of Audio {LLM}s},\nauthor={Jaechul Roh and Virat Shejwalkar and Amir Houmansadr},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yGa8CYT8kS}\n}",
        "github": "",
        "project": "",
        "reviewers": "nGKb;uBUp;57ot;nssE",
        "site": "https://openreview.net/forum?id=yGa8CYT8kS",
        "pdf_size": 0,
        "rating": "6;6;7;7",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            6.5,
            0.5
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "yYk3zK0X6Q",
        "title": "Streaming DiLoCo with overlapping communication",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Training of large language models (LLMs) is typically distributed across a large number of accelerators to reduce training time. Since internal states and parameter gradients need to be exchanged at each and every single gradient step, all devices need to be co-located using low-latency high-bandwidth communication links to support the required high volume of data exchange. Recently, algorithms like DiLoCo have relaxed the constraint that all devices need co-location: accelerators can be grouped into ``workers'', where synchronizations between workers need only occur infrequently. This in turn means that workers can afford being connected by lower bandwidth communication links without affecting learning quality. However, in these methods, communication across workers still requires the same peak bandwidth as before, as the synchronizations require all parameters to be exchanged across all workers. In this paper, we improve DiLoCo in three ways. First, we synchronize only subsets of parameters in sequence, rather than all at once, which greatly reduces peak bandwidth. Second, we allow workers to continue training while synchronizing, which decreases wall clock time. Third, we quantize the data exchanged by workers, which further reduces bandwith across workers. We show experimentally that by properly combining these modifications we can distribute training of billion-scale parameters and attain models of similar quality as before, but reducing required bandwidth by a factor of up to two orders of magnitude.",
        "keywords": "distributed training;large-scale;llm",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Arthur Douillard;Yani Donchev;J Keith Rush;Satyen Kale;Zachary Charles;Gabriel Teston;Zachary Garrett;Jiajun Shen;Ross McIlroy;David Lacey;Alexandre Rame;Arthur Szlam;MarcAurelio Ranzato;Paul R Barham",
        "authorids": "~Arthur_Douillard1;~Yani_Donchev1;~J_Keith_Rush1;~Satyen_Kale2;~Zachary_Charles1;~Gabriel_Teston1;~Zachary_Garrett1;~Jiajun_Shen1;~Ross_McIlroy1;~David_Lacey1;~Alexandre_Rame1;~Arthur_Szlam3;~MarcAurelio_Ranzato1;~Paul_R_Barham2",
        "gender": "M;M;;;;M;M;M;M;M;M;;M;M",
        "homepage": "https://arthurdouillard.com;https://yannidd.github.io/;https://www.jkrush.com;https://www.satyenkale.com;;;;;;https://deepmind.google;https://alexrame.github.io/;;https://ranzato.github.io/;",
        "dblp": "263/9770.html;;249/8135;52/4768;;398/2778;255/5493;120/0742;;;;;28/1732;",
        "google_scholar": "snwgZBIAAAAJ;;OrUyRAcAAAAJ;https://scholar.google.com/citations?hl=en;;teUiJmUAAAAJ;-M22wckAAAAJ;qckHL1AAAAAJ;https://scholar.google.co.uk/citations?user=XUFRWFIAAAAJ;;7znwivwAAAAJ;https://scholar.google.com/citations?hl=en;NbXF7T8AAAAJ;q34R9psAAAAJ",
        "orcid": ";;;;;;0000-0001-8158-3997;;;;;;;",
        "linkedin": ";;;;;;zacharygarrett/;;;;alexandre-ram%C3%A9-05259587;;;",
        "or_profile": "~Arthur_Douillard1;~Yani_Donchev1;~J_Keith_Rush1;~Satyen_Kale2;~Zachary_Charles1;~Gabriel_Teston1;~Zachary_Garrett1;~Jiajun_Shen1;~Ross_McIlroy1;~David_Lacey1;~Alexandre_Rame1;~Arthur_Szlam3;~MarcAurelio_Ranzato1;~Paul_R_Barham2",
        "aff": "Google DeepMind;Google;Google;Apple;;Google;Research, Google;Google DeepMind;Google;Google;Google;Google;Google;Research, Google",
        "aff_domain": "deepmind.com;deepmind.com;google.com;apple.com;;google.com;research.google.com;deepmind.com;google.com;google.com;google.com;google.com;google.com;research.google.com",
        "position": "Researcher;Researcher;Researcher;Researcher;;Researcher;Researcher;Researcher;Researcher;Researcher;research scientist;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\ndouillard2025streaming,\ntitle={Streaming DiLoCo with overlapping communication},\nauthor={Arthur Douillard and Yani Donchev and J Keith Rush and Satyen Kale and Zachary Charles and Gabriel Teston and Zachary Garrett and Jiajun Shen and Ross McIlroy and David Lacey and Alexandre Rame and Arthur Szlam and MarcAurelio Ranzato and Paul R Barham},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yYk3zK0X6Q}\n}",
        "github": "",
        "project": "",
        "reviewers": "xJA5;CTbm;YXr7;Aw6r",
        "site": "https://openreview.net/forum?id=yYk3zK0X6Q",
        "pdf_size": 0,
        "rating": "5;6;6;7",
        "confidence": "3;2;5;4",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            3.5,
            1.118033988749895
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.31622776601683794
    },
    {
        "id": "ybcZEWaM7U",
        "title": "VideoSAVi: Self-Aligned Video Language Models without Human Supervision",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent advances in video-large language models (Video-LLMs) have led to significant progress in video understanding. Current preference optimization methods often rely on proprietary APIs or ground-truth captions to generate preference data (i.e., pairs of model outputs ranked based on their quality or alignment with human judgment), which is then used to train models for video-language alignment. This approach is both costly and labor-intensive. To address this limitation, we introduce $\\textbf{VideoSAVi}$ ($\\underline{\\textbf{S}}$elf-$\\underline{\\textbf{A}}$ligned $\\underline{\\textbf{Vi}}$deo Language Model), a self-training pipeline that enables Video-LLMs to reason over video content without external supervision. Our approach includes a self-critiquing mechanism that identifies reasoning errors in the model's initial responses and generates improved alternatives, creating preference pairs directly from video content. VideoSAVi then applies Direct Preference Optimization (DPO) to iteratively train the model using the preference data, thus enhancing its temporal and spatial reasoning for video understanding. Experiments show that VideoSAVi delivers significant improvements across multiple benchmarks, including a +4.2 percentage point gain on MVBench, +3.9 on PerceptionTest, and +6.8 on the challenging EgoSchema dataset compared to baseline models. Our model-agnostic approach is computationally efficient, requiring only 32 frames, offering a promising direction for self-aligned video understanding without reliance on external models or annotations.",
        "keywords": "Video understanding;Self-alignment;Video-language models;Direct preference optimization;Self-critiquing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yogesh Kulkarni;Pooyan Fazli",
        "authorids": "~Yogesh_Kulkarni1;~Pooyan_Fazli1",
        "gender": "M;",
        "homepage": "https://yogkul2000.github.io/;",
        "dblp": "158/8564;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;",
        "orcid": "0000-0001-9246-3448;",
        "linkedin": "ykulkarn/;",
        "or_profile": "~Yogesh_Kulkarni1;~Pooyan_Fazli1",
        "aff": "Arizona State University;",
        "aff_domain": "asu.edu;",
        "position": "PhD student;",
        "bibtex": "@inproceedings{\nkulkarni2025videosavi,\ntitle={Video{SAV}i: Self-Aligned Video Language Models without Human Supervision},\nauthor={Yogesh Kulkarni and Pooyan Fazli},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=ybcZEWaM7U}\n}",
        "github": "",
        "project": "",
        "reviewers": "XAqh;vG49;CBX8;nQcD",
        "site": "https://openreview.net/forum?id=ybcZEWaM7U",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "4;4;5;3",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.42640143271122083
    },
    {
        "id": "yeVBHPLXxi",
        "title": "Learning to Generate Unit Tests for Automated Debugging",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language models (LLMs), motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and backtracks edits based on multiple generated UTs to avoid overfitting, and helps LLMs debug effectively. We show that UTGen outperforms other LLM-based baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5 32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17% and 12.35% (respectively) over other LLM-based UT generation baselines. Moreover, we observe that feedback from Qwen2.5 32B-based UTGen model can enhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, we demonstrate that UTGen is a better judge for code correctness, outperforming a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10 sampling using Qwen2.5 7B.",
        "keywords": "Unit Tests Generation;LLMs for code generation;LLMs for code debugging",
        "primary_area": "",
        "supplementary_material": "/attachment/f3e3abd874e6c1ad0a461bbcbb6c9cf7fe922262.zip",
        "author": "Archiki Prasad;Elias Stengel-Eskin;Justin Chen;Zaid Khan;Mohit Bansal",
        "authorids": "~Archiki_Prasad1;~Elias_Stengel-Eskin1;~Justin_Chen1;~Zaid_Khan1;~Mohit_Bansal2",
        "gender": "F;M;;Not Specified;M",
        "homepage": "https://archiki.github.io/;https://esteng.github.io;;https://zaidkhan.me;https://www.cs.unc.edu/~mbansal/",
        "dblp": "264/2812;212/6138;;259/1127-1;32/5243.html",
        "google_scholar": "Svcwv-IAAAAJ;gr_ZVSQAAAAJ;;uXXocfgAAAAJ;DN8QtscAAAAJ",
        "orcid": ";0000-0002-6689-505X;;;",
        "linkedin": "archiki-prasad;;;https://linkedin.com/in/khan-zaid;",
        "or_profile": "~Archiki_Prasad1;~Elias_Stengel-Eskin1;~Justin_Chen1;~Zaid_Khan1;~Mohit_Bansal2",
        "aff": "University of North Carolina, Chapel Hill;University of North Carolina at Chapel Hill;;University of North Carolina at Chapel Hill;University of North Carolina at Chapel Hill",
        "aff_domain": "unc.edu;cs.unc.edu;;unc.edu;unc.edu",
        "position": "PhD student;Postdoc;;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nprasad2025learning,\ntitle={Learning to Generate Unit Tests for Automated Debugging},\nauthor={Archiki Prasad and Elias Stengel-Eskin and Justin Chen and Zaid Khan and Mohit Bansal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yeVBHPLXxi}\n}",
        "github": "",
        "project": "",
        "reviewers": "6enx;2H95;h2jQ",
        "site": "https://openreview.net/forum?id=yeVBHPLXxi",
        "pdf_size": 0,
        "rating": "5;6;6",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.4999999999999999
    },
    {
        "id": "yfRkNRFLzl",
        "title": "Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "While the inconsistency of LLMs is not a novel topic, prior research has predominantly addressed two types of generative inconsistencies: i) Randomness Inconsistency: running the same LLM multiple trials, yielding varying responses; ii) Paraphrase Inconsistency: paraphrased prompts result in different responses from the same LLM. Randomness Inconsistency arises from the inherent randomness due to stochastic sampling in generative models, while Paraphrase Inconsistency is a consequence of the language modeling objectives, where paraphrased prompts alter the distribution of vocabulary logits. This research discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM self-inconsistency: given a question and a couple of LLM-generated answer candidates, the LLM often has conflicting responses when prompted \"Which are correct answers?\" and \"Which are incorrect answers?\". PRIN poses a big concern as it undermines the credibility of LLM-as-a-judge, and suggests a challenge for LLMs to adhere to basic logical rules. We conduct a series of experiments to investigate PRIN, examining the extent of PRIN across different LLMs, methods to mitigate it, potential applications, and its relationship with Randomness Inconsistency and Paraphrase Inconsistency. As the first study to explore PRIN, our findings offer valuable insights into the inner workings of LLMs and contribute to advancing trustworthy AI.",
        "keywords": "Large Language Model;Natural Language Process;Inconsistency of LLMs;Prompt-Reverse Inconsistency;Randomness Inconsistency;Paraphrase Inconsistency",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jihyun Janice Ahn;Wenpeng Yin",
        "authorids": "~Jihyun_Janice_Ahn1;~Wenpeng_Yin1",
        "gender": ";",
        "homepage": ";http://wenpengyin.org/",
        "dblp": ";117/7310-1",
        "google_scholar": ";mRg16LkAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Jihyun_Janice_Ahn1;~Wenpeng_Yin1",
        "aff": ";Pennsylvania State University",
        "aff_domain": ";psu.edu",
        "position": ";Assistant Professor",
        "bibtex": "@inproceedings{\nahn2025promptreverse,\ntitle={Prompt-Reverse Inconsistency: {LLM} Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing},\nauthor={Jihyun Janice Ahn and Wenpeng Yin},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yfRkNRFLzl}\n}",
        "github": "",
        "project": "",
        "reviewers": "b7z4;U3Ww;HG1x;63vw",
        "site": "https://openreview.net/forum?id=yfRkNRFLzl",
        "pdf_size": 0,
        "rating": "5;6;6;6",
        "confidence": "4;3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.75,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.5
        ],
        "replies_avg": [
            12,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.5773502691896257
    },
    {
        "id": "yfnaK1pZxu",
        "title": "CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Multilingual vision-language models have made significant strides in image captioning, yet they still lag behind their English counterparts due to limited multilingual training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG) offers a promising alternative by conditioning caption generation on retrieved examples in the target language, reducing the need for extensive multilingual training. \nHowever, multilingual RAG captioning models often depend on retrieved captions translated from English, which can introduce mismatches and linguistic biases relative to the source language. We introduce CONCAP, a multilingual image captioning model that integrates retrieved captions with image-specific concepts, enhancing the contextualization of the input image and grounding the captioning process across different languages. \nExperiments on the XM3600 dataset indicate that CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of concept-aware retrieval augmentation in bridging multilingual performance gaps.",
        "keywords": "Image Captioning;Concepts;Retrieval;RAG;Multilingual",
        "primary_area": "",
        "supplementary_material": "",
        "author": "George Ibrahim;Rita Ramos;Yova Kementchedjhieva",
        "authorids": "~George_Ibrahim1;~Rita_Ramos1;~Yova_Kementchedjhieva1",
        "gender": "M;;F",
        "homepage": ";;",
        "dblp": ";;225/7708",
        "google_scholar": ";;",
        "orcid": ";;0009-0000-7465-5702",
        "linkedin": "george-sherif/;;",
        "or_profile": "~George_Ibrahim1;~Rita_Ramos1;~Yova_Kementchedjhieva1",
        "aff": "Mohamed bin Zayed University of Artificial Intelligence;;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;;mbzuai.ac.ae",
        "position": "MS student;;Assistant Professor",
        "bibtex": "@inproceedings{\nibrahim2025concap,\ntitle={{CONCAP}: Seeing Beyond English with Concepts Retrieval-Augmented Captioning},\nauthor={George Ibrahim and Rita Ramos and Yova Kementchedjhieva},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yfnaK1pZxu}\n}",
        "github": "",
        "project": "",
        "reviewers": "5aBu;H9Ar;HrUv;umB5",
        "site": "https://openreview.net/forum?id=yfnaK1pZxu",
        "pdf_size": 0,
        "rating": "6;7;7;8",
        "confidence": "3;2;3;3",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.7071067811865476
        ],
        "confidence_avg": [
            2.75,
            0.4330127018922193
        ],
        "replies_avg": [
            16,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "yxzVanFoij",
        "title": "D\u00e9j\u00e0 Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models.\nThrough targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.",
        "keywords": "multilingual;evaluation;meta-evaluation;machine translation evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Julia Kreutzer;Eleftheria Briakou;Sweta Agrawal;Marzieh Fadaee;Tom Kocmi",
        "authorids": "~Julia_Kreutzer1;~Eleftheria_Briakou1;~Sweta_Agrawal1;~Marzieh_Fadaee2;~Tom_Kocmi1",
        "gender": ";F;F;F;M",
        "homepage": ";https://elbria.github.io;https://sweta20.github.io/;http://marziehf.github.io/;",
        "dblp": ";217/4858;210/7863.html;159/4868;185/1333",
        "google_scholar": ";bxqqNFEAAAAJ;Avsw9IkAAAAJ;https://scholar.google.nl/citations?user=NZqs0toAAAAJ;https://scholar.google.cz/citations?user=8dUF8YQAAAAJ",
        "orcid": ";;;;0000-0002-7993-9859",
        "linkedin": ";;;marzieh-fadaee-b7393370/;",
        "or_profile": "~Julia_Kreutzer1;~Eleftheria_Briakou1;~Sweta_Agrawal1;~Marzieh_Fadaee2;~Tom_Kocmi1",
        "aff": ";Google;Google;Cohere Labs;Cohere",
        "aff_domain": ";google.com;google.com;cohere.com;cohere.com",
        "position": ";Researcher;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nkreutzer2025dj,\ntitle={D\\'ej\\`a Vu: Multilingual {LLM} Evaluation through the Lens of Machine Translation Evaluation},\nauthor={Julia Kreutzer and Eleftheria Briakou and Sweta Agrawal and Marzieh Fadaee and Tom Kocmi},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=yxzVanFoij}\n}",
        "github": "",
        "project": "",
        "reviewers": "5tiH;ieqc;gL8s;YSt1",
        "site": "https://openreview.net/forum?id=yxzVanFoij",
        "pdf_size": 0,
        "rating": "7;7;8;8",
        "confidence": "5;4;5;4",
        "wc_review": "",
        "rating_avg": [
            7.5,
            0.5
        ],
        "confidence_avg": [
            4.5,
            0.5
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "z1MHB2m3V9",
        "title": "Retrieval-Augmented Generation with Conflicting Evidence",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs \u2013 which requires presenting all valid answers for ambiguous queries \u2013 improving over strong RAG baselines by up to 11.40%, and on FaithEval \u2013 which requires suppressing misinformation \u2013 where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that our proposed RAMDocs dataset poses a challenge for existing RAG baselines (the most performant Llama3.3-70B-Instruct only yields up to a 32.60 exact match score), as it requires handling conflicting information due to ambiguity, noise, and misinformation simultaneously. While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains, especially when increasing the level of imbalance in supporting evidence and misinformation.",
        "keywords": "Retrieval-augmented Generation;Knowledge Conflict;Multi-agent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Han Wang;Archiki Prasad;Elias Stengel-Eskin;Mohit Bansal",
        "authorids": "~Han_Wang9;~Archiki_Prasad1;~Elias_Stengel-Eskin1;~Mohit_Bansal2",
        "gender": "M;F;M;M",
        "homepage": "https://hannight.github.io/;https://archiki.github.io/;https://esteng.github.io;https://www.cs.unc.edu/~mbansal/",
        "dblp": ";264/2812;212/6138;32/5243.html",
        "google_scholar": "xA8AYqkAAAAJ;Svcwv-IAAAAJ;gr_ZVSQAAAAJ;DN8QtscAAAAJ",
        "orcid": ";;0000-0002-6689-505X;",
        "linkedin": ";archiki-prasad;;",
        "or_profile": "~Han_Wang9;~Archiki_Prasad1;~Elias_Stengel-Eskin1;~Mohit_Bansal2",
        "aff": "University of North Carolina at Chapel Hill;University of North Carolina, Chapel Hill;University of North Carolina at Chapel Hill;University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;unc.edu;cs.unc.edu;unc.edu",
        "position": "PhD student;PhD student;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nwang2025retrievalaugmented,\ntitle={Retrieval-Augmented Generation with Conflicting Evidence},\nauthor={Han Wang and Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=z1MHB2m3V9}\n}",
        "github": "",
        "project": "",
        "reviewers": "YVUj;MWio;Zrjy;kbmP",
        "site": "https://openreview.net/forum?id=z1MHB2m3V9",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "3;5;4;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            23,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "z3lG70Azbg",
        "title": "CodeXEmbed: A Generalist Embedding Model Family for Multilingual and Multi-task Code Retrieval",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Despite the success of text retrieval in many NLP tasks, code retrieval remains a largely underexplored area. Most text retrieval systems are tailored for natural language queries, often neglecting the specific challenges of retrieving code. This gap leaves existing models unable to effectively capture the diversity of programming languages and tasks across different domains, highlighting the need for more focused research in code retrieval. To address this, we introduce CodeXEmbed, a family of large-scale code embedding models ranging from 400M to 7B parameters. Our novel training pipeline unifies multiple programming languages and transforms various code-related tasks into a common retrieval framework, enhancing model generalizability and retrieval performance. Our 7B model achieves a new state-of-the-art (SOTA) in code retrieval, topping the CoIR Leaderboard. In addition to excelling in code retrieval, our models demonstrate competitive performance on the widely adopted BeIR text retrieval benchmark, offering versatility across domains. Experimental results demonstrate that improving retrieval performance significantly enhances end-to-end Retrieval-Augmented Generation (RAG) performance for code-related tasks.",
        "keywords": "Code and Text Retrieval; Code Embedding Model; Text Embedding Model; Retrieval-Augmented Code Generation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ye Liu;Rui Meng;Shafiq Joty;silvio savarese;Caiming Xiong;Yingbo Zhou;Semih Yavuz",
        "authorids": "~Ye_Liu4;~Rui_Meng1;~Shafiq_Joty1;~silvio_savarese2;~Caiming_Xiong1;~Yingbo_Zhou1;~Semih_Yavuz1",
        "gender": "F;M;M;M;M;;",
        "homepage": ";http://memray.me;https://raihanjoty.github.io/;;http://cmxiong.com/;;",
        "dblp": "96/2615-6;;62/2078;;80/7282;72/8614;",
        "google_scholar": "QMKD6YMAAAAJ;s6h8L_UAAAAJ;hR249csAAAAJ;https://scholar.google.com/citations?hl=en;vaSdahkAAAAJ;H_6RQ7oAAAAJ;",
        "orcid": ";0000-0001-5583-4924;;;;;",
        "linkedin": ";memray/;;;caiming-xiong-150a1417;yingbozhou/;",
        "or_profile": "~Ye_Liu4;~Rui_Meng1;~Shafiq_Joty1;~silvio_savarese2;~Caiming_Xiong1;~Yingbo_Zhou1;~Semih_Yavuz1",
        "aff": "SalesForce.com;Google+Salesforce Research;Nanyang Technological University+SalesForce.com;SalesForce.com;Salesforce Research;Salesforce Research;",
        "aff_domain": "salesforce.com;google.com+salesforce.com;ntu.edu.sg+salesforce.com;salesforce.com;salesforce.com;salesforce.com;",
        "position": "Researcher;Researcher+Researcher;Associate Professor+Principal Researcher;Principal Researcher;Research Scientist;Research Scientist;",
        "bibtex": "@inproceedings{\nliu2025codexembed,\ntitle={Code{XE}mbed: A Generalist Embedding Model Family for Multilingual and Multi-task Code Retrieval},\nauthor={Ye Liu and Rui Meng and Shafiq Joty and silvio savarese and Caiming Xiong and Yingbo Zhou and Semih Yavuz},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=z3lG70Azbg}\n}",
        "github": "",
        "project": "",
        "reviewers": "XpJL;RZhx;eN6C;H4PJ",
        "site": "https://openreview.net/forum?id=z3lG70Azbg",
        "pdf_size": 0,
        "rating": "5;6;6;9",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.5
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            31,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.19245008972987526
    },
    {
        "id": "z9SbcYYP0M",
        "title": "Probing then Editing Response Personality of Large Language Models",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that simulate consistent personality traits. \nDespite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in simulating personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly simulate personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality.",
        "keywords": "large language model;personality;interpretability;knowledge editing",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tianjie Ju;Zhenyu Shao;Bowen Wang;Yujia Chen;Zhuosheng Zhang;Hao Fei;Mong-Li Lee;Wynne Hsu;Sufeng Duan;Gongshen Liu",
        "authorids": "~Tianjie_Ju1;~Zhenyu_Shao2;~Bowen_Wang10;~Yujia_Chen5;~Zhuosheng_Zhang1;~Hao_Fei1;~Mong-Li_Lee1;~Wynne_Hsu1;~Sufeng_Duan1;~Gongshen_Liu2",
        "gender": "M;M;M;F;M;M;F;F;;M",
        "homepage": "https://github.com/jometeorie;https://github.com/universe-sky;https://github.com/wbw625;https://github.com/EineKline0;https://bcmi.sjtu.edu.cn/~zhangzs/;https://haofei.vip/;https://www.comp.nus.edu.sg/~leeml/;http://www.comp.nus.edu.sg/~whsu/;;https://www.researchgate.net/profile/Gongshen-Liu",
        "dblp": "319/4246;;;;06/9708;81/3569-1;l/MongLiLee;h/WynneHsu;;02/2134",
        "google_scholar": "https://scholar.google.com.hk/citations?user=f8PPcnoAAAAJ;;;;https://scholar.google.co.jp/citations?user=63LTQhgAAAAJ;YGDX46AAAAAJ;https://scholar.google.com.tw/citations?user=_xFTK8wAAAAJ;https://scholar.google.com.tw/citations?user=ljyBjv8AAAAJ;;",
        "orcid": "0009-0006-6978-1935;;;;0000-0002-4183-3645;0000-0003-3026-6347;0000-0002-9636-388X;0000-0002-4142-8893;;0000-0001-5194-1570",
        "linkedin": ";;;;;;;;;",
        "or_profile": "~Tianjie_Ju1;~Zhenyu_Shao2;~Bowen_Wang10;~Yujia_Chen5;~Zhuosheng_Zhang1;~Hao_Fei1;~Mong-Li_Lee1;~Wynne_Hsu1;~Sufeng_Duan1;~Gongshen_Liu2",
        "aff": "Shanghai Jiaotong University+National University of Singapore;Shanghai Jiaotong University;Shanghai Jiaotong University;Sichuan University;Shanghai Jiaotong University;National University of Singapore;National University of Singapore;National University of Singapore;;Shanghai Jiaotong University",
        "aff_domain": "sjtu.edu.cn+nus.edu.sg;sjtu.edu.cn;sjtu.edu.cn;scu.edu.cn;sjtu.edu.cn;nus.edu.sg;nus.edu.sg;nus.edu.sg;;sjtu.edu.cn",
        "position": "PhD student+PhD student;Undergrad student;Undergrad student;Undergrad student;Assistant Professor;Postdoc;Full Professor;Full Professor;;Full Professor",
        "bibtex": "@inproceedings{\nju2025probing,\ntitle={Probing then Editing Response Personality of Large Language Models},\nauthor={Tianjie Ju and Zhenyu Shao and Bowen Wang and Yujia Chen and Zhuosheng Zhang and Hao Fei and Mong-Li Lee and Wynne Hsu and Sufeng Duan and Gongshen Liu},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=z9SbcYYP0M}\n}",
        "github": "",
        "project": "",
        "reviewers": "GTW9;EDKU;xTQc;i7BV",
        "site": "https://openreview.net/forum?id=z9SbcYYP0M",
        "pdf_size": 0,
        "rating": "6;6;6;7",
        "confidence": "4;4;2;4",
        "wc_review": "",
        "rating_avg": [
            6.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.5,
            0.8660254037844386
        ],
        "replies_avg": [
            21,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "zFz1BJu211",
        "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "As large language models become increasingly capable at various tasks including writing, the need to generate unique and creative content arises. Although LLMs have the ability to generate text covering diverse topics, there is an overall sense of repetitiveness across texts that we aim to formalize. Such familiarity between documents is induced through the persistence of underlying discourse structures. However, existing similarity metrics dependent on lexical overlap and syntactic patterns are overly sensitive to volatility in content overlap, thus making them unsuitable for detecting $\\textit{structural}$ similarities. We introduce an abstraction based on linguistics theories in Questions Under Discussion (QUD) and question semantics to help quantify differences in discourse progression. We then use this framework to build $\\textbf{QUDsim}$, a similarity metric that can detect discursive parallels between documents. Using QUDsim, we find that LLMs often reuse discourse structures (more so than humans) to create seemingly new documents by simply swapping content. Furthermore, LLMs are not only repetitive and structurally uniform, but are also divergent from human authors in the types of structures they use.",
        "keywords": "discourse diversity;discourse structure;large language models;Questions Under Discussion",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ramya Namuduri;Yating Wu;Anshun Asher Zheng;Manya Wadhwa;Greg Durrett;Junyi Jessy Li",
        "authorids": "~Ramya_Namuduri1;~Yating_Wu1;~Anshun_Asher_Zheng1;~Manya_Wadhwa1;~Greg_Durrett1;~Junyi_Jessy_Li2",
        "gender": "F;Not Specified;M;;M;F",
        "homepage": ";https://lingchensanwen.github.io/;https://asherz720.github.io/;https://manyawadhwa.github.io/;http://www.cs.utexas.edu/~gdurrett/;https://jessyli.com",
        "dblp": ";23/1500-2;405/2865;192/2524.html;69/7968;148/9553",
        "google_scholar": ";https://scholar.google.com/citations?view_op=list_works;EcgXeOMAAAAJ;4_uMjzgAAAAJ;https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ;tJGm3-YAAAAJ",
        "orcid": ";;;;;",
        "linkedin": "namuduri-ramya/;wuyating;;manya-wadhwa-9798a79a/;;",
        "or_profile": "~Ramya_Namuduri1;~Yating_Wu1;~Anshun_Asher_Zheng1;~Manya_Wadhwa1;~Greg_Durrett1;~Junyi_Jessy_Li2",
        "aff": "University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "position": "Undergrad student;PhD student;PhD student;PhD student;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nnamuduri2025qudsim,\ntitle={{QUD}sim: Quantifying Discourse Similarities in {LLM}-Generated Text},\nauthor={Ramya Namuduri and Yating Wu and Anshun Asher Zheng and Manya Wadhwa and Greg Durrett and Junyi Jessy Li},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zFz1BJu211}\n}",
        "github": "",
        "project": "",
        "reviewers": "biQ6;kFys;UMuD;HZh2",
        "site": "https://openreview.net/forum?id=zFz1BJu211",
        "pdf_size": 0,
        "rating": "6;7;8;8",
        "confidence": "4;4;4;5",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.82915619758885
        ],
        "confidence_avg": [
            4.25,
            0.4330127018922193
        ],
        "replies_avg": [
            13,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.5222329678670935
    },
    {
        "id": "zHdSCtNmM4",
        "title": "Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses during the early phases of survey design. While previous studies have examined whether models can reflect individual opinions or attitudes, we argue that a higher-order binding of virtual personas requires successfully approximating not only the opinions of a user as an identified member of a group, but also the nuanced ways in which that user perceives and evaluates those outside the group. In particular, faithfully simulating how humans perceive different social groups is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user \"backstories\" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual,  compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies.\nAltogether, our work extends the applicability of LLMs beyond estimating individual self-opinions, enabling their use in a broader range of human studies.",
        "keywords": "user approximation;metaperception;social psycholog;democratic backsliding;outgroup hostility",
        "primary_area": "",
        "supplementary_material": "/attachment/887605a60f6440213636e72d74e01e35df30335b.zip",
        "author": "Minwoo Kang;Suhong Moon;Seung Hyeong Lee;Ayush Raj;Joseph Suh;David Chan",
        "authorids": "~Minwoo_Kang1;~Suhong_Moon1;~Seung_Hyeong_Lee1;~Ayush_Raj3;~Joseph_Suh1;~David_Chan3",
        "gender": "M;M;M;M;Not Specified;M",
        "homepage": ";https://suhongmoon.github.io/;https://sites.google.com/view/leeseunghyeong;;;https://people.eecs.berkeley.edu/~davidchan/",
        "dblp": "292/3009;242/2290.html;;;;80/9659",
        "google_scholar": "mqj3xiQAAAAJ;https://scholar.google.com/citations?hl=en;gmAet3MAAAAJ;AiRWt7MAAAAJ;;qa4M89wAAAAJ",
        "orcid": ";;;;0000-0002-1567-1865;",
        "linkedin": ";;leeseunghyeong/;;joseph-suh-42846928b/;",
        "or_profile": "~Minwoo_Kang1;~Suhong_Moon1;~Seung_Hyeong_Lee1;~Ayush_Raj3;~Joseph_Suh1;~David_Chan3",
        "aff": "University of California, Berkeley;University of California, Berkeley;Northwestern University;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;northwestern.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;PhD student;Undergrad student;PhD student;Postdoc",
        "bibtex": "@inproceedings{\nkang2025deep,\ntitle={Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions},\nauthor={Minwoo Kang and Suhong Moon and Seung Hyeong Lee and Ayush Raj and Joseph Suh and David Chan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zHdSCtNmM4}\n}",
        "github": "",
        "project": "",
        "reviewers": "DZ2E;GJSP;8jSZ;3XgY",
        "site": "https://openreview.net/forum?id=zHdSCtNmM4",
        "pdf_size": 0,
        "rating": "7;7;7;7",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            28,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "zJHZJClG1Z",
        "title": "Values in the Wild: Discovering and Mapping Values in Real-World Language Model Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "AI assistants interact with millions of real users everyday, imparting normative judgments that can have significant personal and societal impact\u2014but little is known about what values guide these interactions in practice. To address this, we develop a method to empirically analyze values expressed in hundreds of thousands of real-world conversations with Claude models. We empirically discover and taxonomize 3,308 AI values, and study how model values and responses depend on context. We find that Claude expresses many professional and intellectual values, and typically supports prosocial human values while resisting values like \"moral nihilism.\" While some values appear consistently (e.g. \"professionalism\"), most are highly context-dependent\u2014\"harm prevention\" emerges when the model resists users, \"historical accuracy\" when discussing controversial events, \"healthy boundaries\" in relationship advice, and \"human agency\" in technology ethics discussions. By providing the first large-scale empirical mapping of AI values in deployment, this work creates a foundation for more grounded evaluation and design of values in increasingly influential AI systems.",
        "keywords": "language models;values;AI ethics;AI values;empirical analysis;human-AI interaction;value alignment;privacy-preserving analysis;value pluralism;AI and society",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Saffron Huang;Esin DURMUS;Kunal Handa;Miles McCain;Alex Tamkin;Michael Stern;Jerry Hong;Deep Ganguli",
        "authorids": "~Saffron_Huang1;~Esin_DURMUS2;~Kunal_Handa1;~Miles_McCain1;~Alex_Tamkin1;~Michael_Stern1;~Jerry_Hong1;~Deep_Ganguli2",
        "gender": "Not Specified;F;;M;;M;M;M",
        "homepage": "https://saffronhuang.com;https://esdurmus.github.io/;https://kunhanda.github.io/;https://miles.land;;;http://jerryhong.io/;",
        "dblp": "308/6345;;336/6747.html;;;;;",
        "google_scholar": "1aL6moAAAAAJ;;scdcthMAAAAJ;;;;j1gbH68AAAAJ;rG3xW3UAAAAJ",
        "orcid": ";;;0000-0001-5326-9212;;;;",
        "linkedin": ";;;;;michael-alan-stern/;jerryhong1;",
        "or_profile": "~Saffron_Huang1;~Esin_DURMUS2;~Kunal_Handa1;~Miles_McCain1;~Alex_Tamkin1;~Michael_Stern1;~Jerry_Hong1;~Deep_Ganguli2",
        "aff": "Anthropic;Anthropic;Anthropic+Department of Computer Science, University of Oxford;Anthropic;;Anthropic;Anthropic;Anthropic",
        "aff_domain": "anthropic.com;anthropic.com;anthropic.com+cs.ox.ac.uk;anthropic.com;;anthropic.com;anthropic.com;anthropic.com",
        "position": "Researcher;Researcher;Researcher+MS student;Researcher;;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nhuang2025values,\ntitle={Values in the Wild: Discovering and Mapping Values in Real-World Language Model Interactions},\nauthor={Saffron Huang and Esin DURMUS and Kunal Handa and Miles McCain and Alex Tamkin and Michael Stern and Jerry Hong and Deep Ganguli},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zJHZJClG1Z}\n}",
        "github": "",
        "project": "",
        "reviewers": "D56v;WszB;jrN1",
        "site": "https://openreview.net/forum?id=zJHZJClG1Z",
        "pdf_size": 0,
        "rating": "7;7;7",
        "confidence": "4;3;4",
        "wc_review": "",
        "rating_avg": [
            7.0,
            0.0
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            10,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.0
    },
    {
        "id": "zLbmsdyTiN",
        "title": "MeMAD: Structured Memory of Debates for Enhanced Multi-Agent Reasoning",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable in-context learning capabilities but often struggle with complex, multi-step reasoning. Multi-Agent Debate (MAD) frameworks partially address these limitations by enabling iterative agent interactions. However, they neglect valuable historical insights by treating each new debate independently. In this paper, we propose Memory-Augmented MAD (MeMAD), a parameter-free memory-augmented MAD framework that systematically organizes and reuses past debate transcripts. MeMAD stores structured representations of successful and unsuccessful reasoning attempts enriched with self-reflections and peer feedback. It systematically retrieves them via semantic similarity at inference time to inform new reasoning tasks. Our experiments on challenging mathematical reasoning, scientific question answering, and language understanding benchmarks show that MeMAD achieves significant accuracy gains (up to 3.3\\% over conventional MAD baselines) without parameter updates. Our findings underscore structured memory as a pivotal mechanism for achieving deeper and more reliable multi-agent reasoning in LLMs. Code is available in ~\\url{https://github.com/LSHCoding/MeMAD}.",
        "keywords": "Multi-Agent Debate;Memory Augmentation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Shuai Ling;Lizi Liao;Dongmei Jiang;Weili Guan",
        "authorids": "~Shuai_Ling1;~Lizi_Liao1;~Dongmei_Jiang2;~Weili_Guan4",
        "gender": "M;F;F;",
        "homepage": ";https://liziliao.github.io/;https://scholar.google.com/citations?user=Awsue7sAAAAJ&hl=en;",
        "dblp": ";149/1249;;",
        "google_scholar": "DoXxoQIAAAAJ;https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ;Awsue7sAAAAJ;",
        "orcid": ";;0000-0002-6238-8499;",
        "linkedin": ";;;",
        "or_profile": "~Shuai_Ling1;~Lizi_Liao1;~Dongmei_Jiang2;~Weili_Guan4",
        "aff": "Harbin Institute of Technology;Singapore Management University;Peng Cheng Laboratory;",
        "aff_domain": "stu.hit.edu.cn;smu.edu.sg;pcl.ac.cn;",
        "position": "PhD student;Assistant Professor;Principal Researcher;",
        "bibtex": "@inproceedings{\nling2025memad,\ntitle={Me{MAD}: Structured Memory of Debates for Enhanced Multi-Agent Reasoning},\nauthor={Shuai Ling and Lizi Liao and Dongmei Jiang and Weili Guan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zLbmsdyTiN}\n}",
        "github": "",
        "project": "",
        "reviewers": "Ayn1;nAFs;84jq",
        "site": "https://openreview.net/forum?id=zLbmsdyTiN",
        "pdf_size": 0,
        "rating": "5;6;7",
        "confidence": "4;4;3",
        "wc_review": "",
        "rating_avg": [
            6.0,
            0.816496580927726
        ],
        "confidence_avg": [
            3.6666666666666665,
            0.4714045207910317
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.8660254037844385
    },
    {
        "id": "zOw2it5Ni6",
        "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called \\emph{Coreference Confidence Disparity} which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40\\% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings, such as when assigning transgender women to historically male-dominated occupations. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers (e.g., 'White' or 'cisgender'), indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.",
        "keywords": "fairness;uncertainty;intersectionality",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Falaah Arif Khan;Nivedha Sivakumar;Yinong Oliver Wang;Katherine Metcalf;Cezanne Camacho;Barry-John Theobald;Luca Zappella;Nicholas Apostoloff",
        "authorids": "~Falaah_Arif_Khan1;~Nivedha_Sivakumar1;~Yinong_Oliver_Wang1;~Katherine_Metcalf1;~Cezanne_Camacho1;~Barry-John_Theobald1;~Luca_Zappella1;~Nicholas_Apostoloff1",
        "gender": ";Not Specified;M;;;M;M;",
        "homepage": "https://falaaharifkhan.github.io/research/;;https://oliverow.github.io/;;;;http://www.cis.jhu.edu/~luca/;",
        "dblp": ";218/5499;386/8270;;;86/6624;38/2520;92/3793",
        "google_scholar": "53O3iRgAAAAJ;t6041v4AAAAJ;;;;DNrQd3IAAAAJ;bmh6mxAAAAAJ;p4w7a_kAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": "falaah-arif-khan-b99058154/;nivedha-sivakumar-34574b92;yinongwang/;;cezanne-camacho-422823b2;barry-john-theobald-392a0611a/;zappella?trk=people-guest_profile-result-card_result-card_full-click;apostoloff/",
        "or_profile": "~Falaah_Arif_Khan1;~Nivedha_Sivakumar1;~Yinong_Oliver_Wang1;~Katherine_Metcalf1;~Cezanne_Camacho1;~Barry-John_Theobald1;~Luca_Zappella1;~Nicholas_Apostoloff1",
        "aff": "New York University;Apple;Carnegie Mellon University;;Apple;Apple;Apple;Apple",
        "aff_domain": "nyu.edu;apple.com;cmu.edu;;apple.com;apple.com;apple.com;apple.com",
        "position": "Researcher;Researcher;PhD student;;Instructor;Researcher;Principal Researcher;Principal Researcher",
        "bibtex": "@inproceedings{\nkhan2025investigating,\ntitle={Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution},\nauthor={Falaah Arif Khan and Nivedha Sivakumar and Yinong Oliver Wang and Katherine Metcalf and Cezanne Camacho and Barry-John Theobald and Luca Zappella and Nicholas Apostoloff},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zOw2it5Ni6}\n}",
        "github": "",
        "project": "",
        "reviewers": "z92p;orsS;LZzW;rGgN",
        "site": "https://openreview.net/forum?id=zOw2it5Ni6",
        "pdf_size": 0,
        "rating": "5;6;7;8",
        "confidence": "4;3;4;4",
        "wc_review": "",
        "rating_avg": [
            6.5,
            1.118033988749895
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            19,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.2581988897471611
    },
    {
        "id": "zP6DJaBBcR",
        "title": "REFA: Reference Free Alignment with Fine-Grained Length Control",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the **URSLA shortcut**. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce **REFA**, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a **60.29\\%** win rate and a **52.17\\%** length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.",
        "keywords": "Model Alignment;RLHF;Preference Optimization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Taneesh Gupta;Rahul Madhavan;Xuchao Zhang;Chetan Bansal;Saravan Rajmohan",
        "authorids": "~Taneesh_Gupta1;~Rahul_Madhavan1;~Xuchao_Zhang1;~Chetan_Bansal1;~Saravan_Rajmohan2",
        "gender": "M;M;;;",
        "homepage": ";;https://xuczhang.github.io/;;",
        "dblp": ";290/2008;;;",
        "google_scholar": "WArg3JAAAAAJ;HrM2xRcAAAAJ;;;",
        "orcid": ";;;;",
        "linkedin": "taneesh-gupta;rahul-madhavan/;;;",
        "or_profile": "~Taneesh_Gupta1;~Rahul_Madhavan1;~Xuchao_Zhang1;~Chetan_Bansal1;~Saravan_Rajmohan2",
        "aff": "Microsoft;Indian Institute of Management, Ahmedabad+Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology+Indian Institute of Science, Bangalore;Microsoft;;",
        "aff_domain": "microsoft.com;iima.ac.in+iitm.ac.in+iisc.ac.in;microsoft.com;;",
        "position": "Researcher;MS student+Undergrad student+PhD student;Researcher;;",
        "bibtex": "@inproceedings{\ngupta2025refa,\ntitle={{REFA}: Reference Free Alignment with Fine-Grained Length Control},\nauthor={Taneesh Gupta and Rahul Madhavan and Xuchao Zhang and Chetan Bansal and Saravan Rajmohan},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zP6DJaBBcR}\n}",
        "github": "",
        "project": "",
        "reviewers": "9iEL;wVBW;G1HG;zZqs;AyBr",
        "site": "https://openreview.net/forum?id=zP6DJaBBcR",
        "pdf_size": 0,
        "rating": "6;6;6;7;7",
        "confidence": "4;3;3;5;4",
        "wc_review": "",
        "rating_avg": [
            6.4,
            0.48989794855663565
        ],
        "confidence_avg": [
            3.8,
            0.7483314773547882
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.7637626158259732
    },
    {
        "id": "zSbecER9il",
        "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.",
        "keywords": "LLM agent;multi-agent system;social simulation;public health;AI for health",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abe Bohan Hou;Hongru Du;Yichen Wang;Jingyu Zhang;Zixiao Wang;Paul Pu Liang;Daniel Khashabi;Lauren M Gardner;Tianxing He",
        "authorids": "~Abe_Bohan_Hou1;~Hongru_Du1;~Yichen_Wang4;~Jingyu_Zhang2;~Zixiao_Wang6;~Paul_Pu_Liang1;~Daniel_Khashabi2;~Lauren_M_Gardner1;~Tianxing_He1",
        "gender": "M;M;M;;;M;M;;M",
        "homepage": "https://abehou.com;https://hongru94.github.io/;https://yichenzw.com;https://jackz.io/;;https://pliang279.github.io/;http://danielkhashabi.com/;;https://cloudygoose.github.io/",
        "dblp": "358/7004;;;92/3672.html;;207/9749;71/10515;;149/0111",
        "google_scholar": ";rBkH7h0AAAAJ;86XiOcsAAAAJ;9EC0sDMAAAAJ;;https://scholar.google.com/citations?hl=en;pK2kQvgAAAAJ;https://scholar.google.com.au/citations?user=EO7XqlUAAAAJ;egmfjjwAAAAJ",
        "orcid": ";;;;;;;;",
        "linkedin": "bohanhou/;;;;;;;;",
        "or_profile": "~Abe_Bohan_Hou1;~Hongru_Du1;~Yichen_Wang4;~Jingyu_Zhang2;~Zixiao_Wang6;~Paul_Pu_Liang1;~Daniel_Khashabi2;~Lauren_M_Gardner1;~Tianxing_He1",
        "aff": "Johns Hopkins University;University of Virginia, Charlottesville+Johns Hopkins University;University of Chicago;Johns Hopkins University;;Massachusetts Institute of Technology;Johns Hopkins University;Johns Hopkins University;Tsinghua University",
        "aff_domain": "jh.edu;virginia.edu+johnshopkins.edu;uchicago.edu;cs.jhu.edu;;mit.edu;jhu.edu;jhu.edu;mail.tsinghua.edu.cn",
        "position": "Undergrad student;Assistant Professor+PhD student;PhD student;PhD student;;Assistant Professor;Assistant Professor;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nhou2025can,\ntitle={Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy},\nauthor={Abe Bohan Hou and Hongru Du and Yichen Wang and Jingyu Zhang and Zixiao Wang and Paul Pu Liang and Daniel Khashabi and Lauren M Gardner and Tianxing He},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zSbecER9il}\n}",
        "github": "",
        "project": "",
        "reviewers": "wsJL;eATj;99J4;2uyB",
        "site": "https://openreview.net/forum?id=zSbecER9il",
        "pdf_size": 0,
        "rating": "7;7;7;8",
        "confidence": "3;4;4;4",
        "wc_review": "",
        "rating_avg": [
            7.25,
            0.4330127018922193
        ],
        "confidence_avg": [
            3.75,
            0.4330127018922193
        ],
        "replies_avg": [
            18,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.3333333333333333
    },
    {
        "id": "zTKYKiWzIm",
        "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Recent large language models (LLMs) achieve impressive performance in text generation but often fail to accurately attribute their outputs, undermining trust and verifiability. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. Furthermore, current attributions fail to provide a reason as to how and why the model uses the context to arrive at the final output. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable ``code agent'' architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both document-level and sentence-level granularity across two long-form question-answering tasks. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.",
        "keywords": "long-form qa;rag;summarization;attributed text generation",
        "primary_area": "",
        "supplementary_material": "/attachment/22e800fcc6dc11a936f21b1f04a507d404c9515d.zip",
        "author": "David Wan;Eran Hirsch;Elias Stengel-Eskin;Ido Dagan;Mohit Bansal",
        "authorids": "~David_Wan1;~Eran_Hirsch1;~Elias_Stengel-Eskin1;~Ido_Dagan1;~Mohit_Bansal2",
        "gender": "M;M;M;M;M",
        "homepage": ";https://eranhirs.github.io/;https://esteng.github.io;http://u.cs.biu.ac.il/~dagan/;https://www.cs.unc.edu/~mbansal/",
        "dblp": "17/4695.html;302/4300.html;212/6138;95/284;32/5243.html",
        "google_scholar": "oHznAAYAAAAJ;GPsTrDEAAAAJ;gr_ZVSQAAAAJ;https://scholar.google.com.tw/citations?user=YzGAGtoAAAAJ;DN8QtscAAAAJ",
        "orcid": ";;0000-0002-6689-505X;;",
        "linkedin": ";;;;",
        "or_profile": "~David_Wan1;~Eran_Hirsch1;~Elias_Stengel-Eskin1;~Ido_Dagan1;~Mohit_Bansal2",
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill;Bar-Ilan University;University of North Carolina at Chapel Hill;Bar-Ilan University;University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;biu.ac.il;cs.unc.edu;biu.ac.il;unc.edu",
        "position": "PhD student;PhD student;Postdoc;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwan2025generationprograms,\ntitle={GenerationPrograms:  Fine-grained Attribution with Executable Programs},\nauthor={David Wan and Eran Hirsch and Elias Stengel-Eskin and Ido Dagan and Mohit Bansal},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zTKYKiWzIm}\n}",
        "github": "",
        "project": "",
        "reviewers": "gWx6;TDKU;BPee",
        "site": "https://openreview.net/forum?id=zTKYKiWzIm",
        "pdf_size": 0,
        "rating": "5;6;6",
        "confidence": "3;4;3",
        "wc_review": "",
        "rating_avg": [
            5.666666666666667,
            0.4714045207910317
        ],
        "confidence_avg": [
            3.3333333333333335,
            0.4714045207910317
        ],
        "replies_avg": [
            25,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.4999999999999999
    },
    {
        "id": "zg5is4GJ3R",
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
        "keywords": "Computer Use;GUI Agents;Multimodal Large Language Models;Planning;Grounding;Vision",
        "primary_area": "",
        "supplementary_material": "/attachment/ecc357b402b416c7ea4804242770e4c521a46cfd.zip",
        "author": "Saaket Agashe;Kyle Wong;Vincent Tu;Jiachen Yang;Ang Li;Xin Eric Wang",
        "authorids": "~Saaket_Agashe1;~Kyle_Wong1;~Vincent_Tu1;~Jiachen_Yang1;~Ang_Li1;~Xin_Eric_Wang2",
        "gender": "M;M;M;;M;M",
        "homepage": "https://saa1605.github.io;;;;https://angli.ai;https://eric-xw.github.io",
        "dblp": "326/1062;;;;33/2805-1;10/5630-61",
        "google_scholar": "_23KIVgAAAAJ;;;;6bRXWXEAAAAJ;YjqluE0AAAAJ",
        "orcid": ";;;;;0000-0003-2605-5504",
        "linkedin": ";kylewong288/;vincent-tu-422b18208/;;angli-ai;",
        "or_profile": "~Saaket_Agashe1;~Kyle_Wong1;~Vincent_Tu1;~Jiachen_Yang1;~Ang_Li1;~Xin_Eric_Wang2",
        "aff": "University of California, Santa Cruz;University of California, Santa Barbara;University of California, San Diego+University of California, San Diego;;Simular;University of California, Santa Barbara+Simular+University of California, Santa Cruz",
        "aff_domain": "ucsc.edu;ucsb.edu;ucsd.edu+ucsd.edu;;simular.ai;ucsb.edu+simular.ai+ucsc.edu",
        "position": "PhD student;Undergrad student;MS student+Undergrad student;;Founder CEO;Assistant Professor+Chief Scientist +Assistant Professor",
        "bibtex": "@inproceedings{\nagashe2025agent,\ntitle={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents},\nauthor={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zg5is4GJ3R}\n}",
        "github": "",
        "project": "",
        "reviewers": "u6Sn;YT8u;Ygqb;Cqe8",
        "site": "https://openreview.net/forum?id=zg5is4GJ3R",
        "pdf_size": 0,
        "rating": "6;6;7;8",
        "confidence": "3;4;5;4",
        "wc_review": "",
        "rating_avg": [
            6.75,
            0.82915619758885
        ],
        "confidence_avg": [
            4.0,
            0.7071067811865476
        ],
        "replies_avg": [
            22,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0.42640143271122083
    },
    {
        "id": "zuNM3eoPVi",
        "title": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models in Multi-turn Interactions",
        "track": "main",
        "status": "Poster",
        "tldr": "",
        "abstract": "Large language models (LLMs) have exhibited outstanding performance in engaging with humans and addressing complex questions by leveraging their vast implicit knowledge and robust reasoning capabilities. However, such models are vulnerable to jailbreak attacks, leading to the generation of harmful responses. Despite recent research on single-turn jailbreak strategies to facilitate the development of defence mechanisms, the challenge of revealing vulnerabilities under multi-turn setting remains relatively under-explored. In this work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective multi-turn jailbreak strategy against the advanced LLMs. JSP splits questions into harmless fractions as the input of each turn, and requests LLMs to reconstruct and respond to questions under multi-turn interaction. Our results demonstrate the proposed JSP jailbreak bypasses original safeguards against explicitly harmful content, achieving an average attack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs (Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini), and exhibits consistent performance on jailbreaking benchmarks. Moreover, JSP exhibits strong resistance to input-side and output-side defence tactics. Warning: this paper contains offensive examples.",
        "keywords": "Jailbreak;Red teaming",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hao Yang;Lizhen Qu;Ehsan Shareghi;Gholamreza Haffari",
        "authorids": "~Hao_Yang26;~Lizhen_Qu2;~Ehsan_Shareghi1;~Gholamreza_Haffari2",
        "gender": "M;M;M;",
        "homepage": "https://github.com/YangHao97;https://research.monash.edu/en/persons/lizhen-qu;https://eehsan.github.io/;",
        "dblp": ";58/3601;09/7859;",
        "google_scholar": "UZe8doYAAAAJ;https://scholar.google.com.au/citations?user=cHXZgHUAAAAJ;https://scholar.google.com.au/citations?user=EhnQJFwAAAAJ;",
        "orcid": ";0000-0002-7764-431X;;",
        "linkedin": ";lizhen-qu-50017717/;;",
        "or_profile": "~Hao_Yang26;~Lizhen_Qu2;~Ehsan_Shareghi1;~Gholamreza_Haffari2",
        "aff": "Monash University;Monash University;Monash University;",
        "aff_domain": "monash.edu;monash.edu.au;monash.edu;",
        "position": "PhD student;Lecturer;Assistant Professor;",
        "bibtex": "@inproceedings{\nyang2025jigsaw,\ntitle={Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models in Multi-turn Interactions},\nauthor={Hao Yang and Lizhen Qu and Ehsan Shareghi and Gholamreza Haffari},\nbooktitle={Second Conference on Language Modeling},\nyear={2025},\nurl={https://openreview.net/forum?id=zuNM3eoPVi}\n}",
        "github": "",
        "project": "",
        "reviewers": "9UUp;KWaf;fo3V;shAs",
        "site": "https://openreview.net/forum?id=zuNM3eoPVi",
        "pdf_size": 0,
        "rating": "2;3;7;8",
        "confidence": "5;4;3;3",
        "wc_review": "",
        "rating_avg": [
            5.0,
            2.5495097567963922
        ],
        "confidence_avg": [
            3.75,
            0.82915619758885
        ],
        "replies_avg": [
            14,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": -0.9460998335825322
    }
]